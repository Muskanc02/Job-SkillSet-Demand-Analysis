{
    "4158550552": {
        "title": "Data Engineer - ETL - H\u00edbrido ",
        "company": "Between Technology",
        "location": "Bizkaia, Basque Country, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nBETWEEN TECHNOLOGY es una consultora multinacional que cuenta con avanzadas capacidades tecnol\u00f3gicas en el \u00e1mbito de la Ingenier\u00eda y las TIC. Nuestros servicios llegan a todos los continentes y cubren varios sectores de la econom\u00eda mundial, de la industria y las empresas de servicios. \nEn la actualidad BETWEEN cuenta con un equipo de 1000 personas trabajando en los \u00e1mbitos de la industria del autom\u00f3vil, aeron\u00e1utica, ferrocarril, Oil&Gas, retail, mundo del deporte, educaci\u00f3n, etc.\n\nActualmente seleccionamos un/a Data Engineer especializado(a en el manejo de datos y con amplia experiencia en la creaci\u00f3n, gesti\u00f3n y optimizaci\u00f3n de procesos ETL (Extract, Transform, Load), utilizando diversas tecnolog\u00edas y herramientas como PL/SQL, Teradata o SQL Server, MIcrostrategy; para incorporarse de forma indefinida en uno de los equipos de tecnolog\u00eda que Between tiene implantados en uno de nuestros principales clientes en Mondrag\u00f3n - Arraste.\n\nModelo de trabajo h\u00edbrido: 3 d\u00edas en Mondrag\u00f3n - Arraste y 2 Teletrabajo.\n\n\n\u00bfQu\u00e9 har\u00e9? Est\u00e1s ser\u00e1n tus funciones:\n\nIntegraci\u00f3n y procesamiento de datos: Recopilar, limpiar y transformar grandes vol\u00famenes de datos procedentes de diversas fuentes (como transacciones bancarias, datos de clientes, y sistemas internos) para que puedan ser usados en an\u00e1lisis de negocio y modelos predictivos.\nConstrucci\u00f3n y mantenimiento de pipelines de datos: Dise\u00f1ar flujos de datos (data pipelines) que permitan la transferencia de datos en tiempo real o en lotes a los sistemas de an\u00e1lisis, asegurando que los datos lleguen con rapidez y precisi\u00f3n.\nOptimizaci\u00f3n de almacenamiento de datos: Configurar y gestionar bases de datos y data warehouses, asegurando que el almacenamiento de datos sea eficiente, seguro y accesible.\nColaboraci\u00f3n con Data Scientists y analistas: Proveer datos bien estructurados y de alta calidad que los analistas y cient\u00edficos de datos pueden usar para desarrollar modelos de predicci\u00f3n, scoring de riesgo crediticio, o an\u00e1lisis de comportamiento de clientes.\nGarantizar la seguridad y privacidad de los datos: Trabajar bajo estrictas normas de privacidad y seguridad, asegurando que los datos sensibles de los clientes est\u00e9n protegidos seg\u00fan las normativas financieras.\n\nBeneficios\n\nSer parte de un equipo joven en una comunidad techie.\nTe integrar\u00e1s en un equipo de alto rendimiento y con un alto grado de especializaci\u00f3n.\nPlanes de formaci\u00f3n personalizados con nuestra BTW University (Idiomas con Speexx, The Power Business School, Udemy, Certificaciones t\u00e9cnicas, Pharos, etc).\nSeguro m\u00e9dico gratuito desde el primer d\u00eda. Posibilidad de elegir c\u00f3mo obtener parte de tu salario gracias a las ventajas fiscales de nuestra Retribuci\u00f3n Flexible (tickets restaurante, guarder\u00eda).\nAcceso a variedad de descuentos (Viajes, tecnolog\u00eda... y muchos m\u00e1s).\nPrecios reducidos en bienestar, fitness y nutric\u00f3n con Gympass.\n\u00a1Cumplir a\u00f1os como Betweener tiene premio!\n\n\u00bfApasionado/a de la ingenier\u00eda de datos y con experiencia en procesos ETL, con ganas de continuar desarrollando tu carrera profesional en un entorno de grandes retos? \u00a1No dudes en inscribirte!\n\nSi tienes cualquier duda, puedes consultarnos a trav\u00e9s del WhatsApp 652 067 523.\nBETWEEN TECHNOLOGY es una consultora multinacional que cuenta con avanzadas capacidades tecnol\u00f3gicas en el \u00e1mbito de la Ingenier\u00eda y las TIC. Nuestros servicios llegan a todos los continentes y cubren varios sectores de la econom\u00eda mundial, de la industria y las empresas de servicios.\nEn la actualidad BETWEEN cuenta con un equipo de 1000 personas trabajando en los \u00e1mbitos de la industria del autom\u00f3vil, aeron\u00e1utica, ferrocarril, Oil&Gas, retail, mundo del deporte, educaci\u00f3n, etc.\nActualmente seleccionamos un/a Data Engineer especializado(a en el manejo de datos y con amplia experiencia en la creaci\u00f3n, gesti\u00f3n y optimizaci\u00f3n de procesos ETL (Extract, Transform, Load), utilizando diversas tecnolog\u00edas y herramientas como PL/SQL, Teradata o SQL Server, MIcrostrategy; para incorporarse de forma indefinida en uno de los equipos de tecnolog\u00eda que Between tiene implantados en uno de nuestros principales clientes en Mondrag\u00f3n - Arraste.\nModelo de trabajo h\u00edbrido: 3 d\u00edas en Mondrag\u00f3n - Arraste y 2 Teletrabajo.\n\u00bfQu\u00e9 har\u00e9? Est\u00e1s ser\u00e1n tus funciones:\nIntegraci\u00f3n y procesamiento de datos: Recopilar, limpiar y transformar grandes vol\u00famenes de datos procedentes de diversas fuentes (como transacciones bancarias, datos de clientes, y sistemas internos) para que puedan ser usados en an\u00e1lisis de negocio y modelos predictivos.\nConstrucci\u00f3n y mantenimiento de pipelines de datos: Dise\u00f1ar flujos de datos (data pipelines) que permitan la transferencia de datos en tiempo real o en lotes a los sistemas de an\u00e1lisis, asegurando que los datos lleguen con rapidez y precisi\u00f3n.\nOptimizaci\u00f3n de almacenamiento de datos: Configurar y gestionar bases de datos y data warehouses, asegurando que el almacenamiento de datos sea eficiente, seguro y accesible.\nColaboraci\u00f3n con Data Scientists y analistas: Proveer datos bien estructurados y de alta calidad que los analistas y cient\u00edficos de datos pueden usar para desarrollar modelos de predicci\u00f3n, scoring de riesgo crediticio, o an\u00e1lisis de comportamiento de clientes.\nGarantizar la seguridad y privacidad de los datos: Trabajar bajo estrictas normas de privacidad y seguridad, asegurando que los datos sensibles de los clientes est\u00e9n protegidos seg\u00fan las normativas financieras.\nIntegraci\u00f3n y procesamiento de datos: Recopilar, limpiar y transformar grandes vol\u00famenes de datos procedentes de diversas fuentes (como transacciones bancarias, datos de clientes, y sistemas internos) para que puedan ser usados en an\u00e1lisis de negocio y modelos predictivos.\nConstrucci\u00f3n y mantenimiento de pipelines de datos: Dise\u00f1ar flujos de datos (data pipelines) que permitan la transferencia de datos en tiempo real o en lotes a los sistemas de an\u00e1lisis, asegurando que los datos lleguen con rapidez y precisi\u00f3n.\nOptimizaci\u00f3n de almacenamiento de datos: Configurar y gestionar bases de datos y data warehouses, asegurando que el almacenamiento de datos sea eficiente, seguro y accesible.\nColaboraci\u00f3n con Data Scientists y analistas: Proveer datos bien estructurados y de alta calidad que los analistas y cient\u00edficos de datos pueden usar para desarrollar modelos de predicci\u00f3n, scoring de riesgo crediticio, o an\u00e1lisis de comportamiento de clientes.\nGarantizar la seguridad y privacidad de los datos: Trabajar bajo estrictas normas de privacidad y seguridad, asegurando que los datos sensibles de los clientes est\u00e9n protegidos seg\u00fan las normativas financieras.\nBeneficios\nSer parte de un equipo joven en una comunidad techie.\nTe integrar\u00e1s en un equipo de alto rendimiento y con un alto grado de especializaci\u00f3n.\nPlanes de formaci\u00f3n personalizados con nuestra BTW University (Idiomas con Speexx, The Power Business School, Udemy, Certificaciones t\u00e9cnicas, Pharos, etc).\nSeguro m\u00e9dico gratuito desde el primer d\u00eda. Posibilidad de elegir c\u00f3mo obtener parte de tu salario gracias a las ventajas fiscales de nuestra Retribuci\u00f3n Flexible (tickets restaurante, guarder\u00eda).\nAcceso a variedad de descuentos (Viajes, tecnolog\u00eda... y muchos m\u00e1s).\nPrecios reducidos en bienestar, fitness y nutric\u00f3n con Gympass.\n\u00a1Cumplir a\u00f1os como Betweener tiene premio!\nSer parte de un equipo joven en una comunidad techie.\nTe integrar\u00e1s en un equipo de alto rendimiento y con un alto grado de especializaci\u00f3n.\nPlanes de formaci\u00f3n personalizados con nuestra BTW University (Idiomas con Speexx, The Power Business School, Udemy, Certificaciones t\u00e9cnicas, Pharos, etc).\nSeguro m\u00e9dico gratuito desde el primer d\u00eda. Posibilidad de elegir c\u00f3mo obtener parte de tu salario gracias a las ventajas fiscales de nuestra Retribuci\u00f3n Flexible (tickets restaurante, guarder\u00eda).\nAcceso a variedad de descuentos (Viajes, tecnolog\u00eda... y muchos m\u00e1s).\nPrecios reducidos en bienestar, fitness y nutric\u00f3n con Gympass.\n\u00a1Cumplir a\u00f1os como Betweener tiene premio!\n\u00bfApasionado/a de la ingenier\u00eda de datos y con experiencia en procesos ETL, con ganas de continuar desarrollando tu carrera profesional en un entorno de grandes retos? \u00a1No dudes en inscribirte!\nSi tienes cualquier duda, puedes consultarnos a trav\u00e9s del WhatsApp 652 067 523."
    },
    "4145792727": {
        "title": "Data Engineer ",
        "company": "Micropole",
        "location": "Valencian Community, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nJoining MICROPOLE SPAIN means becoming part of an innovative agency that empowers brands and businesses. With a diverse team of talents, we tackle data engineering challenges head-on, from cloud migration to advanced analytics. Our international clients across various sectors appreciate our commitment to delivering cutting-edge solutions.\nWe are currently seeking an experienced Data Engineer to join our team.\n\nTasks\nDevelop and maintain scalable data pipelines to automate data extraction, transformation, and loading (ETL) processes.\nDesign and implement data architectures that support analytics and reporting requirements.\nParticipate in the deployment of the group AWS data platform, ensuring alignment with the core data model.\nEnsure the performance, security, and reliability of data systems, following best practices.\nCollaborate with cross-functional teams to understand business needs and translate them into technical solutions.\n\nProfile\nMinimum of 3-4 years of experience in a data engineering role\nStrong proficiency in Python and SQL for data manipulation and automation.\nExperience with AWS Cloud services (data storage, data processing, and infrastructure management)\nFamiliarity with data modeling and data integration processes in large-scale environments.\nExcellent analytical skills and a problem-solving mindset, with attention to detail.\nEnglish is mandatory. French is a plus.\n\nBenefits\nCompetitive salary based on experience and skills.\nOpportunity for professional growth with training programs to enhance your expertise.\nRemote work options with flexible compensation plans, including benefits for meals and public transport.\nA supportive environment with a strong culture of teamwork and work-life balance.\nGain international experience by working with professionals from diverse backgrounds, expanding your network.\n\nAt MICROPOLE SPAIN, we value not only your skills and experience but also your personality, interpersonal skills, and enthusiasm for contributing to our company culture. If your qualifications and growth mindset align with our vision, we're excited to welcome you on board!\nJoining MICROPOLE SPAIN means becoming part of an innovative agency that empowers brands and businesses. With a diverse team of talents, we tackle data engineering challenges head-on, from cloud migration to advanced analytics. Our international clients across various sectors appreciate our commitment to delivering cutting-edge solutions.\nWe are currently seeking an experienced Data Engineer to join our team.\nData Engineer\nTasks\nDevelop and maintain scalable data pipelines to automate data extraction, transformation, and loading (ETL) processes.\nDesign and implement data architectures that support analytics and reporting requirements.\nParticipate in the deployment of the group AWS data platform, ensuring alignment with the core data model.\nEnsure the performance, security, and reliability of data systems, following best practices.\nCollaborate with cross-functional teams to understand business needs and translate them into technical solutions.\nDevelop and maintain scalable data pipelines to automate data extraction, transformation, and loading (ETL) processes.\nDevelop and maintain\nDesign and implement data architectures that support analytics and reporting requirements.\nDesign and implement\nParticipate in the deployment of the group AWS data platform, ensuring alignment with the core data model.\ndeployment of the group AWS data platform\nEnsure the performance, security, and reliability of data systems, following best practices.\nCollaborate with cross-functional teams to understand business needs and translate them into technical solutions.\nProfile\nMinimum of 3-4 years of experience in a data engineering role\nStrong proficiency in Python and SQL for data manipulation and automation.\nExperience with AWS Cloud services (data storage, data processing, and infrastructure management)\nFamiliarity with data modeling and data integration processes in large-scale environments.\nExcellent analytical skills and a problem-solving mindset, with attention to detail.\nEnglish is mandatory. French is a plus.\nMinimum of 3-4 years of experience in a data engineering role\nMinimum of 3-4 years of experience\nStrong proficiency in Python and SQL for data manipulation and automation.\nStrong proficiency in Python and SQL\nExperience with AWS Cloud services (data storage, data processing, and infrastructure management)\nFamiliarity with data modeling and data integration processes in large-scale environments.\ndata modeling\ndata integration processes\nExcellent analytical skills and a problem-solving mindset, with attention to detail.\nEnglish is mandatory. French is a plus.\nEnglish is mandatory\nBenefits\nCompetitive salary based on experience and skills.\nOpportunity for professional growth with training programs to enhance your expertise.\nRemote work options with flexible compensation plans, including benefits for meals and public transport.\nA supportive environment with a strong culture of teamwork and work-life balance.\nGain international experience by working with professionals from diverse backgrounds, expanding your network.\nCompetitive salary based on experience and skills.\nCompetitive salary\nOpportunity for professional growth with training programs to enhance your expertise.\nOpportunity for professional growth\nRemote work options with flexible compensation plans, including benefits for meals and public transport.\nRemote\nflexible compensation\nA supportive environment with a strong culture of teamwork and work-life balance.\nGain international experience by working with professionals from diverse backgrounds, expanding your network.\nAt MICROPOLE SPAIN, we value not only your skills and experience but also your personality, interpersonal skills, and enthusiasm for contributing to our company culture. If your qualifications and growth mindset align with our vision, we're excited to welcome you on board!\nMICROPOLE SPAIN"
    },
    "4159767071": {
        "title": "Data Engineer ",
        "company": "Happening",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nThink of a data platform. Handling real-time data from dozens of services like no other. The industry vertical is betting - gaming and sports. Now think of scaling it. And innovating it. Using cutting edge technology. Managing reporting, decision making and personalization solutions for tens of thousands of customers daily. Sounds interesting?\n\nWe're looking for someone who:\n\nHas solid experience with data transformation flows (ETL, RT data pipelines,...)\nHas a programming background and a general understanding of data architectures\nHas some experience in defining data models based on business requests\nKnows how to write clean, maintainable and well-tested code/flows that adheres to best practices, with an emphasis on performance, scalability\nCS or related degree\n\nBonus points if you are:\n\nExperienced with cloud-based DB and technologies\nFamiliar with Snowflake or other analytical DBs\nFamiliar with Python\n\nWhat you'll be doing:\n\nBuild data transformation flows for various parts of our business, on fast and versatile data sources with more than 100mil changes a day\nDevelop parts of our platform that will be used for front end personalization or customer experience and communication enhancement\nCommunicate with data analysts, product owners and development teams in order to understand business processes and system architecture for specific product features\nDefine rules and implementing controls for data quality management\nTake over complex tasks, introduce new technologies and influence changes in data architecture, negotiate design changes, etc. \nDoing code reviews and assuring the best quality of the platform through collaboration with other team members\nWe're looking for someone who:\nHas solid experience with data transformation flows (ETL, RT data pipelines,...)\nHas a programming background and a general understanding of data architectures\nHas some experience in defining data models based on business requests\nKnows how to write clean, maintainable and well-tested code/flows that adheres to best practices, with an emphasis on performance, scalability\nCS or related degree\nHas solid experience with data transformation flows (ETL, RT data pipelines,...)\nHas a programming background and a general understanding of data architectures\nHas some experience in defining data models based on business requests\nKnows how to write clean, maintainable and well-tested code/flows that adheres to best practices, with an emphasis on performance, scalability\nCS or related degree\nBonus points if you are:\nExperienced with cloud-based DB and technologies\nFamiliar with Snowflake or other analytical DBs\nFamiliar with Python\nExperienced with cloud-based DB and technologies\nFamiliar with Snowflake or other analytical DBs\nFamiliar with Python\nWhat you'll be doing:\nBuild data transformation flows for various parts of our business, on fast and versatile data sources with more than 100mil changes a day\nDevelop parts of our platform that will be used for front end personalization or customer experience and communication enhancement\nCommunicate with data analysts, product owners and development teams in order to understand business processes and system architecture for specific product features\nDefine rules and implementing controls for data quality management\nTake over complex tasks, introduce new technologies and influence changes in data architecture, negotiate design changes, etc. \nDoing code reviews and assuring the best quality of the platform through collaboration with other team members\nBuild data transformation flows for various parts of our business, on fast and versatile data sources with more than 100mil changes a day\nDevelop parts of our platform that will be used for front end personalization or customer experience and communication enhancement\nCommunicate with data analysts, product owners and development teams in order to understand business processes and system architecture for specific product features\nDefine rules and implementing controls for data quality management\nTake over complex tasks, introduce new technologies and influence changes in data architecture, negotiate design changes, etc.\nDoing code reviews and assuring the best quality of the platform through collaboration with other team members"
    },
    "4162241797": {
        "title": "Data Engineer ",
        "company": "Ciklum",
        "location": "M\u00e1laga, Andalusia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nIt's you who can make an impact! Join our dynamic team as a Data Engineer and shape tomorrow's innovations! It is the dream job for those passionate about Data and crave a vibrant team atmosphere. Apply yourself or drop us a line if you know someone who fits the role. Let's enjoy the engineering and vibrant international environment together!\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4,000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\n\nAbout the role:\nAs a Data Engineer, you'll become a part of a cross-functional development team engineering experiences of tomorrow, working on a mission critical project for one of our clients. The Engineering team consists of Software Engineering, Data Engineering, DevOps and Infrastructure Engineering across multiple locations.\nThe team structure is flat and close teamwork is pervasive. The Engineering team is responsible for developing and maintaining all in-house software for our client. It has very close interaction with all areas of the business comprising the Client Team, Investment Team and Operations Team. There is a strong demand from the business for innovation through software implementation and improvement of business processes. Team members manage requirements and interact with the business directly and are responsible for delivering against them in an agile way.\nOur Client is a purpose-built pension investment manager with an advisory mindset who offers a dynamic and opportunistic approach to risk management, asset allocation, manager selection and implementation. The company now employs over 400 professionals who work for institutional investors with assets of c. $400bn.\nTechnologies:\nExperience working in SQL, Python.\nUnderstanding of CI/CD and relevant tools.\n\nResponsibilities:\nWork with Big Data technologies\nMaintain legacy data infrastructure\nWork together with the business and communicate with them directly to make sure their needs are met\nProvide the business with clear and easy to understand data\nData validation to make sure the quality matches the high standards\nWorking in a team environment provide pragmatic data solutions to business users\nContribute to the project technology stack using ideas and trends from the industry\nReview code, architecture and lead the team in providing the highest quality solutions\n\nRequirements:\nWe value the curiosity and intrinsic desire to investigate and understand a problem and trying out technologies one is unfamiliar with. Candidates should display strong communicative skills and understand that working in a team across multiple locations has a significant social component, both in interacting daily within your team and with business stakeholders. Candidates see technology as a means, not as an end. They value a certain level of pragmatism and balance: not too many different technologies as this is hard to maintain, but not reinventing the wheel every single time. Candidates know what good architecture and good code are and always strive for it.\n\nDesirable:\nHadoop/Hive/NiFi\nExperience in institutional banking or in the financial risk management industry.\n\nWhat's in it for you?\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance and 4 additional undocumented sick leave days\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy license, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: Own your schedule \u2013 you are the one to decide when to start your working day. Just dont miss your regular team stand-up. We are there to support your work-life balance and provide 23 vacation days & short Fridays\nOpportunities: we value our specialists and always find the best options for them. Our Internal Mobility Program helps change a project if needed to help you grow, excel professionally and fulfill your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\n\nAbout us:\nJoin a well-established company and a friendly team of professionals.\nOur Delivery Center in Malaga is one of the leading IT hubs in Andalusia \u2013 it provides an exceptional mix of vibrant tech talent community, wide-open access to expertise advancement opportunities, hybrid/remote work setup & endless Mediterranean seashore summer.\nSeize the perks of global opportunities, local atmosphere and industry-leading clients.\n\nBe bold, not bored!\nExperiences of tomorrow. Engineered together\nInterested already?\nWe would love to get to know you! Submit your application. Can\u2019t wait to see you at Ciklum.\nIt's you who can make an impact! Join our dynamic team as a Data Engineer and shape tomorrow's innovations! It is the dream job for those passionate about Data and crave a vibrant team atmosphere. Apply yourself or drop us a line if you know someone who fits the role. Let's enjoy the engineering and vibrant international environment together!\nData Engineer\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4,000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\nAbout the role:\nAs a Data Engineer, you'll become a part of a cross-functional development team engineering experiences of tomorrow, working on a mission critical project for one of our clients. The Engineering team consists of Software Engineering, Data Engineering, DevOps and Infrastructure Engineering across multiple locations.\nThe team structure is flat and close teamwork is pervasive. The Engineering team is responsible for developing and maintaining all in-house software for our client. It has very close interaction with all areas of the business comprising the Client Team, Investment Team and Operations Team. There is a strong demand from the business for innovation through software implementation and improvement of business processes. Team members manage requirements and interact with the business directly and are responsible for delivering against them in an agile way.\nOur Client is a purpose-built pension investment manager with an advisory mindset who offers a dynamic and opportunistic approach to risk management, asset allocation, manager selection and implementation. The company now employs over 400 professionals who work for institutional investors with assets of c. $400bn.\nTechnologies:\nExperience working in SQL, Python.\nUnderstanding of CI/CD and relevant tools.\nExperience working in SQL, Python.\nUnderstanding of CI/CD and relevant tools.\nResponsibilities:\nWork with Big Data technologies\nMaintain legacy data infrastructure\nWork together with the business and communicate with them directly to make sure their needs are met\nProvide the business with clear and easy to understand data\nData validation to make sure the quality matches the high standards\nWorking in a team environment provide pragmatic data solutions to business users\nContribute to the project technology stack using ideas and trends from the industry\nReview code, architecture and lead the team in providing the highest quality solutions\nWork with Big Data technologies\nMaintain legacy data infrastructure\nWork together with the business and communicate with them directly to make sure their needs are met\nProvide the business with clear and easy to understand data\nData validation to make sure the quality matches the high standards\nWorking in a team environment provide pragmatic data solutions to business users\nContribute to the project technology stack using ideas and trends from the industry\nReview code, architecture and lead the team in providing the highest quality solutions\nRequirements:\nWe value the curiosity and intrinsic desire to investigate and understand a problem and trying out technologies one is unfamiliar with. Candidates should display strong communicative skills and understand that working in a team across multiple locations has a significant social component, both in interacting daily within your team and with business stakeholders. Candidates see technology as a means, not as an end. They value a certain level of pragmatism and balance: not too many different technologies as this is hard to maintain, but not reinventing the wheel every single time. Candidates know what good architecture and good code are and always strive for it.\nDesirable:\nHadoop/Hive/NiFi\nExperience in institutional banking or in the financial risk management industry.\nHadoop/Hive/NiFi\nExperience in institutional banking or in the financial risk management industry.\nWhat's in it for you?\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance and 4 additional undocumented sick leave days\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy license, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: Own your schedule \u2013 you are the one to decide when to start your working day. Just dont miss your regular team stand-up. We are there to support your work-life balance and provide 23 vacation days & short Fridays\nOpportunities: we value our specialists and always find the best options for them. Our Internal Mobility Program helps change a project if needed to help you grow, excel professionally and fulfill your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance and 4 additional undocumented sick leave days\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy license, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: Own your schedule \u2013 you are the one to decide when to start your working day. Just dont miss your regular team stand-up. We are there to support your work-life balance and provide 23 vacation days & short Fridays\nOpportunities: we value our specialists and always find the best options for them. Our Internal Mobility Program helps change a project if needed to help you grow, excel professionally and fulfill your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\nAbout us:\nJoin a well-established company and a friendly team of professionals.\nOur Delivery Center in Malaga is one of the leading IT hubs in Andalusia \u2013 it provides an exceptional mix of vibrant tech talent community, wide-open access to expertise advancement opportunities, hybrid/remote work setup & endless Mediterranean seashore summer.\nSeize the perks of global opportunities, local atmosphere and industry-leading clients.\nBe bold, not bored!\nExperiences of tomorrow. Engineered together\nInterested already?\nWe would love to get to know you! Submit your application. Can\u2019t wait to see you at Ciklum."
    },
    "4172691490": {
        "title": "Data Engineer / Data Scientist (Tableau + SQL + Python)",
        "company": "Sg Tech",
        "location": "Greater Sevilla Metropolitan Area",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nEn Sg Tech necesitamos un perfil de Data Engineer / Data Scientist, con un m\u00ednimo de 4 a\u00f1os de experiencia, para trabajar en proyectos bancarios.\nSer\u00e1s responsable de crear y mantener ETLs de alta calidad. Esta es una gran oportunidad para construir una nueva soluci\u00f3n, trabajando como parte de un peque\u00f1o equipo, para ayudar a la empresa a mejorar sus capacidades de creaci\u00f3n de mercado dentro del espacio de renta fija. La plataforma en s\u00ed est\u00e1 alojada en el ecosistema de AWS.\n\nResponsabilidades:\nAsumir la responsabilidad de la entrega del software garantizando el cumplimiento de las expectativas de calidad y alcance.\nContribuir y asumir la propiedad del dise\u00f1o t\u00e9cnico y garantizar que todos los aspectos de la arquitectura del sistema est\u00e9n bien documentados.\nTrabajar en estrecha colaboraci\u00f3n con los equipos tecnol\u00f3gicos asociados y colaborar eficazmente.\n\nRequisitos m\u00ednimos:\nExperiencia m\u00ednima de 4 a\u00f1os como Data Engineer / Data Scientist.\nConocimientos muy profundos de Tableau.\nExperiencia en SQL, Hive, Hadoop\nExperiencia en Python, Pyspark, Pandas, JulyterLab (trabajando con notebooks)\nExperiencia en el uso de la plataforma AWS.\nExperiencia con herramientas de integraci\u00f3n continua y entrega continua como Git, Jenkins etc.\nDesarrollo \u00e1gil/ciclo de vida del software.\nExcelentes habilidades interpersonales y de comunicaci\u00f3n en ingl\u00e9s (B2+).\n\nRequisitos deseados:\nExperiencia con Kafka\nEspec\u00edficamente, experiencia usando EMR (Elastic Map Reduce) en * AWS para ejecutar clusters de Spark.\nConocimientos de Terraform\nExperiencia con Ansible, Bash scripting, boto3\nExperiencia configurando herramientas de integraci\u00f3n continua y entrega continua\n\n\u00bfQu\u00e9 ofrecemos?\nUn ambiente de trabajo amigable y relajado donde se valora la creatividad y la colaboraci\u00f3n.\nOportunidades de desarrollo profesional y formaci\u00f3n continua.\nProyectos desafiantes que te permitir\u00e1n crecer y aprender constantemente.\nContrato indefinido con nosotros.\nHorario Lunes a jueves (9:00 a 18:00) Viernes y Verano (Jornada Intensiva)\nModalidad hibrida en Madrid (zona O'Donnell), Granada, C\u00f3rdoba, Sevilla (La Cartuja), Jaen Presencial 2 Remoto 3\nUn equipo diverso y inclusivo donde cada voz cuenta.\nEn Sg Tech necesitamos un perfil de Data Engineer / Data Scientist, con un m\u00ednimo de 4 a\u00f1os de experiencia, para trabajar en proyectos bancarios.\nSg Tech\nData Engineer / Data Scientist\nSer\u00e1s responsable de crear y mantener ETLs de alta calidad. Esta es una gran oportunidad para construir una nueva soluci\u00f3n, trabajando como parte de un peque\u00f1o equipo, para ayudar a la empresa a mejorar sus capacidades de creaci\u00f3n de mercado dentro del espacio de renta fija. La plataforma en s\u00ed est\u00e1 alojada en el ecosistema de AWS.\nResponsabilidades:\nAsumir la responsabilidad de la entrega del software garantizando el cumplimiento de las expectativas de calidad y alcance.\nContribuir y asumir la propiedad del dise\u00f1o t\u00e9cnico y garantizar que todos los aspectos de la arquitectura del sistema est\u00e9n bien documentados.\nTrabajar en estrecha colaboraci\u00f3n con los equipos tecnol\u00f3gicos asociados y colaborar eficazmente.\nAsumir la responsabilidad de la entrega del software garantizando el cumplimiento de las expectativas de calidad y alcance.\nContribuir y asumir la propiedad del dise\u00f1o t\u00e9cnico y garantizar que todos los aspectos de la arquitectura del sistema est\u00e9n bien documentados.\nTrabajar en estrecha colaboraci\u00f3n con los equipos tecnol\u00f3gicos asociados y colaborar eficazmente.\nRequisitos m\u00ednimos:\nExperiencia m\u00ednima de 4 a\u00f1os como Data Engineer / Data Scientist.\nConocimientos muy profundos de Tableau.\nExperiencia en SQL, Hive, Hadoop\nExperiencia en Python, Pyspark, Pandas, JulyterLab (trabajando con notebooks)\nExperiencia en el uso de la plataforma AWS.\nExperiencia con herramientas de integraci\u00f3n continua y entrega continua como Git, Jenkins etc.\nDesarrollo \u00e1gil/ciclo de vida del software.\nExcelentes habilidades interpersonales y de comunicaci\u00f3n en ingl\u00e9s (B2+).\nExperiencia m\u00ednima de 4 a\u00f1os como Data Engineer / Data Scientist.\nConocimientos muy profundos de Tableau.\nTableau.\nExperiencia en SQL, Hive, Hadoop\nSQL,\nExperiencia en Python, Pyspark, Pandas, JulyterLab (trabajando con notebooks)\nPython\nExperiencia en el uso de la plataforma AWS.\nAWS\nExperiencia con herramientas de integraci\u00f3n continua y entrega continua como Git, Jenkins etc.\nDesarrollo \u00e1gil/ciclo de vida del software.\nExcelentes habilidades interpersonales y de comunicaci\u00f3n en ingl\u00e9s (B2+).\ningl\u00e9s\nRequisitos deseados:\nExperiencia con Kafka\nEspec\u00edficamente, experiencia usando EMR (Elastic Map Reduce) en * AWS para ejecutar clusters de Spark.\nConocimientos de Terraform\nExperiencia con Ansible, Bash scripting, boto3\nExperiencia configurando herramientas de integraci\u00f3n continua y entrega continua\nExperiencia con Kafka\nEspec\u00edficamente, experiencia usando EMR (Elastic Map Reduce) en * AWS para ejecutar clusters de Spark.\nConocimientos de Terraform\nExperiencia con Ansible, Bash scripting, boto3\nExperiencia configurando herramientas de integraci\u00f3n continua y entrega continua\n\u00bfQu\u00e9 ofrecemos?\nUn ambiente de trabajo amigable y relajado donde se valora la creatividad y la colaboraci\u00f3n.\nOportunidades de desarrollo profesional y formaci\u00f3n continua.\nProyectos desafiantes que te permitir\u00e1n crecer y aprender constantemente.\nContrato indefinido con nosotros.\nHorario Lunes a jueves (9:00 a 18:00) Viernes y Verano (Jornada Intensiva)\nModalidad hibrida en Madrid (zona O'Donnell), Granada, C\u00f3rdoba, Sevilla (La Cartuja), Jaen Presencial 2 Remoto 3\nUn equipo diverso y inclusivo donde cada voz cuenta.\nUn ambiente de trabajo amigable y relajado donde se valora la creatividad y la colaboraci\u00f3n.\nOportunidades de desarrollo profesional y formaci\u00f3n continua.\nProyectos desafiantes que te permitir\u00e1n crecer y aprender constantemente.\nContrato indefinido con nosotros.\nHorario Lunes a jueves (9:00 a 18:00) Viernes y Verano (Jornada Intensiva)\nModalidad hibrida en Madrid (zona O'Donnell), Granada, C\u00f3rdoba, Sevilla (La Cartuja), Jaen Presencial 2 Remoto 3\nUn equipo diverso y inclusivo donde cada voz cuenta."
    },
    "4102592770": {
        "title": "Data Analytics Engineer (They/She/He) ",
        "company": "Glovo",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nData Analytics Engineer (they/she/he)\n\nIf you\u2019re here, it\u2019s because you\u2019re looking for an exciting ride.\n\nA ride that will fuel up your ambitions to take on a new challenge and stretch yourself beyond your comfort zone.\n\nWe\u2019ll deliver a non-vanilla culture built on talent, where we work to amplify the impact on millions of people, paving the way forward together.\n\nNot your usual app. We are the fastest-growing multi-category app connecting millions of users with businesses, and couriers, offering on-demand services from more than 170,000 local restaurants, grocers and supermarkets, and high street retail stores. We operate in more than 1500 cities across 25 countries.\n\nTogether we revolutionise the way people connect with their everyday needs, from delivering essentials to connecting our ecosystem of users through innovative solutions powered by technology. For us, every day is filled with purpose.\n\nWhat makes our ride unique? \n\n\ud83e\udd1d Our culture and strong values. \n\nWe have an \u2018\u2019office-first\u2019\u2019 culture and we place collaboration at the center of everything we do! \nWe have a non-vanilla personality and feedback mindset. We don\u2019t shy away from difficult conversations - we see them as a gift! \nWe work with high intensity and have fun along the way. We also celebrate the wins (a lot!). \nWe celebrate diversity in all its forms and foster an inclusive culture where everyone can bring their authentic selves to work. \n\n\ud83d\udcaaOur career development philosophy. \n\nWe are building a talent house of high performing teams and leaders. We invest in people who raise the bar and help others reach their full potential. \nWe take ownership of our career development. We don\u2019t believe in linear and predictable career paths - we create the job of our dreams! \nWe embrace opportunities to move the needle and make an impact beyond our scope. \n\n\ud83e\udd1dOur commitment to being a force for good. \n\nOur platform is an important economic tool for millions of people (customers, partners, couriers) and we are taking action to amplify our positive impact. \nWe invest in doing good by dedicating time and resources into social and environmental initiatives. \nWe have the ambition of being DIB role models across the tech industry. We are creating environments, systems, and processes that provide equal opportunities, break biases, and empower our communities.\n\nWe have a vision: To give everyone easy access to anything in their cities. And this is where your ride starts.\n\nYOUR MISSION\n\nJoin our team to craft robust and scalable data products that drive our business forward. Leverage our Data Mesh strategy to deliver actionable insights, optimize operations, and foster a culture of data-driven innovation across teams.\n\nTHE JOURNEY\n\nResponsibilities\n\nDevelop data pipelines and optimize data pipelines: data ingestion from transactional sources and APIs, data transformation with dbt, and workflow orchestration with Airflow\nModel and structure data: Build maintainable data models, including fact tables and slowly changing dimensions, to support analytics and reporting needs.\nPartner with business teams to understand their needs, deliver actionable data solutions, and support self-service analytics. \nDesign user-friendly dashboards and LookML models to enable stakeholders to explore and analyze data effectively.\nStay up to date with the latest data technologies, methodologies, and industry best practices. Experiment with new tools \nContinuously work on improving our data quality\n\nRequirements\n\nWHAT YOU WILL BRING TO THE RIDE\n\nEagerness to work closely with business stakeholders\nAbility to communicate complex data topics to non-technical stakeholders\nStrong analytical skills\nProficiency with SQL and Python\nExperience building ETL workloads\nKnowledge of data modeling and schema design\nExperience with cloud technologies (GCP, AWS)\nExperience with data visualization tools (e.g. Looker, Tableau, Power BI)\n\nNice To Have\n\nExperience with test automation (data testing, unit and system integration testing)\nExperience building data products in a Data Mesh architecture\nExperience in big data technologies (e.g. Spark), SQL optimisation and performance tuning\nExperience with Infrastructure as a Code (Terraform)\n\nIndividuals representing diverse profiles, encompassing various genders, ethnicities, and backgrounds, are less likely to apply for this role if they do not possess solid experience in 100% of these areas. Even if it seems you don't meet our musts don't let it stop you, we are all about finding the best talent out there! Skills can be learned, and embracing diversity is invaluable.\n\nWe Believe Driven Talent Deserves\n\n\ud83c\udf1f An enticing equity plan that lets you own a piece of the action.\n\ud83d\udcaa Top-notch private health insurance to keep you at your peak.\n\ud83c\udf54 Monthly Glovo credit to satisfy your cravings with zero delivery fees on all Glovo orders!\n\ud83d\udcb3 Cobee discounts on transportation, food, and even kindergarten expenses.\n\ud83c\udfca Discounted gym memberships to keep you energized.\n\ud83c\udfd6\ufe0fFlexible time off (uncapped vacation days) , the freedom to work from home two days a week, and the opportunity to work from anywhere for up to three weeks a year!\n\ud83d\udc6a Enhanced parental leave, and office-based nursery.\n\ud83e\udde0 Online therapy and wellbeing benefits to ensure your mental well-being.\n\ud83d\udcda An external learning budget to fuel your thirst for knowledge and personal growth.\n\nHere in Glovo, we thrive on diversity, we believe it enhances our teams, products, and culture. We know that the best ideas come from a mashup of brilliant diverse minds. This is why we are committed to providing equal opportunities to talent from all backgrounds - all genders, racial/diverse backgrounds, abilities, ages, sexual orientations and all other unique characteristics that make you YOU. We will encourage you to bring your authentic self to work, fostering an inclusive environment where everyone feels heard.\n\nFeel free to note your pronouns in your application (e.g., she/her/hers, he/him/his, they/them/theirs, etc).\n\nSo, ready to take the wheel and make this the ride of your life? \n\nDelve into our culture by taking a peek at our Instagram and checking out our Linkedin and website!\nData Analytics Engineer (they/she/he)\nexciting ride\nnew challenge and stretch yourself beyond your comfort zone.\nnon-vanilla culture built on talent, where we work to amplify the impact on millions of people\nNot your usual app\nWhat makes our ride unique?\n\ud83e\udd1d Our culture and strong values.\nWe have an \u2018\u2019office-first\u2019\u2019 culture and we place collaboration at the center of everything we do! \nWe have a non-vanilla personality and feedback mindset. We don\u2019t shy away from difficult conversations - we see them as a gift! \nWe work with high intensity and have fun along the way. We also celebrate the wins (a lot!). \nWe celebrate diversity in all its forms and foster an inclusive culture where everyone can bring their authentic selves to work.\nWe have an \u2018\u2019office-first\u2019\u2019 culture and we place collaboration at the center of everything we do!\nWe have a non-vanilla personality and feedback mindset. We don\u2019t shy away from difficult conversations - we see them as a gift!\nWe work with high intensity and have fun along the way. We also celebrate the wins (a lot!).\nWe celebrate diversity in all its forms and foster an inclusive culture where everyone can bring their authentic selves to work.\n\ud83d\udcaaOur career development philosophy.\nWe are building a talent house of high performing teams and leaders. We invest in people who raise the bar and help others reach their full potential. \nWe take ownership of our career development. We don\u2019t believe in linear and predictable career paths - we create the job of our dreams! \nWe embrace opportunities to move the needle and make an impact beyond our scope.\nWe are building a talent house of high performing teams and leaders. We invest in people who raise the bar and help others reach their full potential.\nWe take ownership of our career development. We don\u2019t believe in linear and predictable career paths - we create the job of our dreams!\nWe embrace opportunities to move the needle and make an impact beyond our scope.\n\ud83e\udd1dOur commitment to being a force for good.\nOur platform is an important economic tool for millions of people (customers, partners, couriers) and we are taking action to amplify our positive impact. \nWe invest in doing good by dedicating time and resources into social and environmental initiatives. \nWe have the ambition of being DIB role models across the tech industry. We are creating environments, systems, and processes that provide equal opportunities, break biases, and empower our communities.\nOur platform is an important economic tool for millions of people (customers, partners, couriers) and we are taking action to amplify our positive impact.\nWe invest in doing good by dedicating time and resources into social and environmental initiatives.\nWe have the ambition of being DIB role models across the tech industry. We are creating environments, systems, and processes that provide equal opportunities, break biases, and empower our communities.\nTo give everyone easy access to anything in their cities.\nYOUR MISSION\nTHE JOURNEY\nResponsibilities\nDevelop data pipelines and optimize data pipelines: data ingestion from transactional sources and APIs, data transformation with dbt, and workflow orchestration with Airflow\nModel and structure data: Build maintainable data models, including fact tables and slowly changing dimensions, to support analytics and reporting needs.\nPartner with business teams to understand their needs, deliver actionable data solutions, and support self-service analytics. \nDesign user-friendly dashboards and LookML models to enable stakeholders to explore and analyze data effectively.\nStay up to date with the latest data technologies, methodologies, and industry best practices. Experiment with new tools \nContinuously work on improving our data quality\nDevelop data pipelines and optimize data pipelines: data ingestion from transactional sources and APIs, data transformation with dbt, and workflow orchestration with Airflow\nModel and structure data: Build maintainable data models, including fact tables and slowly changing dimensions, to support analytics and reporting needs.\nPartner with business teams to understand their needs, deliver actionable data solutions, and support self-service analytics.\nDesign user-friendly dashboards and LookML models to enable stakeholders to explore and analyze data effectively.\nStay up to date with the latest data technologies, methodologies, and industry best practices. Experiment with new tools\nContinuously work on improving our data quality\nRequirements\nWHAT YOU WILL BRING TO THE RIDE\nEagerness to work closely with business stakeholders\nAbility to communicate complex data topics to non-technical stakeholders\nStrong analytical skills\nProficiency with SQL and Python\nExperience building ETL workloads\nKnowledge of data modeling and schema design\nExperience with cloud technologies (GCP, AWS)\nExperience with data visualization tools (e.g. Looker, Tableau, Power BI)\nEagerness to work closely with business stakeholders\nAbility to communicate complex data topics to non-technical stakeholders\nStrong analytical skills\nProficiency with SQL and Python\nExperience building ETL workloads\nKnowledge of data modeling and schema design\nExperience with cloud technologies (GCP, AWS)\nExperience with data visualization tools (e.g. Looker, Tableau, Power BI)\nNice To Have\nExperience with test automation (data testing, unit and system integration testing)\nExperience building data products in a Data Mesh architecture\nExperience in big data technologies (e.g. Spark), SQL optimisation and performance tuning\nExperience with Infrastructure as a Code (Terraform)\nExperience with test automation (data testing, unit and system integration testing)\nExperience building data products in a Data Mesh architecture\nExperience in big data technologies (e.g. Spark), SQL optimisation and performance tuning\nExperience with Infrastructure as a Code (Terraform)\nSkills can be learned, and embracing diversity is invaluable.\nWe Believe Driven Talent Deserves\n\ud83c\udf1f An enticing equity plan that lets you own a piece of the action.\n\ud83d\udcaa Top-notch private health insurance to keep you at your peak.\n\ud83c\udf54 Monthly Glovo credit to satisfy your cravings with zero delivery fees on all Glovo orders!\n\ud83d\udcb3 Cobee discounts on transportation, food, and even kindergarten expenses.\n\ud83c\udfca Discounted gym memberships to keep you energized.\n\ud83c\udfd6\ufe0fFlexible time off (uncapped vacation days) , the freedom to work from home two days a week, and the opportunity to work from anywhere for up to three weeks a year!\n\ud83d\udc6a Enhanced parental leave, and office-based nursery.\n\ud83e\udde0 Online therapy and wellbeing benefits to ensure your mental well-being.\n\ud83d\udcda An external learning budget to fuel your thirst for knowledge and personal growth.\n\ud83c\udf1f An enticing equity plan that lets you own a piece of the action.\n\ud83d\udcaa Top-notch private health insurance to keep you at your peak.\n\ud83c\udf54 Monthly Glovo credit to satisfy your cravings with zero delivery fees on all Glovo orders!\n\ud83d\udcb3 Cobee discounts on transportation, food, and even kindergarten expenses.\n\ud83c\udfca Discounted gym memberships to keep you energized.\n\ud83c\udfd6\ufe0fFlexible time off (uncapped vacation days) , the freedom to work from home two days a week, and the opportunity to work from anywhere for up to three weeks a year!\n\ud83d\udc6a Enhanced parental leave, and office-based nursery.\n\ud83e\udde0 Online therapy and wellbeing benefits to ensure your mental well-being.\n\ud83d\udcda An external learning budget to fuel your thirst for knowledge and personal growth.\nSo, ready to take the wheel and make this the ride of your life?"
    },
    "4172606789": {
        "title": "Data Analyst/Engineer",
        "company": "E-Solutions",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Contract",
        "description": "About the job\nHere are the job details-\n\nRole \u2013 Data Analyst/Engineer\nLocation \u2013 European Union (EU)\nMode of Work: Remote\nJob type- B2B Contract\n\nExperience:\n5+ years of experience in a data analyst, systems analyst, or related role.\nBachelor\u2019s degree in computer science, mathematics, statistics, or related field.\n\nSkills & Expertise:\nKnowledge of data warehouse design and management.\nExperience with scripting/programming for data analysis and ETL pipelines (e.g., Python, PostgreSQL).\nKnowledge of Azure Data Stack and cloud-based data solutions.\nExperience with data visualization tools (e.g., Tableau, Google Data Studio).\nKnowledge of reporting techniques and analytical frameworks.\n\nNice to Have:\nExperience with issue tracking software (e.g., JIRA, Zendesk).\n\nRequired Technical Skills\nPostgreSQL\nPython\n\nPreferred Technical Skill\nJIRA\nZendesk\n\nMain Responsibilities\nWork with project teams to establish business questions and data requirements.\nCollaborate with business analysts and data scientists to perform data mapping and schema validation.\nDesign, implement, and maintain programs for data cleaning, extraction, segmentation, and statistical analysison large datasets using Python, PostgreSQL, and Azure Data Stack.\nAnalyze and draw insights from fraud trends and market trends.\nDesign, implement, test, and tune fraud detection rules and strategies.\nMonitor and assess risk strategy performance, investigate performance issues, and implement improvements.\nCollaborate with internal and external partners to ensure data and insights meet business objectives.\n\n\nThanks & Regards,\nHere are the job details-\nRole \u2013 Data Analyst/Engineer\nLocation \u2013 European Union (EU)\nMode of Work: Remote\nJob type- B2B Contract\nExperience:\n5+ years of experience in a data analyst, systems analyst, or related role.\ndata analyst, systems analyst,\nBachelor\u2019s degree in computer science, mathematics, statistics, or related field.\nBachelor\u2019s degree\nSkills & Expertise:\nKnowledge of data warehouse design and management.\ndata warehouse design\nExperience with scripting/programming for data analysis and ETL pipelines (e.g., Python, PostgreSQL).\nscripting/programming\nETL pipelines\nPython, PostgreSQL\nKnowledge of Azure Data Stack and cloud-based data solutions.\nAzure Data Stack\nExperience with data visualization tools (e.g., Tableau, Google Data Studio).\ndata visualization tools\nTableau, Google Data Studio\nKnowledge of reporting techniques and analytical frameworks.\nreporting techniques\nNice to Have:\nExperience with issue tracking software (e.g., JIRA, Zendesk).\nRequired Technical Skills\nPostgreSQL\nPython\nPreferred Technical Skill\nJIRA\nZendesk\nMain Responsibilities\nWork with project teams to establish business questions and data requirements.\nbusiness questions and data requirements.\nCollaborate with business analysts and data scientists to perform data mapping and schema validation.\nbusiness analysts\ndata scientists\ndata mapping\nschema validation.\nDesign, implement, and maintain programs for data cleaning, extraction, segmentation, and statistical analysison large datasets using Python, PostgreSQL, and Azure Data Stack.\ndata cleaning, extraction, segmentation,\nstatistical analysison\nPython, PostgreSQL,\nAzure Data Stack.\nAnalyze and draw insights from fraud trends and market trends.\nfraud trends\nmarket trends.\nDesign, implement, test, and tune fraud detection rules and strategies.\nfraud detection rules\nMonitor and assess risk strategy performance, investigate performance issues, and implement improvements.\nrisk strategy performance\nCollaborate with internal and external partners to ensure data and insights meet business objectives.\nThanks & Regards,"
    },
    "4169370990": {
        "title": "Data Engineer",
        "company": "SmartWay VP",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfTe apasionan los datos? \u00bfBuscas nuevos retos y aprender nuevas tecnolog\u00edas?\n\nEn SmartWay estamos on fire y para seguir creciendo juntos estamos buscando nuestro/a pr\u00f3ximo/a Data Engineer \u2013 based Barcelona. \n\n\u00bfC\u00f3mo tentarte? \nNuestro modelo de trabajo es h\u00edbrido y combina la vida personal con la laboral. \n Nos gusta trabajar desde casa pero tambi\u00e9n nos apetece compartir un caf\u00e9 juntos de vez en cuando en la oficina de Barcelona. \nTe ofrecemos contrato indefinido \ud83d\ude0a \nLa transparencia forma parte de nuestra cultura, sabr\u00e1s en todo momento c\u00f3mo va la empresa y cuales son nuestros objetivos (vamos con OKRs). \n\n\u00bfQu\u00e9 nos gustar\u00eda de ti? \nGrado universitario en inform\u00e1tica o similar \nColaboraci\u00f3n con el Data Architect para el dise\u00f1o del modelo l\u00f3gico global en Data Vault\nDise\u00f1o e implementaci\u00f3n del modelado f\u00edsico en Data Vault en las Capas de RDV, BDV e IM\nImplementaci\u00f3n de Soft Business Rules\nDesarrollo de workloads de movimiento y transformaci\u00f3n de datos alineado a la estrategia DataOps\nElaboraci\u00f3n de juegos de prueba y validaci\u00f3n de datos\n\n\u00bfQu\u00e9 experiencias y tecnolog\u00edas buscamos?\nConocimientos en modelado Data Vault (experiencia m\u00ednima 1 a\u00f1o)\nConocimientos en SQL\nBBDD: Oracle, Postgresql, Snowflake\nConocimientos en DBT y Airflow\nConocimientos en herramientas de Reporting: PowerBI\n\n\u00bfQu\u00e9 valoramos positivamente? \nPython\nLiquibase\nKafka\nJenkins\nCI/CD\nDisponer de certificaciones en las tecnolog\u00edas indicadas\nBuen nivel de ingl\u00e9s\n\nSPOILER ALERT: Para nosotros la actitud no se negocia. \n \u00a1Take the Smart Way!\n\u00bfTe apasionan los datos? \u00bfBuscas nuevos retos y aprender nuevas tecnolog\u00edas?\nEn SmartWay estamos on fire y para seguir creciendo juntos estamos buscando nuestro/a pr\u00f3ximo/a Data Engineer \u2013 based Barcelona.\nData\nEngineer\n\u00bfC\u00f3mo tentarte?\nNuestro modelo de trabajo es h\u00edbrido y combina la vida personal con la laboral. \n Nos gusta trabajar desde casa pero tambi\u00e9n nos apetece compartir un caf\u00e9 juntos de vez en cuando en la oficina de Barcelona. \nTe ofrecemos contrato indefinido \ud83d\ude0a \nLa transparencia forma parte de nuestra cultura, sabr\u00e1s en todo momento c\u00f3mo va la empresa y cuales son nuestros objetivos (vamos con OKRs).\nNuestro modelo de trabajo es h\u00edbrido y combina la vida personal con la laboral.\nNos gusta trabajar desde casa pero tambi\u00e9n nos apetece compartir un caf\u00e9 juntos de vez en cuando en la oficina de Barcelona.\nTe ofrecemos contrato indefinido \ud83d\ude0a\nLa transparencia forma parte de nuestra cultura, sabr\u00e1s en todo momento c\u00f3mo va la empresa y cuales son nuestros objetivos (vamos con OKRs).\n\u00bfQu\u00e9 nos gustar\u00eda de ti?\nGrado universitario en inform\u00e1tica o similar \nColaboraci\u00f3n con el Data Architect para el dise\u00f1o del modelo l\u00f3gico global en Data Vault\nDise\u00f1o e implementaci\u00f3n del modelado f\u00edsico en Data Vault en las Capas de RDV, BDV e IM\nImplementaci\u00f3n de Soft Business Rules\nDesarrollo de workloads de movimiento y transformaci\u00f3n de datos alineado a la estrategia DataOps\nElaboraci\u00f3n de juegos de prueba y validaci\u00f3n de datos\nGrado universitario en inform\u00e1tica o similar\nColaboraci\u00f3n con el Data Architect para el dise\u00f1o del modelo l\u00f3gico global en Data Vault\nDise\u00f1o e implementaci\u00f3n del modelado f\u00edsico en Data Vault en las Capas de RDV, BDV e IM\nImplementaci\u00f3n de Soft Business Rules\nDesarrollo de workloads de movimiento y transformaci\u00f3n de datos alineado a la estrategia DataOps\nElaboraci\u00f3n de juegos de prueba y validaci\u00f3n de datos\n\u00bfQu\u00e9 experiencias y tecnolog\u00edas buscamos?\nConocimientos en modelado Data Vault (experiencia m\u00ednima 1 a\u00f1o)\nConocimientos en SQL\nBBDD: Oracle, Postgresql, Snowflake\nConocimientos en DBT y Airflow\nConocimientos en herramientas de Reporting: PowerBI\nConocimientos en modelado Data Vault (experiencia m\u00ednima 1 a\u00f1o)\nConocimientos en SQL\nBBDD: Oracle, Postgresql, Snowflake\nConocimientos en DBT y Airflow\nConocimientos en herramientas de Reporting: PowerBI\n\u00bfQu\u00e9 valoramos positivamente?\nPython\nLiquibase\nKafka\nJenkins\nCI/CD\nDisponer de certificaciones en las tecnolog\u00edas indicadas\nBuen nivel de ingl\u00e9s\nPython\nLiquibase\nKafka\nJenkins\nCI/CD\nDisponer de certificaciones en las tecnolog\u00edas indicadas\nBuen nivel de ingl\u00e9s\nSPOILER ALERT: Para nosotros la actitud no se negocia.\n\u00a1Take the Smart Way!"
    },
    "4153814710": {
        "title": "Data Engineer - BI & Analytics",
        "company": "WeDoData",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nData engineer - BI & Analytics \n\nLocation: On-site Madrid\nSalary: 36-40K depending on experience \nFlexible working hours. \n\nJoin our growing Admin & Finance team as a Senior Analytics Engineer and play a key role in shaping data-driven decision-making across the organization.\n\n\ud83d\udca1 What you\u2019ll do:\n\ud83d\udd39 Design & implement scalable data models\n\ud83d\udd39 Develop analytical solutions that drive business impact\n\ud83d\udd39 Work with SQL, Python, Databricks, Snowflake, Power BI/Tableau\n\ud83d\udd39 Ensure data integrity, governance, and best practices.\n\n\ud83c\udfaf What we\u2019re looking for:\n\u2714\ufe0f 6+ years of experience in data & analytics\n\u2714\ufe0f Strong SQL & data modeling skills\n\u2714\ufe0f Experience with cloud data platforms (Snowflake, BigQuery, Redshift)\n\u2714\ufe0f Ability to translate business needs into data solutions\n\nPlease apply as soon as possible!\nData engineer - BI & Analytics\nLocation: On-site Madrid\nSalary: 36-40K depending on experience \nFlexible working hours.\nLocation: On-site Madrid\nSalary: 36-40K depending on experience\nFlexible working hours.\nJoin our growing Admin & Finance team as a Senior Analytics Engineer and play a key role in shaping data-driven decision-making across the organization.\nAdmin & Finance team\nSenior Analytics Engineer\nshaping data-driven decision-making\n\ud83d\udca1 What you\u2019ll do:\nWhat you\u2019ll do:\n\ud83d\udd39 Design & implement scalable data models\ndata models\n\ud83d\udd39 Develop analytical solutions that drive business impact\nanalytical solutions\n\ud83d\udd39 Work with SQL, Python, Databricks, Snowflake, Power BI/Tableau\nSQL, Python, Databricks, Snowflake, Power BI/Tableau\n\ud83d\udd39 Ensure data integrity, governance, and best practices.\ndata integrity, governance, and best practices.\n\ud83c\udfaf What we\u2019re looking for:\nWhat we\u2019re looking for:\n\u2714\ufe0f 6+ years of experience in data & analytics\n6+ years\n\u2714\ufe0f Strong SQL & data modeling skills\nSQL & data modeling\n\u2714\ufe0f Experience with cloud data platforms (Snowflake, BigQuery, Redshift)\ncloud data platforms\n\u2714\ufe0f Ability to translate business needs into data solutions\ntranslate business needs\nPlease apply as soon as possible!"
    },
    "4157832759": {
        "title": "Data Engineer - Fintech ",
        "company": "Ebury",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nEbury is a leading global fintech company that empowers businesses to trade and grow internationally. It offers a comprehensive suite of products, including international payments and collections, FX risk management, trade finance, and API integrations. Founded in 2009 by Juan Lobato and Salvador Garc\u00eda, Ebury is one of the fastest-growing global fintechs, with over 1,700 employees and 38 offices in more than 25 countries.\n\nData Engineer - Fintech \n\nMadrid Office - Hybrid: 4 days in the office, 1 day working from home\n\nJoin Our Data Team at Ebury Madrid Office.\n\nEbury\u2019s strategic growth plan would not be possible without our Data team and we are seeking a Data Engineer to join our Data Engineering team!\n\nOur Data Mission Is To Develop And Maintain Ebury\u2019s Data Warehouse And Serve It To The Whole Company, Where Data Scientists, Data Engineers, Analytics Engineers And Data Analysts Work Collaboratively To\n\nBuild ETLs and data pipelines to serve data in our platform\nProvide clean, transformed data ready for analysis and used by our BI tool\nDevelop department and project specific data models and serve these to teams across the company to drive decision making\nAutomate end solutions so we can all spend time on high-value analysis rather than running data extracts\n\nWhat We Offer\n\nCompetitive salary and benefits package \nDiscretionary bonus based on performance\nContinued personal development through training and certification\nWe are Open Source friendly, following Open Source principles in our internal projects and encouraging contributions to external projects\n\nResponsibilities\n\nBe mentored by one of our outstanding performance team member along a 30/60/90 plan designed only for you\nParticipate in data modelling reviews and discussions to validate the model's accuracy, completeness, and alignment with business objectives.\nDesign, develop, deploy and maintain ELT/ETL data pipelines from a variety of data sources (transactional databases, REST APIs, file-based endpoints).\nServe hands-on delivery of data models using solid software engineering practices (eg. version control, testing, CI/CD)\nManage overall pipeline orchestration using Airflow (hosted in Cloud Composer), as well as execution using GCP hosted services such as Container Registry, Artifact Registry, Cloud Run, Cloud Functions, and GKE.\nWork on reducing technical debt by addressing code that is outdated, inefficient, or no longer aligned with best practices or business needs.\nCollaborate with team members to reinforce best practices across the platform, encouraging a shared commitment to quality.\nHelp to implement data governance policies, including data quality standards, data access control, and data classification.\nIdentify opportunities to optimise and refine existing processes.\n\nAbout You\n\n3+ years of data/analytics engineering experience building, maintaining & optimising data pipelines & ETL processes on big data environments\nProficiency in Python, SQL and Airflow\nKnowledge of software engineering practices in data (SDLC, RFC\u2026)\nStay informed about the latest developments and industry standards in Data\nFluency in English\n\nIf you\u2019re excited about this job opportunity but your background doesn\u2019t match exactly the requirements in the job description, we strongly encourage you to apply anyway. You may be just the right candidate for this or other positions we have.\n\nAbout Us\n\nEbury is a FinTech success story, positioned among the fastest-growing international companies in its sector.\n\nFounded in 2009, we are headquartered in London and have more than 1700 staff with a presence in more than 25 countries worldwide. Cultural diversity is part of what makes Ebury a special place to be. From Sao Paulo to Dubai, Bucharest to Toronto, we enjoy sharing team experiences and celebrating success across the Ebury family.\n\nHard work pays off: in 2019, Ebury received a \u00a3350 million investment from Banco Santander and has won internationally recognised awards including Financial Times: 1000 Europe's Fastest-Growing Companies.\n\nNone of this would have been possible without our proudest achievement: our great people. Enthusiastic, innovative and collaborative teams, always ready to disrupt and revolutionise the fast-paced FinTech sector.\n\nWe believe in inclusion. We stand against discrimination in all forms and have no tolerance for the intolerance of differences that makes us a modern and successful organisation. At Ebury, you can be whoever you want to be and still feel a sense of belonging no matter your story because we want you and your uniqueness to help write our future.\n\nPlease submit your application on the careers website directly, uploading your CV / resume in English.\nData Engineer - Fintech\nMadrid Office - Hybrid: 4 days in the office, 1 day working from home\nMadrid\nData team\nData Engineer\nData Engineering team\nBuild ETLs and data pipelines to serve data in our platform\nProvide clean, transformed data ready for analysis and used by our BI tool\nDevelop department and project specific data models and serve these to teams across the company to drive decision making\nAutomate end solutions so we can all spend time on high-value analysis rather than running data extracts\nBuild ETLs and data pipelines to serve data in our platform\nProvide clean, transformed data ready for analysis and used by our BI tool\nDevelop department and project specific data models and serve these to teams across the company to drive decision making\nAutomate end solutions so we can all spend time on high-value analysis rather than running data extracts\nWhat We Offer\nCompetitive salary and benefits package \nDiscretionary bonus based on performance\nContinued personal development through training and certification\nWe are Open Source friendly, following Open Source principles in our internal projects and encouraging contributions to external projects\nCompetitive salary and benefits package\nDiscretionary bonus based on performance\nContinued personal development through training and certification\nWe are Open Source friendly, following Open Source principles in our internal projects and encouraging contributions to external projects\nResponsibilities\nBe mentored by one of our outstanding performance team member along a 30/60/90 plan designed only for you\nParticipate in data modelling reviews and discussions to validate the model's accuracy, completeness, and alignment with business objectives.\nDesign, develop, deploy and maintain ELT/ETL data pipelines from a variety of data sources (transactional databases, REST APIs, file-based endpoints).\nServe hands-on delivery of data models using solid software engineering practices (eg. version control, testing, CI/CD)\nManage overall pipeline orchestration using Airflow (hosted in Cloud Composer), as well as execution using GCP hosted services such as Container Registry, Artifact Registry, Cloud Run, Cloud Functions, and GKE.\nWork on reducing technical debt by addressing code that is outdated, inefficient, or no longer aligned with best practices or business needs.\nCollaborate with team members to reinforce best practices across the platform, encouraging a shared commitment to quality.\nHelp to implement data governance policies, including data quality standards, data access control, and data classification.\nIdentify opportunities to optimise and refine existing processes.\nBe mentored by one of our outstanding performance team member along a 30/60/90 plan designed only for you\nParticipate in data modelling reviews and discussions to validate the model's accuracy, completeness, and alignment with business objectives.\nDesign, develop, deploy and maintain ELT/ETL data pipelines from a variety of data sources (transactional databases, REST APIs, file-based endpoints).\nServe hands-on delivery of data models using solid software engineering practices (eg. version control, testing, CI/CD)\nManage overall pipeline orchestration using Airflow (hosted in Cloud Composer), as well as execution using GCP hosted services such as Container Registry, Artifact Registry, Cloud Run, Cloud Functions, and GKE.\nWork on reducing technical debt by addressing code that is outdated, inefficient, or no longer aligned with best practices or business needs.\nCollaborate with team members to reinforce best practices across the platform, encouraging a shared commitment to quality.\nHelp to implement data governance policies, including data quality standards, data access control, and data classification.\nIdentify opportunities to optimise and refine existing processes.\nAbout You\n3+ years of data/analytics engineering experience building, maintaining & optimising data pipelines & ETL processes on big data environments\nProficiency in Python, SQL and Airflow\nKnowledge of software engineering practices in data (SDLC, RFC\u2026)\nStay informed about the latest developments and industry standards in Data\nFluency in English\n3+ years of data/analytics engineering experience building, maintaining & optimising data pipelines & ETL processes on big data environments\nProficiency in Python, SQL and Airflow\nKnowledge of software engineering practices in data (SDLC, RFC\u2026)\nStay informed about the latest developments and industry standards in Data\nFluency in English\nAbout Us\nEbury is a FinTech success story, positioned among the fastest-growing international companies in its sector.\nWe believe in inclusion. We stand against discrimination in all forms and have no tolerance for the intolerance of differences that makes us a modern and successful organisation. At Ebury, you can be whoever you want to be and still feel a sense of belonging no matter your story because we want you and your uniqueness to help write our future.\nPlease submit your application on the careers website directly, uploading your CV / resume in English."
    },
    "4161421827": {
        "title": "Data Engineer",
        "company": "ORBIDI",
        "location": "Sant Cugat del Vall\u00e8s, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nSobre nosotros: \nEn ORBIDI, somos pioneros en el uso de tecnolog\u00eda avanzada, inteligencia artificial y modelos predictivos para propulsar el crecimiento de las peque\u00f1as y medianas empresas (PYMEs). Nuestra pasi\u00f3n por la innovaci\u00f3n y el marketing nos motiva a buscar soluciones creativas que permitan a las empresas no solo competir, sino destacar en el mercado actual. Creemos firmemente en el poder de la digitalizaci\u00f3n y c\u00f3mo esta puede transformar los negocios para mejor. Buscamos talentos creativos y estrat\u00e9gicos que compartan nuestra visi\u00f3n y deseo de hacer una diferencia tangible en el mundo empresarial.\n\nEl rol:\nEstamos buscando un  Senior Data Engineer altamente capacitado para unirse a nuestro equipo. Como Ingeniero de Datos Senior, ser\u00e1s responsable de dise\u00f1ar, desarrollar y optimizar nuestra infraestructura de datos para respaldar diversas iniciativas basadas en datos en toda la organizaci\u00f3n. Desempe\u00f1ar\u00e1s un papel clave en la construcci\u00f3n de canalizaciones de datos escalables, garantizando la disponibilidad y confiabilidad de los datos, y colaborando con cient\u00edficos y analistas de datos para generar informaci\u00f3n estrat\u00e9gica para el negocio.\n\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n?\nDesarrollar, probar y mantener procesos ETL/ELT robustos para ingerir y transformar grandes vol\u00famenes de datos estructurados y no estructurados provenientes de diversas fuentes.\nDise\u00f1ar e implementar modelos de datos y soluciones de almacenamiento escalables para respaldar iniciativas de an\u00e1lisis, aprendizaje autom\u00e1tico e inteligencia empresarial.\nIntegrar diversas fuentes de datos de terceros e internas en plataformas de datos centralizadas y automatizar tareas repetitivas para mejorar la eficiencia.\nImplementar y mantener las mejores pr\u00e1cticas de gobernanza de datos, asegurando alta calidad de datos, seguridad y cumplimiento con est\u00e1ndares de la industria (GDPR, HIPAA, etc.).\nColaborar estrechamente con cient\u00edficos de datos, analistas y partes interesadas del negocio para comprender sus necesidades de datos y garantizar la entrega oportuna de datos precisos y relevantes.\nMonitorear continuamente el rendimiento de la infraestructura de datos, identificando cuellos de botella y optimizando los sistemas para mejorar la velocidad y la eficiencia en concurrencia, rendimiento de consultas y eficiencia de almacenamiento.\nTrabajar de cerca con el equipo de ciencia de datos para implementar canalizaciones de an\u00e1lisis de datos.\n\nLo que te har\u00e1 triunfar:\nM\u00e1s de 5 a\u00f1os de experiencia dise\u00f1ando y construyendo sistemas de datos a gran escala, con experiencia en ETL/ELT, modelado de datos y almacenamiento de datos.\nS\u00f3lidas habilidades de programaci\u00f3n en Python y conocimiento avanzado de SQL para manipulaci\u00f3n y an\u00e1lisis de datos.\nExperiencia pr\u00e1ctica con plataformas en la nube como AWS, Google Cloud o Azure, incluyendo servicios como AWS Redshift, BigQuery o Azure Data Lake.\nExperiencia con tecnolog\u00edas de big data como Hadoop, Spark, Kafka u otros marcos de computaci\u00f3n distribuida.\nConocimiento de bases de datos SQL y NoSQL (por ejemplo, PostgreSQL, MySQL, MongoDB, Cassandra) y experiencia en modelado de datos para un rendimiento \u00f3ptimo.\nCompetencia en herramientas de canalizaci\u00f3n y orquestaci\u00f3n de datos como Airflow, Prefect o Luigi.\nS\u00f3lida comprensi\u00f3n de soluciones de almacenamiento de datos, incluyendo lagos de datos (data lakes) y almacenes de datos (data warehouses).\n\nBeneficios:\n\ud83d\udd51Flexibilidad horaria\n\ud83d\udc8eOficinas modernas en Sant Cugat, frente a la estaci\u00f3n de FGC de Mirasol.\n\ud83d\udcb0Salario competitivo y contrato indefinido\n\ud83e\udd11Bonus basado en el cumplimiento de objetivos de tu equipo\n\ud83c\udf3423 d\u00edas laborales de vacaciones + \ud83c\udf82d\u00eda de cumplea\u00f1os\n\ud83c\udfe0Formato H\u00edbrido con teletrabajo 2 d\u00edas a la semana.\n\ud83d\ude0dJornada intensiva todos los viernes del a\u00f1o.\n\ud83d\udcc5Agosto flexible\n\ud83e\udd73Actividades de teambuilding y celebraciones y off-sites\n\ud83e\udd29Referral Program\n\ud83e\udd38\ud83c\udffb\u200d\u2640\ufe0fDescuento en gimnasio\n\ud83c\udf4eFruta en oficina\nSi tienes una pasi\u00f3n por el marketing digital, la creaci\u00f3n de contenido y el dise\u00f1o, y buscas un rol donde puedas dejar tu huella y contribuir al \u00e9xito de las PYMEs, \u00a1ORBIDI es el lugar para ti! Te invitamos a aplicar y ser parte de nuestro equipo creativo y estrat\u00e9gico.\nSobre nosotros:\nEn ORBIDI, somos pioneros en el uso de tecnolog\u00eda avanzada, inteligencia artificial y modelos predictivos para propulsar el crecimiento de las peque\u00f1as y medianas empresas (PYMEs). Nuestra pasi\u00f3n por la innovaci\u00f3n y el marketing nos motiva a buscar soluciones creativas que permitan a las empresas no solo competir, sino destacar en el mercado actual. Creemos firmemente en el poder de la digitalizaci\u00f3n y c\u00f3mo esta puede transformar los negocios para mejor. Buscamos talentos creativos y estrat\u00e9gicos que compartan nuestra visi\u00f3n y deseo de hacer una diferencia tangible en el mundo empresarial.\nEl rol:\nEstamos buscando un  Senior Data Engineer altamente capacitado para unirse a nuestro equipo. Como Ingeniero de Datos Senior, ser\u00e1s responsable de dise\u00f1ar, desarrollar y optimizar nuestra infraestructura de datos para respaldar diversas iniciativas basadas en datos en toda la organizaci\u00f3n. Desempe\u00f1ar\u00e1s un papel clave en la construcci\u00f3n de canalizaciones de datos escalables, garantizando la disponibilidad y confiabilidad de los datos, y colaborando con cient\u00edficos y analistas de datos para generar informaci\u00f3n estrat\u00e9gica para el negocio.\nSenior\nData Engineer\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n?\nDesarrollar, probar y mantener procesos ETL/ELT robustos para ingerir y transformar grandes vol\u00famenes de datos estructurados y no estructurados provenientes de diversas fuentes.\nDise\u00f1ar e implementar modelos de datos y soluciones de almacenamiento escalables para respaldar iniciativas de an\u00e1lisis, aprendizaje autom\u00e1tico e inteligencia empresarial.\nIntegrar diversas fuentes de datos de terceros e internas en plataformas de datos centralizadas y automatizar tareas repetitivas para mejorar la eficiencia.\nImplementar y mantener las mejores pr\u00e1cticas de gobernanza de datos, asegurando alta calidad de datos, seguridad y cumplimiento con est\u00e1ndares de la industria (GDPR, HIPAA, etc.).\nColaborar estrechamente con cient\u00edficos de datos, analistas y partes interesadas del negocio para comprender sus necesidades de datos y garantizar la entrega oportuna de datos precisos y relevantes.\nMonitorear continuamente el rendimiento de la infraestructura de datos, identificando cuellos de botella y optimizando los sistemas para mejorar la velocidad y la eficiencia en concurrencia, rendimiento de consultas y eficiencia de almacenamiento.\nTrabajar de cerca con el equipo de ciencia de datos para implementar canalizaciones de an\u00e1lisis de datos.\nDesarrollar, probar y mantener procesos ETL/ELT robustos para ingerir y transformar grandes vol\u00famenes de datos estructurados y no estructurados provenientes de diversas fuentes.\nDise\u00f1ar e implementar modelos de datos y soluciones de almacenamiento escalables para respaldar iniciativas de an\u00e1lisis, aprendizaje autom\u00e1tico e inteligencia empresarial.\nIntegrar diversas fuentes de datos de terceros e internas en plataformas de datos centralizadas y automatizar tareas repetitivas para mejorar la eficiencia.\nImplementar y mantener las mejores pr\u00e1cticas de gobernanza de datos, asegurando alta calidad de datos, seguridad y cumplimiento con est\u00e1ndares de la industria (GDPR, HIPAA, etc.).\nColaborar estrechamente con cient\u00edficos de datos, analistas y partes interesadas del negocio para comprender sus necesidades de datos y garantizar la entrega oportuna de datos precisos y relevantes.\nMonitorear continuamente el rendimiento de la infraestructura de datos, identificando cuellos de botella y optimizando los sistemas para mejorar la velocidad y la eficiencia en concurrencia, rendimiento de consultas y eficiencia de almacenamiento.\nTrabajar de cerca con el equipo de ciencia de datos para implementar canalizaciones de an\u00e1lisis de datos.\nLo que te har\u00e1 triunfar:\nM\u00e1s de 5 a\u00f1os de experiencia dise\u00f1ando y construyendo sistemas de datos a gran escala, con experiencia en ETL/ELT, modelado de datos y almacenamiento de datos.\nS\u00f3lidas habilidades de programaci\u00f3n en Python y conocimiento avanzado de SQL para manipulaci\u00f3n y an\u00e1lisis de datos.\nExperiencia pr\u00e1ctica con plataformas en la nube como AWS, Google Cloud o Azure, incluyendo servicios como AWS Redshift, BigQuery o Azure Data Lake.\nExperiencia con tecnolog\u00edas de big data como Hadoop, Spark, Kafka u otros marcos de computaci\u00f3n distribuida.\nConocimiento de bases de datos SQL y NoSQL (por ejemplo, PostgreSQL, MySQL, MongoDB, Cassandra) y experiencia en modelado de datos para un rendimiento \u00f3ptimo.\nCompetencia en herramientas de canalizaci\u00f3n y orquestaci\u00f3n de datos como Airflow, Prefect o Luigi.\nS\u00f3lida comprensi\u00f3n de soluciones de almacenamiento de datos, incluyendo lagos de datos (data lakes) y almacenes de datos (data warehouses).\nM\u00e1s de 5 a\u00f1os de experiencia dise\u00f1ando y construyendo sistemas de datos a gran escala, con experiencia en ETL/ELT, modelado de datos y almacenamiento de datos.\nS\u00f3lidas habilidades de programaci\u00f3n en Python y conocimiento avanzado de SQL para manipulaci\u00f3n y an\u00e1lisis de datos.\nExperiencia pr\u00e1ctica con plataformas en la nube como AWS, Google Cloud o Azure, incluyendo servicios como AWS Redshift, BigQuery o Azure Data Lake.\nExperiencia con tecnolog\u00edas de big data como Hadoop, Spark, Kafka u otros marcos de computaci\u00f3n distribuida.\nConocimiento de bases de datos SQL y NoSQL (por ejemplo, PostgreSQL, MySQL, MongoDB, Cassandra) y experiencia en modelado de datos para un rendimiento \u00f3ptimo.\nCompetencia en herramientas de canalizaci\u00f3n y orquestaci\u00f3n de datos como Airflow, Prefect o Luigi.\nS\u00f3lida comprensi\u00f3n de soluciones de almacenamiento de datos, incluyendo lagos de datos (data lakes) y almacenes de datos (data warehouses).\nBeneficios:\n\ud83d\udd51Flexibilidad horaria\n\ud83d\udc8eOficinas modernas en Sant Cugat, frente a la estaci\u00f3n de FGC de Mirasol.\n\ud83d\udcb0Salario competitivo y contrato indefinido\n\ud83e\udd11Bonus basado en el cumplimiento de objetivos de tu equipo\n\ud83c\udf3423 d\u00edas laborales de vacaciones + \ud83c\udf82d\u00eda de cumplea\u00f1os\n\ud83c\udfe0Formato H\u00edbrido con teletrabajo 2 d\u00edas a la semana.\n\ud83d\ude0dJornada intensiva todos los viernes del a\u00f1o.\n\ud83d\udcc5Agosto flexible\n\ud83e\udd73Actividades de teambuilding y celebraciones y off-sites\n\ud83e\udd29Referral Program\n\ud83e\udd38\ud83c\udffb\u200d\u2640\ufe0fDescuento en gimnasio\n\ud83c\udf4eFruta en oficina\n\ud83d\udd51Flexibilidad horaria\n\ud83d\udc8eOficinas modernas en Sant Cugat, frente a la estaci\u00f3n de FGC de Mirasol.\n\ud83d\udcb0Salario competitivo y contrato indefinido\n\ud83e\udd11Bonus basado en el cumplimiento de objetivos de tu equipo\n\ud83c\udf3423 d\u00edas laborales de vacaciones + \ud83c\udf82d\u00eda de cumplea\u00f1os\n\ud83c\udfe0Formato H\u00edbrido con teletrabajo 2 d\u00edas a la semana.\n\ud83d\ude0dJornada intensiva todos los viernes del a\u00f1o.\n\ud83d\udcc5Agosto flexible\n\ud83e\udd73Actividades de teambuilding y celebraciones y off-sites\n\ud83e\udd29Referral Program\n\ud83e\udd38\ud83c\udffb\u200d\u2640\ufe0fDescuento en gimnasio\n\ud83c\udf4eFruta en oficina\nSi tienes una pasi\u00f3n por el marketing digital, la creaci\u00f3n de contenido y el dise\u00f1o, y buscas un rol donde puedas dejar tu huella y contribuir al \u00e9xito de las PYMEs, \u00a1ORBIDI es el lugar para ti! Te invitamos a aplicar y ser parte de nuestro equipo creativo y estrat\u00e9gico."
    },
    "4163543330": {
        "title": "Data Engineer",
        "company": "UMATR",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nPosition Overview\nAs a Data Engineer, you will play a key role in enhancing the data and capabilities of the organisation. You will work within a multidisciplinary team, collaborating with various departments to drive the development of data frameworks and ensure efficient use and integration of these systems across the business.\n\nKey Responsibilities:\nContribute to the scalability, reliability, and efficiency of data systems to support growing needs across the organisation.\nCollaborate with cross-functional teams to gather data requirements and design scalable data solutions.\nDevelop, implement, and deploy data solutions end-to-end to support key business operations.\nFocus on data modelling and pipeline development to ensure high-quality, reliable data workflows.\nOptimise data storage, processing, and retrieval mechanisms to enhance performance.\nCollaborate with data professionals from diverse backgrounds in a dynamic, cross-functional environment.\n\nRequired Skills and Qualifications:\nMaster\u2019s degree in Computer Science, Engineering, or a related technical field.\nStrong experience in data engineering, with expertise in developing data solutions in a cloud environment (preferably Microsoft Azure).\nHands-on experience with Databricks and Spark.\nFamiliarity with data warehousing and data lake architectures.\nKnowledge of data pipeline orchestration, including event-driven architectures and both streaming and batch data processing.\nProficiency in Python and SQL for querying and data manipulation.\nExperience with continuous integration and continuous deployment (CI/CD) practices for automated data solution deployment.\nFamiliar with collaborative environments and version control systems, such as GitHub.\nUnderstanding of Business Intelligence (BI) or data visualisation tools (e.g., Power BI).\nProficiency in English, both spoken and written.\nPosition Overview\nAs a Data Engineer, you will play a key role in enhancing the data and capabilities of the organisation. You will work within a multidisciplinary team, collaborating with various departments to drive the development of data frameworks and ensure efficient use and integration of these systems across the business.\nKey Responsibilities:\nContribute to the scalability, reliability, and efficiency of data systems to support growing needs across the organisation.\nCollaborate with cross-functional teams to gather data requirements and design scalable data solutions.\nDevelop, implement, and deploy data solutions end-to-end to support key business operations.\nFocus on data modelling and pipeline development to ensure high-quality, reliable data workflows.\nOptimise data storage, processing, and retrieval mechanisms to enhance performance.\nCollaborate with data professionals from diverse backgrounds in a dynamic, cross-functional environment.\nContribute to the scalability, reliability, and efficiency of data systems to support growing needs across the organisation.\nCollaborate with cross-functional teams to gather data requirements and design scalable data solutions.\nDevelop, implement, and deploy data solutions end-to-end to support key business operations.\nFocus on data modelling and pipeline development to ensure high-quality, reliable data workflows.\nOptimise data storage, processing, and retrieval mechanisms to enhance performance.\nCollaborate with data professionals from diverse backgrounds in a dynamic, cross-functional environment.\nRequired Skills and Qualifications:\nMaster\u2019s degree in Computer Science, Engineering, or a related technical field.\nStrong experience in data engineering, with expertise in developing data solutions in a cloud environment (preferably Microsoft Azure).\nHands-on experience with Databricks and Spark.\nFamiliarity with data warehousing and data lake architectures.\nKnowledge of data pipeline orchestration, including event-driven architectures and both streaming and batch data processing.\nProficiency in Python and SQL for querying and data manipulation.\nExperience with continuous integration and continuous deployment (CI/CD) practices for automated data solution deployment.\nFamiliar with collaborative environments and version control systems, such as GitHub.\nUnderstanding of Business Intelligence (BI) or data visualisation tools (e.g., Power BI).\nProficiency in English, both spoken and written.\nMaster\u2019s degree in Computer Science, Engineering, or a related technical field.\nStrong experience in data engineering, with expertise in developing data solutions in a cloud environment (preferably Microsoft Azure).\nHands-on experience with Databricks and Spark.\nFamiliarity with data warehousing and data lake architectures.\nKnowledge of data pipeline orchestration, including event-driven architectures and both streaming and batch data processing.\nProficiency in Python and SQL for querying and data manipulation.\nExperience with continuous integration and continuous deployment (CI/CD) practices for automated data solution deployment.\nFamiliar with collaborative environments and version control systems, such as GitHub.\nUnderstanding of Business Intelligence (BI) or data visualisation tools (e.g., Power BI).\nProficiency in English, both spoken and written."
    },
    "4130991821": {
        "title": "Data Engineer II ",
        "company": "Expedia Group",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nExpedia Group brands power global travel for everyone, everywhere. We design cutting-edge tech to make travel smoother and more memorable, and we create groundbreaking solutions for our partners. Our diverse, vibrant, and welcoming community is essential in driving our success.\n\nWhy Join Us?\n\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\n\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a global hybrid work setup (with some pretty cool offices), and career development resources, all to fuel our employees' passion for travel and ensure a rewarding career journey. We\u2019re building a more open world. Join us.\n\nAre you a Data engineer looking to join a leading technology company to build products and features? Are you passionate about data and everything it powers? Are you ready to help people travel, go from anywhere to somewhere, possible? If this sounds like you, keep reading!\n\nExpedia Partner Solutions (EPS) is seeking a Software Engineer for its Data Engineering team in Madrid or London.\n\nWe're constantly looking for opportunities to improve the breadth and depth of the products we offer. From a strategic standpoint, our ambition is to unlock the power of Expedia for partners. Our mission is to fuel our partners\u2019 growth through our unparalleled technology, travel supply and support services. We are about travel, but also about state-of-the-art technology, AI and ML. Data is our underlying foundation, and the only way to operate at our immense scale is to develop our own technologies and use cloud capabilities on positive and exciting test and learn environment.\n\nAs today, the streaming platform ingests more than 3.6b domain event messages per day, 11.5 TB, that we enrich and transform, making it available for all internal functions, people and services.\n\nIf you are someone with a growth mindset and love solving complex and challenging problems, come join us on this exciting journey.\n\nOur Team Really Enjoy\n\nA friendly and collaborative team who are keen to share their knowledge and constantly learn new things\nA bias for action\nYou will get the opportunity to work on a cloud-based data lake backed by the full suite technologies and including Kafka, Spark, Scala, Airflow, Hive, Kinesis, EMR and Docker.\n\nResponsibilities\n\nDeveloping various facets of data capture, data processing, storage, distribution and visualization\nUnderstanding and applying AWS standard methodologies and products (compute, storage, databases)\nWriting clean, maintainable and well-tested code\nDevelop scalable and highly-performant distributed systems with everything this entails (availability, monitoring, resiliency)\nCommunicating and documenting solutions and design decisions\nProposing new ways of doing things and contribute to the system architecture\nBe part of an agile team that is continuously learning and improving.\n\nExperience And Qualification\n\nExperience with at least one modern programming language (e.g. Scala, Python or Java etc)\nExperience delivering and supporting highly available and scalable solutions with large transaction volume\nExperience in working with Relational Databases and proven knowledge of SQL\nExperience of using cloud services (e.g. AWS)\nA passion for learning, especially in the areas of data, technology, service design patterns and system architecture\nBe self-motivated with drive and good organizational skills to work in a multi-disciplinary team.\n\nAccommodation requests\n\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request.\n\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\n\nExpedia Group's family of brands includes: Brand Expedia\u00ae, Hotels.com\u00ae, Expedia\u00ae Partner Solutions, Vrbo\u00ae, trivago\u00ae, Orbitz\u00ae, Travelocity\u00ae, Hotwire\u00ae, Wotif\u00ae, ebookers\u00ae, CheapTickets\u00ae, Expedia Group\u2122 Media Solutions, Expedia Local Expert\u00ae, CarRentals.com\u2122, and Expedia Cruises\u2122. \u00a9 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. CST: 2029030-50\n\nEmployment opportunities and job offers at Expedia Group will always come from Expedia Group\u2019s Talent Acquisition and hiring teams. Never provide sensitive, personal information to someone unless you\u2019re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs.\n\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.\nWhy Join Us?\nAre you a Data engineer looking to join a leading technology company to build products and features? Are you passionate about data and everything it powers? Are you ready to help people travel, go from anywhere to somewhere, possible? If this sounds like you, keep reading!\nOur Team Really Enjoy\nA friendly and collaborative team who are keen to share their knowledge and constantly learn new things\nA bias for action\nYou will get the opportunity to work on a cloud-based data lake backed by the full suite technologies and including Kafka, Spark, Scala, Airflow, Hive, Kinesis, EMR and Docker.\nA friendly and collaborative team who are keen to share their knowledge and constantly learn new things\nA bias for action\nYou will get the opportunity to work on a cloud-based data lake backed by the full suite technologies and including Kafka, Spark, Scala, Airflow, Hive, Kinesis, EMR and Docker.\nResponsibilities\nDeveloping various facets of data capture, data processing, storage, distribution and visualization\nUnderstanding and applying AWS standard methodologies and products (compute, storage, databases)\nWriting clean, maintainable and well-tested code\nDevelop scalable and highly-performant distributed systems with everything this entails (availability, monitoring, resiliency)\nCommunicating and documenting solutions and design decisions\nProposing new ways of doing things and contribute to the system architecture\nBe part of an agile team that is continuously learning and improving.\nDeveloping various facets of data capture, data processing, storage, distribution and visualization\nUnderstanding and applying AWS standard methodologies and products (compute, storage, databases)\nWriting clean, maintainable and well-tested code\nDevelop scalable and highly-performant distributed systems with everything this entails (availability, monitoring, resiliency)\nCommunicating and documenting solutions and design decisions\nProposing new ways of doing things and contribute to the system architecture\nBe part of an agile team that is continuously learning and improving.\nExperience And Qualification\nExperience with at least one modern programming language (e.g. Scala, Python or Java etc)\nExperience delivering and supporting highly available and scalable solutions with large transaction volume\nExperience in working with Relational Databases and proven knowledge of SQL\nExperience of using cloud services (e.g. AWS)\nA passion for learning, especially in the areas of data, technology, service design patterns and system architecture\nBe self-motivated with drive and good organizational skills to work in a multi-disciplinary team.\nExperience with at least one modern programming language (e.g. Scala, Python or Java etc)\nExperience delivering and supporting highly available and scalable solutions with large transaction volume\nExperience in working with Relational Databases and proven knowledge of SQL\nExperience of using cloud services (e.g. AWS)\nA passion for learning, especially in the areas of data, technology, service design patterns and system architecture\nBe self-motivated with drive and good organizational skills to work in a multi-disciplinary team.\nAccommodation requests"
    },
    "4151469398": {
        "title": "Data Engineer ",
        "company": "Tata Consultancy Services",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nAre you a Data Engineer seeking a new interesting challenge? \ud83d\udd0d\nIf your answer is yes, it\u2019s your lucky day so keep reading, it can be just what you're looking for! \ud83d\udc40\n\n\u270d\ufe0fWHAT WILL YOU DO?\nWe are looking for a dynamic, proactive and talented person to join our team and perform the following tasks:\n\u2022 Design and build data pipelines using Spark-SQL and Pyspark in Azure Databricks. \n\u2022 Build and maintain Lakehouse architecture in ADLS / Databricks. \n\u2022 Perform data preparation tasks including data cleaning, normalization, deduplication, type conversion etc. \n\u2022 Work with DevOps team to deploy solutions in production environments. \n\u2022 Collaborate with Data Science and Business Intelligence teams to share key learnings, leverage ideas and solutions and to propagate best practices. \n\n\ud83e\uddd0 WHAT ARE WE LOOKING FOR?\n\u2022 Knowledge of distributed systems (e.g., Apache Spark or Databricks) \n\u2022 Knowledge of Delta Lake and Lakehouse architecture \n\u2022 Applying the spark optimization technique on high volume while data processing\n\u2022 Troubleshooting DAG failures\n\u2022 Should have hands-on experience in Databricks Unity Catalog latest features and new optimized features on Delta Tables.\n\u2022 Should have a good understanding of metadata driven data pipeline and leveraged notebook framework using pyspark programming\n\u2022 Should have understanding Data governance, Data Lineage, Data accessibility understanding using Databricks or other cloud tools.\n\u2022 Experience with multiple data programming languages (Spark-SQL, Pyspark, Pandas) \n\u2022 Experience with On-premises databases such as SQL server, Oracle etc. \n\u2022 Experience with version control (e.g., Git), DevOps, CI/CD\nAt least 3 years in a similar role \nFluent English\n\n\ud83d\udcc5 WHERE AND WHEN?\nWorkplace: Madrid In-Office\nWork Schedule: Business Hours\n\n\ud83e\udd1d WHAT CAN WE OFFER YOU?\nPermanent contract \ud83d\udccb - We offer indefinite contracts from the first day. Pay and benefits \ud83d\udcb8 - Competitive salary and a flexible compensation plan adapted to your needs (Ticket restaurant plan, Childcare Ticket, Transport Ticket and Health Insurance).Opportunity knocks \ud83d\udc4d\ud83c\udffb - Being a part of a growing company, we want to support your path with a career development plan and annual performance-based compensation reviews. Learn as you grow \ud83d\udcda - Starting with a fantastic onboarding program, TCS has robust learning platforms that will allow you to learn and grow personal as professionally. Bring your buddy \ud83d\udc6b - If you have referred a friend for an open position under the BYB Scheme and she/he is hired you\u2019ll receive a very attractive cash award. Connect globally \ud83c\udf0f - Work with people from all over the world. You can feel the multicultural workforce. Benefit from being a TCSer \ud83c\udf9f\ufe0f \u2013 By being part of the TCS Spain family you can enjoy benefits, offers and corporate discounts on the best brands. And so on \ud83c\udf89 - Appreciations, incentives, Team Building activities, diversity and inclusion programs, sustainability activities, corporative events... This has only just begun!\n\n\ud83d\udca1WHO ARE WE?\nTata Consultancy Services (TCS) is an Information Technology (IT) company founded in 1968 as part of the Indian Tata Group and is one of the top 3 technology companies globally \ud83d\ude80\n\nWith a presence in 55 countries and more than 600,000 employees, TCS is considered one of the 10 best companies to work for worldwide in 2024 according to the Top Employers Institute \ud83e\udd47\n\nTCS Spain started operations in 2001 and currently has a diverse workforce that collaborates with the main Spanish and multinational companies \ud83c\udf0f\n\nTCS Spain has been certified as a Top Employer 2024 and has also been chosen as one of the 100 Best Companies to Work for in Spain in 2024 according to Forbes\ud83c\udfc6\n\nAmong the portfolio of services, TCS has information technology services, asset-based solutions, global consulting, engineering and industrial services, digital solutions and services, application maintenance and development, quality assurance and testing services, IT infrastructure and BPS \ud83c\udfaf\n\nResponsible for development, TCS Spain is committed to inclusion, diversity and sustainability, and promotes flexibility policies that support wellbeing and work-life balance \ud83d\udcaf\n\nWELCOME, WE ARE WAITING FOR YOU! \ud83d\ude80\nAre you a Data Engineer seeking a new interesting challenge? \ud83d\udd0d\nData Engineer\nnew interesting challenge\nIf your answer is yes, it\u2019s your lucky day so keep reading, it can be just what you're looking for! \ud83d\udc40\nit can be just what you're looking for\n\u270d\ufe0fWHAT WILL YOU DO?\nWHAT WILL YOU DO?\nWe are looking for a dynamic, proactive and talented person to join our team and perform the following tasks:\ndynamic, proactive and talented person\ntasks\n\u2022 Design and build data pipelines using Spark-SQL and Pyspark in Azure Databricks.\n\u2022 Build and maintain Lakehouse architecture in ADLS / Databricks.\n\u2022 Perform data preparation tasks including data cleaning, normalization, deduplication, type conversion etc.\n\u2022 Work with DevOps team to deploy solutions in production environments.\n\u2022 Collaborate with Data Science and Business Intelligence teams to share key learnings, leverage ideas and solutions and to propagate best practices.\n\ud83e\uddd0 WHAT ARE WE LOOKING FOR?\nWHAT ARE WE LOOKING FOR?\n\u2022 Knowledge of distributed systems (e.g., Apache Spark or Databricks)\n\u2022 Knowledge of Delta Lake and Lakehouse architecture\n\u2022 Applying the spark optimization technique on high volume while data processing\n\u2022 Troubleshooting DAG failures\n\u2022 Should have hands-on experience in Databricks Unity Catalog latest features and new optimized features on Delta Tables.\n\u2022 Should have a good understanding of metadata driven data pipeline and leveraged notebook framework using pyspark programming\n\u2022 Should have understanding Data governance, Data Lineage, Data accessibility understanding using Databricks or other cloud tools.\n\u2022 Experience with multiple data programming languages (Spark-SQL, Pyspark, Pandas)\n\u2022 Experience with On-premises databases such as SQL server, Oracle etc.\n\u2022 Experience with version control (e.g., Git), DevOps, CI/CD\nAt least 3 years in a similar role \nFluent English\nAt least 3 years in a similar role\nFluent English\n\ud83d\udcc5 WHERE AND WHEN?\nWHERE AND WHEN?\nWorkplace: Madrid In-Office\nWork Schedule: Business Hours\nWorkplace: Madrid In-Office\nWork Schedule: Business Hours\n\ud83e\udd1d WHAT CAN WE OFFER YOU?\nWHAT CAN WE OFFER YOU?\nPermanent contract \ud83d\udccb - We offer indefinite contracts from the first day. Pay and benefits \ud83d\udcb8 - Competitive salary and a flexible compensation plan adapted to your needs (Ticket restaurant plan, Childcare Ticket, Transport Ticket and Health Insurance).Opportunity knocks \ud83d\udc4d\ud83c\udffb - Being a part of a growing company, we want to support your path with a career development plan and annual performance-based compensation reviews. Learn as you grow \ud83d\udcda - Starting with a fantastic onboarding program, TCS has robust learning platforms that will allow you to learn and grow personal as professionally. Bring your buddy \ud83d\udc6b - If you have referred a friend for an open position under the BYB Scheme and she/he is hired you\u2019ll receive a very attractive cash award. Connect globally \ud83c\udf0f - Work with people from all over the world. You can feel the multicultural workforce. Benefit from being a TCSer \ud83c\udf9f\ufe0f \u2013 By being part of the TCS Spain family you can enjoy benefits, offers and corporate discounts on the best brands. And so on \ud83c\udf89 - Appreciations, incentives, Team Building activities, diversity and inclusion programs, sustainability activities, corporative events... This has only just begun!\nPermanent contract\nindefinite contracts\nPay and benefits\nCompetitive salary\nflexible compensation plan\nOpportunity knocks\ncareer development plan\nannual performance-based compensation reviews\nLearn as you grow\nonboarding program\nrobust learning platforms\nBring your buddy\nyou have referred a friend for an open position\nshe/he\nis hired\nvery attractive\ncash award.\nConnect globally\npeople from all over the world\nmulticultural workforce\nBenefit from being a TCSer\nbenefits, offers and corporate discounts on the best brands\nAnd so on\nAppreciations, incentives, Team Building activities, diversity and inclusion programs, sustainability activities, corporative events...\n\ud83d\udca1WHO ARE WE?\nWHO ARE WE?\nTata Consultancy Services (TCS) is an Information Technology (IT) company founded in 1968 as part of the Indian Tata Group and is one of the top 3 technology companies globally \ud83d\ude80\nTata Consultancy Services (TCS)\nInformation Technology (IT) company\nIndian Tata Group\none of the top 3 technology companies globally\nWith a presence in 55 countries and more than 600,000 employees, TCS is considered one of the 10 best companies to work for worldwide in 2024 according to the Top Employers Institute \ud83e\udd47\n55 countries\nemployees\none of the 10 best companies to work for worldwide in 2024\nTop Employers Institute\nTCS Spain started operations in 2001 and currently has a diverse workforce that collaborates with the main Spanish and multinational companies \ud83c\udf0f\nTCS Spain\ndiverse workforce\nmain Spanish and multinational companies\n\ud83c\udf0f\nTCS Spain has been certified as a Top Employer 2024 and has also been chosen as one of the 100 Best Companies to Work for in Spain in 2024 according to Forbes\ud83c\udfc6\nTop Employer 2024\none of the 100 Best Companies to Work for in Spain in 2024\nForbes\nAmong the portfolio of services, TCS has information technology services, asset-based solutions, global consulting, engineering and industrial services, digital solutions and services, application maintenance and development, quality assurance and testing services, IT infrastructure and BPS \ud83c\udfaf\ninformation technology services\nasset-based solutions\nglobal consulting\nengineering and industrial services\ndigital solutions and services\napplication maintenance and development\nquality assurance and testing services\nIT infrastructure\nBPS\nResponsible for development, TCS Spain is committed to inclusion, diversity and sustainability, and promotes flexibility policies that support wellbeing and work-life balance \ud83d\udcaf\ndevelopment\ninclusion, diversity\nsustainability\nflexibility policies\nwellbeing and work-life balance\nWELCOME, WE ARE WAITING FOR YOU! \ud83d\ude80"
    },
    "4150461562": {
        "title": "Junior Data Engineer",
        "company": "Shalion",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nJoin Shalion as a Jr. Data Engineer and turn data into powerful insights for our clients! Work with Snowflake, dbt, and Airflow and be part of our modern Data Stack to deliver cutting-edge solutions.\n\nShalion is a data-driven company that works for world-class manufacturers and brands worldwide. With its Digital Shelf Optimization and Retail Media solutions, Shalion provides innovative tools for e-commerce monitoring.\n\nWe, at Shalion, enable brands to make smarter business decisions in eCommerce, based on actionable data and insights providing information on online product prices, availability, share of search, etc. in more than 800 e-retailers and 50 different countries. Our client portfolio includes industry giants such as Heineken, Diageo, and JDE.\n\nAt Shalion we\u2019ve built a modern data platform (with technologies like Airflow, dbt, Snowflake, Looker, Airbyte and Github actions) that is critical for the company revenue generation: we sell data to our customers who consume it either through our dashboards in Looker embedded or through an API.\n\nThe role\n\nAs a Junior Data Engineer at Shalion, you will play a key role in building and optimizing data solutions. You will work with modern data technologies, automate workflows, and ensure high-quality data delivery to empower business decisions.\n\nYour Responsibilities\n\nDevelop and maintain data models in Snowflake, Looker, and dbt.\nAutomate data pipelines using Apache Airflow.\nApply software engineering best practices such as version control and continuous integration.\nCollaborate with product teams to understand business needs and deliver analysis-ready data.\n\nYour profile should include the following:\n\nProficiency in SQL.\nAt least 1 year of experience with Python.\nFamiliarity with GitHub best practices and daily usage.\n1+ year of experience in data or analytics engineering roles.\nUnderstanding of Data Engineering fundamentals (ETL, Data Modeling, etc.).\nExperience working with cloud platforms like AWS or GCP.\nStrong communication skills and ability to collaborate with stakeholders.\nEnglish level: B1 or higher.\n\nNice-to-Have Skills\n\nExperience with data modeling in dbt or similar tools.\nFamiliarity with data modeling techniques like Kimball.\nKnowledge of modern data table formats (Delta, Iceberg, or Hudi).\nExperience with additional programming languages like Java or Scala.\n\nSoft Skills We Value\n\nAdaptability \u2013 Ability to find solutions in uncertain situations.\nCuriosity \u2013 Eagerness to learn and engage in constructive discussions.\nQuality-Oriented \u2013 Awareness of data quality and the importance of code testing.\nAnalytical Thinking \u2013 Strong problem-solving skills.\nCollaboration \u2013 Experience working in Agile environments.\n\nWhat We Offer\n\n\ud83c\udf0d An international team \u2013 Work with colleagues from 10+ nationalities.\n\ud83d\ude80 A dynamic and innovative environment \u2013 Be part of a high-performing team.\n\ud83c\udfe2 Hybrid work model \u2013 4 days in-office, 1 day remote.\n\ud83d\udca1 Impactful work \u2013 Contribute to a challenging and innovative project.\n\ud83c\udf9f Flexible benefits \u2013 Meal & transport allowance, healthcare at a preferred price.\n\ud83c\udf4f Office perks \u2013 Fresh fruit and coffee daily.\n\ud83d\udcb0 Salary: \u20ac30,000 - \u20ac35,000 gross per year, according to experience.\n\ud83d\udcc4 Contract: Permanent\n\ud83d\udccd Great location \u2013 Office in Barcelona\u2019s Sarri\u00e0-Sant Gervasi district.\n\nInterview process\n\nFirst interview: Team screening\nSecond interview: SQL Assignment and technical interview.\nThird interview: Interview with the hiring manager\n\nShalion Data Services Limited is an equal-opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\nJoin Shalion as a Jr. Data Engineer and turn data into powerful insights for our clients! Work with Snowflake, dbt, and Airflow and be part of our modern Data Stack to deliver cutting-edge solutions.\nShalion is a data-driven company that works for world-class manufacturers and brands worldwide. With its Digital Shelf Optimization and Retail Media solutions, Shalion provides innovative tools for e-commerce monitoring.\nWe, at Shalion, enable brands to make smarter business decisions in eCommerce, based on actionable data and insights providing information on online product prices, availability, share of search, etc. in more than 800 e-retailers and 50 different countries. Our client portfolio includes industry giants such as Heineken, Diageo, and JDE.\nAt Shalion we\u2019ve built a modern data platform (with technologies like Airflow, dbt, Snowflake, Looker, Airbyte and Github actions) that is critical for the company revenue generation: we sell data to our customers who consume it either through our dashboards in Looker embedded or through an API.\nThe role\nAs a Junior Data Engineer at Shalion, you will play a key role in building and optimizing data solutions. You will work with modern data technologies, automate workflows, and ensure high-quality data delivery to empower business decisions.\nJunior Data Engineer\nYour Responsibilities\nDevelop and maintain data models in Snowflake, Looker, and dbt.\nAutomate data pipelines using Apache Airflow.\nApply software engineering best practices such as version control and continuous integration.\nCollaborate with product teams to understand business needs and deliver analysis-ready data.\nDevelop and maintain data models in Snowflake, Looker, and dbt.\nSnowflake, Looker, and dbt\nAutomate data pipelines using Apache Airflow.\nApache Airflow\nApply software engineering best practices such as version control and continuous integration.\nversion control and continuous integration\nCollaborate with product teams to understand business needs and deliver analysis-ready data.\nanalysis-ready data\nYour profile should include the following:\nProficiency in SQL.\nAt least 1 year of experience with Python.\nFamiliarity with GitHub best practices and daily usage.\n1+ year of experience in data or analytics engineering roles.\nUnderstanding of Data Engineering fundamentals (ETL, Data Modeling, etc.).\nExperience working with cloud platforms like AWS or GCP.\nStrong communication skills and ability to collaborate with stakeholders.\nEnglish level: B1 or higher.\nProficiency in SQL.\nSQL\nAt least 1 year of experience with Python.\n1 year of experience with Python\nFamiliarity with GitHub best practices and daily usage.\nGitHub best practices\n1+ year of experience in data or analytics engineering roles.\n1+ year of experience\nUnderstanding of Data Engineering fundamentals (ETL, Data Modeling, etc.).\nData Engineering fundamentals\nExperience working with cloud platforms like AWS or GCP.\nAWS or GCP\nStrong communication skills and ability to collaborate with stakeholders.\nEnglish level: B1 or higher.\nEnglish level:\nNice-to-Have Skills\nExperience with data modeling in dbt or similar tools.\nFamiliarity with data modeling techniques like Kimball.\nKnowledge of modern data table formats (Delta, Iceberg, or Hudi).\nExperience with additional programming languages like Java or Scala.\nExperience with data modeling in dbt or similar tools.\ndata modeling in dbt\nFamiliarity with data modeling techniques like Kimball.\ndata modeling techniques\nKnowledge of modern data table formats (Delta, Iceberg, or Hudi).\nDelta, Iceberg, or Hudi\nExperience with additional programming languages like Java or Scala.\nJava or Scala\nSoft Skills We Value\nAdaptability \u2013 Ability to find solutions in uncertain situations.\nCuriosity \u2013 Eagerness to learn and engage in constructive discussions.\nQuality-Oriented \u2013 Awareness of data quality and the importance of code testing.\nAnalytical Thinking \u2013 Strong problem-solving skills.\nCollaboration \u2013 Experience working in Agile environments.\nAdaptability \u2013 Ability to find solutions in uncertain situations.\nAdaptability\nCuriosity \u2013 Eagerness to learn and engage in constructive discussions.\nCuriosity\nQuality-Oriented \u2013 Awareness of data quality and the importance of code testing.\nQuality-Oriented\nAnalytical Thinking \u2013 Strong problem-solving skills.\nAnalytical Thinking\nCollaboration \u2013 Experience working in Agile environments.\nCollaboration\nWhat We Offer\n\ud83c\udf0d An international team \u2013 Work with colleagues from 10+ nationalities.\nAn international team\n\ud83d\ude80 A dynamic and innovative environment \u2013 Be part of a high-performing team.\nA dynamic and innovative environment\n\ud83c\udfe2 Hybrid work model \u2013 4 days in-office, 1 day remote.\nHybrid work model\n\ud83d\udca1 Impactful work \u2013 Contribute to a challenging and innovative project.\nImpactful work\n\ud83c\udf9f Flexible benefits \u2013 Meal & transport allowance, healthcare at a preferred price.\nFlexible benefits\n\ud83c\udf4f Office perks \u2013 Fresh fruit and coffee daily.\nOffice perks\n\ud83d\udcb0 Salary: \u20ac30,000 - \u20ac35,000 gross per year, according to experience.\nSalary:\n\ud83d\udcc4 Contract: Permanent\nContract:\n\ud83d\udccd Great location \u2013 Office in Barcelona\u2019s Sarri\u00e0-Sant Gervasi district.\nGreat location\nSarri\u00e0-Sant Gervasi\nInterview process\nFirst interview: Team screening\nSecond interview: SQL Assignment and technical interview.\nThird interview: Interview with the hiring manager\nFirst interview: Team screening\nSecond interview: SQL Assignment and technical interview.\nThird interview: Interview with the hiring manager\nShalion Data Services Limited is an equal-opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees."
    },
    "4131109265": {
        "title": "Data Engineer",
        "company": "Intelequia",
        "location": "Canary Islands, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfQuieres un desaf\u00edo profesional en una consultora IT de primer nivel? \u00a1Esta es tu oportunidad! En Intelequia, nos encontramos en b\u00fasqueda de talento innovador y apasionado por las tecnolog\u00edas de vanguardia para unirse a nuestro equipo.\n\nCon m\u00e1s de 15 a\u00f1os de experiencia en el sector y reconocidos como una de las mejores empresas de tecnolog\u00eda para trabajar en Espa\u00f1a, seg\u00fan Great Place To Work, estamos especializados en realizar servicios TIC en la nube operando a nivel nacional e internacional, abarcando \u00e1reas como Infraestructura, Ciberseguridad, Inteligencia Artificial y Desarrollos en .NET y Low Code.\n\nNuestra misi\u00f3n es acompa\u00f1ar a nuestros clientes en cada etapa de sus proyectos de IT, liderando sus iniciativas y ofreciendo soluciones punteras que impulsen su crecimiento y eficiencia en un entorno tecnol\u00f3gico cada vez m\u00e1s competitivo.\n\nSi est\u00e1s dispuesto a asumir nuevos retos y formar parte de un equipo comprometido con la excelencia, \u00a1queremos conocerte!\n\n\u00bfQU\u00c9 BUSCAMOS?\nUn/a Data Engineer, con al menos 5 a\u00f1os de experiencia en el sector.\n\nFUNCIONES\nDise\u00f1ar, construir y mantener la infraestructura de datos de la empresa.\nGestionar pipelines de datos y asegurarse de que los datos est\u00e9n disponibles y sean de alta calidad.\nTrabajar con grandes vol\u00famenes de datos y optimizar el rendimiento de las bases de datos.\nImplementar y gestionar soluciones de datos utilizando Microsoft Fabric y sus artefactos.\nColaborar con cient\u00edficos/as de datos y analistas para comprender sus necesidades de datos.\nDocumentar y mantener la documentaci\u00f3n de la infraestructura de datos.\nSeguir las mejores pr\u00e1cticas en la ingenier\u00eda de datos.\n\nREQUISITOS M\u00cdNIMOS\nFormaci\u00f3n en Ingenier\u00eda T\u00e9cnica/Inform\u00e1tica o similares.\nValorable estar en posici\u00f3n de master\nConocimientos en SQL y bases de datos (SQL, NoSQL).\nConocimientos en API Rest\nConocimientos en modelado de datos\nExperiencia con herramientas de ETL (Extract, Transform, Load) y ELT.\nFamiliaridad con tecnolog\u00edas en la nube (Azure).\nFamiliaridad con Microsoft Fabric, Azure Synapse, Azure Data Lake, etc.\nConocimientos en Python o R.\nHabilidades de comunicaci\u00f3n y colaboraci\u00f3n.\nCapacidad para trabajar de forma independiente y como parte de un equipo.\nOrientaci\u00f3n a los detalles y capacidad para resolver problemas.\n\nPorque en Intelequia creemos firmemente que el bienestar de nuestro equipo influye directamente en el trabajo realizado, para ello TE OFRECEMOS:\n\nIncorporarte a un proyecto ilusionante y motivador, que despertar\u00e1 la pasi\u00f3n que hay en ti.\nPlan de desarrollo profesional para potenciar tu perfil (certificaciones, formaciones en idiomas y competencias profesionales, etc.)\nModelo de trabajo h\u00edbrido (s\u00f3lo 2 d\u00edas de trabajo al mes en nuestra sede de Tenerife).\nJornada laboral de 38 horas/semana.\n25 d\u00edas h\u00e1biles de vacaciones al a\u00f1o.\nTarde de cumplea\u00f1os libre.\nPlan de retribuci\u00f3n flexible para tu beneficio.\nFormar parte de una empresa Great Place to Work.\nCrecer y aportar junto a grandes profesionales, dentro de un buen clima laboral\n\u00bfQuieres un desaf\u00edo profesional en una consultora IT de primer nivel? \u00a1Esta es tu oportunidad! En Intelequia, nos encontramos en b\u00fasqueda de talento innovador y apasionado por las tecnolog\u00edas de vanguardia para unirse a nuestro equipo.\nCon m\u00e1s de 15 a\u00f1os de experiencia en el sector y reconocidos como una de las mejores empresas de tecnolog\u00eda para trabajar en Espa\u00f1a, seg\u00fan Great Place To Work, estamos especializados en realizar servicios TIC en la nube operando a nivel nacional e internacional, abarcando \u00e1reas como Infraestructura, Ciberseguridad, Inteligencia Artificial y Desarrollos en .NET y Low Code.\nNuestra misi\u00f3n es acompa\u00f1ar a nuestros clientes en cada etapa de sus proyectos de IT, liderando sus iniciativas y ofreciendo soluciones punteras que impulsen su crecimiento y eficiencia en un entorno tecnol\u00f3gico cada vez m\u00e1s competitivo.\nSi est\u00e1s dispuesto a asumir nuevos retos y formar parte de un equipo comprometido con la excelencia, \u00a1queremos conocerte!\n\u00bfQU\u00c9 BUSCAMOS?\nUn/a Data Engineer, con al menos 5 a\u00f1os de experiencia en el sector.\nData Engineer\nFUNCIONES\nDise\u00f1ar, construir y mantener la infraestructura de datos de la empresa.\nGestionar pipelines de datos y asegurarse de que los datos est\u00e9n disponibles y sean de alta calidad.\nTrabajar con grandes vol\u00famenes de datos y optimizar el rendimiento de las bases de datos.\nImplementar y gestionar soluciones de datos utilizando Microsoft Fabric y sus artefactos.\nColaborar con cient\u00edficos/as de datos y analistas para comprender sus necesidades de datos.\nDocumentar y mantener la documentaci\u00f3n de la infraestructura de datos.\nSeguir las mejores pr\u00e1cticas en la ingenier\u00eda de datos.\nDise\u00f1ar, construir y mantener la infraestructura de datos de la empresa.\nGestionar pipelines de datos y asegurarse de que los datos est\u00e9n disponibles y sean de alta calidad.\nTrabajar con grandes vol\u00famenes de datos y optimizar el rendimiento de las bases de datos.\nImplementar y gestionar soluciones de datos utilizando Microsoft Fabric y sus artefactos.\nColaborar con cient\u00edficos/as de datos y analistas para comprender sus necesidades de datos.\nDocumentar y mantener la documentaci\u00f3n de la infraestructura de datos.\nSeguir las mejores pr\u00e1cticas en la ingenier\u00eda de datos.\nREQUISITOS M\u00cdNIMOS\nFormaci\u00f3n en Ingenier\u00eda T\u00e9cnica/Inform\u00e1tica o similares.\nValorable estar en posici\u00f3n de master\nConocimientos en SQL y bases de datos (SQL, NoSQL).\nConocimientos en API Rest\nConocimientos en modelado de datos\nExperiencia con herramientas de ETL (Extract, Transform, Load) y ELT.\nFamiliaridad con tecnolog\u00edas en la nube (Azure).\nFamiliaridad con Microsoft Fabric, Azure Synapse, Azure Data Lake, etc.\nConocimientos en Python o R.\nHabilidades de comunicaci\u00f3n y colaboraci\u00f3n.\nCapacidad para trabajar de forma independiente y como parte de un equipo.\nOrientaci\u00f3n a los detalles y capacidad para resolver problemas.\nFormaci\u00f3n en Ingenier\u00eda T\u00e9cnica/Inform\u00e1tica o similares.\nValorable estar en posici\u00f3n de master\nConocimientos en SQL y bases de datos (SQL, NoSQL).\nConocimientos en API Rest\nConocimientos en modelado de datos\nExperiencia con herramientas de ETL (Extract, Transform, Load) y ELT.\nFamiliaridad con tecnolog\u00edas en la nube (Azure).\nFamiliaridad con Microsoft Fabric, Azure Synapse, Azure Data Lake, etc.\nConocimientos en Python o R.\nHabilidades de comunicaci\u00f3n y colaboraci\u00f3n.\nCapacidad para trabajar de forma independiente y como parte de un equipo.\nOrientaci\u00f3n a los detalles y capacidad para resolver problemas.\nPorque en Intelequia creemos firmemente que el bienestar de nuestro equipo influye directamente en el trabajo realizado, para ello TE OFRECEMOS:\nTE OFRECEMOS\nIncorporarte a un proyecto ilusionante y motivador, que despertar\u00e1 la pasi\u00f3n que hay en ti.\nPlan de desarrollo profesional para potenciar tu perfil (certificaciones, formaciones en idiomas y competencias profesionales, etc.)\nModelo de trabajo h\u00edbrido (s\u00f3lo 2 d\u00edas de trabajo al mes en nuestra sede de Tenerife).\nJornada laboral de 38 horas/semana.\n25 d\u00edas h\u00e1biles de vacaciones al a\u00f1o.\nTarde de cumplea\u00f1os libre.\nPlan de retribuci\u00f3n flexible para tu beneficio.\nFormar parte de una empresa Great Place to Work.\nCrecer y aportar junto a grandes profesionales, dentro de un buen clima laboral\nIncorporarte a un proyecto ilusionante y motivador, que despertar\u00e1 la pasi\u00f3n que hay en ti.\nproyecto ilusionante y motivador,\ndespertar\u00e1 la pasi\u00f3n\nPlan de desarrollo profesional para potenciar tu perfil (certificaciones, formaciones en idiomas y competencias profesionales, etc.)\nPlan de desarrollo profesional para potenciar tu perfil\nModelo de trabajo h\u00edbrido (s\u00f3lo 2 d\u00edas de trabajo al mes en nuestra sede de Tenerife).\nModelo de trabajo h\u00edbrido\nJornada laboral de 38 horas/semana.\nlaboral de 38 horas/semana.\n25 d\u00edas h\u00e1biles de vacaciones al a\u00f1o.\n25 d\u00edas h\u00e1biles de vacaciones\nTarde de cumplea\u00f1os libre.\ncumplea\u00f1os libre.\nPlan de retribuci\u00f3n flexible para tu beneficio.\nPlan de retribuci\u00f3n flexible\nFormar parte de una empresa Great Place to Work.\nempresa Great Place to Work.\nCrecer y aportar junto a grandes profesionales, dentro de un buen clima laboral\nCrecer y aportar"
    },
    "4081153982": {
        "title": "Data Engineer ",
        "company": "ClearPeaks",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nClearPeaks is a specialist consulting firm delivering services and solutions in \"Everything Data\" - Business Intelligence, Advanced Analytics, Big Data & Cloud, and Web & Mobile Applications. Founded in 2000, we have been a trusted partner to our customers in over 15 industry verticals and functional areas, with operations spanning Europe, Middle East, the United States, and Africa.\n\nClearPeaks is part of a strategic alliance with synvert, a group of six successful full-service Data & Analytics (D&A) consulting firms, with a clear goal to become one of EMEA's largest D&A consulting companies.\n\nOur services are based on the latest market-leading enterprise technology platforms, and delivered by a dynamic team of expert consultants. Our strength lies in our ability to efficiently deliver customer insight and value, gained through our decades of experience with real-world challenges.\n\nAs a Data Engineer, you will be responsible securely transfer the data between different network domains and environments. Besides that, you will be improving the performance and enhancing the structure of the data pipelines and data models.\n\nAt ClearPeaks, there are endless opportunities to get involved in different projects bringing innovative ideas while being part of leading-edge teams who are always innovating and evolving in the Data Management field.\n\nResponsibilities\n\nDevelop, deploy and manage data pipelines following CI/CD practices\nOrchestrate data pipelines with tools such as Airflow, Oozie, Azure Data Factory, etc.\nDevelop Python scripts or Scala code for data processing\nCollect and analyse pipeline requirements\nConsult about best architectural and design principles in developing pipelines\n\n\nRequirements\n\nA degree in Statistics, Computer Science or Telecommunication Engineering, Informatics or related degree.\nProven experience with Python or Scala\nExperience with orchestration tools like Airflow or Oozie\nAbility to use CI/CD & DevOps tools such as Git, AWS CI/CD Pipelines, Azure DevOps, etc.\nKnowledge on SQL\nKNIME knowledge is nice to have\nAbility in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers.\nTechnology expertise of solutioning in cloud data platforms in AWS, Azure or GCP\n\nOur offer to you!\n\nWork with leading edge technologies that will enable you to accelerate your career development.\nEnjoy an excellent work environment where people love what they do.\nBe part of an international and ambitious team whilst having fun.\nAttractive salary and flexible compensation\nClearPeaks is a specialist consulting firm delivering services and solutions in \"Everything Data\" - Business Intelligence, Advanced Analytics, Big Data & Cloud, and Web & Mobile Applications. Founded in 2000, we have been a trusted partner to our customers in over 15 industry verticals and functional areas, with operations spanning Europe, Middle East, the United States, and Africa.\nClearPeaks is part of a strategic alliance with synvert, a group of six successful full-service Data & Analytics (D&A) consulting firms, with a clear goal to become one of EMEA's largest D&A consulting companies.\nOur services are based on the latest market-leading enterprise technology platforms, and delivered by a dynamic team of expert consultants. Our strength lies in our ability to efficiently deliver customer insight and value, gained through our decades of experience with real-world challenges.\nAs a Data Engineer, you will be responsible securely transfer the data between different network domains and environments. Besides that, you will be improving the performance and enhancing the structure of the data pipelines and data models.\nData Engineer\nAt ClearPeaks, there are endless opportunities to get involved in different projects bringing innovative ideas while being part of leading-edge teams who are always innovating and evolving in the Data Management field.\nResponsibilities\nDevelop, deploy and manage data pipelines following CI/CD practices\nOrchestrate data pipelines with tools such as Airflow, Oozie, Azure Data Factory, etc.\nDevelop Python scripts or Scala code for data processing\nCollect and analyse pipeline requirements\nConsult about best architectural and design principles in developing pipelines\nDevelop, deploy and manage data pipelines following CI/CD practices\nOrchestrate data pipelines with tools such as Airflow, Oozie, Azure Data Factory, etc.\nDevelop Python scripts or Scala code for data processing\nCollect and analyse pipeline requirements\nConsult about best architectural and design principles in developing pipelines\nRequirements\nA degree in Statistics, Computer Science or Telecommunication Engineering, Informatics or related degree.\nProven experience with Python or Scala\nExperience with orchestration tools like Airflow or Oozie\nAbility to use CI/CD & DevOps tools such as Git, AWS CI/CD Pipelines, Azure DevOps, etc.\nKnowledge on SQL\nKNIME knowledge is nice to have\nAbility in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers.\nTechnology expertise of solutioning in cloud data platforms in AWS, Azure or GCP\nA degree in Statistics, Computer Science or Telecommunication Engineering, Informatics or related degree.\nProven experience with Python or Scala\nExperience with orchestration tools like Airflow or Oozie\nAbility to use CI/CD & DevOps tools such as Git, AWS CI/CD Pipelines, Azure DevOps, etc.\nKnowledge on SQL\nKNIME knowledge is nice to have\nAbility in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers.\nTechnology expertise of solutioning in cloud data platforms in AWS, Azure or GCP\nOur offer to you!\nWork with leading edge technologies that will enable you to accelerate your career development.\nEnjoy an excellent work environment where people love what they do.\nBe part of an international and ambitious team whilst having fun.\nAttractive salary and flexible compensation\nWork with leading edge technologies that will enable you to accelerate your career development.\nEnjoy an excellent work environment where people love what they do.\nBe part of an international and ambitious team whilst having fun.\nAttractive salary and flexible compensation"
    },
    "4168388682": {
        "title": "Data Engineer",
        "company": "Restb.ai",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nWho we are \ud83d\ude80\n\nAre you a Data Engineer looking for an opportunity to work on cutting-edge AI solutions that push the boundaries of computer vision? \n\nAt Restb.ai, we specialize in industry-specific visual recognition, helping businesses unlock powerful insights through AI-driven automation. We don\u2019t just recognize objects (that\u2019s so 2016), we teach computers to understand intangible visual concepts like \u201cthis room has natural light.\u201d\n\nBy joining our team, you\u2019ll play a critical role in shaping our data infrastructure, ensuring that our AI models are fueled by high-quality, scalable, and efficient data pipelines. If you thrive on solving complex challenges and want to be part of a fast-growing AI company, we\u2019d love to meet you!\n\nYour role \ud83d\udc64\n\nWe are looking for a Data Engineer who is passionate about technology, and always looking for new ways to tackle complex issues. As our Data Engineer, you would be in charge of our backend, data pipelines, and automation duties. You\u2019ll be a key player managing our ETL operations to help us bring our state-of-the-art AI solutions to market.\n\nThe usual day-to-day tasks include managing ETL implementations, and data pipelines, implementing automation workflows, and designing API endpoints for extracting Intelligence from the Data Warehouse meanwhile understanding all needed Real Estate industry concepts.\n\nThe role will be completely in English, and CVs/resumes in other languages will not be considered.\n\nWhat you\u2019ll do \u2b50\ufe0f\n\nWork closely with the Product team for all the related aspects regarding the Data Intelligence platform.\nWork closely with the CTO for all the backend/ETL-related developments and improvements.\nTake charge of the design and development of data pipeline architectures based on the Cloud.\nMonitor our data pipelines, bringing automation and alert management into the picture.\nSupport design and delivery of CI/CD pipelines in cooperation with Software and QA teams.\nDeploy robust pipelines to enable developers to build and test releases before they are promoted to production.\nProvide your technical vision in discussions around software architecture and product definition.\n\nWhat we\u2019re looking for \ud83d\udca1\n\nA strong candidate will ideally possess at least one strong expertise in the following areas and at least a familiarity in others.\n\nExperience building and maintaining data pipelines.\nFamiliarity with ETL workflows and data transformation processes.\nHands-on experience with workflow orchestration tools (e.g., Apache Airflow).\nKnowledge of distributed data storage and processing systems.\nExposure to Elasticsearch/OpenSearch or NoSQL databases.\nExperience working with cloud-based data warehouses such as Snowflake.\nProficiency in Python (especially pandas) and SQL for data processing.\nUnderstanding of APIs and experience building or consuming Restful APIs.\nFamiliarity with containerization tools like Docker and Linux-based systems\nInterest in learning and applying cloud architecture best practices.\nPassion for staying up-to-date with industry best practices, tools, and frameworks.\nAn active GitHub portfolio or contributions to open-source projects is a plus.\n\nWhat makes working at Restb.ai great \ud83d\udd25\n\nMake an Impact: Your work will directly shape AI-driven real estate technology. \nInnovation-Driven Culture: Work with cutting-edge AI and data engineering technologies.\nBe part of an international company that works with some of the largest companies in the world.\nThrive in an open environment with young, driven, and dynamic team members.\nFree in-office snacks, beverages, hot drinks, and salads.\nFree team lunch every Wednesday.\nFree in-office physical therapy sessions.\nQuarterly team-building event.\nCareer development training.\nHybrid Office/remote working.\nHealth insurance.\nBarcelona-based, working worldwide!! Do we need to say more?\n\nEmployment type\n\nPermanent position\nReady to start ASAP\nWho we are \ud83d\ude80\nAre you a Data Engineer looking for an opportunity to work on cutting-edge AI solutions that push the boundaries of computer vision?\nAt Restb.ai, we specialize in industry-specific visual recognition, helping businesses unlock powerful insights through AI-driven automation. We don\u2019t just recognize objects (that\u2019s so 2016), we teach computers to understand intangible visual concepts like \u201cthis room has natural light.\u201d\nBy joining our team, you\u2019ll play a critical role in shaping our data infrastructure, ensuring that our AI models are fueled by high-quality, scalable, and efficient data pipelines. If you thrive on solving complex challenges and want to be part of a fast-growing AI company, we\u2019d love to meet you!\nYour role \ud83d\udc64\nWe are looking for a Data Engineer who is passionate about technology, and always looking for new ways to tackle complex issues. As our Data Engineer, you would be in charge of our backend, data pipelines, and automation duties. You\u2019ll be a key player managing our ETL operations to help us bring our state-of-the-art AI solutions to market.\nThe usual day-to-day tasks include managing ETL implementations, and data pipelines, implementing automation workflows, and designing API endpoints for extracting Intelligence from the Data Warehouse meanwhile understanding all needed Real Estate industry concepts.\nThe role will be completely in English, and CVs/resumes in other languages will not be considered.\nWhat you\u2019ll do \u2b50\ufe0f\nWhat you\u2019ll do\nWork closely with the Product team for all the related aspects regarding the Data Intelligence platform.\nWork closely with the CTO for all the backend/ETL-related developments and improvements.\nTake charge of the design and development of data pipeline architectures based on the Cloud.\nMonitor our data pipelines, bringing automation and alert management into the picture.\nSupport design and delivery of CI/CD pipelines in cooperation with Software and QA teams.\nDeploy robust pipelines to enable developers to build and test releases before they are promoted to production.\nProvide your technical vision in discussions around software architecture and product definition.\nWork closely with the Product team for all the related aspects regarding the Data Intelligence platform.\nWork closely with the CTO for all the backend/ETL-related developments and improvements.\nTake charge of the design and development of data pipeline architectures based on the Cloud.\nMonitor our data pipelines, bringing automation and alert management into the picture.\nSupport design and delivery of CI/CD pipelines in cooperation with Software and QA teams.\nDeploy robust pipelines to enable developers to build and test releases before they are promoted to production.\nProvide your technical vision in discussions around software architecture and product definition.\nWhat we\u2019re looking for \ud83d\udca1\nA strong candidate will ideally possess at least one strong expertise in the following areas and at least a familiarity in others.\nExperience building and maintaining data pipelines.\nFamiliarity with ETL workflows and data transformation processes.\nHands-on experience with workflow orchestration tools (e.g., Apache Airflow).\nKnowledge of distributed data storage and processing systems.\nExposure to Elasticsearch/OpenSearch or NoSQL databases.\nExperience working with cloud-based data warehouses such as Snowflake.\nProficiency in Python (especially pandas) and SQL for data processing.\nUnderstanding of APIs and experience building or consuming Restful APIs.\nFamiliarity with containerization tools like Docker and Linux-based systems\nInterest in learning and applying cloud architecture best practices.\nPassion for staying up-to-date with industry best practices, tools, and frameworks.\nAn active GitHub portfolio or contributions to open-source projects is a plus.\nExperience building and maintaining data pipelines.\nFamiliarity with ETL workflows and data transformation processes.\nHands-on experience with workflow orchestration tools (e.g., Apache Airflow).\nKnowledge of distributed data storage and processing systems.\nExposure to Elasticsearch/OpenSearch or NoSQL databases.\nExperience working with cloud-based data warehouses such as Snowflake.\nProficiency in Python (especially pandas) and SQL for data processing.\nUnderstanding of APIs and experience building or consuming Restful APIs.\nFamiliarity with containerization tools like Docker and Linux-based systems\nInterest in learning and applying cloud architecture best practices.\nPassion for staying up-to-date with industry best practices, tools, and frameworks.\nAn active GitHub portfolio or contributions to open-source projects is a plus.\nWhat makes working at Restb.ai great \ud83d\udd25\nMake an Impact: Your work will directly shape AI-driven real estate technology. \nInnovation-Driven Culture: Work with cutting-edge AI and data engineering technologies.\nBe part of an international company that works with some of the largest companies in the world.\nThrive in an open environment with young, driven, and dynamic team members.\nFree in-office snacks, beverages, hot drinks, and salads.\nFree team lunch every Wednesday.\nFree in-office physical therapy sessions.\nQuarterly team-building event.\nCareer development training.\nHybrid Office/remote working.\nHealth insurance.\nBarcelona-based, working worldwide!! Do we need to say more?\nMake an Impact: Your work will directly shape AI-driven real estate technology.\nInnovation-Driven Culture: Work with cutting-edge AI and data engineering technologies.\nBe part of an international company that works with some of the largest companies in the world.\nThrive in an open environment with young, driven, and dynamic team members.\nFree in-office snacks, beverages, hot drinks, and salads.\nFree team lunch every Wednesday.\nFree in-office physical therapy sessions.\nQuarterly team-building event.\nCareer development training.\nHybrid Office/remote working.\nHealth insurance.\nBarcelona-based, working worldwide!! Do we need to say more?\nEmployment type\nPermanent position\nReady to start ASAP\nPermanent position\nReady to start ASAP"
    },
    "4161463588": {
        "title": "Data Engineer ",
        "company": "Clarivate",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nWe are looking for a Business Intelligence - Data Engineer to join our Life Sciences and Healthcare Consulting Operations Team in Barcelona (Spain ) \u2013 Hybrid .\n\nThis is an amazing opportunity to work on building and optimizing data solutions across multiple domains, including business intelligence, data integration, and governance. We have a strong focus on innovation and efficiency, and we would love to speak with you if you have expertise in data engineering, ETL processes, and business intelligence tools.\n\nAbout You \u2013 Experience, Education, Skills, And Accomplishments\n\nBachelor's Degree or equivalent in Engineering, IT, or a related field with a strong quantitative and analytical focus.\nAt least 3 years of relevant work experience in data engineering or business intelligence.\nProficiency in SQL and experience with relational databases like MySQL, PostgreSQL.\nProficient in Git for version control.\n2+ years of experience with Python.\n2+ years of experience with BI and analytics tools (e.g., Power BI, Tableau, Looker).\nExperience with ETL technologies (e.g., dbt, Apache NiFi, Talend) and data pipeline management tools (e.g., Airflow, Dagster).\nExperience with Data Quality frameworks: Great Expectations (gx), Soda, etc.\nCorporate Data modelling: conceptual, logical, physical.\nFluency in English, both written and spoken (C1).\n\nIt Would Be Great If You Also Had\n\nExperience with cloud platforms (e.g., AWS, Azure, GCP).\nFamiliarity with big data technologies (Hadoop, Spark).\nUnderstanding of data governance and data security best practices.\nExperience working with global teams.\nExperience in reporting and financial and operations analysis\n\nWhat Will You Be Doing In This Role\n\nDesign, develop, and maintain ETL processes to integrate data from various sources.\nBuild and optimize data pipelines and architectures to support business intelligence and analytics.\nDevelop and maintain data warehouse solutions.\nImplement and manage data governance and data quality processes.\nCreate and maintain documentation for data processes, architecture, and workflows.\nVisualize data insights (dashboards) and communicate findings to teams across the organization.\nTroubleshoot and resolve data-related issues to ensure data integrity and availability.\nCollaborate with stakeholders including Executive, Product, Data, and Design teams to resolve data-related technical issues and support data infrastructure needs.\nManage and expand the current Business Intelligence platform on the Consulting Operations team, primarily based on Power BI.\n\nProjects you will be working on\n\nYou will be part of various data-driven projects such as data integration, business intelligence dashboards, process optimization, and data governance. The team works on transforming data into valuable insights for internal and external stakeholders, ensuring efficiency, compliance, and data integrity.\n\nAbout The Team\n\nYou will be joining a talented and diverse global team within the Consulting Operations group. Our team is dedicated to enhancing data management and optimizing data processes to ensure high-quality business intelligence solutions. You will collaborate with professionals across multiple regions to deliver impactful data-driven solutions.\n\nHours of Work\n\nThis is a permanent full-time position at 40 hours per week. The working schedule is flexible, primarily centered on CET time zone with occasional need to accommodate meetings with US or East Asia time zones. The position is hybrid with office visits required 2-3 times per week.\n\nWhat We Can Offer You\n\nA modern culture environment combined with all the financial and stability advantages of working for a large business.\nActive volunteering community, with 40 annual paid hours of volunteering time.\nPrivate Health and Life & Disability insurances.\nTax-free benefits (Ticket Restaurant scheme, kindergarten and transport cards).\nFree Language classes (Catalan, Spanish and English).\nGlobal and dynamic employee base (more than 20 nationalities).\n30 working days of vacation. \n\n,\n\nAt Clarivate, we are committed to providing equal employment opportunities for all persons with respect to hiring, compensation, promotion, training, and other terms, conditions, and privileges of employment. We comply with applicable laws and regulations governing non-discrimination in all locations.\nWe are looking for a Business Intelligence - Data Engineer\nBarcelona (Spain ) \u2013 Hybrid\nAbout You \u2013 Experience, Education, Skills, And Accomplishments\nBachelor's Degree or equivalent in Engineering, IT, or a related field with a strong quantitative and analytical focus.\nAt least 3 years of relevant work experience in data engineering or business intelligence.\nProficiency in SQL and experience with relational databases like MySQL, PostgreSQL.\nProficient in Git for version control.\n2+ years of experience with Python.\n2+ years of experience with BI and analytics tools (e.g., Power BI, Tableau, Looker).\nExperience with ETL technologies (e.g., dbt, Apache NiFi, Talend) and data pipeline management tools (e.g., Airflow, Dagster).\nExperience with Data Quality frameworks: Great Expectations (gx), Soda, etc.\nCorporate Data modelling: conceptual, logical, physical.\nFluency in English, both written and spoken (C1).\nBachelor's Degree or equivalent in Engineering, IT, or a related field with a strong quantitative and analytical focus.\nAt least 3 years of relevant work experience in data engineering or business intelligence.\nProficiency in SQL and experience with relational databases like MySQL, PostgreSQL.\nProficient in Git for version control.\n2+ years of experience with Python.\n2+ years of experience with BI and analytics tools (e.g., Power BI, Tableau, Looker).\nExperience with ETL technologies (e.g., dbt, Apache NiFi, Talend) and data pipeline management tools (e.g., Airflow, Dagster).\nExperience with Data Quality frameworks: Great Expectations (gx), Soda, etc.\nCorporate Data modelling: conceptual, logical, physical.\nFluency in English, both written and spoken (C1).\nIt Would Be Great If You Also Had\nExperience with cloud platforms (e.g., AWS, Azure, GCP).\nFamiliarity with big data technologies (Hadoop, Spark).\nUnderstanding of data governance and data security best practices.\nExperience working with global teams.\nExperience in reporting and financial and operations analysis\nExperience with cloud platforms (e.g., AWS, Azure, GCP).\nFamiliarity with big data technologies (Hadoop, Spark).\nUnderstanding of data governance and data security best practices.\nExperience working with global teams.\nExperience in reporting and financial and operations analysis\nWhat Will You Be Doing In This Role\nDesign, develop, and maintain ETL processes to integrate data from various sources.\nBuild and optimize data pipelines and architectures to support business intelligence and analytics.\nDevelop and maintain data warehouse solutions.\nImplement and manage data governance and data quality processes.\nCreate and maintain documentation for data processes, architecture, and workflows.\nVisualize data insights (dashboards) and communicate findings to teams across the organization.\nTroubleshoot and resolve data-related issues to ensure data integrity and availability.\nCollaborate with stakeholders including Executive, Product, Data, and Design teams to resolve data-related technical issues and support data infrastructure needs.\nManage and expand the current Business Intelligence platform on the Consulting Operations team, primarily based on Power BI.\nDesign, develop, and maintain ETL processes to integrate data from various sources.\nBuild and optimize data pipelines and architectures to support business intelligence and analytics.\nDevelop and maintain data warehouse solutions.\nImplement and manage data governance and data quality processes.\nCreate and maintain documentation for data processes, architecture, and workflows.\nVisualize data insights (dashboards) and communicate findings to teams across the organization.\nTroubleshoot and resolve data-related issues to ensure data integrity and availability.\nCollaborate with stakeholders including Executive, Product, Data, and Design teams to resolve data-related technical issues and support data infrastructure needs.\nManage and expand the current Business Intelligence platform on the Consulting Operations team, primarily based on Power BI.\nProjects you will be working on\nAbout The Team\nHours of Work\nWhat We Can Offer You\nA modern culture environment combined with all the financial and stability advantages of working for a large business.\nActive volunteering community, with 40 annual paid hours of volunteering time.\nPrivate Health and Life & Disability insurances.\nTax-free benefits (Ticket Restaurant scheme, kindergarten and transport cards).\nFree Language classes (Catalan, Spanish and English).\nGlobal and dynamic employee base (more than 20 nationalities).\n30 working days of vacation.\nA modern culture environment combined with all the financial and stability advantages of working for a large business.\nActive volunteering community, with 40 annual paid hours of volunteering time.\nPrivate Health and Life & Disability insurances.\nTax-free benefits (Ticket Restaurant scheme, kindergarten and transport cards).\nFree Language classes (Catalan, Spanish and English).\nGlobal and dynamic employee base (more than 20 nationalities).\n30 working days of vacation."
    },
    "4174486030": {
        "title": "Data Engineer ",
        "company": "Dow Jones",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nJOB TITLE: Data Engineer\n\nDow Jones Location: Barcelona Job ID: TBD\n\nAbout Us\n\nOPIS, a Dow Jones company, is one of the world\u2019s most comprehensive sources for petroleum pricing and news information. OPIS provides real-time and historical spot, wholesale/rack and retail fuel prices for the refined products, renewable fuels and natural gas and gas liquids (LPG) industries. We deliver award-winning news, market intelligence and transparency to the entire refined energy marketplace and companies looking to go carbon neutral. At its core, OPIS uses a set of complex IT systems and tools, handling huge amounts of data in a reliable way, and providing customers with business applications to use this data as efficiently as possible.\n\nYour Role\n\nWe are looking for a Data Engineer that will design, develop and maintain database architecture and code initiatives supporting internal and third-party software solutions, delivering excellent products in scope and on time, and meeting current and long-term business needs for the business area. Participate in an agile team of motivated, driven, high-energy engineers, building trust and confidence with team members and business stakeholders. Lead, mentor, and coach junior data engineers to build stronger and faster teams.\n\nYour Responsibilities\n\nAs an experienced Data Engineer, your responsibilities will include, but are not limited to: working with our product teams to provide timely, effective and performant solutions; contributing to the design of scalable data solutions, including the development of automated pipelines; implementing and/or maintaining database systems to ensure the availability and consistent performance of our application suite; understand and define business data requirements and identify data needs and data sources to create scalable data architecture and support enhancements, new product and implementation. You will be a strong collaborator working with other team members, driving both employee and team development and growth.\n\nYour Expertise\n\nDegree in computer science, related technical field, or equivalent experience\nMinimum of 1 year database developer experience (SQL Server or equivalent)\nAbility to create data models for the integration of multiple systems\nAbility to create and maintain stored procedures\nAbility to write SQL queries and statements to explore source data and data issues\nKnowledge of cloud data technologies and architecture\nKnowledge of Python for data analysis and automation\nKnowledge of building CI/CD pipelines for code deployment\nKnowledge of BI tooling, Power BI preferred\nFamiliarity with data-specific unit testing frameworks\nFamiliarity with PowerShell scripting for system administration and automation tasks\nMust speak and write in English fluently; Effective communicator\n\nDoes this sound like you?\n\nA strong analytical thinker with a love for planning and organization who can manage competing demands with minimal supervision\nA self-starter with a never-ending curiosity to learn and solve problems\nA critical thinker, with mad problem-solving skills\nA team player with great interpersonal and communication skills\nA structured developer, with good time-management skills\n\nIf these things strike a chord with you, we should talk.\n\nWe\u2019d also love to hear about your experiences with any of the following:\n\nDegrees, Certifications, and/or awards/recognition that you have achieved\nKnowledge of SDLC, Agile methodology and/or SCRUM\nExperience working with scripts under source control (Git/TFVC/Mercurial/Svn/etc)\nExperience being part of a continuous integration and deployment process\nExperience using database monitoring tools (like tools from SolarWinds or RedGate)\n\nOur Benefits\n\nPension Scheme: Company contributes up to 8% of your salary with matching contributions.\nHealthcare Coverage: Comprehensive health plan with unlimited coverage, including outpatient care, dental, and maternity.\nLife & Disability Insurance: Coverage from day one, including life insurance at 2X salary and accident coverage up to 4X salary.\nWellness Programs: Access to wellness platforms, mindfulness apps, and free fitness classes.\nMeal & Teleworking Allowance: Monthly meal card and teleworking allowance.\nFlexible Working: Ability to work from home 2 days per week with the option to work remotely for 1 week per quarter.\nFamily Care Support: Up to 4 weeks of paid compassionate leave and a generous reimbursement for emergency care.\nReferral Bonus: Earn a bonus for recommending great talent to join us.\n\nInclusion and diversity are critical to the success of Dow Jones, and we actively encourage applications from people of all backgrounds. We are committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, status as a protected veteran, or any other protected category. For more information on the many ways in which we enthusiastically support inclusion and diversity efforts for both candidates and employees. \n\n\n\nCurrent Colleagues\n\nIf you are currently employed by Dow Jones, please apply internally via the Workday internal careers site.\n\nReasonable accommodation: Dow Jones, Making Careers Newsworthy - We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law. EEO/AA/M/F/Disabled/Vets. We strongly encourage applications from all qualified individuals, including women, people with disabilities, and those from underrepresented groups. Dow Jones is committed to providing reasonable accommodation for qualified individuals with disabilities, in our job application and/or interview process. If you need assistance or accommodation in completing your application, due to a disability, email us at talentresourceteam@dowjones.com. Please put \"Reasonable Accommodation\" in the subject line and provide a brief description of the type of assistance you need. This inbox will not be monitored for application status updates.\n\nBusiness Area: Dow Jones - OPIS\n\nJob Category: Data Analytics/Warehousing & Business Intelligence\n\nUnion Status\n\nNon-Union role\n\nSince 1882, Dow Jones has been finding new ways to bring information to the world\u2019s top business entities. Beginning as a niche news agency in an obscure Wall Street basement, Dow Jones has grown to be a worldwide news and information powerhouse, with prestigious brands including The Wall Street Journal, Dow Jones Newswires, Factiva, Barron\u2019s, MarketWatch and Financial News.\n\nThis longevity and success is due to a relentless pursuit of accuracy, depth and innovation, enhanced by the wisdom of past experience and a solid grasp on the future ahead. More than its individual brands, Dow Jones is a modern gateway to intelligence, with innovative technology, advanced data feeds, integrated solutions, expert research, award-winning journalism and customizable apps and delivery systems to bring the information that matters most to customers, when and where they need it, every day.\n\nReq ID: 45287\nAbout Us\nDegree in computer science, related technical field, or equivalent experience\nMinimum of 1 year database developer experience (SQL Server or equivalent)\nAbility to create data models for the integration of multiple systems\nAbility to create and maintain stored procedures\nAbility to write SQL queries and statements to explore source data and data issues\nKnowledge of cloud data technologies and architecture\nKnowledge of Python for data analysis and automation\nKnowledge of building CI/CD pipelines for code deployment\nKnowledge of BI tooling, Power BI preferred\nFamiliarity with data-specific unit testing frameworks\nFamiliarity with PowerShell scripting for system administration and automation tasks\nMust speak and write in English fluently; Effective communicator\nDegree in computer science, related technical field, or equivalent experience\nMinimum of 1 year database developer experience (SQL Server or equivalent)\nAbility to create data models for the integration of multiple systems\nAbility to create and maintain stored procedures\nAbility to write SQL queries and statements to explore source data and data issues\nKnowledge of cloud data technologies and architecture\nKnowledge of Python for data analysis and automation\nKnowledge of building CI/CD pipelines for code deployment\nKnowledge of BI tooling, Power BI preferred\nFamiliarity with data-specific unit testing frameworks\nFamiliarity with PowerShell scripting for system administration and automation tasks\nMust speak and write in English fluently; Effective communicator\nA strong analytical thinker with a love for planning and organization who can manage competing demands with minimal supervision\nA self-starter with a never-ending curiosity to learn and solve problems\nA critical thinker, with mad problem-solving skills\nA team player with great interpersonal and communication skills\nA structured developer, with good time-management skills\nA strong analytical thinker with a love for planning and organization who can manage competing demands with minimal supervision\nA self-starter with a never-ending curiosity to learn and solve problems\nA critical thinker, with mad problem-solving skills\nA team player with great interpersonal and communication skills\nA structured developer, with good time-management skills\nDegrees, Certifications, and/or awards/recognition that you have achieved\nKnowledge of SDLC, Agile methodology and/or SCRUM\nExperience working with scripts under source control (Git/TFVC/Mercurial/Svn/etc)\nExperience being part of a continuous integration and deployment process\nExperience using database monitoring tools (like tools from SolarWinds or RedGate)\nDegrees, Certifications, and/or awards/recognition that you have achieved\nKnowledge of SDLC, Agile methodology and/or SCRUM\nExperience working with scripts under source control (Git/TFVC/Mercurial/Svn/etc)\nExperience being part of a continuous integration and deployment process\nExperience using database monitoring tools (like tools from SolarWinds or RedGate)\nOur Benefits\nPension Scheme: Company contributes up to 8% of your salary with matching contributions.\nHealthcare Coverage: Comprehensive health plan with unlimited coverage, including outpatient care, dental, and maternity.\nLife & Disability Insurance: Coverage from day one, including life insurance at 2X salary and accident coverage up to 4X salary.\nWellness Programs: Access to wellness platforms, mindfulness apps, and free fitness classes.\nMeal & Teleworking Allowance: Monthly meal card and teleworking allowance.\nFlexible Working: Ability to work from home 2 days per week with the option to work remotely for 1 week per quarter.\nFamily Care Support: Up to 4 weeks of paid compassionate leave and a generous reimbursement for emergency care.\nReferral Bonus: Earn a bonus for recommending great talent to join us.\nPension Scheme: Company contributes up to 8% of your salary with matching contributions.\nHealthcare Coverage: Comprehensive health plan with unlimited coverage, including outpatient care, dental, and maternity.\nLife & Disability Insurance: Coverage from day one, including life insurance at 2X salary and accident coverage up to 4X salary.\nWellness Programs: Access to wellness platforms, mindfulness apps, and free fitness classes.\nMeal & Teleworking Allowance: Monthly meal card and teleworking allowance.\nFlexible Working: Ability to work from home 2 days per week with the option to work remotely for 1 week per quarter.\nFamily Care Support: Up to 4 weeks of paid compassionate leave and a generous reimbursement for emergency care.\nReferral Bonus: Earn a bonus for recommending great talent to join us.\nUnion Status"
    },
    "4156172212": {
        "title": "Data Engineer ",
        "company": "BASE life science",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nThe life sciences industry plays a significant role in advancing healthcare and improving the quality of life - in many cases, saving millions of lives globally. At BASE life science, this is the motivation for everything we do, aimed at pushing boundaries, seeking excellence, and making a lasting difference through consulting expertise and data-driven solutions.\n\nWho is BASE life science?\nWe are a fast-growing consultancy in the life sciences industry and are looking for innovative, enthusiastic, and versatile data engineers to join us. This is an exciting opportunity to work on cutting-edge projects and gain experience in a dynamic and fast-paced environment.\n\nFrom our headquarters in Denmark and offices in France, Germany, Italy, Spain, Switzerland, the United Kingdom, and the United States of America, we serve customers across the globe. With a deep understanding of the life sciences industry, we work hand in hand with customers to identify, develop, and implement strategic initiatives that drive growth, efficiency, and overall success through tailored solutions.\n\nOur partnership with industry-leading solution providers, including Veeva, Salesforce, IQVIA, Benchling, and MediSpend ensures that our clients harness the full potential of the most innovative solutions on the market.\n\nAbout the role\nWe are a fast-growing consultancy in the life science industry, and we are looking for innovative, enthusiastic, and versatile data engineers to join us. This is an exciting opportunity to work on cutting-edge projects and gain experience in a dynamic and fast-paced environment.\n\nAs a Data Engineer your responsibilities include:\nBuilding and maintaining data pipelines and data architecture for our clients\nCollaborating with cross-functional teams to design and implement data-driven solutions\nHelping to improve data quality and data governance practices\nParticipating in the development and testing of data-driven solutions\n\nWe offer this position in Barcelona or Madrid, Spain.\n\nWhat makes you the best person for this job? \nMaster\u2019s degree in STEM (Science, Technology, Engineering, and Mathematics) or another quantitative field.\n3+ years of working experience with Python and/or other programming languages (e.g., R, Java, C++).\nKnowledge of databases, data warehousing, ETL, and/or data modelling concepts.\nKnowledge of using a bash-terminal.\nStrong problem-solving skills and ability to work independently.\nStrong communication and teamworking skills.\n\nAdditionally, you might have experience with:\nCollaborative software development using Git, UNIX and in working with GxP or other regulated data, preferably from the pharmaceutical industry.\nYou have an advantage if you come with an understanding of pharmaceutical data in the R&D space (clinical, regulatory, quality, safety).\nWe further presume that you have experience working in a client-facing role.\n\nWhat makes us the best employer for you?\nBASE life science has been recognized as an outstanding workplace, earning us the Great Place To Work certification in Denmark, France, Germany, Italy, Spain, Switzerland, and the United Kingdom.\n\nWe are proud to represent 35+ nationalities among our colleagues, and firmly believe in the power of diverse perspectives and characteristics to drive truly innovative solutions and achieving excellence.\n\nAt the heart of our organization lies a foundation grounded in five core values: team spirit, trust, openness, ambition, and execution. We cultivate an environment that champions flexibility, placing confidence in our employees to successfully complete tasks while upholding a healthy work-life balance. Additionally, we place significance in cross-functional collaboration, where niche skills and specialization are valued and challenging the status quo is encouraged.\n\nAdditionally, we offer you:\nHealth insurance.\nRemote work friendliness, with an expectation that you are in the office at least once a week\nOngoing learning and development.\nContributions to physical, social, and emotional health.\nHome office setup with laptop and other electronic devices.\nThe chance to help make a difference for patients around the world.\n\nInterested? \nIf you find this position intriguing, don't delay\u2014submit your application at your earliest convenience. We are continuously reviewing and assessing all incoming candidates and eagerly await your application.\n\nBy submitting your application, you consent to the processing of your personal data by BASE life science for the purposes of recruitment and selection. This includes the collection, storage, and use of your personal data as outlined in our Privacy Policy.\nThe life sciences industry plays a significant role in advancing healthcare and improving the quality of life - in many cases, saving millions of lives globally. At BASE life science, this is the motivation for everything we do, aimed at pushing boundaries, seeking excellence, and making a lasting difference through consulting expertise and data-driven solutions.\nWho is BASE life science?\nWe are a fast-growing consultancy in the life sciences industry and are looking for innovative, enthusiastic, and versatile data engineers to join us. This is an exciting opportunity to work on cutting-edge projects and gain experience in a dynamic and fast-paced environment.\nFrom our headquarters in Denmark and offices in France, Germany, Italy, Spain, Switzerland, the United Kingdom, and the United States of America, we serve customers across the globe. With a deep understanding of the life sciences industry, we work hand in hand with customers to identify, develop, and implement strategic initiatives that drive growth, efficiency, and overall success through tailored solutions.\nOur partnership with industry-leading solution providers, including Veeva, Salesforce, IQVIA, Benchling, and MediSpend ensures that our clients harness the full potential of the most innovative solutions on the market.\nAbout the role\nWe are a fast-growing consultancy in the life science industry, and we are looking for innovative, enthusiastic, and versatile data engineers to join us. This is an exciting opportunity to work on cutting-edge projects and gain experience in a dynamic and fast-paced environment.\nAs a Data Engineer your responsibilities include:\nBuilding and maintaining data pipelines and data architecture for our clients\nCollaborating with cross-functional teams to design and implement data-driven solutions\nHelping to improve data quality and data governance practices\nParticipating in the development and testing of data-driven solutions\nBuilding and maintaining data pipelines and data architecture for our clients\nCollaborating with cross-functional teams to design and implement data-driven solutions\nHelping to improve data quality and data governance practices\nParticipating in the development and testing of data-driven solutions\nWe offer this position in Barcelona or Madrid, Spain.\nWhat makes you the best person for this job?\nMaster\u2019s degree in STEM (Science, Technology, Engineering, and Mathematics) or another quantitative field.\n3+ years of working experience with Python and/or other programming languages (e.g., R, Java, C++).\nKnowledge of databases, data warehousing, ETL, and/or data modelling concepts.\nKnowledge of using a bash-terminal.\nStrong problem-solving skills and ability to work independently.\nStrong communication and teamworking skills.\nMaster\u2019s degree in STEM (Science, Technology, Engineering, and Mathematics) or another quantitative field.\n3+ years of working experience with Python and/or other programming languages (e.g., R, Java, C++).\nKnowledge of databases, data warehousing, ETL, and/or data modelling concepts.\nKnowledge of using a bash-terminal.\nStrong problem-solving skills and ability to work independently.\nStrong communication and teamworking skills.\nAdditionally, you might have experience with:\nCollaborative software development using Git, UNIX and in working with GxP or other regulated data, preferably from the pharmaceutical industry.\nYou have an advantage if you come with an understanding of pharmaceutical data in the R&D space (clinical, regulatory, quality, safety).\nWe further presume that you have experience working in a client-facing role.\nCollaborative software development using Git, UNIX and in working with GxP or other regulated data, preferably from the pharmaceutical industry.\nYou have an advantage if you come with an understanding of pharmaceutical data in the R&D space (clinical, regulatory, quality, safety).\nWe further presume that you have experience working in a client-facing role.\nWhat makes us the best employer for you?\nBASE life science has been recognized as an outstanding workplace, earning us the Great Place To Work certification in Denmark, France, Germany, Italy, Spain, Switzerland, and the United Kingdom.\nWe are proud to represent 35+ nationalities among our colleagues, and firmly believe in the power of diverse perspectives and characteristics to drive truly innovative solutions and achieving excellence.\nAt the heart of our organization lies a foundation grounded in five core values: team spirit, trust, openness, ambition, and execution. We cultivate an environment that champions flexibility, placing confidence in our employees to successfully complete tasks while upholding a healthy work-life balance. Additionally, we place significance in cross-functional collaboration, where niche skills and specialization are valued and challenging the status quo is encouraged.\nAdditionally, we offer you:\nHealth insurance.\nRemote work friendliness, with an expectation that you are in the office at least once a week\nOngoing learning and development.\nContributions to physical, social, and emotional health.\nHome office setup with laptop and other electronic devices.\nThe chance to help make a difference for patients around the world.\nHealth insurance.\nRemote work friendliness, with an expectation that you are in the office at least once a week\nOngoing learning and development.\nContributions to physical, social, and emotional health.\nHome office setup with laptop and other electronic devices.\nThe chance to help make a difference for patients around the world.\nInterested?\nIf you find this position intriguing, don't delay\u2014submit your application at your earliest convenience. We are continuously reviewing and assessing all incoming candidates and eagerly await your application.\nBy submitting your application, you consent to the processing of your personal data by BASE life science for the purposes of recruitment and selection. This includes the collection, storage, and use of your personal data as outlined in our Privacy Policy.\n."
    },
    "4175506952": {
        "title": "Data Engineer",
        "company": "Source Technology",
        "location": "Seville, Andalusia, Spain",
        "work_mode": "On-site",
        "job_type": "Contract",
        "description": "About the job\nData Engineer - (Marketing Intelligence)\n\nKey Responsibilities:\n\u2022 Design, develop, and maintain data pipelines in AWS to support marketing intelligence needs.\n\u2022 Optimize and enhance existing data pipelines for performance, scalability, and efficiency.\n\u2022 Integrate new data sources and adjust pipelines as requirements evolve.\n\u2022 Work with tools like Docebo, Qualtrics, and SAP Marketing to streamline data ingestion.\n\u2022 Collaborate with marketing and data teams to ensure data accuracy and availability.\n\nKey Skills & Experience:\n\u2022 Strong experience in AWS data services (Glue, AppFlow, Step Functions, Athena, Redshift).\n\u2022 Proficiency in Python and Spark for data processing.\n\u2022 Experience with marketing data platforms like Docebo, Qualtrics, SAP Marketing is a plus.\n\u2022 Ability to work with structured and unstructured data.\n\u2022 Strong problem-solving skills and ability to work in a fast-paced environment.\nData Engineer - (Marketing Intelligence)\nKey Responsibilities:\n\u2022 Design, develop, and maintain data pipelines in AWS to support marketing intelligence needs.\n\u2022 Optimize and enhance existing data pipelines for performance, scalability, and efficiency.\n\u2022 Integrate new data sources and adjust pipelines as requirements evolve.\n\u2022 Work with tools like Docebo, Qualtrics, and SAP Marketing to streamline data ingestion.\n\u2022 Collaborate with marketing and data teams to ensure data accuracy and availability.\nKey Skills & Experience:\n\u2022 Strong experience in AWS data services (Glue, AppFlow, Step Functions, Athena, Redshift).\n\u2022 Proficiency in Python and Spark for data processing.\n\u2022 Experience with marketing data platforms like Docebo, Qualtrics, SAP Marketing is a plus.\n\u2022 Ability to work with structured and unstructured data.\n\u2022 Strong problem-solving skills and ability to work in a fast-paced environment."
    },
    "4173573932": {
        "title": "Data Engineer ",
        "company": "Nuvolar Works",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\ud83d\udc40 Looking for your next challenge? This might be it!\n\nData is transforming industries at an incredible pace, and at Nuvolar Works, we\u2019re at the forefront of innovation. Whether it\u2019s advancing healthcare with AI-powered solutions or revolutionizing pharmaceutical data management, our innovative projects are shaping the future of the industry.\n\nAre you passionate about data engineering, cloud architectures, and tackling large-scale challenges? If so, we want you on our team! \ud83d\ude80\n\nWhat you\u2019ll be doing? \ud83d\udee0\ufe0f\nAs a Data Engineer, your impact will be felt across multiple industries. You\u2019ll be responsible for:\nDesigning and delivering complex data flows to support AI/ML/RAG applications, aligned with product roadmaps.\nCollaborating with peers and stakeholders to design and present scalable data solutions.\nEnsuring high-quality, maintainable code for all deliverables.\nTroubleshooting and resolving technical issues effectively.\nMentoring junior team members and supporting their growth.\nAssisting in task allocation based on team members\u2019 strengths and goals.\nIdentifying technical spikes and leading their execution.\nProposing improvements to workflows, processes, and the development stack.\nParticipating in the technical assessment of new candidates.\n\nWhat will make you succeed with us? \u2705\nWe\u2019re looking for someone who has:\n3+ years of proven experience as a Data Engineer.\nStrong programming skills in Python.\nSolid understanding of Azure Cloud solutions, including:\nData Storage: Azure Data Lake, Blob Storage, Cosmos DB, Azure SQL\nData Processing: Databricks, Azure Stream Analytics\nAI & ML: Azure AI Studio, Azure ML\nReporting: Power BI\nExperience with data modeling, ETL processes, and data warehousing concepts.\nStrong problem-solving and analytical skills.\nProficiency with Git, Docker, Bash, and Linux.\nFamiliarity with Agile methodologies.\nExcellent communication skills in English, both written and spoken.\n\nWhy you\u2019ll love working with us \ud83d\udc99\n\ud83d\udcbb Flexible work options \u2013 Choose remote work in Spain or come to our offices in Madrid or Barcelona.\n\ud83d\udd51 Work-life balance \u2013 Enjoy every other Friday off and summer schedule in July and August.\n\ud83d\udcc8 Growth opportunities \u2013 We invest in continuous learning and professional development.\n\ud83c\udfe5 Top-tier benefits \u2013 Perks like gym subsidies, private health insurance, language classes and a birthday day off.\n\ud83c\udf8a A dynamic company culture \u2013 Funday Fridays, knowledge-sharing sessions (Lunch & Learns), and more!\n\nHiring process \ud83d\ude80\nWe keep things simple and efficient:\n1\ufe0f\u20e3 HR Interview \u2013 Let\u2019s talk about your experience and aspirations.\n2\ufe0f\u20e3 Technical Test \u2013 Solve a real-world problem and showcase your skills.\n3\ufe0f\u20e3 Technical Interview \u2013 Meet the team and dive deeper into our data challenges.\n\nExcited to make an impact? Join us and help shape the future of data as a great team together!\n\nIf you want to be aware of all of our latest updates, follow us via LinkedIn | Instagram | Glassdoor | Medium\n\nNuvolar Works is proud to be an equal-opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees, regardless of age, gender, sexual orientation, disability, national origin, race, or religion.\n\ud83d\udc40 Looking for your next challenge? This might be it!\nLooking for your next challenge? This might be it!\nData is transforming industries at an incredible pace, and at Nuvolar Works, we\u2019re at the forefront of innovation. Whether it\u2019s advancing healthcare with AI-powered solutions or revolutionizing pharmaceutical data management, our innovative projects are shaping the future of the industry.\nNuvolar Works\nAre you passionate about data engineering, cloud architectures, and tackling large-scale challenges? If so, we want you on our team! \ud83d\ude80\nwe want you on our team!\nWhat you\u2019ll be doing? \ud83d\udee0\ufe0f\nAs a Data Engineer, your impact will be felt across multiple industries. You\u2019ll be responsible for:\nData Engineer\nDesigning and delivering complex data flows to support AI/ML/RAG applications, aligned with product roadmaps.\nCollaborating with peers and stakeholders to design and present scalable data solutions.\nEnsuring high-quality, maintainable code for all deliverables.\nTroubleshooting and resolving technical issues effectively.\nMentoring junior team members and supporting their growth.\nAssisting in task allocation based on team members\u2019 strengths and goals.\nIdentifying technical spikes and leading their execution.\nProposing improvements to workflows, processes, and the development stack.\nParticipating in the technical assessment of new candidates.\nDesigning and delivering complex data flows to support AI/ML/RAG applications, aligned with product roadmaps.\nAI/ML/RAG applications\nCollaborating with peers and stakeholders to design and present scalable data solutions.\nEnsuring high-quality, maintainable code for all deliverables.\nhigh-quality, maintainable code\nTroubleshooting and resolving technical issues effectively.\nMentoring junior team members and supporting their growth.\nAssisting in task allocation based on team members\u2019 strengths and goals.\nIdentifying technical spikes and leading their execution.\nProposing improvements to workflows, processes, and the development stack.\nParticipating in the technical assessment of new candidates.\nWhat will make you succeed with us? \u2705\nWe\u2019re looking for someone who has:\n3+ years of proven experience as a Data Engineer.\nStrong programming skills in Python.\nSolid understanding of Azure Cloud solutions, including:\nData Storage: Azure Data Lake, Blob Storage, Cosmos DB, Azure SQL\nData Processing: Databricks, Azure Stream Analytics\nAI & ML: Azure AI Studio, Azure ML\nReporting: Power BI\nExperience with data modeling, ETL processes, and data warehousing concepts.\nStrong problem-solving and analytical skills.\nProficiency with Git, Docker, Bash, and Linux.\nFamiliarity with Agile methodologies.\nExcellent communication skills in English, both written and spoken.\n3+ years of proven experience as a Data Engineer.\n3+ years\nStrong programming skills in Python.\nPython.\nSolid understanding of Azure Cloud solutions, including:\nAzure Cloud solutions\nData Storage: Azure Data Lake, Blob Storage, Cosmos DB, Azure SQL\nData Storage\nData Processing: Databricks, Azure Stream Analytics\nData Processing\nAI & ML: Azure AI Studio, Azure ML\nAI & ML\nReporting: Power BI\nExperience with data modeling, ETL processes, and data warehousing concepts.\nStrong problem-solving and analytical skills.\nProficiency with Git, Docker, Bash, and Linux.\nFamiliarity with Agile methodologies.\nAgile methodologies.\nExcellent communication skills in English, both written and spoken.\nWhy you\u2019ll love working with us \ud83d\udc99\n\ud83d\udcbb Flexible work options \u2013 Choose remote work in Spain or come to our offices in Madrid or Barcelona.\n\ud83d\udd51 Work-life balance \u2013 Enjoy every other Friday off and summer schedule in July and August.\n\ud83d\udcc8 Growth opportunities \u2013 We invest in continuous learning and professional development.\n\ud83c\udfe5 Top-tier benefits \u2013 Perks like gym subsidies, private health insurance, language classes and a birthday day off.\n\ud83c\udf8a A dynamic company culture \u2013 Funday Fridays, knowledge-sharing sessions (Lunch & Learns), and more!\nFunday Fridays,\nLunch & Learns\nHiring process \ud83d\ude80\nWe keep things simple and efficient:\n1\ufe0f\u20e3 HR Interview \u2013 Let\u2019s talk about your experience and aspirations.\nHR Interview\n2\ufe0f\u20e3 Technical Test \u2013 Solve a real-world problem and showcase your skills.\nTechnical Test\n3\ufe0f\u20e3 Technical Interview \u2013 Meet the team and dive deeper into our data challenges.\nTechnical Interview\nExcited to make an impact? Join us and help shape the future of data as a great team together!\nExcited to make an impact?\nIf you want to be aware of all of our latest updates, follow us via LinkedIn | Instagram | Glassdoor | Medium\nNuvolar Works is proud to be an equal-opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees, regardless of age, gender, sexual orientation, disability, national origin, race, or religion."
    },
    "4173028387": {
        "title": "Data Engineer",
        "company": "Selecta Digital",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfQuieres formar parte de una empresa que te ayude a crecer tecnol\u00f3gicamente con proyectos innovadores? \u00a1Esta es tu propuesta laboral!\n\nDesde Selecta Digital, estamos buscando \ud83d\udd0e un Data Engineer (H/M) para un cliente final del sector Seguros \ud83d\udc68\u200d\ud83d\udcbb\n\nLa empresa tiene una misi\u00f3n muy clara: dise\u00f1ar y ofrecer servicios de previsi\u00f3n que permitan mejorar la calidad de vida de las personas.\n\nEs el reto profesional que, con tu pasi\u00f3n y conocimiento, puede suponer el salto profesional que estas buscando. Desde Selecta, \u00a1te acompa\u00f1aremos en todo el proceso de selecci\u00f3n!\n\n\u00bfEn qu\u00e9 consistir\u00e1 tu trabajo? \ud83d\udcf1\nLa principal responsabilidad ser\u00e1 la creaci\u00f3n y mantenimiento de flujos de datos (ETL), permitiendo que los datos solicitados est\u00e9n disponibles para los compa\u00f1eros del equipo y/o para otros departamentos. Esto implicar\u00e1 la b\u00fasqueda de fuentes de datos originales, as\u00ed como la generaci\u00f3n de nuevas tablas para crear innovadores KPIs que ayuden a obtener insights a nuestros equipos.\n\nLo que buscamos\ud83d\udd75\ufe0f\u200d\u2640\ufe0f \ud83d\udc49:\n\ud83d\udd39 Experiencia demostrable en el desarrollo de pipelines de datos utilizando Talend u otras herramientas.\n\ud83d\udd39 Conocimiento s\u00f3lido de SQL y bases de datos relacionales.\n\ud83d\udd39 Capacidad para trabajar de forma independiente y en equipo en un entorno \u00e1gil y r\u00e1pido.\n\ud83d\udd39 Se valorar\u00e1 experiencia con software de gesti\u00f3n de proyectos JIRA y nociones b\u00e1sicas de programaci\u00f3n en lenguaje Java.\n\n\u00a1Condiciones y Beneficios! \ud83e\udd1d\ud83c\udffd\n\ud83d\udcddContrato indefinido.\n\ud83d\udccd Presencial Barcelona - Horario flexible - Jornada 37.5 horas a la semana.\n\ud83c\udfd6\ufe0f28 d\u00edas de vacaciones.\n\ud83d\udcb0 Salario seg\u00fan perfil. \u00a1\u00a1Cu\u00e9ntanos tus expectativas!!\n\ud83c\udfe5Beneficios sociales como seguro m\u00e9dico y de vida.\n\nSi crees que encajas en el perfil o conoces a alguien que pudiera estar interesad@, no dudes en inscribirte.\n\n\u00a1Esperamos tu postulaci\u00f3n! \u263a\n\nTanto Selecta como nuestros clientes estamos comprometidos con la igualdad de oportunidades y nos discriminamos por raz\u00f3n de edad, raza, religi\u00f3n o sexo.\n\u00bfQuieres formar parte de una empresa que te ayude a crecer tecnol\u00f3gicamente con proyectos innovadores? \u00a1Esta es tu propuesta laboral!\nDesde Selecta Digital, estamos buscando \ud83d\udd0e un Data Engineer (H/M) para un cliente final del sector Seguros \ud83d\udc68\u200d\ud83d\udcbb\nData Engineer (H/M)\nLa empresa tiene una misi\u00f3n muy clara: dise\u00f1ar y ofrecer servicios de previsi\u00f3n que permitan mejorar la calidad de vida de las personas.\nEs el reto profesional que, con tu pasi\u00f3n y conocimiento, puede suponer el salto profesional que estas buscando. Desde Selecta, \u00a1te acompa\u00f1aremos en todo el proceso de selecci\u00f3n!\n\u00bfEn qu\u00e9 consistir\u00e1 tu trabajo? \ud83d\udcf1\nLa principal responsabilidad ser\u00e1 la creaci\u00f3n y mantenimiento de flujos de datos (ETL), permitiendo que los datos solicitados est\u00e9n disponibles para los compa\u00f1eros del equipo y/o para otros departamentos. Esto implicar\u00e1 la b\u00fasqueda de fuentes de datos originales, as\u00ed como la generaci\u00f3n de nuevas tablas para crear innovadores KPIs que ayuden a obtener insights a nuestros equipos.\nLo que buscamos\ud83d\udd75\ufe0f\u200d\u2640\ufe0f \ud83d\udc49:\n\ud83d\udd39 Experiencia demostrable en el desarrollo de pipelines de datos utilizando Talend u otras herramientas.\n\ud83d\udd39 Conocimiento s\u00f3lido de SQL y bases de datos relacionales.\n\ud83d\udd39 Capacidad para trabajar de forma independiente y en equipo en un entorno \u00e1gil y r\u00e1pido.\n\ud83d\udd39 Se valorar\u00e1 experiencia con software de gesti\u00f3n de proyectos JIRA y nociones b\u00e1sicas de programaci\u00f3n en lenguaje Java.\n\u00a1Condiciones y Beneficios! \ud83e\udd1d\ud83c\udffd\n\ud83d\udcddContrato indefinido.\n\ud83d\udccd Presencial Barcelona - Horario flexible - Jornada 37.5 horas a la semana.\n\ud83c\udfd6\ufe0f28 d\u00edas de vacaciones.\n\ud83d\udcb0 Salario seg\u00fan perfil. \u00a1\u00a1Cu\u00e9ntanos tus expectativas!!\n\ud83c\udfe5Beneficios sociales como seguro m\u00e9dico y de vida.\nSi crees que encajas en el perfil o conoces a alguien que pudiera estar interesad@, no dudes en inscribirte.\n\u00a1Esperamos tu postulaci\u00f3n! \u263a\nTanto Selecta como nuestros clientes estamos comprometidos con la igualdad de oportunidades y nos discriminamos por raz\u00f3n de edad, raza, religi\u00f3n o sexo."
    },
    "3673673371": {
        "title": "Data Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\n\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca \n\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \u270d\n\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n? \ud83d\udc47\n\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada semestre, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario competitivo seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \u270d\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\ndesarrollo y ejecuci\u00f3n de diversos proyectos\ndefinici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nSQL\nETL\ndatawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nformaci\u00f3n interna y externa en las herramientas relevantes para el equipo y\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\ntendencias tecnol\u00f3gicas\nData & Analytics\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\ncultura de SDG y su unidad de trabajo\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\ndin\u00e1micas de compa\u00f1\u00eda y de equipo\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\nctividades operativas de la compa\u00f1\u00eda\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n? \ud83d\udc47\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nSQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\n\u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\nvisualizaci\u00f3n con foco en Tableau y PowerBI.\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada semestre, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario competitivo seg\u00fan tu formaci\u00f3n y experiencia\nSalario competitivo\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5"
    },
    "4163554950": {
        "title": "Data Engineer - Quicksight ",
        "company": "Ayesa",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1En @Ayesa crecemos juntos! \ud83d\ude80 \u2728\nCada profesional de nuestra empresa es importante para nosotros, nos ayudan a crecer de manera diversa y gracias a ellos somos m\u00e1s de 12.000 personas trabajando con el mismo objetivo en m\u00e1s de 40 pa\u00edses.\nSi eres una persona entusiasta, te apasiona la #tecnolog\u00eda y buscas un nuevo reto profesional \u00a1Este es tu sitio!\nBuscamos ampliar nuestro equipo con un/a Data Engineer para incorporase en el \u00e1rea de Data & IA para un proyecto de uno de nuestros clientes bancarios en nuestras oficinas de \ud83d\udccdMadrid en modalidad h\u00edbrida (disponibilidad en otras zonas geogr\u00e1ficas).\n\n\u00a1An\u00edmate a dar el salto!\u00a1Te estamos esperando!\n\nFunciones:\nTe incorporar\u00e1s para a un equipo multidisciplinar dentro de un proyecto de un cliente del sector banca. Existe la necesidad para crear cuadros de mando y engineer con Python para acceder a datos.\n\nRequisitos imprescindibles: \nExperiencia de al menos 2 a\u00f1os como Data Engineer.\nExperiencia con Python.\nExperiencia con Pyspark.\nExperiencia con Quicksight.\n\nDeseable: \nMicrostrategy.\nHerramientas Data.\nEntorno Financiero.\n\nTe ofrecemos un camino lleno de aventuras:\n\ud83d\udd39 \u00a1Llega hasta donde t\u00fa quieras! Crece con nuestros programas de desarrollo! Te ayudaremos a marcar tu itinerario de futuro anualmente participando en proyectos vanguardistas con los mejores profesionales del sector.\n\ud83d\udd39 Porque lo importante sois t\u00fa y los tuyos : contamos con pol\u00edticas que facilitan la conciliaci\u00f3n entre la vida profesional y personal: Flexibilidad horaria, Smart Job y jornada intensiva en verano.\n\ud83d\udd39 T\u00fa decides c\u00f3mo usar tu dinero: disfruta de nuestra retribuci\u00f3n flexible, contamos con: Seguro m\u00e9dico, Ticket Restaurant, Guarder\u00eda y Transporte.\n\ud83d\udd39 Plan Bienestar descuentos en viajes, tecnolog\u00eda, formaci\u00f3n, deporte, belleza y \u00a1muchos m\u00e1s.\n\ud83d\udd39 Divi\u00e9rtete participando en los diferentes concursos y actividades que organizamos: \u00bfte gusta el deporte, escribir, la fotograf\u00eda? Todo tiene cabida.\n\ud83d\udd39\u00a1Actual\u00edzate! apostamos por un aprendizaje continuo, contamos con formaci\u00f3n especializada, certificaciones e idiomas.\n\ud83d\udd39\u00a1Deja tu huella! Impacta con nuestros proyectos de voluntariado social.\n\u00a1\u00danete a la multinacional espa\u00f1ola tecnol\u00f3gica referente en el mundo! \ud83d\udcab\ud83c\udf0e\n\nAtendiendo a la Ley Org\u00e1nica 3/2007, de 22 de marzo, Ayesa se ha marcado como objetivo promover la defensa y aplicaci\u00f3n efectiva del principio de igualdad entre hombres y mujeres, evitando cualquier tipo de discriminaci\u00f3n laboral por raz\u00f3n de sexo, garantizando as\u00ed las mismas oportunidades de ingreso. Asimismo, fomentamos la diversidad, rechazando cualquier tipo de discriminaci\u00f3n por razones de raza, sexo, diversidad funcional, religi\u00f3n, orientaci\u00f3n sexual, identidad sexual, o cualquier otra condici\u00f3n personal o social, apostando por construir un entorno inclusivo y enriquecedor.\n\u00a1En @Ayesa crecemos juntos! \ud83d\ude80 \u2728\nCada profesional de nuestra empresa es importante para nosotros, nos ayudan a crecer de manera diversa y gracias a ellos somos m\u00e1s de 12.000 personas trabajando con el mismo objetivo en m\u00e1s de 40 pa\u00edses.\n12.000\nSi eres una persona entusiasta, te apasiona la #tecnolog\u00eda y buscas un nuevo reto profesional \u00a1Este es tu sitio!\n#tecnolog\u00eda\nBuscamos ampliar nuestro equipo con un/a Data Engineer para incorporase en el \u00e1rea de Data & IA para un proyecto de uno de nuestros clientes bancarios en nuestras oficinas de \ud83d\udccdMadrid en modalidad h\u00edbrida (disponibilidad en otras zonas geogr\u00e1ficas).\nData Engineer\nMadrid\n\u00a1An\u00edmate a dar el salto!\u00a1Te estamos esperando!\nFunciones:\nFunciones\n:\nTe incorporar\u00e1s para a un equipo multidisciplinar dentro de un proyecto de un cliente del sector banca. Existe la necesidad para crear cuadros de mando y engineer con Python para acceder a datos.\nRequisitos imprescindibles:\nExperiencia de al menos 2 a\u00f1os como Data Engineer.\nExperiencia con Python.\nExperiencia con Pyspark.\nExperiencia con Quicksight.\nExperiencia de al menos 2 a\u00f1os como Data Engineer.\nExperiencia con Python.\nExperiencia con Pyspark.\nExperiencia con Quicksight.\nDeseable:\nMicrostrategy.\nHerramientas Data.\nEntorno Financiero.\nMicrostrategy.\nHerramientas Data.\nEntorno Financiero.\nTe ofrecemos un camino lleno de aventuras:\nTe ofrecemos un camino lleno de aventuras\n\ud83d\udd39 \u00a1Llega hasta donde t\u00fa quieras! Crece con nuestros programas de desarrollo! Te ayudaremos a marcar tu itinerario de futuro anualmente participando en proyectos vanguardistas con los mejores profesionales del sector.\n\u00a1Llega hasta donde t\u00fa quieras!\n\ud83d\udd39 Porque lo importante sois t\u00fa y los tuyos : contamos con pol\u00edticas que facilitan la conciliaci\u00f3n entre la vida profesional y personal: Flexibilidad horaria, Smart Job y jornada intensiva en verano.\nPorque lo importante sois t\u00fa y los tuyos :\n\ud83d\udd39 T\u00fa decides c\u00f3mo usar tu dinero: disfruta de nuestra retribuci\u00f3n flexible, contamos con: Seguro m\u00e9dico, Ticket Restaurant, Guarder\u00eda y Transporte.\nT\u00fa decides c\u00f3mo usar tu dinero:\n\ud83d\udd39 Plan Bienestar descuentos en viajes, tecnolog\u00eda, formaci\u00f3n, deporte, belleza y \u00a1muchos m\u00e1s.\nPlan Bienestar\n\ud83d\udd39 Divi\u00e9rtete participando en los diferentes concursos y actividades que organizamos: \u00bfte gusta el deporte, escribir, la fotograf\u00eda? Todo tiene cabida.\nDivi\u00e9rtete\n\ud83d\udd39\u00a1Actual\u00edzate! apostamos por un aprendizaje continuo, contamos con formaci\u00f3n especializada, certificaciones e idiomas.\n\u00a1Actual\u00edzate!\n\ud83d\udd39\u00a1Deja tu huella! Impacta con nuestros proyectos de voluntariado social.\n\u00a1Deja tu huella!\n\u00a1\u00danete a la multinacional espa\u00f1ola tecnol\u00f3gica referente en el mundo! \ud83d\udcab\ud83c\udf0e\nAtendiendo a la Ley Org\u00e1nica 3/2007, de 22 de marzo, Ayesa se ha marcado como objetivo promover la defensa y aplicaci\u00f3n efectiva del principio de igualdad entre hombres y mujeres, evitando cualquier tipo de discriminaci\u00f3n laboral por raz\u00f3n de sexo, garantizando as\u00ed las mismas oportunidades de ingreso. Asimismo, fomentamos la diversidad, rechazando cualquier tipo de discriminaci\u00f3n por razones de raza, sexo, diversidad funcional, religi\u00f3n, orientaci\u00f3n sexual, identidad sexual, o cualquier otra condici\u00f3n personal o social, apostando por construir un entorno inclusivo y enriquecedor."
    },
    "4162245373": {
        "title": "Cloud Data Engineer ",
        "company": "Hays",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nYour New Company\nTech Hub located in Barcelona\n\nYour Tasks\nThe development, implementation and maintenance of Google Cloud infrastructure for data analytics pipelines (e.g. ETL/ELT, data science and reporting) for processing large datasets\nThe configuration of cloud infrastructure and its services with Terraform \nThe development and maintenance of CI/CD pipelines\nSupport product owner, BI experts and data engineers in your team with cloud engineering tasks\nImplement data transformation workflows and manage dependencies efficiently using Dataform.\n\nYour Profile\nA degree in computer sciences, business informatics or comparable qualification\nAt least 3+ years of professional experience in software engineering, data analytics or business intelligence\nProven track of managing cloud infrastructure and services, ideally with hands-on experience in Google Cloud Platform, Terraform \nExperienced in data analytics for retail or consumer electronics industry, ideally with hands-on experience in SQL, python with a solid understanding of best practices and libraries commonly used in data engineering. Experience with other programming languages, such as Java, is welcome and considered a plus.\nFamiliarity with Looker as a reporting tool, including dashboard creation and LookML development.\nExtensive experience with BigQuery, SQL performance tuning, and cost optimization strategies to manage and reduce query and storage expenses effectively. \nExcellent English language skills\nYour New Company\nTech Hub located in Barcelona\nYour Tasks\nThe development, implementation and maintenance of Google Cloud infrastructure for data analytics pipelines (e.g. ETL/ELT, data science and reporting) for processing large datasets\nThe configuration of cloud infrastructure and its services with Terraform \nThe development and maintenance of CI/CD pipelines\nSupport product owner, BI experts and data engineers in your team with cloud engineering tasks\nImplement data transformation workflows and manage dependencies efficiently using Dataform.\nThe development, implementation and maintenance of Google Cloud infrastructure for data analytics pipelines (e.g. ETL/ELT, data science and reporting) for processing large datasets\nThe configuration of cloud infrastructure and its services with Terraform\nThe development and maintenance of CI/CD pipelines\nSupport product owner, BI experts and data engineers in your team with cloud engineering tasks\nImplement data transformation workflows and manage dependencies efficiently using Dataform.\nDataform\nYour Profile\nA degree in computer sciences, business informatics or comparable qualification\nAt least 3+ years of professional experience in software engineering, data analytics or business intelligence\nProven track of managing cloud infrastructure and services, ideally with hands-on experience in Google Cloud Platform, Terraform \nExperienced in data analytics for retail or consumer electronics industry, ideally with hands-on experience in SQL, python with a solid understanding of best practices and libraries commonly used in data engineering. Experience with other programming languages, such as Java, is welcome and considered a plus.\nFamiliarity with Looker as a reporting tool, including dashboard creation and LookML development.\nExtensive experience with BigQuery, SQL performance tuning, and cost optimization strategies to manage and reduce query and storage expenses effectively. \nExcellent English language skills\nA degree in computer sciences, business informatics or comparable qualification\nAt least 3+ years of professional experience in software engineering, data analytics or business intelligence\nProven track of managing cloud infrastructure and services, ideally with hands-on experience in Google Cloud Platform, Terraform\nExperienced in data analytics for retail or consumer electronics industry, ideally with hands-on experience in SQL, python with a solid understanding of best practices and libraries commonly used in data engineering. Experience with other programming languages, such as Java, is welcome and considered a plus.\nFamiliarity with Looker as a reporting tool, including dashboard creation and LookML development.\nExtensive experience with BigQuery, SQL performance tuning, and cost optimization strategies to manage and reduce query and storage expenses effectively.\nExcellent English language skills"
    },
    "4143578012": {
        "title": "Data Engineer ",
        "company": "Linde Material Handling",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nWhat We Offer:\n\nWe offer an interesting job and exciting international career opportunity, both in our regional operating unit and in our overarching group functions within an innovative, forward-looking and fast-growing sector. Our culture is driven by our corporate values: Integrity, Collaboration, Courage and Excellence.\n\nIf you want to be part of keeping the world moving, this is the right place.\n\nTasks And Qualifications:\n\nLinde Material Handling is a leading global manufacturer of forklift trucks and warehouse trucks, and a provider of intralogistics solutions and services. As an innovation leader, we deliver progressive products and tailored solutions, which meet all individual requirements that an application or company may have.\n\nAdvanced products and solutions require increasingly digitized systems.\n\nThe Digital Hub is the place where new solutions are developed from idea to prototype and then incorporated into products. We combine methods such as design thinking and agile application development with cutting-edge digital technologies.\n\nBecome part of the team!\n\nYour role: \n\n Develop and operate data-driven features in an interdisciplinary team in the context of larger software development using SAFe.\n Design and implement (big) data pipelines and infrastructures using modern, cloud-based technologies.\n Prepare, clean and model structured and unstructured data.\n Support and advise product teams on the use of big data for their specific use cases.\n Participate in the development of overarching data-related architectures.\n Collaborate closely in interdisciplinary teams with data analysts, data scientists, product owners, designers and IT departments. \n\nYour Profile:\n\n \n\n Completed studies in computer science, information systems, mathematics or a comparable education\n Experience in design, implementation and maintenance of data-intensive applications and data pipelines\n Strong Python coding capabilities\n Experience in the end-to-end development of data-driven products\n Knowledge and experience in several of the following areas:\nData modeling and data management, databases and database queries (data warehouses, SQL, NoSQL, timeseries DBs) \nDistributed systems, ETL/ELT tools und common big data technologies (Spark/Databricks, Kafka) \nModern data infrastructures und data integration processes \nAnalytics/BI tools, ideally Power BI \nDeployments using CI/CD pipelines, ideally using Docker / Kubernetes \nUse of cloud-based platforms, ideally Azure and GCP \n Experience with agile software development methods, ideally Scrum and SAFe\n Strong conceptual and analytical skills\n High degree of initiative and solution orientation\n Fluent in Spanish and English, We are International Team!\n\nWe are looking for you!\n\n Invent digital applications from scratch!\n Solve customer problems with the latest technology!\n Have freedom to innovate and make a difference!\n Bring product ideas to the customer to perfect them to the smallest detail!\n\nWhat We Offer:\n\n Attractive remuneration based on your experience, skills and development\n The opportunity to participate in the international projects and a significant influence on company IT development. Work in a high-tech environment with 5 hubs in Europe!\n Stable employment and social package (private medical care and other benefits)\n Extensive trainings in your area of responsibility\n Committed to work-life balance: Flexible hours and the possibility of intensive Fridays. Possibility of 50% remote work.\n Coffee and fruit available in the office\n Committed to Diversity and Society: Support for employees with children with disabilities.\n\nAbout The Company:\n\nWe are a socially responsible company that offers equal employment opportunities, fosters diversity, and respects differences within our organization. We do not tolerate any form of discrimination, harassment, or aggression\u2014verbal or physical, direct or indirect\u2014toward individuals or material objects.\nWhat We Offer:\nIf you want to be part of keeping the world moving, this is the right place.\nTasks And Qualifications:\nAdvanced products and solutions require increasingly digitized systems\nBecome part of the team!\nYour role:\nDevelop and operate data-driven features in an interdisciplinary team in the context of larger software development using SAFe.\n Design and implement (big) data pipelines and infrastructures using modern, cloud-based technologies.\n Prepare, clean and model structured and unstructured data.\n Support and advise product teams on the use of big data for their specific use cases.\n Participate in the development of overarching data-related architectures.\n Collaborate closely in interdisciplinary teams with data analysts, data scientists, product owners, designers and IT departments.\nDevelop and operate data-driven features in an interdisciplinary team in the context of larger software development using SAFe.\nDesign and implement (big) data pipelines and infrastructures using modern, cloud-based technologies.\nPrepare, clean and model structured and unstructured data.\nSupport and advise product teams on the use of big data for their specific use cases.\nParticipate in the development of overarching data-related architectures.\nCollaborate closely in interdisciplinary teams with data analysts, data scientists, product owners, designers and IT departments.\nYour Profile:\nCompleted studies in computer science, information systems, mathematics or a comparable education\n Experience in design, implementation and maintenance of data-intensive applications and data pipelines\n Strong Python coding capabilities\n Experience in the end-to-end development of data-driven products\n Knowledge and experience in several of the following areas:\nData modeling and data management, databases and database queries (data warehouses, SQL, NoSQL, timeseries DBs) \nDistributed systems, ETL/ELT tools und common big data technologies (Spark/Databricks, Kafka) \nModern data infrastructures und data integration processes \nAnalytics/BI tools, ideally Power BI \nDeployments using CI/CD pipelines, ideally using Docker / Kubernetes \nUse of cloud-based platforms, ideally Azure and GCP \n Experience with agile software development methods, ideally Scrum and SAFe\n Strong conceptual and analytical skills\n High degree of initiative and solution orientation\n Fluent in Spanish and English, We are International Team!\nCompleted studies in computer science, information systems, mathematics or a comparable education\nExperience in design, implementation and maintenance of data-intensive applications and data pipelines\nStrong Python coding capabilities\nExperience in the end-to-end development of data-driven products\nKnowledge and experience in several of the following areas:\nData modeling and data management, databases and database queries (data warehouses, SQL, NoSQL, timeseries DBs)\nDistributed systems, ETL/ELT tools und common big data technologies (Spark/Databricks, Kafka)\nModern data infrastructures und data integration processes\nAnalytics/BI tools, ideally Power BI\nDeployments using CI/CD pipelines, ideally using Docker / Kubernetes\nUse of cloud-based platforms, ideally Azure and GCP\nExperience with agile software development methods, ideally Scrum and SAFe\nStrong conceptual and analytical skills\nHigh degree of initiative and solution orientation\nFluent in Spanish and English, We are International Team!\nWe are looking for you!\nInvent digital applications from scratch!\n Solve customer problems with the latest technology!\n Have freedom to innovate and make a difference!\n Bring product ideas to the customer to perfect them to the smallest detail!\nInvent digital applications from scratch!\nSolve customer problems with the latest technology!\nHave freedom to innovate and make a difference!\nBring product ideas to the customer to perfect them to the smallest detail!\nAttractive remuneration based on your experience, skills and development\n The opportunity to participate in the international projects and a significant influence on company IT development. Work in a high-tech environment with 5 hubs in Europe!\n Stable employment and social package (private medical care and other benefits)\n Extensive trainings in your area of responsibility\n Committed to work-life balance: Flexible hours and the possibility of intensive Fridays. Possibility of 50% remote work.\n Coffee and fruit available in the office\n Committed to Diversity and Society: Support for employees with children with disabilities.\nAttractive remuneration based on your experience, skills and development\nThe opportunity to participate in the international projects and a significant influence on company IT development. Work in a high-tech environment with 5 hubs in Europe!\nStable employment and social package (private medical care and other benefits)\nExtensive trainings in your area of responsibility\nCommitted to work-life balance: Flexible hours and the possibility of intensive Fridays. Possibility of 50% remote work.\nCoffee and fruit available in the office\nCommitted to Diversity and Society: Support for employees with children with disabilities.\nAbout The Company:"
    },
    "4175115790": {
        "title": "BACKEND & DATA ENGINEER - TELECOM ",
        "company": "Telef\u00f3nica",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nMain Activities and Responsabilities:\n\nCollect requirements from business layers and translate them into technical solutions.\nPropose solutions based on PoC techniques to validate the value they deliver.\nGather data from various sources deployed in different technologies and domain contexts.\nDeploy the data platform components within a cloud provider environment.\nOptimization of SQL Queries.\nOptimization of current solutions to improve FinOps.\nUpgrade current solutions to the latest stable version of the technology stack.\nIncidents resolution of the data platform.\nUnit, Integration and Functional Testing.\nMaintain continuous relationships with team members, Product Owners, Product Managers, and other related areas.\n\n\nAcademic Training :\n\nComputer Engineering, Telecommunications Engineering, Information Systems engineering or a relevant discipline.\n\n\nSpecific Skills:\n\nComplex problem-solving and analytical thinking.\nCollaboration and teamwork.\nExcellent written and verbal communication skills.\nCustomer and results orientation\nLeadership skills in a changing environment\nAbility to work under pressure and time constrains.\nAbility to self-train and jump on new technologies.\nAbility to transform and evolve.\nTelco business view\nFlexibility to work in an international environment with customers (Internal and External) in LATAM, EMEA and APAC.\nAble to work independently with initiative and proactivity.\n\n\nTechnical Knowledge:\n\nLinux\nGit\nPython\nApi Rest\nSQL\nNo SQL\nCloud Providers (AWS, GCP, AZURE, at least one of them)\nDocker or Kubernetes\nSolid Principles\nPort and Adapters (Hexagonal Architecture)\nTesting\n\nExperience working with Network OSS (VIPTELA, CISCO, FORTINET) would be an advantage.\n\nCron experience or other orchestation tools (AIRFLOW, PREFECT) would be an advantage.\n\nAWS Certifications, such as AWS Certified Solutions Architect, AWS Certified Developer would be an advantage.\n\nTelecom Operations knowledge would be an advantage.\n\n\n\nProfessional Experience:\nAt least 3 years as a backend position.\n\n\nLanguages: \n\nFluent in both spoken and written English\nFluent in both spoken and written Spanish\n\n\n\nIf you join Telef\u00f3nica\n\nYou join almost 100 years of history, you join a team of 106 nationalities present in more tan 35 countries. You join a team that works to connect people wherever they are. We are leading the digital revolution with the enthusiasm of the first day in all our businesses, creating the best digital ecosystem for our clients: network, IoT, cloud, security, innovation etc. Here, at Telef\u00f3nica you count with everything you need to be the best you. We need people like you that wants to take this challenge of creating the Telef\u00f3nica of tomorrow.\n\nTelefonica is committed to the new working ways. Under our proposition \u201cDisconnect to Reconnect\u201d we have applied the Digital Disconnection.\nYou join a company whose activity is governed by its code of ethics, Our Responsible Business Principles. We are looking for people who identify with them, who help us make decisions based on integrity, commitment and transparency and who are committed to ethical management, promoting fairer and more sustainable social and environmental development.\n\n#WeAreInclusive\n\nWe strongly believe diverse and inclusive teams become more innovative and they achieve improved results. Telefonica aims to promote and guarantee a place for everyone, just the way we are: gender, age range, sexual orientation and/or identity, culture, creed, disability or any other personal condition.\nMain Activities and Responsabilities:\nCollect requirements from business layers and translate them into technical solutions.\nPropose solutions based on PoC techniques to validate the value they deliver.\nGather data from various sources deployed in different technologies and domain contexts.\nDeploy the data platform components within a cloud provider environment.\nOptimization of SQL Queries.\nOptimization of current solutions to improve FinOps.\nUpgrade current solutions to the latest stable version of the technology stack.\nIncidents resolution of the data platform.\nUnit, Integration and Functional Testing.\nMaintain continuous relationships with team members, Product Owners, Product Managers, and other related areas.\nCollect requirements from business layers and translate them into technical solutions.\nPropose solutions based on PoC techniques to validate the value they deliver.\nGather data from various sources deployed in different technologies and domain contexts.\nDeploy the data platform components within a cloud provider environment.\nOptimization of SQL Queries.\nOptimization of current solutions to improve FinOps.\nUpgrade current solutions to the latest stable version of the technology stack.\nIncidents resolution of the data platform.\nUnit, Integration and Functional Testing.\nMaintain continuous relationships with team members, Product Owners, Product Managers, and other related areas.\nAcademic Training :\nComputer Engineering, Telecommunications Engineering, Information Systems engineering or a relevant discipline.\nSpecific Skills:\nComplex problem-solving and analytical thinking.\nCollaboration and teamwork.\nExcellent written and verbal communication skills.\nCustomer and results orientation\nLeadership skills in a changing environment\nAbility to work under pressure and time constrains.\nAbility to self-train and jump on new technologies.\nAbility to transform and evolve.\nTelco business view\nFlexibility to work in an international environment with customers (Internal and External) in LATAM, EMEA and APAC.\nAble to work independently with initiative and proactivity.\nComplex problem-solving and analytical thinking.\nCollaboration and teamwork.\nExcellent written and verbal communication skills.\nCustomer and results orientation\nLeadership skills in a changing environment\nAbility to work under pressure and time constrains.\nAbility to self-train and jump on new technologies.\nAbility to transform and evolve.\nTelco business view\nFlexibility to work in an international environment with customers (Internal and External) in LATAM, EMEA and APAC.\nAble to work independently with initiative and proactivity.\nTechnical Knowledge:\nLinux\nGit\nPython\nApi Rest\nSQL\nNo SQL\nCloud Providers (AWS, GCP, AZURE, at least one of them)\nDocker or Kubernetes\nSolid Principles\nPort and Adapters (Hexagonal Architecture)\nTesting\nLinux\nGit\nPython\nApi Rest\nSQL\nNo SQL\nCloud Providers (AWS, GCP, AZURE, at least one of them)\nDocker or Kubernetes\nSolid Principles\nPort and Adapters (Hexagonal Architecture)\nTesting\nExperience working with Network OSS (VIPTELA, CISCO, FORTINET) would be an advantage.\nCron experience or other orchestation tools (AIRFLOW, PREFECT) would be an advantage.\nAWS Certifications, such as AWS Certified Solutions Architect, AWS Certified Developer would be an advantage.\nTelecom Operations knowledge would be an advantage.\nProfessional Experience:\nAt least 3 years as a backend position.\nLanguages:\nFluent in both spoken and written English\nFluent in both spoken and written Spanish\nFluent in both spoken and written English\nFluent in both spoken and written Spanish\nIf you join Telef\u00f3nica\nYou join almost 100 years of history, you join a team of 106 nationalities present in more tan 35 countries. You join a team that works to connect people wherever they are. We are leading the digital revolution with the enthusiasm of the first day in all our businesses, creating the best digital ecosystem for our clients: network, IoT, cloud, security, innovation etc. Here, at Telef\u00f3nica you count with everything you need to be the best you. We need people like you that wants to take this challenge of creating the Telef\u00f3nica of tomorrow.\nTelefonica is committed to the new working ways. Under our proposition \u201cDisconnect to Reconnect\u201d we have applied the Digital Disconnection.\nYou join a company whose activity is governed by its code of ethics, Our Responsible Business Principles. We are looking for people who identify with them, who help us make decisions based on integrity, commitment and transparency and who are committed to ethical management, promoting fairer and more sustainable social and environmental development.\n#WeAreInclusive\nWe strongly believe diverse and inclusive teams become more innovative and they achieve improved results. Telefonica aims to promote and guarantee a place for everyone, just the way we are: gender, age range, sexual orientation and/or identity, culture, creed, disability or any other personal condition."
    },
    "4012335446": {
        "title": "Data Engineer ",
        "company": "Veeva Systems",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nVeeva Systems is a mission-driven organization and pioneer in industry cloud, helping life sciences companies bring therapies to patients faster. As one of the fastest-growing SaaS companies in history, we surpassed $2B in revenue in our last fiscal year with extensive growth potential ahead.\n\nAt the heart of Veeva are our values: Do the Right Thing, Customer Success, Employee Success, and Speed. We're not just any public company \u2013 we made history in 2021 by becoming a public benefit corporation (PBC), legally bound to balancing the interests of customers, employees, society, and investors.\n\nAs a Work Anywhere company, we support your flexibility to work from home or in the office, so you can thrive in your ideal environment.\n\nJoin us in transforming the life sciences industry, committed to making a positive impact on its customers, employees, and communities.\n\nThe Role\n\nVeeva Link supports the life sciences industry to connect with key people to improve research and care. It helps professionals to find the right people for, e.g., clinical trials, education programs, or advisory boards. This streamlined access helps to reduce the time-to-market of important drugs, conduct trials with the most relevant experts in the respective field, and spread information about new treatments to key people in the life science community. You can read more about Veeva Link on our product pages at https://www.veeva.com/products/veeva-link/.\n\nAs a data engineer, you focus on our data pipelines and take responsibility for a major part of the Link data processing platform. We value end-to-end ownership, which puts you into the sweet spot of finding, designing, and implementing improvements to the product's data pipelines and adjusting them to changing demands of the market.\n\nYou take responsibility for features and innovation using SOLID and clean software principles, take part in the architectural enhancement process and care for the quality of the outcome. Monitoring, metrics, and general observability are part of the feature design process.\n\nWe hire the same role for different engineering domains: Sourcing, Tagging, Matching, and Provisioning.\n\nDepending on the engineering domain, you will focus on different aspects. We decide together which domains fit best for you.\n\nWhat You'll Do\n\nWork on Veeva Link\u2019s next-gen Data Platform\nImprove our current environment with features, refactoring, and innovation\nWork with JVM-based languages or Python on Spark-based data pipelines\nOperate ML models in close cooperation with our data science team\nExperiment in your domain to improve precision, recall, or cost savings\n\nRequirements\n\nExpert skills in Java or Python\nExperience with Apache Spark or PySpark \nExperience writing software for the cloud (AWS or GCP)\nSpeaking and writing in English enables you to take part in day-to-day conversations in the team and contribute to deep technical discussions \n\nNice to Have\n\nExperience with operating machine learning models (e.g., MLFlow)\nExperience with Data Lakes, Lakehouses, and Warehouses (e.g., DeltaLake, Redshift)\nDevOps skills, including terraform and general CI/CD experience\nPreviously worked in agile environments\nExperience with expert systems\n\nPerks & Benefits\n\nComprehensive benefits package \nFitness reimbursement \nVeeva Work-Anywhere\n\n#RemoteSpain\n\nVeeva\u2019s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world.\n\nAs an equal opportunity employer, Veeva is committed to fostering a culture of inclusion and growing a diverse workforce. Diversity makes us stronger. It comes in many forms. Gender, race, ethnicity, religion, politics, sexual orientation, age, disability and life experience shape us all into unique individuals. We value people for the individuals they are and the contributions they can bring to our teams.\n\nIf you need assistance or accommodation due to a disability or special need when applying for a role or in our recruitment process, please contact us at talent_accommodations@veeva.com.\nThe Role\nWhat You'll Do\nWork on Veeva Link\u2019s next-gen Data Platform\nImprove our current environment with features, refactoring, and innovation\nWork with JVM-based languages or Python on Spark-based data pipelines\nOperate ML models in close cooperation with our data science team\nExperiment in your domain to improve precision, recall, or cost savings\nWork on Veeva Link\u2019s next-gen Data Platform\nImprove our current environment with features, refactoring, and innovation\nWork with JVM-based languages or Python on Spark-based data pipelines\nOperate ML models in close cooperation with our data science team\nExperiment in your domain to improve precision, recall, or cost savings\nRequirements\nExpert skills in Java or Python\nExperience with Apache Spark or PySpark \nExperience writing software for the cloud (AWS or GCP)\nSpeaking and writing in English enables you to take part in day-to-day conversations in the team and contribute to deep technical discussions\nExpert skills in Java or Python\nExperience with Apache Spark or PySpark\nExperience writing software for the cloud (AWS or GCP)\nSpeaking and writing in English enables you to take part in day-to-day conversations in the team and contribute to deep technical discussions\nNice to Have\nExperience with operating machine learning models (e.g., MLFlow)\nExperience with Data Lakes, Lakehouses, and Warehouses (e.g., DeltaLake, Redshift)\nDevOps skills, including terraform and general CI/CD experience\nPreviously worked in agile environments\nExperience with expert systems\nExperience with operating machine learning models (e.g., MLFlow)\nExperience with Data Lakes, Lakehouses, and Warehouses (e.g., DeltaLake, Redshift)\nDevOps skills, including terraform and general CI/CD experience\nPreviously worked in agile environments\nExperience with expert systems\nPerks & Benefits\nComprehensive benefits package \nFitness reimbursement \nVeeva Work-Anywhere\nComprehensive benefits package\nFitness reimbursement\nVeeva Work-Anywhere"
    },
    "4122316886": {
        "title": "Data Operations Engineer Specialist ",
        "company": "Clarity AI",
        "location": "Valencia, Valencian Community, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nAbout Clarity AI \ud83e\udeb4\nClarity AI is a global tech company founded in 2017 with a unique mission: bringing societal impact to markets.\n\nWe leverage AI and machine learning technologies to provide top international investors, governments, companies, and consumers with the right data, methodologies, and tools to make more informed decisions.\n\nWe are now a team of more than 300 highly passionate and curious individuals from all over the world, with offices in New York, Madrid, London, Paris, and Abu Dhabi. Together, we have established Clarity AI as a leading sustainability tech AI company backed by investors and strategic partners such as BlackRock, SoftBank, and Deutsche B\u00f6rse, who believe in us and share our goals.\n\nWe are dedicated to cultivating an exceptional workplace environment, and we take pride in our culture, defined by our commitment to being fact-based, diverse, transparent, meritocratic, and flexible. \n\nWe have plans to continue growing our teams globally, so if you would like to join us on this rocket ship, keep reading! Your work will shape and guide the sustainable decisions of investors, companies and consumers worldwide.\n\nAbout The Role \ud83d\udcbb\nWe are looking for a Data Operations Engineer Specialist to join our Data Strategy team. If you\u2019re a Data Engineer and you enjoy leveraging code to quickly adapt to changing needs in an operative environment, or if you are a Data Analyst who can code in python to solve operative issues, we are looking for you.\n\nYou will be part of the Data Strategy team in charge of growing our Sustainability database, allowing us to enhance the productivity of the research team and reduce the operation burden of the development team. You will interact with the development team as well as with the research team.\n\nWhat You\u2019ll Be Doing \ud83d\ude80\nWorking as a Data Operations Engineer Specialist you will be responsible for:\nConsulting data and using your coding skills (Python and SQL) to manage data from different data sources (S3, postgreSQL, etc.) in order to define the information to retrieve in our collection efforts\nCreating dynamic data assets required for different use cases using DBT and Airflow for analytic consumption\nCreating automated and refreshable dashboards and reports for different stakeholders using your SQL and Python coding skills and using tools and services such as DBT and Quicksight\nDeveloping process operations and process automations using orchestration tools like Airflow, APIs and overall best practices in streamlining business processes\nDelivering data through existing pipelines, or supporting the creation of new pipelines while including necessary soda validation checks\nServing and explaining data to non-technical and technical teams\nDeveloping solution architecture diagrams and supporting the build using best practices\nOffering technical support to business stakeholders around their day to day operational work\nModifying and creating new ETLs helped by the data-engineering team (we use Airflow to manage ETLs)\nSupporting the configuration/integration of third party tools and work on web scraping activities and tasks\nWriting maintainable code with tests, applying version control, debugging correctly, and uploading it to Gitlab\nInterfacing with the Data Science team on serving data for modeling and predictions and helping deliver high quality data based on business rules to specific data stores\n\nLocation \ud83c\udf0d\nThe role is based in Madrid, Barcelona, or Spain (remote)\nWay of Working: Remote / Hybrid\n\nWhat You\u2019ll Need \ud83d\udc40\n3+ years industry experience in the data analytics and data engineering field\nYou hold a degree in a quantitative field (CS, Engineer, Math, Physics, \u2026 ) and have great coding skills.\nYou possess very strong SQL, Python database skills, and can make invocations to APIs\nYou are very familiar with using tools such as SnowFlake, Airflow, AWS Services (ie. S3, Quicksight), and DBT\nExperience in advanced analytics, data engineering and a similar related field of data management\nExperience in collaborating with engineering teams and stakeholders to build key datasets and executing data pipelines using SQL/Python/ETL frameworks\nCan design data models and are familiar with best practices around data design and architecture\nCan work well with the Business and Technology stakeholders to deliver business initiatives on time by understanding the business need\nSelf-starter, able to take ownership and initiative, with high energy and stamina\nDecisive and action-oriented, able to make rapid decisions even when they are short of information\nHighly motivated, independent and deeply passionate about sustainability and impact\nExcellent oral and written English communication skills (minimum C1 level-proficient user)\n\nNice To Have \u2728\nExperience in the financial sector\nKnowledge of sustainability aspects\n\nWhat we offer \ud83e\udd41\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events\nAbout Clarity AI \ud83e\udeb4\nAbout Clarity AI\nClarity AI is a global tech company founded in 2017 with a unique mission: bringing societal impact to markets.\nbringing societal impact to markets.\nWe leverage AI and machine learning technologies to provide top international investors, governments, companies, and consumers with the right data, methodologies, and tools to make more informed decisions.\ntop international\ninvestors, governments, companies, and consumers\nWe are now a team of more than 300 highly passionate and curious individuals from all over the world, with offices in New York, Madrid, London, Paris, and Abu Dhabi. Together, we have established Clarity AI as a leading sustainability tech AI company backed by investors and strategic partners such as BlackRock, SoftBank, and Deutsche B\u00f6rse, who believe in us and share our goals.\n300 highly passionate and curious individuals\nleading sustainability tech AI company\nBlackRock, SoftBank, and Deutsche B\u00f6rse\nWe are dedicated to cultivating an exceptional workplace environment, and we take pride in our culture, defined by our commitment to being fact-based, diverse, transparent, meritocratic, and flexible.\nfact-based, diverse, transparent, meritocratic, and flexible.\nWe have plans to continue growing our teams globally, so if you would like to join us on this rocket ship, keep reading! Your work will shape and guide the sustainable decisions of investors, companies and consumers worldwide.\nAbout The Role \ud83d\udcbb\nAbout The Role\nWe are looking for a Data Operations Engineer Specialist to join our Data Strategy team. If you\u2019re a Data Engineer and you enjoy leveraging code to quickly adapt to changing needs in an operative environment, or if you are a Data Analyst who can code in python to solve operative issues, we are looking for you.\nData Operations Engineer Specialist\nYou will be part of the Data Strategy team in charge of growing our Sustainability database, allowing us to enhance the productivity of the research team and reduce the operation burden of the development team. You will interact with the development team as well as with the research team.\nWhat You\u2019ll Be Doing \ud83d\ude80\nWhat You\u2019ll Be Doing\nWorking as a Data Operations Engineer Specialist you will be responsible for:\nConsulting data and using your coding skills (Python and SQL) to manage data from different data sources (S3, postgreSQL, etc.) in order to define the information to retrieve in our collection efforts\nCreating dynamic data assets required for different use cases using DBT and Airflow for analytic consumption\nCreating automated and refreshable dashboards and reports for different stakeholders using your SQL and Python coding skills and using tools and services such as DBT and Quicksight\nDeveloping process operations and process automations using orchestration tools like Airflow, APIs and overall best practices in streamlining business processes\nDelivering data through existing pipelines, or supporting the creation of new pipelines while including necessary soda validation checks\nServing and explaining data to non-technical and technical teams\nDeveloping solution architecture diagrams and supporting the build using best practices\nOffering technical support to business stakeholders around their day to day operational work\nModifying and creating new ETLs helped by the data-engineering team (we use Airflow to manage ETLs)\nSupporting the configuration/integration of third party tools and work on web scraping activities and tasks\nWriting maintainable code with tests, applying version control, debugging correctly, and uploading it to Gitlab\nInterfacing with the Data Science team on serving data for modeling and predictions and helping deliver high quality data based on business rules to specific data stores\nConsulting data and using your coding skills (Python and SQL) to manage data from different data sources (S3, postgreSQL, etc.) in order to define the information to retrieve in our collection efforts\nCreating dynamic data assets required for different use cases using DBT and Airflow for analytic consumption\nCreating automated and refreshable dashboards and reports for different stakeholders using your SQL and Python coding skills and using tools and services such as DBT and Quicksight\nDeveloping process operations and process automations using orchestration tools like Airflow, APIs and overall best practices in streamlining business processes\nDelivering data through existing pipelines, or supporting the creation of new pipelines while including necessary soda validation checks\nServing and explaining data to non-technical and technical teams\nDeveloping solution architecture diagrams and supporting the build using best practices\nOffering technical support to business stakeholders around their day to day operational work\nModifying and creating new ETLs helped by the data-engineering team (we use Airflow to manage ETLs)\nSupporting the configuration/integration of third party tools and work on web scraping activities and tasks\nWriting maintainable code with tests, applying version control, debugging correctly, and uploading it to Gitlab\nInterfacing with the Data Science team on serving data for modeling and predictions and helping deliver high quality data based on business rules to specific data stores\nLocation \ud83c\udf0d\nLocation\nThe role is based in Madrid, Barcelona, or Spain (remote)\nWay of Working: Remote / Hybrid\nWhat You\u2019ll Need \ud83d\udc40\nWhat You\u2019ll Need\n3+ years industry experience in the data analytics and data engineering field\nYou hold a degree in a quantitative field (CS, Engineer, Math, Physics, \u2026 ) and have great coding skills.\nYou possess very strong SQL, Python database skills, and can make invocations to APIs\nYou are very familiar with using tools such as SnowFlake, Airflow, AWS Services (ie. S3, Quicksight), and DBT\nExperience in advanced analytics, data engineering and a similar related field of data management\nExperience in collaborating with engineering teams and stakeholders to build key datasets and executing data pipelines using SQL/Python/ETL frameworks\nCan design data models and are familiar with best practices around data design and architecture\nCan work well with the Business and Technology stakeholders to deliver business initiatives on time by understanding the business need\nSelf-starter, able to take ownership and initiative, with high energy and stamina\nDecisive and action-oriented, able to make rapid decisions even when they are short of information\nHighly motivated, independent and deeply passionate about sustainability and impact\nExcellent oral and written English communication skills (minimum C1 level-proficient user)\n3+ years industry experience in the data analytics and data engineering field\nYou hold a degree in a quantitative field (CS, Engineer, Math, Physics, \u2026 ) and have great coding skills.\nYou possess very strong SQL, Python database skills, and can make invocations to APIs\nYou are very familiar with using tools such as SnowFlake, Airflow, AWS Services (ie. S3, Quicksight), and DBT\nExperience in advanced analytics, data engineering and a similar related field of data management\nExperience in collaborating with engineering teams and stakeholders to build key datasets and executing data pipelines using SQL/Python/ETL frameworks\nCan design data models and are familiar with best practices around data design and architecture\nCan work well with the Business and Technology stakeholders to deliver business initiatives on time by understanding the business need\nSelf-starter, able to take ownership and initiative, with high energy and stamina\nDecisive and action-oriented, able to make rapid decisions even when they are short of information\nHighly motivated, independent and deeply passionate about sustainability and impact\nExcellent oral and written English communication skills (minimum C1 level-proficient user)\nNice To Have \u2728\nExperience in the financial sector\nKnowledge of sustainability aspects\nExperience in the financial sector\nKnowledge of sustainability aspects\nWhat we offer \ud83e\udd41\nWhat we offer\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events"
    },
    "4149624277": {
        "title": "Data Engineer en Ufinet \ud83c\udf0e",
        "company": "Candee (by VIKO)",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nDesde Candee, Consultor\u00eda de Talento, estamos gestionando esta posici\u00f3n de Data Engineer para Ufinet.\n\n\u00bfQu\u00e9 es UFINET? \ud83d\udd0e\nUfinet es un operador mayorista de fibra \u00f3ptica neutral de telecomunicaciones.\nBrindan conectividad de datos, capacidad e internet por cable \u00f3ptico principalmente a LATAM. Proporcionan servicios de conectividad a sus clientes, con amplia cobertura de red y gran capilaridad en las principales ciudades de los 17 pa\u00edses d\u00f3nde operan desde hace m\u00e1s de 24 a\u00f1os.\n\n\u00bfQu\u00e9 hace que UFINET sea un sitio \u00fanico? \ud83d\ude0d\nSeg\u00fan nos ha contado el equipo de UFINET lo mejor de trabajar aqu\u00ed es:\n\ud83c\udf0e Trabajo en equipo: Mucho compa\u00f1erismo y respeto entre todos. Adem\u00e1s de mucho apoyo entre todos los miembros del equipo.\n\ud83c\udf0e Crecimiento: Empresa con gran crecimiento, lo que ayuda a potenciar la carrera profesional.\n\ud83c\udf0e Aprendizaje: Oportunidad de aprender constantemente nuevas tecnolog\u00edas y habilidades dando soluciones a proyectos y problemas reales.\n\n\u00bfC\u00f3mo ser\u00e1 tu equipo? \ud83d\udc6f\nFormar\u00e1s parte del equipo de Software y Data, bajo el liderazgo de su Chief Data Officer. El equipo tiene una l\u00ednea de Data Analyst y una l\u00ednea de Data Engineering liderada por sus respectivos Team Leads y dentro de los cu\u00e1les los perfiles est\u00e1n especializados en las diferentes \u00e1reas de negocio.\n\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \ud83d\ude80\nTu misi\u00f3n ser\u00e1 la de realizar la construcci\u00f3n de pipeline de datos; soporte y monitorizaci\u00f3n de las cargas diarias, ejecuci\u00f3n de campa\u00f1as del proceso de calidad, enriquecimiento de datos y test plans. Tus funciones consistir\u00e1n en:\nPlanificar, monitorizar y revisar procesamiento diario de procesos planificados: procesos batch y procesos real time.\nDiagnosticar, y solucionar problemas de carga de datos.\nSoporte en Cargas de Datos.\nExtracci\u00f3n, limpieza, preparaci\u00f3n y estructuraci\u00f3n de datos de diferentes sistemas.\nConstrucci\u00f3n de conjuntos de datos consolidados de diferentes fuentes.\nDise\u00f1ar, desarrollar y desplegar estructuras de datos para consumo de sistemas anal\u00edticos.\nParticipaci\u00f3n en la estrategia de gobernanza, calidad y almacenamiento de datos.\nParticipaci\u00f3n en la estrategia del ciclo CI/CD.\n\n\u00bfQu\u00e9 est\u00e1n buscando? \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\nM\u00e1s de 2 a\u00f1os de experiencia en roles con responsabilidades similares.\nTengas estudios de ingenier\u00eda de sistemas / matem\u00e1ticas o similares.\nTengas experiencia avanzada en SQL, Azure Data Factory, Azure Functions (se valora el conocimiento del servicio independientemente del lenguaje) y Git. \nQue est\u00e9s familiarizado/a con la Metodolog\u00eda Scrum.\nQue seas una persona con buenas habilidades de comunicaci\u00f3n, que sepa trabajar en equipo.\nSon una compa\u00f1\u00eda en constante cambio, por lo que ser\u00e1 imprescindible que seas una persona flexible y que se adapte al cambio.\nEsperamos que seas una persona proactiva, con ganas de aprender y mejorar de manera continua.\nOrientaci\u00f3n a resultados.\nValoraremos positivamente que... \nCuentes con alguno/s de estos conocimientos: Azure Synapse y/o Azure Blob Storage.\nOrientaci\u00f3n a Programaci\u00f3n Funcional.\nConocimientos de DevOps (Boards, Repo, Pipeline y Releases y Test Plans).\nTe defiendas con el ingl\u00e9s (B2).\n\n\u00bfQu\u00e9 ofrecen? \ud83c\udf6c\nSalario competitivo.\n3 d\u00edas de teletrabajo y 2 en oficina por semana.\nOficinas situadas en el barrio de Salamanca, Madrid.\nHorario flexible: Entrada de 8h a 11h y horario de salida en consecuencia.\nMes de Agosto con Jornada Intensiva y Reducida. \nTicket Restaurant.\nSeguro M\u00e9dico con Adeslas.\nFormaci\u00f3n a demanda en funci\u00f3n de tu inter\u00e9s/necesidades. Son una compa\u00f1\u00eda abierta a sugerencias de formaci\u00f3n, recursos y tecnolog\u00eda. No cuentan con restricciones en este aspecto.\n\n\u00bfC\u00f3mo ser\u00e1 tu proceso de selecci\u00f3n? \ud83d\udcc6\nAntes de nada, aplica a la vacante \ud83d\ude42.\nEntrevista semi-estructurada: Si cumples los requisitos, tu Talent Agent se pondr\u00e1 en contacto contigo para que puedas resolver dudas sobre la posici\u00f3n y para empezar a conocerte m\u00e1s all\u00e1 de tu CV.\nCaso pr\u00e1ctico: En caso que ambas partes consider\u00e9is que hay encaje, te enviaremos un caso pr\u00e1ctico para que podamos verte en acci\u00f3n\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb.\nEntrevista con Ufinet: Si has bordado el caso, agendaremos una entrevista con la compa\u00f1\u00eda para que puedas entrevistar a tus potenciales empleadores y para que ellos puedan entrevistarte a ti.\nDurante todo el proceso tendr\u00e1s contacto directo con tu Talent Agent para poder resolver cualquier duda o inquietud.\n\nSi has llegado hasta aqu\u00ed, puedes ser la persona que buscamos. \u00a1Conectemos!\nDesde Candee, Consultor\u00eda de Talento, estamos gestionando esta posici\u00f3n de Data Engineer para Ufinet.\nCandee\nData Engineer\nUfinet.\n\u00bfQu\u00e9 es UFINET? \ud83d\udd0e\nUfinet es un operador mayorista de fibra \u00f3ptica neutral de telecomunicaciones.\nUfinet\nBrindan conectividad de datos, capacidad e internet por cable \u00f3ptico principalmente a LATAM. Proporcionan servicios de conectividad a sus clientes, con amplia cobertura de red y gran capilaridad en las principales ciudades de los 17 pa\u00edses d\u00f3nde operan desde hace m\u00e1s de 24 a\u00f1os.\n\u00bfQu\u00e9 hace que UFINET sea un sitio \u00fanico? \ud83d\ude0d\nSeg\u00fan nos ha contado el equipo de UFINET lo mejor de trabajar aqu\u00ed es:\n\ud83c\udf0e Trabajo en equipo: Mucho compa\u00f1erismo y respeto entre todos. Adem\u00e1s de mucho apoyo entre todos los miembros del equipo.\nTrabajo en equipo:\n\ud83c\udf0e Crecimiento: Empresa con gran crecimiento, lo que ayuda a potenciar la carrera profesional.\nCrecimiento:\n\ud83c\udf0e Aprendizaje: Oportunidad de aprender constantemente nuevas tecnolog\u00edas y habilidades dando soluciones a proyectos y problemas reales.\n\ud83c\udf0e Aprendizaje:\n\u00bfC\u00f3mo ser\u00e1 tu equipo? \ud83d\udc6f\nFormar\u00e1s parte del equipo de Software y Data, bajo el liderazgo de su Chief Data Officer. El equipo tiene una l\u00ednea de Data Analyst y una l\u00ednea de Data Engineering liderada por sus respectivos Team Leads y dentro de los cu\u00e1les los perfiles est\u00e1n especializados en las diferentes \u00e1reas de negocio.\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \ud83d\ude80\nTu misi\u00f3n ser\u00e1 la de realizar la construcci\u00f3n de pipeline de datos; soporte y monitorizaci\u00f3n de las cargas diarias, ejecuci\u00f3n de campa\u00f1as del proceso de calidad, enriquecimiento de datos y test plans. Tus funciones consistir\u00e1n en:\nconstrucci\u00f3n de pipeline de datos; soporte y monitorizaci\u00f3n de las cargas diarias, ejecuci\u00f3n de campa\u00f1as del proceso de calidad, enriquecimiento de datos y test plans\nPlanificar, monitorizar y revisar procesamiento diario de procesos planificados: procesos batch y procesos real time.\nDiagnosticar, y solucionar problemas de carga de datos.\nSoporte en Cargas de Datos.\nExtracci\u00f3n, limpieza, preparaci\u00f3n y estructuraci\u00f3n de datos de diferentes sistemas.\nConstrucci\u00f3n de conjuntos de datos consolidados de diferentes fuentes.\nDise\u00f1ar, desarrollar y desplegar estructuras de datos para consumo de sistemas anal\u00edticos.\nParticipaci\u00f3n en la estrategia de gobernanza, calidad y almacenamiento de datos.\nParticipaci\u00f3n en la estrategia del ciclo CI/CD.\nPlanificar, monitorizar y revisar procesamiento diario de procesos planificados: procesos batch y procesos real time.\nDiagnosticar, y solucionar problemas de carga de datos.\nSoporte en Cargas de Datos.\nExtracci\u00f3n, limpieza, preparaci\u00f3n y estructuraci\u00f3n de datos de diferentes sistemas.\nConstrucci\u00f3n de conjuntos de datos consolidados de diferentes fuentes.\nDise\u00f1ar, desarrollar y desplegar estructuras de datos para consumo de sistemas anal\u00edticos.\nParticipaci\u00f3n en la estrategia de gobernanza, calidad y almacenamiento de datos.\nParticipaci\u00f3n en la estrategia del ciclo CI/CD.\n\u00bfQu\u00e9 est\u00e1n buscando? \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\nM\u00e1s de 2 a\u00f1os de experiencia en roles con responsabilidades similares.\nTengas estudios de ingenier\u00eda de sistemas / matem\u00e1ticas o similares.\nTengas experiencia avanzada en SQL, Azure Data Factory, Azure Functions (se valora el conocimiento del servicio independientemente del lenguaje) y Git. \nQue est\u00e9s familiarizado/a con la Metodolog\u00eda Scrum.\nQue seas una persona con buenas habilidades de comunicaci\u00f3n, que sepa trabajar en equipo.\nSon una compa\u00f1\u00eda en constante cambio, por lo que ser\u00e1 imprescindible que seas una persona flexible y que se adapte al cambio.\nEsperamos que seas una persona proactiva, con ganas de aprender y mejorar de manera continua.\nOrientaci\u00f3n a resultados.\nM\u00e1s de 2 a\u00f1os de experiencia en roles con responsabilidades similares.\nM\u00e1s de 2 a\u00f1os\nTengas estudios de ingenier\u00eda de sistemas / matem\u00e1ticas o similares.\ningenier\u00eda de sistemas / matem\u00e1ticas\nTengas experiencia avanzada en SQL, Azure Data Factory, Azure Functions (se valora el conocimiento del servicio independientemente del lenguaje) y Git.\nSQL, Azure Data Factory, Azure Functions (se valora el conocimiento del servicio independientemente del lenguaje) y Git.\nQue est\u00e9s familiarizado/a con la Metodolog\u00eda Scrum.\nQue seas una persona con buenas habilidades de comunicaci\u00f3n, que sepa trabajar en equipo.\ncomunicaci\u00f3n\ntrabajar en equipo.\nSon una compa\u00f1\u00eda en constante cambio, por lo que ser\u00e1 imprescindible que seas una persona flexible y que se adapte al cambio.\npersona flexible y que se adapte al cambio\nEsperamos que seas una persona proactiva, con ganas de aprender y mejorar de manera continua.\naprender y mejorar de manera continua\nOrientaci\u00f3n a resultados.\nValoraremos positivamente que...\nCuentes con alguno/s de estos conocimientos: Azure Synapse y/o Azure Blob Storage.\nOrientaci\u00f3n a Programaci\u00f3n Funcional.\nConocimientos de DevOps (Boards, Repo, Pipeline y Releases y Test Plans).\nTe defiendas con el ingl\u00e9s (B2).\nCuentes con alguno/s de estos conocimientos: Azure Synapse y/o Azure Blob Storage.\nOrientaci\u00f3n a Programaci\u00f3n Funcional.\nConocimientos de DevOps (Boards, Repo, Pipeline y Releases y Test Plans).\nTe defiendas con el ingl\u00e9s (B2).\n\u00bfQu\u00e9 ofrecen? \ud83c\udf6c\nSalario competitivo.\n3 d\u00edas de teletrabajo y 2 en oficina por semana.\nOficinas situadas en el barrio de Salamanca, Madrid.\nHorario flexible: Entrada de 8h a 11h y horario de salida en consecuencia.\nMes de Agosto con Jornada Intensiva y Reducida. \nTicket Restaurant.\nSeguro M\u00e9dico con Adeslas.\nFormaci\u00f3n a demanda en funci\u00f3n de tu inter\u00e9s/necesidades. Son una compa\u00f1\u00eda abierta a sugerencias de formaci\u00f3n, recursos y tecnolog\u00eda. No cuentan con restricciones en este aspecto.\nSalario competitivo.\n3 d\u00edas de teletrabajo y 2 en oficina por semana.\nOficinas situadas en el barrio de Salamanca, Madrid.\nOficinas\nHorario flexible: Entrada de 8h a 11h y horario de salida en consecuencia.\nflexible\nMes de Agosto con Jornada Intensiva y Reducida.\nAgosto\nJornada Intensiva y Reducida.\nTicket Restaurant.\nRestaurant\nSeguro M\u00e9dico con Adeslas.\nSeguro M\u00e9dico\nFormaci\u00f3n a demanda en funci\u00f3n de tu inter\u00e9s/necesidades. Son una compa\u00f1\u00eda abierta a sugerencias de formaci\u00f3n, recursos y tecnolog\u00eda. No cuentan con restricciones en este aspecto.\nFormaci\u00f3n\n\u00bfC\u00f3mo ser\u00e1 tu proceso de selecci\u00f3n? \ud83d\udcc6\n\u00bfC\u00f3mo ser\u00e1 tu proceso de selecci\u00f3n?\nAntes de nada, aplica a la vacante \ud83d\ude42.\nEntrevista semi-estructurada: Si cumples los requisitos, tu Talent Agent se pondr\u00e1 en contacto contigo para que puedas resolver dudas sobre la posici\u00f3n y para empezar a conocerte m\u00e1s all\u00e1 de tu CV.\nCaso pr\u00e1ctico: En caso que ambas partes consider\u00e9is que hay encaje, te enviaremos un caso pr\u00e1ctico para que podamos verte en acci\u00f3n\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb.\nEntrevista con Ufinet: Si has bordado el caso, agendaremos una entrevista con la compa\u00f1\u00eda para que puedas entrevistar a tus potenciales empleadores y para que ellos puedan entrevistarte a ti.\nDurante todo el proceso tendr\u00e1s contacto directo con tu Talent Agent para poder resolver cualquier duda o inquietud.\nAntes de nada, aplica a la vacante \ud83d\ude42.\nEntrevista semi-estructurada: Si cumples los requisitos, tu Talent Agent se pondr\u00e1 en contacto contigo para que puedas resolver dudas sobre la posici\u00f3n y para empezar a conocerte m\u00e1s all\u00e1 de tu CV.\nEntrevista semi-estructurada:\nCaso pr\u00e1ctico: En caso que ambas partes consider\u00e9is que hay encaje, te enviaremos un caso pr\u00e1ctico para que podamos verte en acci\u00f3n\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb.\nCaso pr\u00e1ctico:\nEntrevista con Ufinet: Si has bordado el caso, agendaremos una entrevista con la compa\u00f1\u00eda para que puedas entrevistar a tus potenciales empleadores y para que ellos puedan entrevistarte a ti.\nEntrevista con Ufinet:\nDurante todo el proceso tendr\u00e1s contacto directo con tu Talent Agent para poder resolver cualquier duda o inquietud.\nSi has llegado hasta aqu\u00ed, puedes ser la persona que buscamos. \u00a1Conectemos!\n\u00a1Conectemos!"
    },
    "3931414992": {
        "title": "Data Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\n\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca \n\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \u270d\n\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n? \ud83d\udc47\n\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada semestre, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario competitivo seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \u270d\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\ndesarrollo y ejecuci\u00f3n de diversos proyectos\ndefinici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nSQL\nETL\ndatawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nformaci\u00f3n interna y externa en las herramientas relevantes para el equipo y\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\ntendencias tecnol\u00f3gicas\nData & Analytics\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\ncultura de SDG y su unidad de trabajo\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\ndin\u00e1micas de compa\u00f1\u00eda y de equipo\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\nctividades operativas de la compa\u00f1\u00eda\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n? \ud83d\udc47\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nSQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\n\u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\nvisualizaci\u00f3n con foco en Tableau y PowerBI.\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada semestre, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario competitivo seg\u00fan tu formaci\u00f3n y experiencia\nSalario competitivo\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5"
    },
    "4169795011": {
        "title": "Senior Data Engineer ",
        "company": "Fever",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nBehind the user-friendly iOS and Android apps and webpage that work across the world is the engineering team. We are in charge of creating, developing, improving, and maintaining all Fever services so that more people can have an amazing experience. \n\nAbout the role\n\nYou\u2019ll be part of the Data organization, building and operating the core technologies that enable data scientists, analysts and the different business units to leverage rich data in efficient and innovative ways to generate impact and connect people to the most relevant real-world experiences.\nYou'll own critical data pipelines of our data warehouse and the resulting data products that are used daily across the company to inform all sorts of decisions and models.\nYou'll ideate and implement tools and processes that increase our ability to exploit our diverse sources of data to solve business problems, understand behaviors, \u2026\nYou'll work closely with other business units to understand the challenges they face and apply an engineering vision to create structured and scalable solutions to those challenges.\nYou'll contribute to the development of a complex data and software ecosystem using the latest technologies in the data and software engineering stack.\n\nOn your first month in Fever:\nYou will be fully integrated into the team. During this month you will have already participated in onboarding, pair programming, one to one, Scrum sessions, and you will have met the different departments at Fever\nYou will get familiar with Fever\u2019s tech stack and frameworks used to develop our data strategy\nYou will attend some of the Fever Original\u2019s experiences like Candlelight\n\nAfter 3 months in Fever:\nYou\u2019ll be able to come up with solutions to new difficult problems and you'll be generating impact and creating new business opportunities.\nYou\u2019ll have responsibilities and ownership over parts of our Data Warehouse or other critical tools.\nYou will participate in some of the hackdays or hackathons we organize with other teams, and you will mostly know everybody from the data and engineering communities.\n\nOn your 6th month in Fever:\nYou\u2019ll contribute to the overall health of our data ecosystem, improving performance, scalability, robustness, \u2026\nYou'll be able to identify gaps in our platforms and processes and be a champion for continuous improvement.\nYou\u2019ll be mentoring other new joiners to the team.\nYou will participate in some of the team buildings we organise for your team or the whole engineering team.\n\nKey responsibilities\nHave a data-oriented mindset to understand complex data assets and business challenges and use engineering skills to solve it.\nBuild trusted data assets that power Fever's decision making.\nBuild automatizations to create huge business opportunities.\nDesign, build and support modern and scalable data infrastructure, e.g.:\nWrite robust, maintainable code to orchestrate our ETL workflows and build data quality monitoring processes\nExtend our data APIs\nBuild data tools to make the company more data-driven\nUnderstand the technical trade offs of different solutions, implement them and make them scalable\nCollaborate with other engineers, and stakeholders to understand what data is required and how best to make it available in our Data Platform.\n\nAbout you\nYou have a strong background in at least two of the following areas: data engineering, business intelligence, software engineering.\nYou are an expert in Python3 and its data ecosystem\nYou have proven experience working with SQL languages.\nYou have worked with complex data pipelines.\nYou are smart, get stuff done, have great energy, and thrive in a fast paced environment\nYou are a collaborative team player with strong communication skills, adaptable to a multidisciplinary, international, and fast-paced environment.\nYou are proactive, driven, and bring positive energy to your work, thriving in dynamic settings.\nYou possess strong analytical and problem-solving abilities, backed by solid software engineering skills.\nYou are proficient in business English, ensuring clear and effective communication in a professional setting.\n\nIt would be a plus if you...\n\nCollaborated effectively in a multidisciplinary team, interacting with roles like data analysts, data scientists, marketing, and product managers to meet project goals and deliver actionable insights.\nWorked with scheduling and workflow orchestration tools, such as Airflow, or similar technologies, to manage data pipelines and automate tasks.\nWorked databases like Snowflake and PostgreSQL for data storage, retrieval, and management, ensuring efficient and accurate data handling.\nUtilized Business Intelligence (BI) tools, such as Metabase or Superset, for data visualization and reporting to support decision-making processes.\nIntegrated and interacted with APIs from popular marketing platforms (e.g., Facebook, Google, Instagram) to extract and process data relevant for analysis.\nDeveloped data-powered tools and applications, either as part of a professional setting or through personal projects, showcasing hands-on skills in practical data applications.\nGained familiarity with tools and processes designed to support reproducible, production-ready machine learning applications, contributing to ML workflows.\nAcquired knowledge of backend frameworks, including Django, and their use cases in data engineering and application development.\n\nBenefits & Perks\nAttractive compensation package consisting of base salary and the potential to earn a significant bonus for top performance.\nStock options.\nOpportunity to have a real impact in a high-growth global category leader\n40% discount on all Fever events and experiences\nHome office friendly\nResponsibility from day one and professional and personal growth\nGreat work environment with a young, international team of talented people to work with!\nHealth insurance and other benefits such as Flexible remuneration with a 100% tax exemption through Cobee.\nEnglish Lessons\nGympass Membership\nPossibility to receive in advance part of your salary by Payflow.\nBehind the user-friendly iOS and Android apps and webpage that work across the world is the engineering team. We are in charge of creating, developing, improving, and maintaining all Fever services so that more people can have an amazing experience.\nAbout the role\nYou\u2019ll be part of the Data organization, building and operating the core technologies that enable data scientists, analysts and the different business units to leverage rich data in efficient and innovative ways to generate impact and connect people to the most relevant real-world experiences.\nYou'll own critical data pipelines of our data warehouse and the resulting data products that are used daily across the company to inform all sorts of decisions and models.\nYou'll ideate and implement tools and processes that increase our ability to exploit our diverse sources of data to solve business problems, understand behaviors, \u2026\nYou'll work closely with other business units to understand the challenges they face and apply an engineering vision to create structured and scalable solutions to those challenges.\nYou'll contribute to the development of a complex data and software ecosystem using the latest technologies in the data and software engineering stack.\nYou\u2019ll be part of the Data organization, building and operating the core technologies that enable data scientists, analysts and the different business units to leverage rich data in efficient and innovative ways to generate impact and connect people to the most relevant real-world experiences.\nYou'll own critical data pipelines of our data warehouse and the resulting data products that are used daily across the company to inform all sorts of decisions and models.\nYou'll ideate and implement tools and processes that increase our ability to exploit our diverse sources of data to solve business problems, understand behaviors, \u2026\nYou'll work closely with other business units to understand the challenges they face and apply an engineering vision to create structured and scalable solutions to those challenges.\nYou'll contribute to the development of a complex data and software ecosystem using the latest technologies in the data and software engineering stack.\nOn your first month in Fever:\nYou will be fully integrated into the team. During this month you will have already participated in onboarding, pair programming, one to one, Scrum sessions, and you will have met the different departments at Fever\nYou will get familiar with Fever\u2019s tech stack and frameworks used to develop our data strategy\nYou will attend some of the Fever Original\u2019s experiences like Candlelight\nYou will be fully integrated into the team. During this month you will have already participated in onboarding, pair programming, one to one, Scrum sessions, and you will have met the different departments at Fever\nYou will get familiar with Fever\u2019s tech stack and frameworks used to develop our data strategy\nYou will attend some of the Fever Original\u2019s experiences like Candlelight\nAfter 3 months in Fever:\nYou\u2019ll be able to come up with solutions to new difficult problems and you'll be generating impact and creating new business opportunities.\nYou\u2019ll have responsibilities and ownership over parts of our Data Warehouse or other critical tools.\nYou will participate in some of the hackdays or hackathons we organize with other teams, and you will mostly know everybody from the data and engineering communities.\nYou\u2019ll be able to come up with solutions to new difficult problems and you'll be generating impact and creating new business opportunities.\nYou\u2019ll have responsibilities and ownership over parts of our Data Warehouse or other critical tools.\nYou will participate in some of the hackdays or hackathons we organize with other teams, and you will mostly know everybody from the data and engineering communities.\nOn your 6th month in Fever:\nYou\u2019ll contribute to the overall health of our data ecosystem, improving performance, scalability, robustness, \u2026\nYou'll be able to identify gaps in our platforms and processes and be a champion for continuous improvement.\nYou\u2019ll be mentoring other new joiners to the team.\nYou will participate in some of the team buildings we organise for your team or the whole engineering team.\nYou\u2019ll contribute to the overall health of our data ecosystem, improving performance, scalability, robustness, \u2026\nYou'll be able to identify gaps in our platforms and processes and be a champion for continuous improvement.\nYou\u2019ll be mentoring other new joiners to the team.\nYou will participate in some of the team buildings we organise for your team or the whole engineering team.\nKey responsibilities\nHave a data-oriented mindset to understand complex data assets and business challenges and use engineering skills to solve it.\nBuild trusted data assets that power Fever's decision making.\nBuild automatizations to create huge business opportunities.\nDesign, build and support modern and scalable data infrastructure, e.g.:\nWrite robust, maintainable code to orchestrate our ETL workflows and build data quality monitoring processes\nExtend our data APIs\nBuild data tools to make the company more data-driven\nUnderstand the technical trade offs of different solutions, implement them and make them scalable\nCollaborate with other engineers, and stakeholders to understand what data is required and how best to make it available in our Data Platform.\nHave a data-oriented mindset to understand complex data assets and business challenges and use engineering skills to solve it.\nBuild trusted data assets that power Fever's decision making.\nBuild automatizations to create huge business opportunities.\nDesign, build and support modern and scalable data infrastructure, e.g.:\nWrite robust, maintainable code to orchestrate our ETL workflows and build data quality monitoring processes\nExtend our data APIs\nBuild data tools to make the company more data-driven\nUnderstand the technical trade offs of different solutions, implement them and make them scalable\nCollaborate with other engineers, and stakeholders to understand what data is required and how best to make it available in our Data Platform.\nAbout you\nYou have a strong background in at least two of the following areas: data engineering, business intelligence, software engineering.\nYou are an expert in Python3 and its data ecosystem\nYou have proven experience working with SQL languages.\nYou have worked with complex data pipelines.\nYou are smart, get stuff done, have great energy, and thrive in a fast paced environment\nYou are a collaborative team player with strong communication skills, adaptable to a multidisciplinary, international, and fast-paced environment.\nYou are proactive, driven, and bring positive energy to your work, thriving in dynamic settings.\nYou possess strong analytical and problem-solving abilities, backed by solid software engineering skills.\nYou are proficient in business English, ensuring clear and effective communication in a professional setting.\nYou have a strong background in at least two of the following areas: data engineering, business intelligence, software engineering.\nYou are an expert in Python3 and its data ecosystem\nYou have proven experience working with SQL languages.\nYou have worked with complex data pipelines.\nYou are smart, get stuff done, have great energy, and thrive in a fast paced environment\nYou are a collaborative team player with strong communication skills, adaptable to a multidisciplinary, international, and fast-paced environment.\nYou are proactive, driven, and bring positive energy to your work, thriving in dynamic settings.\nYou possess strong analytical and problem-solving abilities, backed by solid software engineering skills.\nYou are proficient in business English, ensuring clear and effective communication in a professional setting.\nIt would be a plus if you...\nCollaborated effectively in a multidisciplinary team, interacting with roles like data analysts, data scientists, marketing, and product managers to meet project goals and deliver actionable insights.\nWorked with scheduling and workflow orchestration tools, such as Airflow, or similar technologies, to manage data pipelines and automate tasks.\nWorked databases like Snowflake and PostgreSQL for data storage, retrieval, and management, ensuring efficient and accurate data handling.\nUtilized Business Intelligence (BI) tools, such as Metabase or Superset, for data visualization and reporting to support decision-making processes.\nIntegrated and interacted with APIs from popular marketing platforms (e.g., Facebook, Google, Instagram) to extract and process data relevant for analysis.\nDeveloped data-powered tools and applications, either as part of a professional setting or through personal projects, showcasing hands-on skills in practical data applications.\nGained familiarity with tools and processes designed to support reproducible, production-ready machine learning applications, contributing to ML workflows.\nAcquired knowledge of backend frameworks, including Django, and their use cases in data engineering and application development.\nCollaborated effectively in a multidisciplinary team, interacting with roles like data analysts, data scientists, marketing, and product managers to meet project goals and deliver actionable insights.\nWorked with scheduling and workflow orchestration tools, such as Airflow, or similar technologies, to manage data pipelines and automate tasks.\nWorked databases like Snowflake and PostgreSQL for data storage, retrieval, and management, ensuring efficient and accurate data handling.\nUtilized Business Intelligence (BI) tools, such as Metabase or Superset, for data visualization and reporting to support decision-making processes.\nIntegrated and interacted with APIs from popular marketing platforms (e.g., Facebook, Google, Instagram) to extract and process data relevant for analysis.\nDeveloped data-powered tools and applications, either as part of a professional setting or through personal projects, showcasing hands-on skills in practical data applications.\nGained familiarity with tools and processes designed to support reproducible, production-ready machine learning applications, contributing to ML workflows.\nAcquired knowledge of backend frameworks, including Django, and their use cases in data engineering and application development.\nBenefits & Perks\nAttractive compensation package consisting of base salary and the potential to earn a significant bonus for top performance.\nStock options.\nOpportunity to have a real impact in a high-growth global category leader\n40% discount on all Fever events and experiences\nHome office friendly\nResponsibility from day one and professional and personal growth\nGreat work environment with a young, international team of talented people to work with!\nHealth insurance and other benefits such as Flexible remuneration with a 100% tax exemption through Cobee.\nEnglish Lessons\nGympass Membership\nPossibility to receive in advance part of your salary by Payflow.\nAttractive compensation package consisting of base salary and the potential to earn a significant bonus for top performance.\nStock options.\nOpportunity to have a real impact in a high-growth global category leader\n40% discount on all Fever events and experiences\nHome office friendly\nResponsibility from day one and professional and personal growth\nGreat work environment with a young, international team of talented people to work with!\nHealth insurance and other benefits such as Flexible remuneration with a 100% tax exemption through Cobee.\nEnglish Lessons\nGympass Membership\nPossibility to receive in advance part of your salary by Payflow."
    },
    "4157738648": {
        "title": "Data Engineer (AWS) ",
        "company": "FDS, A DXC Technology Company",
        "location": "Zaragoza, Aragon, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n*Se valoran candidatos a nivel nacional\n\n\u00bfQu\u00e9 esperamos de ti?\n\nFormaci\u00f3n de al menos Ciclo Formativo de Grado Superior en Inform\u00e1tica o similar.\nExperiencia profesional de 1 a\u00f1o en el desarrollo de soluciones de procesamiento de datos sobre AWS.\nBuena base de lenguaje SQL y BBDD.\nValorables conocimientos en entornos BI (herramientas ETL, limpieza de datos, modelado\u2026\n\n\n\u00bfQu\u00e9 tareas llevar\u00e1s a cabo?\n\nCodificaci\u00f3n y revisi\u00f3n y testeo de programas y aplicaciones de procesamiento y explotaci\u00f3n de datos.\nAn\u00e1lisis y resoluci\u00f3n de incidencias.\nDise\u00f1o, planificaci\u00f3n y pruebas del sistema con supervisi\u00f3n.\n\n\n\u00bfQuieres trabajar con nosotros en nuestro \u00e1rea de Analytics and Data Management? Ap\u00fantate a nuestras ofertas de trabajo.\n\n\n\u00bfQu\u00e9 te ofrecemos?\n\n\u00b7 Incorporaci\u00f3n a una compa\u00f1\u00eda l\u00edder a nivel internacional en servicios globales de IT, puntera en el sector y socialmente responsable.\n\u00b7 Integraci\u00f3n en un equipo de profesionales altamente cualificados, en proyectos innovadores, din\u00e1micos y con un excelente clima laboral.\n\u00b7 Formaci\u00f3n en nuevas tecnolog\u00edas y competencias a trav\u00e9s de plataformas internas de formaci\u00f3n.\n\u00b7 Horario flexible y jornada intensiva en verano, sujeto a proyecto\n\u00b7 Contrato indefinido\n\u00b7 Seguro de vida y accidentes\n\u00b7 Modelo de trabajo flexible.\n\nUbicaci\u00f3n : Preferencia de residencia en Zaragoza o alrededores, aunque se valoran candidatos que residan en otras provincias.\n*Se valoran candidatos a nivel nacional\n\u00bfQu\u00e9 esperamos de ti?\nFormaci\u00f3n de al menos Ciclo Formativo de Grado Superior en Inform\u00e1tica o similar.\nExperiencia profesional de 1 a\u00f1o en el desarrollo de soluciones de procesamiento de datos sobre AWS.\nBuena base de lenguaje SQL y BBDD.\nValorables conocimientos en entornos BI (herramientas ETL, limpieza de datos, modelado\u2026\nFormaci\u00f3n de al menos Ciclo Formativo de Grado Superior en Inform\u00e1tica o similar.\nExperiencia profesional de 1 a\u00f1o en el desarrollo de soluciones de procesamiento de datos sobre AWS.\nBuena base de lenguaje SQL y BBDD.\nValorables conocimientos en entornos BI (herramientas ETL, limpieza de datos, modelado\u2026\n\u00bfQu\u00e9 tareas llevar\u00e1s a cabo?\nCodificaci\u00f3n y revisi\u00f3n y testeo de programas y aplicaciones de procesamiento y explotaci\u00f3n de datos.\nAn\u00e1lisis y resoluci\u00f3n de incidencias.\nDise\u00f1o, planificaci\u00f3n y pruebas del sistema con supervisi\u00f3n.\nCodificaci\u00f3n y revisi\u00f3n y testeo de programas y aplicaciones de procesamiento y explotaci\u00f3n de datos.\nAn\u00e1lisis y resoluci\u00f3n de incidencias.\nDise\u00f1o, planificaci\u00f3n y pruebas del sistema con supervisi\u00f3n.\n\u00bfQuieres trabajar con nosotros en nuestro \u00e1rea de Analytics and Data Management? Ap\u00fantate a nuestras ofertas de trabajo.\n\u00bfQu\u00e9 te ofrecemos?\n\u00b7 Incorporaci\u00f3n a una compa\u00f1\u00eda l\u00edder a nivel internacional en servicios globales de IT, puntera en el sector y socialmente responsable.\n\u00b7 Integraci\u00f3n en un equipo de profesionales altamente cualificados, en proyectos innovadores, din\u00e1micos y con un excelente clima laboral.\n\u00b7 Formaci\u00f3n en nuevas tecnolog\u00edas y competencias a trav\u00e9s de plataformas internas de formaci\u00f3n.\n\u00b7 Horario flexible y jornada intensiva en verano, sujeto a proyecto\n\u00b7 Contrato indefinido\n\u00b7 Seguro de vida y accidentes\n\u00b7 Modelo de trabajo flexible.\nUbicaci\u00f3n : Preferencia de residencia en Zaragoza o alrededores, aunque se valoran candidatos que residan en otras provincias.\nUbicaci\u00f3n :"
    },
    "4024721358": {
        "title": "Senior Data Engineer ",
        "company": "EPAM Systems",
        "location": "M\u00e1laga, Andalusia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nDo you have a software engineering background and strong knowledge and experience in Data? Are you an open-minded professional with good English skills? If it sounds like you, this could be the perfect opportunity to join EPAM as a Senior Data Engineer.\n\nEPAM is shaping the digital future for Fortune 1000 companies, building complex solutions using modern technologies. We are looking for a Senior Data Engineer with an open-minded personality who can join our friendly environment and become a core contributor to our team of experts. The candidate needs to have experience in Databricks, Python and SQL to match this position.\n\nWe are looking for candidates from Malaga or ready to relocate. Successful candidate should be ready to work from office 4 days/week.\n\n\nResponsibilities\n\n\nDesign, develop, monitor, and operate data pipelines \nIntegrate high-quality datasets for analytical use-cases \nEnable other data teams to follow the client standards \nTesting (preparation and execution) \nMaintenance of existing data pipelines incl. alerting, bug fixing, etc. \n\n\nRequirements\n\n\nWide experience in Azure Basis, using Databricks \nDatabricks delta live / pipeline build experience \nStrong skills in Python Core, PySpark and SQL \nRESTful services concepts \nUnderstanding of CI/CD \nStrong communication skills \nCode management tools and experience of working from a checked-in code base \n\n\nNice to have\n\n\nTDD concepts awareness\n\n\nWe offer\n\n\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\n\n\nEPAM is a leading digital transformation services and product engineering company with over 52,650 EPAMers in more than 55 countries and regions. Since 1993, our multidisciplinary teams have been helping make the future real for our clients and communities around the world. In 2018, we opened an office in Spain that quickly grew to over 1,450 EPAMers distributed between the offices in M\u00e1laga and Madrid as well as remotely across the country. Here you will collaborate with multinational teams, contribute to numerous innovative projects, and have an opportunity to learn and grow continuously.\n\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nSenior Data Engineer\nWe are looking for candidates from Malaga or ready to relocate. Successful candidate should be ready to work from office 4 days/week.\nResponsibilities\nDesign, develop, monitor, and operate data pipelines \nIntegrate high-quality datasets for analytical use-cases \nEnable other data teams to follow the client standards \nTesting (preparation and execution) \nMaintenance of existing data pipelines incl. alerting, bug fixing, etc.\nDesign, develop, monitor, and operate data pipelines\nIntegrate high-quality datasets for analytical use-cases\nEnable other data teams to follow the client standards\nTesting (preparation and execution)\nMaintenance of existing data pipelines incl. alerting, bug fixing, etc.\nRequirements\nWide experience in Azure Basis, using Databricks \nDatabricks delta live / pipeline build experience \nStrong skills in Python Core, PySpark and SQL \nRESTful services concepts \nUnderstanding of CI/CD \nStrong communication skills \nCode management tools and experience of working from a checked-in code base\nWide experience in Azure Basis, using Databricks\nDatabricks delta live / pipeline build experience\nStrong skills in Python Core, PySpark and SQL\nRESTful services concepts\nUnderstanding of CI/CD\nStrong communication skills\nCode management tools and experience of working from a checked-in code base\nNice to have\nTDD concepts awareness\nWe offer\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends."
    },
    "4152975193": {
        "title": "Senior Data Engineer",
        "company": "Nextlane",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\ud83c\udf10 About Us\n\nAt Nextlane, we don\u2019t just develop software solutions \u2013 we create the future of the automotive industry. \n\nWe are a company that combines advanced technology with a clear vision: simplifying and digitizing every step of the automotive customer journey, empowering manufacturers and dealerships to thrive in a constantly evolving market.\n\nWe believe in the value of every team member, offering opportunities for you to develop and contribute to meaningful solutions.\nSo\u2026 What does it mean to be a #Nextlaner?\nBe part of a growth-oriented culture.\nCollaborate with colleagues from all over the world.\nBelieve in the Power of ideas and the diversity of thought. \nBe committed to provide an environment where you can learn, grow, and collaborate on projects that make a global impact. \n\nOur success is measured not just by results, but also by the growth and satisfaction of those who are part of our company. \nAt Nextlane, you\u2019ll have the opportunity to innovate, push boundaries, and work on solutions that are transforming the automotive world.\n\ud83c\udfaf Your Responsibilities:\n\n As a key member of the team, you will be responsible for:\nCollaborating in the design and implementation of next-generation data platforms based on Data Mesh architecture principles.\nLeading the development of data ingestion pipelines (batch and streaming) for structured and unstructured data sources.\nBuilding scalable, high-performance data storage solutions, including working with AWS services like S3, Glue, Redshift, and Athena.\nDeveloping ETL/ELT processes using AWS Glue, Apache Spark, and Databricks to support data processing at scale.\nOptimizing data systems for performance and cost using AWS best practices, such as partitioning, compression, and caching.\nImplementing robust security controls using AWS Lake Formation and IAM policies to ensure data privacy and compliance with regulations like GDPR and ISO27001.\n\n\ud83d\udd75\ufe0f\u200d\u2642\ufe0f What We're Looking For:\n\n \u2022 Experience: 6+ years of hands-on data engineering experience, preferably building large-scale data platforms in AWS.\n \u2022 Languages: Proficiency in English (Interview will be conducted in English).\n \u2022 Communication: Strong communication skills, both verbal and written.\n \u2022 Technical Skills:\nProficiency in AWS data services (Glue, S3, Redshift, Kinesis, Athena).\nAdvanced knowledge of Data Mesh architecture and domain-driven data products.\nExperience with data ingestion pipelines (batch and streaming).\nExpertise in data formats such as Parquet, ORC, and Iceberg.\nProficiency in Apache Spark (PySpark) and distributed SQL engines like Presto.\nExperience with orchestration tools like AWS Step Functions or Airflow.\n\nInterpersonal Skills: Ability to collaborate in a diverse and dynamic environment.\n\n\n\ud83e\udd1d Our Recruitment Process:\n15-minute introductory call with our Talent Acquisition Specialist.\n[Additional steps in the recruitment process]\n\n\ud83d\udc8e What We Offer:\nWe understand that flexibility and trust are essential for our teams. Here are some of the benefits we offer:\n\ud83c\udfe1 Remote Work: Up to 3 days a week! \nWork-life balance: We have flexible entrance and departure time. \nSummer schedule on Fridays: Start early your weekends! \n\ud83e\udd1d Referral Bonus: \u20ac1500 for every talent you refer (after 6 months). \n\ud83c\udf71 Flexible Remuneration: COBEE platform. \n\u2695\ufe0f Private Medical Insurance: Adeslas. \n\ud83d\udcda Continuous Learning: Access to our internal platform for continuous development.\n\ud83c\udf34 Vacation: 23 vacation days plus an intensive schedule in July and August. \nTeambuilding: Our afterworks and activities are memorable!\n\n\ud83c\udf0d Diversity, Inclusion & Belonging\nAt Nextlane, we are committed to creating a space where everyone feels valued and respected. We firmly believe that diversity in experiences and backgrounds strengthens our culture and drives innovation.\nSupport for people with disabilities. If you need any adjustments during the recruitment process, let us know so we can provide the best possible experience.\nEqual opportunities for all: We welcome applications regardless of age, gender, origin, disability, or any other characteristic protected by law.\n\nJoin Nextlane and become part of the technological revolution in the automotive industry. \nDiscover why we are a great place to develop your talent!\n\ud83c\udf10 About Us\nAt Nextlane, we don\u2019t just develop software solutions \u2013 we create the future of the automotive industry.\nNextlane\ncreate the future\nWe are a company that combines advanced technology with a clear vision: simplifying and digitizing every step of the automotive customer journey, empowering manufacturers and dealerships to thrive in a constantly evolving market.\nadvanced technology\nsimplifying and digitizing\nWe believe in the value of every team member, offering opportunities for you to develop and contribute to meaningful solutions.\ndevelop\nSo\u2026 What does it mean to be a #Nextlaner?\nBe part of a growth-oriented culture.\nCollaborate with colleagues from all over the world.\nBelieve in the Power of ideas and the diversity of thought. \nBe committed to provide an environment where you can learn, grow, and collaborate on projects that make a global impact.\nBe part of a growth-oriented culture.\ngrowth-oriented culture\nCollaborate with colleagues from all over the world.\ncolleagues from all over the world.\nBelieve in the Power of ideas and the diversity of thought.\nthe Power of ideas\ndiversity of thought\nBe committed to provide an environment where you can learn, grow, and collaborate on projects that make a global impact.\nlearn, grow, and collaborate\nOur success is measured not just by results, but also by the growth and satisfaction of those who are part of our company.\nAt Nextlane, you\u2019ll have the opportunity to innovate, push boundaries, and work on solutions that are transforming the automotive world.\ninnovate\npush boundaries\nwork on solutions that are transforming\n\ud83c\udfaf Your Responsibilities:\nYour Responsibilities:\nAs a key member of the team, you will be responsible for:\nCollaborating in the design and implementation of next-generation data platforms based on Data Mesh architecture principles.\nLeading the development of data ingestion pipelines (batch and streaming) for structured and unstructured data sources.\nBuilding scalable, high-performance data storage solutions, including working with AWS services like S3, Glue, Redshift, and Athena.\nDeveloping ETL/ELT processes using AWS Glue, Apache Spark, and Databricks to support data processing at scale.\nOptimizing data systems for performance and cost using AWS best practices, such as partitioning, compression, and caching.\nImplementing robust security controls using AWS Lake Formation and IAM policies to ensure data privacy and compliance with regulations like GDPR and ISO27001.\nCollaborating in the design and implementation of next-generation data platforms based on Data Mesh architecture principles.\nLeading the development of data ingestion pipelines (batch and streaming) for structured and unstructured data sources.\nBuilding scalable, high-performance data storage solutions, including working with AWS services like S3, Glue, Redshift, and Athena.\nDeveloping ETL/ELT processes using AWS Glue, Apache Spark, and Databricks to support data processing at scale.\nOptimizing data systems for performance and cost using AWS best practices, such as partitioning, compression, and caching.\nImplementing robust security controls using AWS Lake Formation and IAM policies to ensure data privacy and compliance with regulations like GDPR and ISO27001.\n\ud83d\udd75\ufe0f\u200d\u2642\ufe0f What We're Looking For:\nWhat We're Looking For:\n\u2022 Experience: 6+ years of hands-on data engineering experience, preferably building large-scale data platforms in AWS.\nExperience:\n\u2022 Languages: Proficiency in English (Interview will be conducted in English).\nLanguages:\n\u2022 Communication: Strong communication skills, both verbal and written.\nCommunication:\n\u2022 Technical Skills:\nTechnical Skills:\nProficiency in AWS data services (Glue, S3, Redshift, Kinesis, Athena).\nAdvanced knowledge of Data Mesh architecture and domain-driven data products.\nExperience with data ingestion pipelines (batch and streaming).\nExpertise in data formats such as Parquet, ORC, and Iceberg.\nProficiency in Apache Spark (PySpark) and distributed SQL engines like Presto.\nExperience with orchestration tools like AWS Step Functions or Airflow.\nProficiency in AWS data services (Glue, S3, Redshift, Kinesis, Athena).\nAdvanced knowledge of Data Mesh architecture and domain-driven data products.\nExperience with data ingestion pipelines (batch and streaming).\nExpertise in data formats such as Parquet, ORC, and Iceberg.\nProficiency in Apache Spark (PySpark) and distributed SQL engines like Presto.\nExperience with orchestration tools like AWS Step Functions or Airflow.\nInterpersonal Skills: Ability to collaborate in a diverse and dynamic environment.\nInterpersonal Skills:\n\ud83e\udd1d Our Recruitment Process:\n15-minute introductory call with our Talent Acquisition Specialist.\n[Additional steps in the recruitment process]\n15-minute introductory call with our Talent Acquisition Specialist.\n15-minute introductory call\n[Additional steps in the recruitment process]\n\ud83d\udc8e What We Offer:\nWe understand that flexibility and trust are essential for our teams. Here are some of the benefits we offer:\nflexibility and trust\n\ud83c\udfe1 Remote Work: Up to 3 days a week! \nWork-life balance: We have flexible entrance and departure time. \nSummer schedule on Fridays: Start early your weekends! \n\ud83e\udd1d Referral Bonus: \u20ac1500 for every talent you refer (after 6 months). \n\ud83c\udf71 Flexible Remuneration: COBEE platform. \n\u2695\ufe0f Private Medical Insurance: Adeslas. \n\ud83d\udcda Continuous Learning: Access to our internal platform for continuous development.\n\ud83c\udf34 Vacation: 23 vacation days plus an intensive schedule in July and August. \nTeambuilding: Our afterworks and activities are memorable!\n\ud83c\udfe1 Remote Work: Up to 3 days a week!\n\ud83c\udfe1 Remote Work:\nWork-life balance: We have flexible entrance and departure time.\nWork-life balance:\nSummer schedule on Fridays: Start early your weekends!\nSummer schedule on Fridays:\n\ud83e\udd1d Referral Bonus: \u20ac1500 for every talent you refer (after 6 months).\n\ud83e\udd1d Referral Bonus:\n\ud83c\udf71 Flexible Remuneration: COBEE platform.\n\ud83c\udf71 Flexible Remuneration:\n\u2695\ufe0f Private Medical Insurance: Adeslas.\n\u2695\ufe0f Private Medical Insurance:\n\ud83d\udcda Continuous Learning: Access to our internal platform for continuous development.\n\ud83d\udcda Continuous Learning:\n\ud83c\udf34 Vacation: 23 vacation days plus an intensive schedule in July and August.\n\ud83c\udf34 Vacation:\nTeambuilding: Our afterworks and activities are memorable!\nTeambuilding:\n\ud83c\udf0d Diversity, Inclusion & Belonging\nAt Nextlane, we are committed to creating a space where everyone feels valued and respected. We firmly believe that diversity in experiences and backgrounds strengthens our culture and drives innovation.\neveryone feels valued and respected\nSupport for people with disabilities. If you need any adjustments during the recruitment process, let us know so we can provide the best possible experience.\nEqual opportunities for all: We welcome applications regardless of age, gender, origin, disability, or any other characteristic protected by law.\nSupport for people with disabilities. If you need any adjustments during the recruitment process, let us know so we can provide the best possible experience.\nSupport for people with disabilities.\nEqual opportunities for all: We welcome applications regardless of age, gender, origin, disability, or any other characteristic protected by law.\nEqual opportunities for all:\nJoin Nextlane and become part of the technological revolution in the automotive industry.\nDiscover why we are a great place to develop your talent!"
    },
    "4173563437": {
        "title": "Data Engineer ",
        "company": "Inetum",
        "location": "Greater Madrid Metropolitan Area",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nInetum is an agile IT services company that provides digital services and solutions, and a global group that helps companies and institutions to get the most out of digital flow. In a context of perpetual movement, where needs and usages are constantly being reinvented, the Inetum group is committed to all these players to innovate, continue to adapt, and stay ahead. With its multi-expert profile, Inetum offers its clients a unique combination of proximity, a breakdown by sector, and state of the art solutions.\n\n\ud83d\udca1 We are looking for a Data Engineer! \ud83d\udca1\nAs a Data Engineer, you'll be responsible for building and maintaining data pipelines, processing information from various sources, and ensuring data quality and security. You will work with Spark on Scala, CI/CD tools (GitLab, Jenkins\u2026), and HDFS/SQL databases, among others.\n\n\ud83d\udccd Hybrid work model in the client's offices in Madrid.\n\n\n\ud83d\udd39 Key Responsibilities:\n\u2705 Develop and optimize data pipelines using Spark on Scala\n\u2705 Ensure data quality and consistency\n\u2705 Implement CI/CD pipelines for automation\n\u2705 Manage and improve orchestration with Apache Airflow\n\u2705 Support production, incident resolution, and continuous improvement\n\n\n\ud83d\udd39 Required Skills:\n\ud83d\udcbb Strong experience with Spark on Scala\n\ud83d\udd27 CI/CD tools (GitLab, Jenkins\u2026)\n\ud83d\udcc2 HDFS and structured databases (SQL)\n\ud83c\udf10 Knowledge of Apache Airflow\n\ud83d\udce1 Experience with streaming processes (Kafka, event stream\u2026)\n\ud83d\udde3\ufe0f English level B2 or higher\n\n\nIf you are passionate about data engineering and want to work in a dynamic environment, let\u2019s connect! \u2728 Join us! @Inetum. #WeAreInetum\nInetum is an agile IT services company that provides digital services and solutions, and a global group that helps companies and institutions to get the most out of digital flow. In a context of perpetual movement, where needs and usages are constantly being reinvented, the Inetum group is committed to all these players to innovate, continue to adapt, and stay ahead. With its multi-expert profile, Inetum offers its clients a unique combination of proximity, a breakdown by sector, and state of the art solutions.\nInetum\nis an agile IT services company that provides digital services and solutions, and a global group that helps companies and institutions to get the most out of digital flow. In a context of perpetual movement, where needs and usages are constantly being reinvented, the Inetum group is committed to all these players to innovate, continue to adapt, and stay ahead. With its multi-expert profile, Inetum offers its clients a unique combination of proximity, a breakdown by sector, and state of the art solutions.\n\ud83d\udca1 We are looking for a Data Engineer! \ud83d\udca1\nWe are looking for a Data Engineer!\nAs a Data Engineer, you'll be responsible for building and maintaining data pipelines, processing information from various sources, and ensuring data quality and security. You will work with Spark on Scala, CI/CD tools (GitLab, Jenkins\u2026), and HDFS/SQL databases, among others.\nSpark on Scala\nCI/CD tools (GitLab, Jenkins\u2026)\nHDFS/SQL databases\n\ud83d\udccd Hybrid work model in the client's offices in Madrid.\nHybrid work model in the client's offices in Madrid.\n\ud83d\udd39 Key Responsibilities:\nKey Responsibilities:\n\u2705 Develop and optimize data pipelines using Spark on Scala\n\u2705 Ensure data quality and consistency\n\u2705 Implement CI/CD pipelines for automation\n\u2705 Manage and improve orchestration with Apache Airflow\n\u2705 Support production, incident resolution, and continuous improvement\n\ud83d\udd39 Required Skills:\nRequired Skills:\n\ud83d\udcbb Strong experience with Spark on Scala\n\ud83d\udd27 CI/CD tools (GitLab, Jenkins\u2026)\n\ud83d\udcc2 HDFS and structured databases (SQL)\n\ud83c\udf10 Knowledge of Apache Airflow\n\ud83d\udce1 Experience with streaming processes (Kafka, event stream\u2026)\n\ud83d\udde3\ufe0f English level B2 or higher\nIf you are passionate about data engineering and want to work in a dynamic environment, let\u2019s connect! \u2728 Join us! @Inetum. #WeAreInetum\ndata engineering\n@Inetum"
    },
    "4152781622": {
        "title": "Data Engineer",
        "company": "Sumauto",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nVocento es uno de los principales grupos multimedia de Espa\u00f1a, dedicado a la actividad en medios de comunicaci\u00f3n, con posicionamiento relevante nacional y regional, y actividades adicionales de diversificaci\u00f3n, como la gastronom\u00eda que tiene un alcance internacional, as\u00ed como una s\u00f3lida red de clasificados en los sectores de motor e inmobiliario.\n\n\n\n\ud83d\ude80 \u00a1\u00daNETE A NUESTRO EQUIPO DE DATOS EN SUMAUTO! \ud83d\udcca\ud83d\udcbc\n\nComo Ingeniero/a de Datos, ser\u00e1s una pieza clave en nuestro equipo de Data para impulsar la eficiencia y escalabilidad de nuestros procesos y herramientas. Este rol es fundamental para el desarrollo de una infraestructura de datos s\u00f3lida que permita tener una visi\u00f3n 360\u00ba de nuestras operaciones. \u00a1Estamos buscando a alguien como t\u00fa! \ud83c\udf1f\n\n\n\n\u00bfQU\u00c9 FUNCIONES VAS A DESEMPE\u00d1AR? \n\n\ud83d\udd27 Dise\u00f1o y desarrollo de pipelines de datos: Crear, gestionar y optimizar pipelines robustos que aseguren el procesamiento eficiente de grandes vol\u00famenes de datos.\n\n\ud83d\udd04 Gesti\u00f3n de flujos de datos: Asegurar la integraci\u00f3n, disponibilidad y consistencia de los datos en todas nuestras plataformas.\n\n\u26a1 Optimizaci\u00f3n en BigQuery y manejo de Big Data:Trabajar en entornos de datos a gran escala utilizando BigQuery para optimizar consultas y an\u00e1lisis masivos.\n\n\ud83e\udd1d Colaboraci\u00f3n interdepartamental: Trabajar de cerca con otros equipos como ciencia de datos, an\u00e1lisis de negocio, y m\u00e1s para crear soluciones de datos personalizadas.\n\n\ud83d\udca1 Mejoras de eficiencia en datos: Identificar y recomendar mejoras en la infraestructura de datos que optimicen el uso de datos en distintas \u00e1reas de la empresa.\n\n\ud83d\udcda Evangelizar la cultura del dato: Promover el uso de datos para la toma de decisiones a trav\u00e9s de proyectos concretos.\n\n\n\n\n\u00bfQU\u00c9 BUSCAMOS EN T\u00cd? \n\n\ud83c\udf93 Estudios: \nGrado o FP en inform\u00e1tica, ingenier\u00eda de software, ciencia de datos, matem\u00e1ticas, o afines.\n\n\ud83d\udccc Experiencia t\u00e9cnica:\nMindset \u00e1gil: Conocimiento en metodolog\u00edas \u00e1giles como SCRUM o Kanban.\nExperiencia en Ingenier\u00eda de Datos: Conocimientos s\u00f3lidos en el desarrollo y mantenimiento de pipelines de datos a gran escala.\nPython avanzado: Experiencia en el procesamiento y automatizaci\u00f3n de flujos de datos.\nBigQuery y GCP (Google Cloud Platform): Experiencia en la optimizaci\u00f3n de datos y an\u00e1lisis de grandes vol\u00famenes.\nManejo de Big Data: Experiencia en el procesamiento y an\u00e1lisis de grandes vol\u00famenes de datos.\n\n\ud83c\udfaf Habilidades personales:\nCapacidad anal\u00edtica y atenci\u00f3n al detalle\nExcelentes habilidades interpersonales y capacidad de comunicaci\u00f3n.\nTrabajo en equipo en un entorno din\u00e1mico y en constante cambio.\nExcelentes habilidades organizativas y capacidad para trabajar bajo presi\u00f3n.\nIngl\u00e9s avanzado tanto hablado como escrito.\n\n\n\u2b50 Valoraremos positivamente que tengas...\nConocimientos en ciencia de datos.\nExperiencia en el sector de la automoci\u00f3n o en marketplaces y negocios digitales.\nEres tenaz, entusiasta, auto-motivado/a y orientado/a a resultados.\nMentalidad de crecimiento y mejora continua.\n\n\n\u00bfQU\u00c9 TE OFRECEMOS?\n\n\u2728 Estabilidad: Contrato indefinido.\n\n\ud83d\udc69\u200d\ud83d\udcbb Ambiente laboral:\nEquipo unido, colaborativo y en constante mejora.\nOficinas abiertas y excelente ambiente.\n\n\ud83d\udcda Formaci\u00f3n continua:\nFormaci\u00f3n interna y charlas tecnol\u00f3gicas.\nPosibilidades de aprendizaje constante.\n\n\ud83d\udcc5 Flexibilidad:\nHorario flexible de entrada y salida.\nJornada intensiva en julio y agosto.\nTrabajo h\u00edbrido..\n\n\ud83d\udca1 Beneficios:\nRetribuci\u00f3n flexible: mutua, ticket restaurant, guarder\u00eda, transporte.\nWeb descuentos en hoteles, cine, teatro, ropa... \u00a1s\u00f3lo por ser empleado/a de Vocento!\nGimnasio subvencionado en la oficina o acceso Gimpass/Wellhub\n\nSi tienes experiencia en ingenier\u00eda de datos y te apasiona el sector del motor, la tecnolog\u00eda y la innovaci\u00f3n, \u00a1nos encantar\u00eda saber de ti!\n\n\nEn virtud del Reglamento General de Protecci\u00f3n de Datos, le informamos de que la sociedad a cuya vacante se postula usted como candidato es la responsable del tratamiento de los datos de car\u00e1cter personal que se traten con objeto de llevar a cabo el proceso de selecci\u00f3n. El tratamiento se realiza en base al inter\u00e9s leg\u00edtimo de la sociedad. Sus datos podr\u00e1n ser comunicados a Administraciones p\u00fablicas en cumplimiento de la legislaci\u00f3n aplicable, y a prestadores de servicios tecnol\u00f3gicos y de selecci\u00f3n. Usted tiene derecho a acceder, rectificar y suprimir sus datos, entre otros.\nVocento es uno de los principales grupos multimedia de Espa\u00f1a, dedicado a la actividad en medios de comunicaci\u00f3n, con posicionamiento relevante nacional y regional, y actividades adicionales de diversificaci\u00f3n, como la gastronom\u00eda que tiene un alcance internacional, as\u00ed como una s\u00f3lida red de clasificados en los sectores de motor e inmobiliario.\nVocento\n\ud83d\ude80 \u00a1\u00daNETE A NUESTRO EQUIPO DE DATOS EN SUMAUTO! \ud83d\udcca\ud83d\udcbc\n\ud83d\ude80 \u00a1\u00daNETE A NUESTRO EQUIPO DE DATOS EN SUMAUTO!\nComo Ingeniero/a de Datos, ser\u00e1s una pieza clave en nuestro equipo de Data para impulsar la eficiencia y escalabilidad de nuestros procesos y herramientas. Este rol es fundamental para el desarrollo de una infraestructura de datos s\u00f3lida que permita tener una visi\u00f3n 360\u00ba de nuestras operaciones. \u00a1Estamos buscando a alguien como t\u00fa! \ud83c\udf1f\nIngeniero/a de Datos\nData\neficiencia\nescalabilidad\nvisi\u00f3n 360\u00ba\n\u00bfQU\u00c9 FUNCIONES VAS A DESEMPE\u00d1AR?\n\ud83d\udd27 Dise\u00f1o y desarrollo de pipelines de datos: Crear, gestionar y optimizar pipelines robustos que aseguren el procesamiento eficiente de grandes vol\u00famenes de datos.\n\ud83d\udd27 Dise\u00f1o y desarrollo de pipelines de datos\n\ud83d\udd04 Gesti\u00f3n de flujos de datos: Asegurar la integraci\u00f3n, disponibilidad y consistencia de los datos en todas nuestras plataformas.\n\ud83d\udd04 Gesti\u00f3n de flujos de datos\n\u26a1 Optimizaci\u00f3n en BigQuery y manejo de Big Data:Trabajar en entornos de datos a gran escala utilizando BigQuery para optimizar consultas y an\u00e1lisis masivos.\n\u26a1 Optimizaci\u00f3n en BigQuery y manejo de Big Data\n\ud83e\udd1d Colaboraci\u00f3n interdepartamental: Trabajar de cerca con otros equipos como ciencia de datos, an\u00e1lisis de negocio, y m\u00e1s para crear soluciones de datos personalizadas.\n\ud83e\udd1d Colaboraci\u00f3n interdepartamental\n\ud83d\udca1 Mejoras de eficiencia en datos: Identificar y recomendar mejoras en la infraestructura de datos que optimicen el uso de datos en distintas \u00e1reas de la empresa.\n\ud83d\udca1 Mejoras de eficiencia en datos\n\ud83d\udcda Evangelizar la cultura del dato: Promover el uso de datos para la toma de decisiones a trav\u00e9s de proyectos concretos.\n\ud83d\udcda Evangelizar la cultura del dato\n\u00bfQU\u00c9 BUSCAMOS EN T\u00cd?\n\ud83c\udf93 Estudios:\nEstudios:\nGrado o FP en inform\u00e1tica, ingenier\u00eda de software, ciencia de datos, matem\u00e1ticas, o afines.\n\ud83d\udccc Experiencia t\u00e9cnica:\nExperiencia t\u00e9cnica\nMindset \u00e1gil: Conocimiento en metodolog\u00edas \u00e1giles como SCRUM o Kanban.\nExperiencia en Ingenier\u00eda de Datos: Conocimientos s\u00f3lidos en el desarrollo y mantenimiento de pipelines de datos a gran escala.\nPython avanzado: Experiencia en el procesamiento y automatizaci\u00f3n de flujos de datos.\nBigQuery y GCP (Google Cloud Platform): Experiencia en la optimizaci\u00f3n de datos y an\u00e1lisis de grandes vol\u00famenes.\nManejo de Big Data: Experiencia en el procesamiento y an\u00e1lisis de grandes vol\u00famenes de datos.\nMindset \u00e1gil: Conocimiento en metodolog\u00edas \u00e1giles como SCRUM o Kanban.\nMindset \u00e1gil\nSCRUM\nKanban\nExperiencia en Ingenier\u00eda de Datos: Conocimientos s\u00f3lidos en el desarrollo y mantenimiento de pipelines de datos a gran escala.\nExperiencia en Ingenier\u00eda de Datos\nPython avanzado: Experiencia en el procesamiento y automatizaci\u00f3n de flujos de datos.\nPython avanzado\nBigQuery y GCP (Google Cloud Platform): Experiencia en la optimizaci\u00f3n de datos y an\u00e1lisis de grandes vol\u00famenes.\nBigQuery y GCP (Google Cloud Platform)\nManejo de Big Data: Experiencia en el procesamiento y an\u00e1lisis de grandes vol\u00famenes de datos.\nManejo de Big Data\n\ud83c\udfaf Habilidades personales:\nHabilidades personales\nCapacidad anal\u00edtica y atenci\u00f3n al detalle\nExcelentes habilidades interpersonales y capacidad de comunicaci\u00f3n.\nTrabajo en equipo en un entorno din\u00e1mico y en constante cambio.\nExcelentes habilidades organizativas y capacidad para trabajar bajo presi\u00f3n.\nIngl\u00e9s avanzado tanto hablado como escrito.\nCapacidad anal\u00edtica y atenci\u00f3n al detalle\nExcelentes habilidades interpersonales y capacidad de comunicaci\u00f3n.\nTrabajo en equipo en un entorno din\u00e1mico y en constante cambio.\nExcelentes habilidades organizativas y capacidad para trabajar bajo presi\u00f3n.\nIngl\u00e9s avanzado tanto hablado como escrito.\nIngl\u00e9s avanzado\n\u2b50 Valoraremos positivamente que tengas...\nConocimientos en ciencia de datos.\nExperiencia en el sector de la automoci\u00f3n o en marketplaces y negocios digitales.\nEres tenaz, entusiasta, auto-motivado/a y orientado/a a resultados.\nMentalidad de crecimiento y mejora continua.\nConocimientos en ciencia de datos.\nciencia de datos\nExperiencia en el sector de la automoci\u00f3n o en marketplaces y negocios digitales.\nExperiencia en el sector de la automoci\u00f3n\nmarketplaces\nnegocios digitales\nEres tenaz, entusiasta, auto-motivado/a y orientado/a a resultados.\nMentalidad de crecimiento y mejora continua.\n\u00bfQU\u00c9 TE OFRECEMOS?\n\u2728 Estabilidad: Contrato indefinido.\nEstabilidad\n\ud83d\udc69\u200d\ud83d\udcbb Ambiente laboral:\nAmbiente laboral\nEquipo unido, colaborativo y en constante mejora.\nOficinas abiertas y excelente ambiente.\nEquipo unido, colaborativo y en constante mejora.\nOficinas abiertas y excelente ambiente.\n\ud83d\udcda Formaci\u00f3n continua:\nFormaci\u00f3n continua\nFormaci\u00f3n interna y charlas tecnol\u00f3gicas.\nPosibilidades de aprendizaje constante.\nFormaci\u00f3n interna y charlas tecnol\u00f3gicas.\nPosibilidades de aprendizaje constante.\n\ud83d\udcc5 Flexibilidad:\nFlexibilidad\nHorario flexible de entrada y salida.\nJornada intensiva en julio y agosto.\nTrabajo h\u00edbrido..\nHorario flexible de entrada y salida.\nJornada intensiva en julio y agosto.\nTrabajo h\u00edbrido..\n\ud83d\udca1 Beneficios:\nBeneficios\nRetribuci\u00f3n flexible: mutua, ticket restaurant, guarder\u00eda, transporte.\nWeb descuentos en hoteles, cine, teatro, ropa... \u00a1s\u00f3lo por ser empleado/a de Vocento!\nGimnasio subvencionado en la oficina o acceso Gimpass/Wellhub\nRetribuci\u00f3n flexible: mutua, ticket restaurant, guarder\u00eda, transporte.\nWeb descuentos en hoteles, cine, teatro, ropa... \u00a1s\u00f3lo por ser empleado/a de Vocento!\nGimnasio subvencionado en la oficina o acceso Gimpass/Wellhub\nSi tienes experiencia en ingenier\u00eda de datos y te apasiona el sector del motor, la tecnolog\u00eda y la innovaci\u00f3n, \u00a1nos encantar\u00eda saber de ti!\nEn virtud del Reglamento General de Protecci\u00f3n de Datos, le informamos de que la sociedad a cuya vacante se postula usted como candidato es la responsable del tratamiento de los datos de car\u00e1cter personal que se traten con objeto de llevar a cabo el proceso de selecci\u00f3n. El tratamiento se realiza en base al inter\u00e9s leg\u00edtimo de la sociedad. Sus datos podr\u00e1n ser comunicados a Administraciones p\u00fablicas en cumplimiento de la legislaci\u00f3n aplicable, y a prestadores de servicios tecnol\u00f3gicos y de selecci\u00f3n. Usted tiene derecho a acceder, rectificar y suprimir sus datos, entre otros."
    },
    "4163514879": {
        "title": "Senior Data Engineer - EY GDS Spain - Hybrid ",
        "company": "EY",
        "location": "M\u00e1laga, Andalusia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nData Engineer \n\nAs a Data Engineer you will oversee data ingestion processes and deployment of ML/IA solutions for predictive analysis for an energy sector customer. You will collaborate with Data Scientist and Data Engineers to achieve project and client goals on a very dynamic environment.\n\nThe opportunity\n\nAs a member of our team in GDS office in Malaga, Spain, you\u2019ll have a chance to extend your knowledge & experience by working on the interesting projects with the newest technologies and approaches. You\u2019ll support clients in choosing the most suitable business solution and take part in digital transformation.\n\nYour Key Responsibilities\n\nAs a Data Engineer, you will work on the design of end-to-end pipelines for enterprise data loading from different sources, deployment of ML/IA solutions for predictive analysis and improvements on quality of data.\n\nYou will also collaborate with the product development teams (business analysts, product owners, developers) to deliver high quality solutions for our customers.\n\nTo qualify for the role, you must have\n\nDegree on Statistics, Maths, Informatics or similar.\n3-4 years of experience as Data Engineer\nExperience in Azure: Databricks, DataFactory, Synapse\nDatabase knowledge, SQL, OLTP/OLAP\nExperience of Big Data (i.e Spark, Hadoop)\nVery good communication skills in terms of capturing requirements, describing architecture, data flows, etc.\n\nIdeally, you\u2019ll also have\n\nKnowledge of cloud solutions\nExperience on Agile and SCRUM methodology.\n\nWhat We Look For\n\nWe are a dynamic team of passionate specialists, working in international teams all over the world. As part of the Data team, we look for \u201cout of the box thinking\" professionals that will build good relationships with our clients. If you want to be a part of this journey, we are looking forward to seeing you on board!\n\nWhat We Offer\n\nEY Global Delivery Services (GDS) is a dynamic and truly global delivery network. We work across eight locations \u2013 Argentina, China, India, Philippines, Poland, UK, Hungary and Spain \u2013 and with teams from all EY service lines, geographies and sectors, playing a vital role in the delivery of the EY growth strategy. From accountants to coders to advisory consultants, we offer a wide variety of fulfilling career opportunities that span all business disciplines. In GDS, you will collaborate with EY teams on exciting projects and work with well-known brands from across the globe. We\u2019ll introduce you to an ever-expanding ecosystem of people, learning, skills and insights that will stay with you throughout your career. \n\nContinuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next. \nSuccess as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way. \nTransformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs. \nDiverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs. \n\nAbout EY\n\nEY | Building a better working world\n\nEY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets.\n\nEnabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate.\n\nWorking across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.\n\nIf you can demonstrate that you meet the criteria above, please contact us as soon as possible.\nData Engineer\nThe opportunity\nYour Key Responsibilities\nTo qualify for the role, you must have\nDegree on Statistics, Maths, Informatics or similar.\n3-4 years of experience as Data Engineer\nExperience in Azure: Databricks, DataFactory, Synapse\nDatabase knowledge, SQL, OLTP/OLAP\nExperience of Big Data (i.e Spark, Hadoop)\nVery good communication skills in terms of capturing requirements, describing architecture, data flows, etc.\nDegree on Statistics, Maths, Informatics or similar.\n3-4 years of experience as Data Engineer\nExperience in Azure: Databricks, DataFactory, Synapse\nDatabase knowledge, SQL, OLTP/OLAP\nExperience of Big Data (i.e Spark, Hadoop)\nVery good communication skills in terms of capturing requirements, describing architecture, data flows, etc.\nIdeally, you\u2019ll also have\nKnowledge of cloud solutions\nExperience on Agile and SCRUM methodology.\nKnowledge of cloud solutions\nExperience on Agile and SCRUM methodology.\nWhat We Look For\nWhat We Offer\nEY Global Delivery Services (GDS) is a dynamic and truly global delivery network. We work across eight locations \u2013 Argentina, China, India, Philippines, Poland, UK, Hungary and Spain \u2013 and with teams from all EY service lines, geographies and sectors, playing a vital role in the delivery of the EY growth strategy. From accountants to coders to advisory consultants, we offer a wide variety of fulfilling career opportunities that span all business disciplines. In GDS, you will collaborate with EY teams on exciting projects and work with well-known brands from across the globe. We\u2019ll introduce you to an ever-expanding ecosystem of people, learning, skills and insights that will stay with you throughout your career.\nContinuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next. \nSuccess as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way. \nTransformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs. \nDiverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.\nContinuous learning: You\u2019ll develop the mindset and skills to navigate whatever comes next.\nSuccess as defined by you: We\u2019ll provide the tools and flexibility, so you can make a meaningful impact, your way.\nTransformative leadership: We\u2019ll give you the insights, coaching and confidence to be the leader the world needs.\nDiverse and inclusive culture: You\u2019ll be embraced for who you are and empowered to use your voice to help others find theirs.\nAbout EY\nEY | Building a better working world\nIf you can demonstrate that you meet the criteria above, please contact us as soon as possible."
    },
    "4038805663": {
        "title": "Data Engineer - OpenData Commercial ",
        "company": "Veeva Systems",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nVeeva Systems is a mission-driven organization and pioneer in industry cloud, helping life sciences companies bring therapies to patients faster. As one of the fastest-growing SaaS companies in history, we surpassed $2B in revenue in our last fiscal year with extensive growth potential ahead.\n\nAt the heart of Veeva are our values: Do the Right Thing, Customer Success, Employee Success, and Speed. We're not just any public company \u2013 we made history in 2021 by becoming a public benefit corporation (PBC), legally bound to balancing the interests of customers, employees, society, and investors.\n\nAs a Work Anywhere company, we support your flexibility to work from home or in the office, so you can thrive in your ideal environment.\n\nJoin us in transforming the life sciences industry, committed to making a positive impact on its customers, employees, and communities.\n\nThe Role\n\nVeeva OpenData Commercial supports the industry by providing reference data across the complete healthcare ecosystem to support commercial sales execution, compliance, and business analytics. We drive value to our customers through constant innovation, using cloud-based solutions and state-of-the-art technologies to deliver product excellence and customer success. The Data Platform Engineering team delivers the tools and data processing pipelines to build the global data core for life sciences in 100+ countries.\n\nWorking as a data engineer in our global Data Platform Engineering team at OpenData, you will drive impactful change by harmonizing data operations globally and enhancing data quality. Your responsibilities will include creating the tools and processes necessary to efficiently store, manage, and compile data, directly contributing to the success of OpenData.\n\nYou must be based in Spain and already hold legal work authorization, as Veeva does not sponsor employment visa processes for this role.\n\nWhat You'll Do\n\nBuild data tools to streamline data operations globally\nCollaborate with cross-functional teams to deliver data solutions\nContribute to the Data Engineering community at OpenData to influence tooling and standards to improve data quality and productivity\nDevelop a data validation framework to automate quality control processes\n\nRequirements\n\nOver 3+ years of hands-on Python development experience for production systems \nProficient in data cloud computing technologies (EMR, Databricks) within leading cloud platforms (AWS, Google Cloud, or Azure)\nSkilled in designing and implementing data pipelines using distributed storage platforms \nDemonstrated analytical skills and structured approach to software design\nHave a strong intrinsic desire to learn and fill in missing skills\nHighly proactive finding ways to overcome challenges\n\nNice to Have\n\nProficiency in Git, including knowledge of branching strategies, merge mechanisms, and repository management\nDemonstrated ability to build and maintain test automation frameworks and CI/CD pipelines\nExperience with Polars\n\nPerks & Benefits\n\nBenefits package including Restricted Stock Units (RSUs), family health insurance, and contributions to private pension plans\nFitness reimbursement\nWork anywhere\n\n#RemoteSpain\n\nVeeva\u2019s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world.\n\nAs an equal opportunity employer, Veeva is committed to fostering a culture of inclusion and growing a diverse workforce. Diversity makes us stronger. It comes in many forms. Gender, race, ethnicity, religion, politics, sexual orientation, age, disability and life experience shape us all into unique individuals. We value people for the individuals they are and the contributions they can bring to our teams.\n\nIf you need assistance or accommodation due to a disability or special need when applying for a role or in our recruitment process, please contact us at talent_accommodations@veeva.com.\nThe Role\nWhat You'll Do\nBuild data tools to streamline data operations globally\nCollaborate with cross-functional teams to deliver data solutions\nContribute to the Data Engineering community at OpenData to influence tooling and standards to improve data quality and productivity\nDevelop a data validation framework to automate quality control processes\nBuild data tools to streamline data operations globally\nCollaborate with cross-functional teams to deliver data solutions\nContribute to the Data Engineering community at OpenData to influence tooling and standards to improve data quality and productivity\nDevelop a data validation framework to automate quality control processes\nRequirements\nOver 3+ years of hands-on Python development experience for production systems \nProficient in data cloud computing technologies (EMR, Databricks) within leading cloud platforms (AWS, Google Cloud, or Azure)\nSkilled in designing and implementing data pipelines using distributed storage platforms \nDemonstrated analytical skills and structured approach to software design\nHave a strong intrinsic desire to learn and fill in missing skills\nHighly proactive finding ways to overcome challenges\nOver 3+ years of hands-on Python development experience for production systems\nProficient in data cloud computing technologies (EMR, Databricks) within leading cloud platforms (AWS, Google Cloud, or Azure)\nSkilled in designing and implementing data pipelines using distributed storage platforms\nDemonstrated analytical skills and structured approach to software design\nHave a strong intrinsic desire to learn and fill in missing skills\nHighly proactive finding ways to overcome challenges\nNice to Have\nProficiency in Git, including knowledge of branching strategies, merge mechanisms, and repository management\nDemonstrated ability to build and maintain test automation frameworks and CI/CD pipelines\nExperience with Polars\nProficiency in Git, including knowledge of branching strategies, merge mechanisms, and repository management\nDemonstrated ability to build and maintain test automation frameworks and CI/CD pipelines\nExperience with Polars\nPerks & Benefits\nBenefits package including Restricted Stock Units (RSUs), family health insurance, and contributions to private pension plans\nFitness reimbursement\nWork anywhere\nBenefits package including Restricted Stock Units (RSUs), family health insurance, and contributions to private pension plans\nFitness reimbursement\nWork anywhere"
    },
    "3982411695": {
        "title": "Spark Data Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\n\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\n\ud83d\udd04 Desarrollar\u00e1s soluciones de datos end-to-end, desde el entendimiento del problema hasta la puesta en producci\u00f3n.\n\n\ud83d\udca1 Aportar\u00e1s tu visi\u00f3n t\u00e9cnica en el dise\u00f1o de arquitecturas y procesos, participando en debates y decisiones clave.\n\n\ud83d\udccc Definir\u00e1s e implementar\u00e1s estrategias de modelado y transformaci\u00f3n de datos seg\u00fan las necesidades del proyecto.\n\n\ud83d\udee0\ufe0f Colaborar\u00e1s en la toma de requerimientos para arquitecturas y pipelines de datos.\n\n\ud83e\uddea Probar\u00e1s nuevas tecnolog\u00edas y servicios cloud, aportando innovaci\u00f3n a nuestros proyectos.\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n?\n\n\ud83d\udcda Formaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o cualquier titulaci\u00f3n t\u00e9cnica relacionada con la ingenier\u00eda del software.\n\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Engineer, desarrollando soluciones de ingesta y transformaci\u00f3n de datos en entornos on-premise o cloud.\n\n\ud83c\udfd7\ufe0f Experiencia en arquitectura de datos, trabajando con estructuras, particionado, modelado y ciclo de vida en Data Lake.\n\n\ud83c\udf0d Ingl\u00e9s para trabajar en un entorno global.\n\n\nValoramos tambi\u00e9n:\n\n\u2601\ufe0f Plataforma: Experiencia en plataformas modernas como Databricks, Dataproc o EMR; o en plataformas tradicionales como Cloudera o Stratio.\n\n\u2699\ufe0f Procesos dirigidos por metadatos.\n\n\ud83d\udd04 Integraci\u00f3n de datos: Frameworks de procesamiento Spark o Beam y lenguajes Python, Scala o Java.\n\n\ud83d\udee0\ufe0f Orquestaci\u00f3n: Experiencia con Apache Airflow o Databricks Workflows.\n\n\ud83d\udcdd Gesti\u00f3n del c\u00f3digo: Buenas pr\u00e1cticas y metodolog\u00edas.\n\n\ud83d\udcca Familiaridad con conceptos como DataOps, Data Observability y Data Mesh.\n\n\ud83d\udcbc Experiencia en consultor\u00eda dentro del mundo Data & Analytics.\n\n\ud83c\udf93 Formaci\u00f3n extra como un M\u00e1ster en Big Data & Analytics u otras certificaciones que potencien tus habilidades.\n\n\ud83d\udee0\ufe0f Conocimiento en tecnolog\u00edas como SQL, Snowflake, dbt, Azure Data Factory o Microsoft Fabric.\n\n\ud83d\udce6 Experiencia con contenedores y orquestaci\u00f3n.\n\n\u2601\ufe0f Cloud & Automatizaci\u00f3n: Infraestructura, pipelines de datos, operaci\u00f3n y monitorizaci\u00f3n.\n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\u2705 Acceso a formaciones y certificaciones\n\n\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\ud83d\udd04 Desarrollar\u00e1s soluciones de datos end-to-end, desde el entendimiento del problema hasta la puesta en producci\u00f3n.\nDesarrollar\u00e1s soluciones de datos end-to-end\n\ud83d\udca1 Aportar\u00e1s tu visi\u00f3n t\u00e9cnica en el dise\u00f1o de arquitecturas y procesos, participando en debates y decisiones clave.\nAportar\u00e1s tu visi\u00f3n t\u00e9cnica\n\ud83d\udccc Definir\u00e1s e implementar\u00e1s estrategias de modelado y transformaci\u00f3n de datos seg\u00fan las necesidades del proyecto.\nDefinir\u00e1s e implementar\u00e1s estrategias\n\ud83d\udee0\ufe0f Colaborar\u00e1s en la toma de requerimientos para arquitecturas y pipelines de datos.\nColaborar\u00e1s en la toma de requerimientos\n\ud83e\uddea Probar\u00e1s nuevas tecnolog\u00edas y servicios cloud, aportando innovaci\u00f3n a nuestros proyectos.\nProbar\u00e1s nuevas tecnolog\u00edas y servicios cloud\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n?\n\ud83d\udcda Formaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o cualquier titulaci\u00f3n t\u00e9cnica relacionada con la ingenier\u00eda del software.\nFormaci\u00f3n\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Engineer, desarrollando soluciones de ingesta y transformaci\u00f3n de datos en entornos on-premise o cloud.\n+4 a\u00f1os de experiencia\nData Engineer\ningesta y transformaci\u00f3n de datos\non-premise o cloud\n\ud83c\udfd7\ufe0f Experiencia en arquitectura de datos, trabajando con estructuras, particionado, modelado y ciclo de vida en Data Lake.\nExperiencia en arquitectura de datos\nData Lake\n\ud83c\udf0d Ingl\u00e9s para trabajar en un entorno global.\nIngl\u00e9s\nValoramos tambi\u00e9n:\n\u2601\ufe0f Plataforma: Experiencia en plataformas modernas como Databricks, Dataproc o EMR; o en plataformas tradicionales como Cloudera o Stratio.\nPlataforma\nDatabricks, Dataproc o EMR\nCloudera o Stratio\n\u2699\ufe0f Procesos dirigidos por metadatos.\nProcesos dirigidos por metadatos\n\ud83d\udd04 Integraci\u00f3n de datos: Frameworks de procesamiento Spark o Beam y lenguajes Python, Scala o Java.\nIntegraci\u00f3n de datos\nSpark o Beam\nPython, Scala o Java.\n\ud83d\udee0\ufe0f Orquestaci\u00f3n: Experiencia con Apache Airflow o Databricks Workflows.\nOrquestaci\u00f3n\nApache Airflow o Databricks Workflows\n\ud83d\udcdd Gesti\u00f3n del c\u00f3digo: Buenas pr\u00e1cticas y metodolog\u00edas.\nGesti\u00f3n del c\u00f3digo\n\ud83d\udcca Familiaridad con conceptos como DataOps, Data Observability y Data Mesh.\nDataOps, Data Observability y Data Mesh\n\ud83d\udcbc Experiencia en consultor\u00eda dentro del mundo Data & Analytics.\nExperiencia en consultor\u00eda\nData & Analytics\n\ud83c\udf93 Formaci\u00f3n extra como un M\u00e1ster en Big Data & Analytics u otras certificaciones que potencien tus habilidades.\nM\u00e1ster en Big Data & Analytics\n\ud83d\udee0\ufe0f Conocimiento en tecnolog\u00edas como SQL, Snowflake, dbt, Azure Data Factory o Microsoft Fabric.\nSQL, Snowflake, dbt, Azure Data Factory o Microsoft Fabric\n\ud83d\udce6 Experiencia con contenedores y orquestaci\u00f3n.\ncontenedores y orquestaci\u00f3n\n\u2601\ufe0f Cloud & Automatizaci\u00f3n: Infraestructura, pipelines de datos, operaci\u00f3n y monitorizaci\u00f3n.\nCloud & Automatizaci\u00f3n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\nSalario\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\n\u2705 Acceso a formaciones y certificaciones\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group"
    },
    "4167005594": {
        "title": "Lead Data Engineer ",
        "company": "Winning",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nAt Winning Consulting, we are on the lookout for Lead Data Engineer to be part of our dynamic team. This role involves working on an exciting project for one of our key clients in the industry sector, based in our Madrid office. (Hybrid or remote model)\n\nAs an AWS Migration Specialist, your responsibilities will be:\n\nLead cloud migration: Direct the migration of the BI platform and all on-premises processes to AWS, ensuring a smooth and efficient transition.\nETL optimization and maintenance: Develop, optimize, and maintain ETL processes to improve data efficiency and availability.\nProcess monitoring: Supervise daily processes to ensure their proper functioning, proactively identifying and resolving issues.\nData management: Extract, transform, and load data from various sources to generate key insights that drive decision-making.\n\nWhat We Are Looking For:\n\nExperience in migrating BI platforms and ETL processes to AWS.\nExtensive knowledge and experience with AWS services (AWS S3, AWS Lambda, AWS Glue, AWS RDS, AWS CloudWatch, AWS Transfer).\nStrong background in databases and tools such as SQL Server, SSIS, Snowflake, and scripting experience with Python.\nFamiliarity with version control and collaboration tools like Bitbucket, GitHub, Jira, and Confluence.\nKnowledge of automation and deployment using Terraform and Jenkins.\nExperience with API integration and tools like Mulesoft.\n What We Offer:\nOpportunities for growth and professional development in a multinational environment.\nWork in a dynamic and collaborative team with access to the latest technologies.\nFlexible working arrangements, benefits, and an environment that fosters innovation.\n \n\n\n\u00bfQui\u00e9nes somos? \nWinning Consulting es una empresa de consultor\u00eda que ofrece servicios de \nconsultor\u00eda, formaci\u00f3n, reclutamiento e investigaci\u00f3n. Apoyamos a nuestros clientes \nen la b\u00fasqueda de soluciones innovadoras y sostenibles, desde la aplicaci\u00f3n del \nconocimiento cient\u00edfico a la resoluci\u00f3n de problemas complejos de gesti\u00f3n hasta la \ntransformaci\u00f3n digital y tecnol\u00f3gica de las organizaciones. \nSi quieres saber m\u00e1s sobre nosotros, visita nuestra web https://www.winning-consulting.com/\n\nTodas las candidaturas se tratan de forma confidencial en virtud del GDPR. Al enviar su candidatura, acepta el tratamiento de su informaci\u00f3n \nen el contexto del reclutamiento y su inclusi\u00f3n en nuestra base de datos de candidatos. Si no consiente el tratamiento de estos datos, le \nrogamos que no presente su candidatura a este anuncio\nAt Winning Consulting, we are on the lookout for Lead Data Engineer to be part of our dynamic team. This role involves working on an exciting project for one of our key clients in the industry sector, based in our Madrid office. (Hybrid or remote model)\nLead Data Engineer\nAs an AWS Migration Specialist, your responsibilities will be:\nAWS Migration Specialist,\nLead cloud migration: Direct the migration of the BI platform and all on-premises processes to AWS, ensuring a smooth and efficient transition.\nETL optimization and maintenance: Develop, optimize, and maintain ETL processes to improve data efficiency and availability.\nProcess monitoring: Supervise daily processes to ensure their proper functioning, proactively identifying and resolving issues.\nData management: Extract, transform, and load data from various sources to generate key insights that drive decision-making.\nLead cloud migration: Direct the migration of the BI platform and all on-premises processes to AWS, ensuring a smooth and efficient transition.\nLead cloud migration:\nETL optimization and maintenance: Develop, optimize, and maintain ETL processes to improve data efficiency and availability.\nETL optimization and maintenance:\nProcess monitoring: Supervise daily processes to ensure their proper functioning, proactively identifying and resolving issues.\nProcess monitoring:\nData management: Extract, transform, and load data from various sources to generate key insights that drive decision-making.\nData management:\nWhat We Are Looking For:\nExperience in migrating BI platforms and ETL processes to AWS.\nExtensive knowledge and experience with AWS services (AWS S3, AWS Lambda, AWS Glue, AWS RDS, AWS CloudWatch, AWS Transfer).\nStrong background in databases and tools such as SQL Server, SSIS, Snowflake, and scripting experience with Python.\nFamiliarity with version control and collaboration tools like Bitbucket, GitHub, Jira, and Confluence.\nKnowledge of automation and deployment using Terraform and Jenkins.\nExperience with API integration and tools like Mulesoft.\nExperience in migrating BI platforms and ETL processes to AWS.\nAWS\nExtensive knowledge and experience with AWS services (AWS S3, AWS Lambda, AWS Glue, AWS RDS, AWS CloudWatch, AWS Transfer).\nStrong background in databases and tools such as SQL Server, SSIS, Snowflake, and scripting experience with Python.\nSnowflake\nFamiliarity with version control and collaboration tools like Bitbucket, GitHub, Jira, and Confluence.\nKnowledge of automation and deployment using Terraform and Jenkins.\nExperience with API integration and tools like Mulesoft.\nWhat We Offer:\nOpportunities for growth and professional development in a multinational environment.\nWork in a dynamic and collaborative team with access to the latest technologies.\nFlexible working arrangements, benefits, and an environment that fosters innovation.\nOpportunities for growth and professional development in a multinational environment.\nWork in a dynamic and collaborative team with access to the latest technologies.\nFlexible working arrangements, benefits, and an environment that fosters innovation.\n\u00bfQui\u00e9nes somos?\nWinning Consulting es una empresa de consultor\u00eda que ofrece servicios de\nconsultor\u00eda, formaci\u00f3n, reclutamiento e investigaci\u00f3n. Apoyamos a nuestros clientes\nen la b\u00fasqueda de soluciones innovadoras y sostenibles, desde la aplicaci\u00f3n del\nconocimiento cient\u00edfico a la resoluci\u00f3n de problemas complejos de gesti\u00f3n hasta la\ntransformaci\u00f3n digital y tecnol\u00f3gica de las organizaciones.\nSi quieres saber m\u00e1s sobre nosotros, visita nuestra web https://www.winning-consulting.com/\nTodas las candidaturas se tratan de forma confidencial en virtud del GDPR. Al enviar su candidatura, acepta el tratamiento de su informaci\u00f3n\nen el contexto del reclutamiento y su inclusi\u00f3n en nuestra base de datos de candidatos. Si no consiente el tratamiento de estos datos, le\nrogamos que no presente su candidatura a este anuncio"
    },
    "4150251162": {
        "title": "Data & BI Engineer (German speaker) ",
        "company": "isolutions AG",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nAre you a Data Engineer with a passion for Microsoft technologies \u2753\nIf so, join us in our journey of innovation and shape the future of data engineering! We look forward to welcoming you to our team.\nAs a Data Engineer, you will thrive within our collaborative team, participating in high-end projects alongside skilled professionals. Your role will be pivotal as you design, develop, and implement data solutions for diverse customer environments. Leveraging your expertise in Microsoft technologies such as Azure Synapse, Databricks, Azure Storage Account (Data Lake), and SQL Server, you will create robust and efficient data pipelines using Python and SQL.\nFurthermore, your expertise in DevOps practices will ensure seamless data engineering processes, including CI/CD pipelines and version control.\n\nHow your day will look like \ud83d\udca1\nCollaborate with the DATA & AI team distributed among Switzerland and Barcelona, participating in high-end projects\nDesign, develop, and implement data solutions using Microsoft technologies (Azure Synapse, Databricks, Azure Storage Account, SQL Server)\nCreate and optimize data pipelines using Python and SQL\nImplement DevOps practices for data engineering, including CI/CD pipelines and version control\nUtilize concepts such as snowflake and star schema in data modeling\nVisualize data effectively using Power BI\n\nYour profile \ud83e\uddd1\u200d\ud83d\udcbb\nat least 1 year of hands-on experience as a Data Engineer, with a strong focus on Microsoft technologies\nProficiency in Azure Synapse, Databricks, Azure Storage Account (Data Lake), and SQL Server\nStrong programming skills in Python and SQL\nExperience with DevOps practices for data engineering\nFamiliarity with data modeling concepts such as snowflake and star schema\nAbility to create insightful visualizations using Power BI\nExcellent problem-solving and communication skills\n\"Can do\" mindset and proactive attitude\nBachelor's or Master's degree in Computer Science, Engineering, or related field\nGood level of German (at least B1) and excellent communication skills in English, enabling effective collaboration with colleagues and clients\nEU citizen or possessing a valid Spanish work visa\n\nIf your experience looks a little different from what we\u2019ve identified and you think you can add value to our crew, we\u2019d love to learn more about you.\n\nWhat we offer \ud83d\ude80\nPermanent contract, with competitive salary package, plus interesting firing benefits\nFlexible working hours for an optimal work-life balance\nBeing based in Barcelona or nearby, you have the flexibility to work from home according to your needs\nWork and evolve with the latest technologies like Microsoft Fabric, Azure Databricks and Snowflake using Python, PySpark and SQL within a Microsoft-focused ecosystem\nStructured career development plan based on your professional goals\nYearly training budget, Microsoft Certifications and bi-weekly \u201cTech Lunches\u201c\nChoose your own hardware\nFree language classes\nOne additional week of vacation for Corporate Social Responsibility\nA dynamic environment and a unique team spirit!\n\nShape the future of data engineering with us. Apply now and become part of our exciting journey!\n\nHow to find out if this is the next step of your career\nApply \ud83d\udce9\nDiscovery call with HR \ud83e\uddd0\nInterview - video call with the Data team \ud83e\udd1d\nFinal assessment \ud83d\udc69\u200d\ud83d\udcbb\n\n\ud83e\udde1 Our commitment: to give you feedback on each step, both in positive and negative cases\nAre you a Data Engineer with a passion for Microsoft technologies \u2753\nIf so, join us in our journey of innovation and shape the future of data engineering! We look forward to welcoming you to our team.\nAs a Data Engineer, you will thrive within our collaborative team, participating in high-end projects alongside skilled professionals. Your role will be pivotal as you design, develop, and implement data solutions for diverse customer environments. Leveraging your expertise in Microsoft technologies such as Azure Synapse, Databricks, Azure Storage Account (Data Lake), and SQL Server, you will create robust and efficient data pipelines using Python and SQL.\nFurthermore, your expertise in DevOps practices will ensure seamless data engineering processes, including CI/CD pipelines and version control.\nHow your day will look like \ud83d\udca1\nCollaborate with the DATA & AI team distributed among Switzerland and Barcelona, participating in high-end projects\nDesign, develop, and implement data solutions using Microsoft technologies (Azure Synapse, Databricks, Azure Storage Account, SQL Server)\nCreate and optimize data pipelines using Python and SQL\nImplement DevOps practices for data engineering, including CI/CD pipelines and version control\nUtilize concepts such as snowflake and star schema in data modeling\nVisualize data effectively using Power BI\nCollaborate with the DATA & AI team distributed among Switzerland and Barcelona, participating in high-end projects\nDesign, develop, and implement data solutions using Microsoft technologies (Azure Synapse, Databricks, Azure Storage Account, SQL Server)\nCreate and optimize data pipelines using Python and SQL\nImplement DevOps practices for data engineering, including CI/CD pipelines and version control\nUtilize concepts such as snowflake and star schema in data modeling\nVisualize data effectively using Power BI\nYour profile \ud83e\uddd1\u200d\ud83d\udcbb\nat least 1 year of hands-on experience as a Data Engineer, with a strong focus on Microsoft technologies\nProficiency in Azure Synapse, Databricks, Azure Storage Account (Data Lake), and SQL Server\nStrong programming skills in Python and SQL\nExperience with DevOps practices for data engineering\nFamiliarity with data modeling concepts such as snowflake and star schema\nAbility to create insightful visualizations using Power BI\nExcellent problem-solving and communication skills\n\"Can do\" mindset and proactive attitude\nBachelor's or Master's degree in Computer Science, Engineering, or related field\nGood level of German (at least B1) and excellent communication skills in English, enabling effective collaboration with colleagues and clients\nEU citizen or possessing a valid Spanish work visa\nat least 1 year of hands-on experience as a Data Engineer, with a strong focus on Microsoft technologies\nat least 1 year of hands-on experience as a Data Engineer\nProficiency in Azure Synapse, Databricks, Azure Storage Account (Data Lake), and SQL Server\nStrong programming skills in Python and SQL\nExperience with DevOps practices for data engineering\nFamiliarity with data modeling concepts such as snowflake and star schema\nAbility to create insightful visualizations using Power BI\nExcellent problem-solving and communication skills\n\"Can do\" mindset and proactive attitude\nBachelor's or Master's degree in Computer Science, Engineering, or related field\nGood level of German (at least B1) and excellent communication skills in English, enabling effective collaboration with colleagues and clients\nGood level of German\nEU citizen or possessing a valid Spanish work visa\nIf your experience looks a little different from what we\u2019ve identified and you think you can add value to our crew, we\u2019d love to learn more about you.\nWhat we offer \ud83d\ude80\nPermanent contract, with competitive salary package, plus interesting firing benefits\nFlexible working hours for an optimal work-life balance\nBeing based in Barcelona or nearby, you have the flexibility to work from home according to your needs\nWork and evolve with the latest technologies like Microsoft Fabric, Azure Databricks and Snowflake using Python, PySpark and SQL within a Microsoft-focused ecosystem\nStructured career development plan based on your professional goals\nYearly training budget, Microsoft Certifications and bi-weekly \u201cTech Lunches\u201c\nChoose your own hardware\nFree language classes\nOne additional week of vacation for Corporate Social Responsibility\nA dynamic environment and a unique team spirit!\nPermanent contract, with competitive salary package, plus interesting firing benefits\nFlexible working hours for an optimal work-life balance\nBeing based in Barcelona or nearby, you have the flexibility to work from home according to your needs\nWork and evolve with the latest technologies like Microsoft Fabric, Azure Databricks and Snowflake using Python, PySpark and SQL within a Microsoft-focused ecosystem\nStructured career development plan based on your professional goals\nYearly training budget, Microsoft Certifications and bi-weekly \u201cTech Lunches\u201c\nChoose your own hardware\nFree language classes\nOne additional week of vacation for Corporate Social Responsibility\nA dynamic environment and a unique team spirit!\nShape the future of data engineering with us. Apply now and become part of our exciting journey!\nHow to find out if this is the next step of your career\nApply \ud83d\udce9\nDiscovery call with HR \ud83e\uddd0\nInterview - video call with the Data team \ud83e\udd1d\nFinal assessment \ud83d\udc69\u200d\ud83d\udcbb\nApply \ud83d\udce9\nDiscovery call with HR \ud83e\uddd0\nInterview - video call with the Data team \ud83e\udd1d\nFinal assessment \ud83d\udc69\u200d\ud83d\udcbb\n\ud83e\udde1 Our commitment: to give you feedback on each step, both in positive and negative cases"
    },
    "4157897178": {
        "title": "Junior Data Engineer",
        "company": "Canonical",
        "location": "M\u00e1laga, Andalusia, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nBring your data analytics and data mining skills to a unique team seeking to understand and shape the future of marketing technology. We are interested in technology adoption patterns, the respect of visitors' data and the use of open source in marketing. We are also interested in those marketing data analysts who are curious enough to embrace new technologies and are ready to work with unfamiliar tools, if needed.\n\nThe role of a Junior Data Engineer at Canonical\n\nCanonical has provided developers with open source since 2004, helping them build innovations such as public cloud, machine learning, robotics or blockchain. Marketing at Canonical means being at the forefront of innovation, for our customers and for our own martech stack. We're on the look out for a marketing data analyst to join our team and execute on our growth hacking strategy.\n\nThe ideal candidate will be passionate about technology, technology marketing and the use of technology in marketing. You will prefer to work in an environment that has emphasis on ownership of campaigns, collaboration, learning, curiosity and a drive to continually improve oneself / the team / the organisation. You will also love to problem solve, get hands-on, experiment, measure and use automation to make daily life easier.\n\nThe Marketing team at Canonical drives commercial outcomes for the company across its portfolio of products and grows the addressable market through digital marketing campaigns, lifecycle management, events, partnerships and community development. If these things are important to you and you're motivated by driving data engineering, delighting customers and filling the sales funnel, we want to talk with you.\n\nThis role sits in the Marketing team reporting to the Growth Engineering Manager.\n\nLocation: This role will be based remotely in the EMEA region.\n\nWhat your day will look like\n\nUtilise advanced data analytics to grow Canonical's product adoption and market penetration\nFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomes\nDesign and conduct experiments with data, visualisation and insights into Canonical's target audiences\nCollaborate with stakeholder teams (Product Management, Engineering, Information Systems, Finance, RevOps, etc) to improve the data and tool ecosystem\nPut in place and maintain systems to ensure teams across the company have self-service access to data dashboards\n\nWhat we are looking for in you?\n\nBackground in data science, mathematics, actuarial science, or engineering\nKnowledge in advanced statistics, data sciences, coding/scripting languages (Python, JS, etc), and databases (SQL, etc)\nStrength in data analytics and visualisation (Looker Studio, Tableau, Apache Superset, etc)\nAbility to translate business questions to key research objectives\nAbility to identify the best methodology to execute research, synthesise and analyse findings\nExcellent writing and communication skills\nWillingness to examine the status quo and resilient in the face of challenges\n\nWhat we offer you\n\nYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilisation. Our compensation philosophy is to ensure equity right across our global workforce.\n\nIn addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.\n\n\ud83c\udfe0Fully remote working environment - we've been working remotely since 2004!\n\n\ud83d\udcdaPersonal learning and development budget of 2,000 USD per annum\n\n\ud83d\udcb0Annual compensation review\n\n\ud83c\udfc6Recognition rewards\n\n\ud83c\udfdd Annual holiday leave\n\n\ud83d\udc76Parental Leave\n\n\ud83e\uddd1\u200d\ud83d\udcbcEmployee Assistance Programme\n\n\ud83e\uddf3Opportunity to travel to new locations to meet colleagues at 'sprints'\n\n\u2708\ufe0fPriority Pass for travel and travel upgrades for long haul company events\n\nAbout Canonical\n\nCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.\n\nCanonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.\n\nCanonical is an equal opportunity employer\n\nWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration.\nData Engineer\nLocation:\nUtilise advanced data analytics to grow Canonical's product adoption and market penetration\nFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomes\nDesign and conduct experiments with data, visualisation and insights into Canonical's target audiences\nCollaborate with stakeholder teams (Product Management, Engineering, Information Systems, Finance, RevOps, etc) to improve the data and tool ecosystem\nPut in place and maintain systems to ensure teams across the company have self-service access to data dashboards\nUtilise advanced data analytics to grow Canonical's product adoption and market penetration\nFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomes\nDesign and conduct experiments with data, visualisation and insights into Canonical's target audiences\nCollaborate with stakeholder teams (Product Management, Engineering, Information Systems, Finance, RevOps, etc) to improve the data and tool ecosystem\nPut in place and maintain systems to ensure teams across the company have self-service access to data dashboards\nBackground in data science, mathematics, actuarial science, or engineering\nKnowledge in advanced statistics, data sciences, coding/scripting languages (Python, JS, etc), and databases (SQL, etc)\nStrength in data analytics and visualisation (Looker Studio, Tableau, Apache Superset, etc)\nAbility to translate business questions to key research objectives\nAbility to identify the best methodology to execute research, synthesise and analyse findings\nExcellent writing and communication skills\nWillingness to examine the status quo and resilient in the face of challenges\nBackground in data science, mathematics, actuarial science, or engineering\nKnowledge in advanced statistics, data sciences, coding/scripting languages (Python, JS, etc), and databases (SQL, etc)\nStrength in data analytics and visualisation (Looker Studio, Tableau, Apache Superset, etc)\nAbility to translate business questions to key research objectives\nAbility to identify the best methodology to execute research, synthesise and analyse findings\nExcellent writing and communication skills\nWillingness to examine the status quo and resilient in the face of challenges\nAbout Canonical"
    },
    "4143648426": {
        "title": "Data Analyst / Data Engineer ",
        "company": "GMV",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nIf you can process data in Python faster than making a coffee... Your place is at GMV!\n\nWe are expanding our Big Data and Artificial Intelligence team to prevent money laundering and financial violations for a banking sector company!\n\nWe like to get straight to the point, so we'll tell you what you won't find online. If you want to know more about us, visit GMV\n\nWHAT CHALLENGES WILL YOU FACE?\n\nIn our artificial intelligence team, you will provide services to a leading banking client and perform tasks such as:\n\n\ud83d\udce1 Developing and maintaining real-time data streaming processes.\n\n\ud83d\udcbb Extracting insights from large amounts of data.\n\n\ud83e\udd1d Working in a team environment under Agile methodologies.\n\nWHAT DO WE NEED FROM YOU?\n\nFor this role, we are looking for engineering graduates with at least 2 years of previous experience in:\n\n\ud83d\uddc3\ufe0f SQL and databases\n\n\ud83d\udd04 Data processing\n\nWe will also value previous experience in fraud prevention in the banking sector and knowledge in:\n\n\u2705 Functional knowledge of the banking or insurance sector\n\n\ud83d\udd0d Experience in fraud prevention\n\n\ud83d\udcca Data governance\n\n\u2714\ufe0f Data quality\n\n\ud83d\udda5\ufe0f IBM DataStage and IBM Cognos\n\n\ud83d\udcac Kafka\n\n\ud83d\udd04 Nifi\n\n\ud83e\udd16 Watson Knowledge Catalog\n\nWHAT DO WE OFFER YOU?\n\n\ud83d\udcbb Hybrid work model and 8 weeks of remote work per year outside your usual geographic area.\n\n\ud83d\udd51 Flexible working hours, with early Friday and summer schedules.\n\n\ud83d\ude80 Development of a personalized career plan, training, and language learning support.\n\n\ud83c\udf0d National and international mobility. Coming from another country? We offer a relocation package.\n\n\ud83d\udcb0 Competitive compensation with continuous reviews, flexible compensation, and brand discounts.\n\n\ud83d\udcaa Well-being program: medical, dental, and accident insurance; free fruit and coffee; physical, mental, and financial health training, and much more!\n\n\u26a0\ufe0f In our recruitment process, you will always have direct contact with our talent acquisition team via phone or personal meetings (either in person or online). We will never request bank transfers or card details. If you are contacted through any other process, please email our team at privacy@gmv.com\n\n\u2764\ufe0f We promote equal employment opportunities and are committed to inclusion and diversity.\n\nWHAT ARE YOU WAITING FOR? JOIN US!\n\nIf you have any questions, feel free to contact Clara S\u00e1nchez Lobato, the person responsible for this vacancy.\n\nClara S\u00e1nchez Lobato\nBig Data and Artificial Intelligence\nmoney laundering\nfinancial violations\nbanking sector\nWHAT CHALLENGES WILL YOU FACE?\nbanking\nreal-time data streaming\nAgile\nWHAT DO WE NEED FROM YOU?\nengineering graduates\n2 years of previous experience\nSQL and databases\nData processing\nprevious experience\nWHAT DO WE OFFER YOU?\nHybrid work model\n8 weeks\nremote work\nFlexible working hours\npersonalized career plan\nNational and international mobility.\nrelocation package\nCompetitive compensation\ncontinuous reviews\ndirect contact\nequal employment opportunities\ninclusion and diversity\nWHAT ARE YOU WAITING FOR? JOIN US!"
    },
    "4122317870": {
        "title": "Data Operations Engineer Specialist ",
        "company": "Clarity AI",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nAbout The Role \ud83d\udcbb\nWe are looking for a Data Operations Engineer Specialist to join our Data Strategy team. If you\u2019re a Data Engineer and you enjoy leveraging code to quickly adapt to changing needs in an operative environment, or if you are a Data Analyst who can code in python to solve operative issues, we are looking for you.\n\nYou will be part of the Data Strategy team in charge of growing our Sustainability database, allowing us to enhance the productivity of the research team and reduce the operation burden of the development team. You will interact with the development team as well as with the research team.\n\nWhat You\u2019ll Be Doing \ud83d\ude80\n\nWorking as a Data Operations Engineer Specialist you will be responsible for:\n\nConsulting data and using your coding skills (Python and SQL) to manage data from different data sources (S3, postgreSQL, etc.) in order to define the information to retrieve in our collection efforts\nCreating dynamic data assets required for different use cases using DBT and Airflow for analytic consumption\nCreating automated and refreshable dashboards and reports for different stakeholders using your SQL and Python coding skills and using tools and services such as DBT and Quicksight\nDeveloping process operations and process automations using orchestration tools like Airflow, APIs and overall best practices in streamlining business processes\nDelivering data through existing pipelines, or supporting the creation of new pipelines while including necessary soda validation checks\nServing and explaining data to non-technical and technical teams\nDeveloping solution architecture diagrams and supporting the build using best practices\nOffering technical support to business stakeholders around their day to day operational work\nModifying and creating new ETLs helped by the data-engineering team (we use Airflow to manage ETLs)\nSupporting the configuration/integration of third party tools and work on web scraping activities and tasks\nWriting maintainable code with tests, applying version control, debugging correctly, and uploading it to Gitlab\nInterfacing with the Data Science team on serving data for modeling and predictions and helping deliver high quality data based on business rules to specific data stores\n\nLocation \ud83c\udf0d\nThe role is based in Barcelona\nWay of Working: Remote / Hybrid\n\nWhat You\u2019ll Need \ud83d\udc40\n3+ years industry experience in the data analytics and data engineering field\nYou hold a degree in a quantitative field (CS, Engineer, Math, Physics, \u2026 ) and have great coding skills.\nYou possess very strong SQL, Python database skills, and can make invocations to APIs\nYou are very familiar with using tools such as SnowFlake, Airflow, AWS Services (ie. S3, Quicksight), and DBT\nExperience in advanced analytics, data engineering and a similar related field of data management\nExperience in collaborating with engineering teams and stakeholders to build key datasets and executing data pipelines using SQL/Python/ETL frameworks\nCan design data models and are familiar with best practices around data design and architecture\nCan work well with the Business and Technology stakeholders to deliver business initiatives on time by understanding the business need\nSelf-starter, able to take ownership and initiative, with high energy and stamina\nDecisive and action-oriented, able to make rapid decisions even when they are short of information\nHighly motivated, independent and deeply passionate about sustainability and impact\nExcellent oral and written English communication skills (minimum C1 level-proficient user)\n\nNice To Have \u2728\nExperience in the financial sector\nKnowledge of sustainability aspects\n\nWhat we offer \ud83e\udd41\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events\nAbout The Role \ud83d\udcbb\nAbout The Role\nWe are looking for a Data Operations Engineer Specialist to join our Data Strategy team. If you\u2019re a Data Engineer and you enjoy leveraging code to quickly adapt to changing needs in an operative environment, or if you are a Data Analyst who can code in python to solve operative issues, we are looking for you.\nData Operations Engineer Specialist\nYou will be part of the Data Strategy team in charge of growing our Sustainability database, allowing us to enhance the productivity of the research team and reduce the operation burden of the development team. You will interact with the development team as well as with the research team.\nWhat You\u2019ll Be Doing \ud83d\ude80\nWhat You\u2019ll Be Doing\nWorking as a Data Operations Engineer Specialist you will be responsible for:\nConsulting data and using your coding skills (Python and SQL) to manage data from different data sources (S3, postgreSQL, etc.) in order to define the information to retrieve in our collection efforts\nCreating dynamic data assets required for different use cases using DBT and Airflow for analytic consumption\nCreating automated and refreshable dashboards and reports for different stakeholders using your SQL and Python coding skills and using tools and services such as DBT and Quicksight\nDeveloping process operations and process automations using orchestration tools like Airflow, APIs and overall best practices in streamlining business processes\nDelivering data through existing pipelines, or supporting the creation of new pipelines while including necessary soda validation checks\nServing and explaining data to non-technical and technical teams\nDeveloping solution architecture diagrams and supporting the build using best practices\nOffering technical support to business stakeholders around their day to day operational work\nModifying and creating new ETLs helped by the data-engineering team (we use Airflow to manage ETLs)\nSupporting the configuration/integration of third party tools and work on web scraping activities and tasks\nWriting maintainable code with tests, applying version control, debugging correctly, and uploading it to Gitlab\nInterfacing with the Data Science team on serving data for modeling and predictions and helping deliver high quality data based on business rules to specific data stores\nConsulting data and using your coding skills (Python and SQL) to manage data from different data sources (S3, postgreSQL, etc.) in order to define the information to retrieve in our collection efforts\nCreating dynamic data assets required for different use cases using DBT and Airflow for analytic consumption\nCreating automated and refreshable dashboards and reports for different stakeholders using your SQL and Python coding skills and using tools and services such as DBT and Quicksight\nDeveloping process operations and process automations using orchestration tools like Airflow, APIs and overall best practices in streamlining business processes\nDelivering data through existing pipelines, or supporting the creation of new pipelines while including necessary soda validation checks\nServing and explaining data to non-technical and technical teams\nDeveloping solution architecture diagrams and supporting the build using best practices\nOffering technical support to business stakeholders around their day to day operational work\nModifying and creating new ETLs helped by the data-engineering team (we use Airflow to manage ETLs)\nSupporting the configuration/integration of third party tools and work on web scraping activities and tasks\nWriting maintainable code with tests, applying version control, debugging correctly, and uploading it to Gitlab\nInterfacing with the Data Science team on serving data for modeling and predictions and helping deliver high quality data based on business rules to specific data stores\nLocation \ud83c\udf0d\nLocation\nThe role is based in Barcelona\nWay of Working: Remote / Hybrid\nWhat You\u2019ll Need \ud83d\udc40\nWhat You\u2019ll Need\n3+ years industry experience in the data analytics and data engineering field\nYou hold a degree in a quantitative field (CS, Engineer, Math, Physics, \u2026 ) and have great coding skills.\nYou possess very strong SQL, Python database skills, and can make invocations to APIs\nYou are very familiar with using tools such as SnowFlake, Airflow, AWS Services (ie. S3, Quicksight), and DBT\nExperience in advanced analytics, data engineering and a similar related field of data management\nExperience in collaborating with engineering teams and stakeholders to build key datasets and executing data pipelines using SQL/Python/ETL frameworks\nCan design data models and are familiar with best practices around data design and architecture\nCan work well with the Business and Technology stakeholders to deliver business initiatives on time by understanding the business need\nSelf-starter, able to take ownership and initiative, with high energy and stamina\nDecisive and action-oriented, able to make rapid decisions even when they are short of information\nHighly motivated, independent and deeply passionate about sustainability and impact\nExcellent oral and written English communication skills (minimum C1 level-proficient user)\n3+ years industry experience in the data analytics and data engineering field\nYou hold a degree in a quantitative field (CS, Engineer, Math, Physics, \u2026 ) and have great coding skills.\nYou possess very strong SQL, Python database skills, and can make invocations to APIs\nYou are very familiar with using tools such as SnowFlake, Airflow, AWS Services (ie. S3, Quicksight), and DBT\nExperience in advanced analytics, data engineering and a similar related field of data management\nExperience in collaborating with engineering teams and stakeholders to build key datasets and executing data pipelines using SQL/Python/ETL frameworks\nCan design data models and are familiar with best practices around data design and architecture\nCan work well with the Business and Technology stakeholders to deliver business initiatives on time by understanding the business need\nSelf-starter, able to take ownership and initiative, with high energy and stamina\nDecisive and action-oriented, able to make rapid decisions even when they are short of information\nHighly motivated, independent and deeply passionate about sustainability and impact\nExcellent oral and written English communication skills (minimum C1 level-proficient user)\nNice To Have \u2728\nExperience in the financial sector\nKnowledge of sustainability aspects\nExperience in the financial sector\nKnowledge of sustainability aspects\nWhat we offer \ud83e\udd41\nWhat we offer\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events"
    },
    "3982411847": {
        "title": "AWS Data Solutions Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\n\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\n\ud83d\udd27 Dise\u00f1ar soluciones de software enfocadas en la gesti\u00f3n de datos, asegurando calidad, seguridad y trazabilidad\n\n\ud83d\udcca Dise\u00f1ar e implementar soluciones impulsadas por metadatos para optimizar y automatizar procesos a lo largo del ciclo de vida del dato.\n\n\u2601\ufe0f Implementar soluciones cloud (Azure, AWS y GCP) asegurando el uso \u00f3ptimo de sus componentes.\n\n\ud83d\udcbb Programar y optimizar procesos de tratamiento de datos, utilizando Python, para el manejo eficiente del ciclo vida del dato.\n\n\ud83d\udcc8 Coordinar el dise\u00f1o y desarrollo de soluciones para la gesti\u00f3n del ciclo del dato, garantizando eficiencia y escalabilidad.\n\n\ud83e\udd1d Colaborar con equipos t\u00e9cnicos para definir estrategias y buenas pr\u00e1cticas en entornos cloud.\n\n\ud83d\udd0d Evaluar y probar servicios cloud innovadores que puedan aportar valor a los proyectos\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n?\n\n\ud83c\udf93 Titulaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o formaci\u00f3n equivalente.\n\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Software Engineer o roles similares dentro del mundo del dato.\n\n\ud83d\udd04 +2 a\u00f1os en integraci\u00f3n y procesamiento de datos, desarrollando pipelines de ingesta y transformaci\u00f3n.\n\n\ud83d\udcbb Conocimiento s\u00f3lido en Python\n\n\ud83d\udcc2 Bases de datos relacionales y no relacionales, con capacidad para optimizar consultas y estructuras de almacenamiento.\n\n\u2601\ufe0f Experiencia en entornos cloud, principalmente Azure, AWS y GCP.\n\n\ud83c\udfd7\ufe0f Conocimientos en arquitectura de datos, orquestaci\u00f3n de procesos y observabilidad.\n\n\ud83d\udcc8 Desarrollo de soluciones escalables y eficientes para la gesti\u00f3n del ciclo de vida del dato.\n\n\ud83d\ude80 Habilidad para trabajar en entornos de integraci\u00f3n continua y despliegue en la nube.\n\n\ud83e\uddd0 Experiencia en liderazgo t\u00e9cnico.\n\nValoramos tambi\u00e9n:\n\n\ud83d\udd27 Dise\u00f1o de soluciones end-to-end y comprensi\u00f3n de su base t\u00e9cnica.\n\n\ud83d\udcbb Experiencia en programaci\u00f3n para integraci\u00f3n de datos, con conocimiento de frameworks y herramientas relevantes.\n\n\u2601\ufe0f Certificaci\u00f3n cloud o experiencia con servicios en la nube.\n\n\ud83c\udf10 Conocimientos en DataOps, Data Observability y Data Mesh.\n\n\ud83c\udf93 Formaci\u00f3n complementaria como un M\u00e1ster en Big Data y Analytics es un plus.\n\n\ud83d\udde3\ufe0f Ingl\u00e9s para trabajar en un entorno global.\n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\u2705 Acceso a formaciones y certificaciones.\n\n\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\n\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\ud83d\udd27 Dise\u00f1ar soluciones de software enfocadas en la gesti\u00f3n de datos, asegurando calidad, seguridad y trazabilidad\nDise\u00f1ar soluciones\ngesti\u00f3n de datos\n\ud83d\udcca Dise\u00f1ar e implementar soluciones impulsadas por metadatos para optimizar y automatizar procesos a lo largo del ciclo de vida del dato.\nDise\u00f1ar e implementar\nimpulsadas por metadatos\n\u2601\ufe0f Implementar soluciones cloud (Azure, AWS y GCP) asegurando el uso \u00f3ptimo de sus componentes.\nImplementar soluciones cloud\n\ud83d\udcbb Programar y optimizar procesos de tratamiento de datos, utilizando Python, para el manejo eficiente del ciclo vida del dato.\nProgramar y optimizar\nPython\n\ud83d\udcc8 Coordinar el dise\u00f1o y desarrollo de soluciones para la gesti\u00f3n del ciclo del dato, garantizando eficiencia y escalabilidad.\nCoordinar el dise\u00f1o y desarrollo\n\ud83e\udd1d Colaborar con equipos t\u00e9cnicos para definir estrategias y buenas pr\u00e1cticas en entornos cloud.\nColaborar con equipos t\u00e9cnicos\n\ud83d\udd0d Evaluar y probar servicios cloud innovadores que puedan aportar valor a los proyectos\nEvaluar y probar\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n?\n\ud83c\udf93 Titulaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o formaci\u00f3n equivalente.\nTitulaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Software Engineer o roles similares dentro del mundo del dato.\n+4 a\u00f1os de experiencia\nData Software Engineer\n\ud83d\udd04 +2 a\u00f1os en integraci\u00f3n y procesamiento de datos, desarrollando pipelines de ingesta y transformaci\u00f3n.\n+2 a\u00f1os en integraci\u00f3n y procesamiento de datos\n\ud83d\udcbb Conocimiento s\u00f3lido en Python\nConocimiento s\u00f3lido en Python\n\ud83d\udcc2 Bases de datos relacionales y no relacionales, con capacidad para optimizar consultas y estructuras de almacenamiento.\nBases de datos relacionales y no relacionales\n\u2601\ufe0f Experiencia en entornos cloud, principalmente Azure, AWS y GCP.\nExperiencia en entornos cloud\nAzure, AWS y GCP\n\ud83c\udfd7\ufe0f Conocimientos en arquitectura de datos, orquestaci\u00f3n de procesos y observabilidad.\nConocimientos en arquitectura de datos, orquestaci\u00f3n de procesos y observabilidad\n\ud83d\udcc8 Desarrollo de soluciones escalables y eficientes para la gesti\u00f3n del ciclo de vida del dato.\nDesarrollo de soluciones escalables y eficientes\n\ud83d\ude80 Habilidad para trabajar en entornos de integraci\u00f3n continua y despliegue en la nube.\nHabilidad para trabajar en entornos de integraci\u00f3n continua\n\ud83e\uddd0 Experiencia en liderazgo t\u00e9cnico.\nliderazgo t\u00e9cnico.\nValoramos tambi\u00e9n:\n\ud83d\udd27 Dise\u00f1o de soluciones end-to-end y comprensi\u00f3n de su base t\u00e9cnica.\nDise\u00f1o de soluciones end-to-end\n\ud83d\udcbb Experiencia en programaci\u00f3n para integraci\u00f3n de datos, con conocimiento de frameworks y herramientas relevantes.\nExperiencia en programaci\u00f3n\n\u2601\ufe0f Certificaci\u00f3n cloud o experiencia con servicios en la nube.\nCertificaci\u00f3n cloud\nservicios en la nube\n\ud83c\udf10 Conocimientos en DataOps, Data Observability y Data Mesh.\nDataOps, Data Observability y Data Mesh\n\ud83c\udf93 Formaci\u00f3n complementaria como un M\u00e1ster en Big Data y Analytics es un plus.\nFormaci\u00f3n complementaria\nM\u00e1ster en Big Data y Analytics\n\ud83d\udde3\ufe0f Ingl\u00e9s para trabajar en un entorno global.\nIngl\u00e9s\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\nSalario\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\n\u2705 Acceso a formaciones y certificaciones.\nAcceso a formaciones y certificaciones.\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group"
    },
    "4145946412": {
        "title": "Data Operations Engineer Specialist ",
        "company": "Clarity AI",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nAbout Clarity AI \ud83e\udeb4\nClarity AI is a global tech company founded in 2017 with a unique mission: bringing societal impact to markets.\n\nWe leverage AI and machine learning technologies to provide top international investors, governments, companies, and consumers with the right data, methodologies, and tools to make more informed decisions.\n\nWe are now a team of more than 300 highly passionate and curious individuals from all over the world, with offices in New York, Madrid, London, Paris, and Abu Dhabi. Together, we have established Clarity AI as a leading sustainability tech AI company backed by investors and strategic partners such as BlackRock, SoftBank, and Deutsche B\u00f6rse, who believe in us and share our goals.\n\nWe are dedicated to cultivating an exceptional workplace environment, and we take pride in our culture, defined by our commitment to being fact-based, diverse, transparent, meritocratic, and flexible. \n\nWe have plans to continue growing our teams globally, so if you would like to join us on this rocket ship, keep reading! Your work will shape and guide the sustainable decisions of investors, companies and consumers worldwide.\n\nAbout The Role \ud83d\udcbb\nWe are looking for a Data Operations Engineer Specialist to join our Data Strategy team. If you\u2019re a Data Engineer and you enjoy leveraging code to quickly adapt to changing needs in an operative environment, or if you are a Data Analyst who can code in python to solve operative issues, we are looking for you.\n\nYou will be part of the Data Strategy team in charge of growing our Sustainability database, allowing us to enhance the productivity of the research team and reduce the operation burden of the development team. You will interact with the development team as well as with the research team.\n\nWhat You\u2019ll Be Doing \ud83d\ude80\nWorking as a Data Operations Engineer Specialist you will be responsible for:\nConsulting data and using your coding skills (Python and SQL) to manage data from different data sources (S3, postgreSQL, etc.) in order to define the information to retrieve in our collection efforts\nCreating dynamic data assets required for different use cases using DBT and Airflow for analytic consumption\nCreating automated and refreshable dashboards and reports for different stakeholders using your SQL and Python coding skills and using tools and services such as DBT and Quicksight\nDeveloping process operations and process automations using orchestration tools like Airflow, APIs and overall best practices in streamlining business processes\nDelivering data through existing pipelines, or supporting the creation of new pipelines while including necessary soda validation checks\nServing and explaining data to non-technical and technical teams\nDeveloping solution architecture diagrams and supporting the build using best practices\nOffering technical support to business stakeholders around their day to day operational work\nModifying and creating new ETLs helped by the data-engineering team (we use Airflow to manage ETLs)\nSupporting the configuration/integration of third party tools and work on web scraping activities and tasks\nWriting maintainable code with tests, applying version control, debugging correctly, and uploading it to Gitlab\nInterfacing with the Data Science team on serving data for modeling and predictions and helping deliver high quality data based on business rules to specific data stores\n\nLocation \ud83c\udf0d\nThe role is based in Madrid, Barcelona, or Spain (remote)\nWay of Working: Remote / Hybrid\n\nWhat You\u2019ll Need \ud83d\udc40\n3+ years industry experience in the data analytics and data engineering field\nYou hold a degree in a quantitative field (CS, Engineer, Math, Physics, \u2026 ) and have great coding skills.\nYou possess very strong SQL, Python database skills, and can make invocations to APIs\nYou are very familiar with using tools such as SnowFlake, Airflow, AWS Services (ie. S3, Quicksight), and DBT\nExperience in advanced analytics, data engineering and a similar related field of data management\nExperience in collaborating with engineering teams and stakeholders to build key datasets and executing data pipelines using SQL/Python/ETL frameworks\nCan design data models and are familiar with best practices around data design and architecture\nCan work well with the Business and Technology stakeholders to deliver business initiatives on time by understanding the business need\nSelf-starter, able to take ownership and initiative, with high energy and stamina\nDecisive and action-oriented, able to make rapid decisions even when they are short of information\nHighly motivated, independent and deeply passionate about sustainability and impact\nExcellent oral and written English communication skills (minimum C1 level-proficient user)\n\nNice To Have \u2728\nExperience in the financial sector\nKnowledge of sustainability aspects\n\nWhat we offer \ud83e\udd41\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events\nAbout Clarity AI \ud83e\udeb4\nAbout Clarity AI\nClarity AI is a global tech company founded in 2017 with a unique mission: bringing societal impact to markets.\nbringing societal impact to markets.\nWe leverage AI and machine learning technologies to provide top international investors, governments, companies, and consumers with the right data, methodologies, and tools to make more informed decisions.\ntop international\ninvestors, governments, companies, and consumers\nWe are now a team of more than 300 highly passionate and curious individuals from all over the world, with offices in New York, Madrid, London, Paris, and Abu Dhabi. Together, we have established Clarity AI as a leading sustainability tech AI company backed by investors and strategic partners such as BlackRock, SoftBank, and Deutsche B\u00f6rse, who believe in us and share our goals.\n300 highly passionate and curious individuals\nleading sustainability tech AI company\nBlackRock, SoftBank, and Deutsche B\u00f6rse\nWe are dedicated to cultivating an exceptional workplace environment, and we take pride in our culture, defined by our commitment to being fact-based, diverse, transparent, meritocratic, and flexible.\nfact-based, diverse, transparent, meritocratic, and flexible.\nWe have plans to continue growing our teams globally, so if you would like to join us on this rocket ship, keep reading! Your work will shape and guide the sustainable decisions of investors, companies and consumers worldwide.\nAbout The Role \ud83d\udcbb\nAbout The Role\nWe are looking for a Data Operations Engineer Specialist to join our Data Strategy team. If you\u2019re a Data Engineer and you enjoy leveraging code to quickly adapt to changing needs in an operative environment, or if you are a Data Analyst who can code in python to solve operative issues, we are looking for you.\nData Operations Engineer Specialist\nYou will be part of the Data Strategy team in charge of growing our Sustainability database, allowing us to enhance the productivity of the research team and reduce the operation burden of the development team. You will interact with the development team as well as with the research team.\nWhat You\u2019ll Be Doing \ud83d\ude80\nWhat You\u2019ll Be Doing\nWorking as a Data Operations Engineer Specialist you will be responsible for:\nConsulting data and using your coding skills (Python and SQL) to manage data from different data sources (S3, postgreSQL, etc.) in order to define the information to retrieve in our collection efforts\nCreating dynamic data assets required for different use cases using DBT and Airflow for analytic consumption\nCreating automated and refreshable dashboards and reports for different stakeholders using your SQL and Python coding skills and using tools and services such as DBT and Quicksight\nDeveloping process operations and process automations using orchestration tools like Airflow, APIs and overall best practices in streamlining business processes\nDelivering data through existing pipelines, or supporting the creation of new pipelines while including necessary soda validation checks\nServing and explaining data to non-technical and technical teams\nDeveloping solution architecture diagrams and supporting the build using best practices\nOffering technical support to business stakeholders around their day to day operational work\nModifying and creating new ETLs helped by the data-engineering team (we use Airflow to manage ETLs)\nSupporting the configuration/integration of third party tools and work on web scraping activities and tasks\nWriting maintainable code with tests, applying version control, debugging correctly, and uploading it to Gitlab\nInterfacing with the Data Science team on serving data for modeling and predictions and helping deliver high quality data based on business rules to specific data stores\nConsulting data and using your coding skills (Python and SQL) to manage data from different data sources (S3, postgreSQL, etc.) in order to define the information to retrieve in our collection efforts\nCreating dynamic data assets required for different use cases using DBT and Airflow for analytic consumption\nCreating automated and refreshable dashboards and reports for different stakeholders using your SQL and Python coding skills and using tools and services such as DBT and Quicksight\nDeveloping process operations and process automations using orchestration tools like Airflow, APIs and overall best practices in streamlining business processes\nDelivering data through existing pipelines, or supporting the creation of new pipelines while including necessary soda validation checks\nServing and explaining data to non-technical and technical teams\nDeveloping solution architecture diagrams and supporting the build using best practices\nOffering technical support to business stakeholders around their day to day operational work\nModifying and creating new ETLs helped by the data-engineering team (we use Airflow to manage ETLs)\nSupporting the configuration/integration of third party tools and work on web scraping activities and tasks\nWriting maintainable code with tests, applying version control, debugging correctly, and uploading it to Gitlab\nInterfacing with the Data Science team on serving data for modeling and predictions and helping deliver high quality data based on business rules to specific data stores\nLocation \ud83c\udf0d\nLocation\nThe role is based in Madrid, Barcelona, or Spain (remote)\nWay of Working: Remote / Hybrid\nWhat You\u2019ll Need \ud83d\udc40\nWhat You\u2019ll Need\n3+ years industry experience in the data analytics and data engineering field\nYou hold a degree in a quantitative field (CS, Engineer, Math, Physics, \u2026 ) and have great coding skills.\nYou possess very strong SQL, Python database skills, and can make invocations to APIs\nYou are very familiar with using tools such as SnowFlake, Airflow, AWS Services (ie. S3, Quicksight), and DBT\nExperience in advanced analytics, data engineering and a similar related field of data management\nExperience in collaborating with engineering teams and stakeholders to build key datasets and executing data pipelines using SQL/Python/ETL frameworks\nCan design data models and are familiar with best practices around data design and architecture\nCan work well with the Business and Technology stakeholders to deliver business initiatives on time by understanding the business need\nSelf-starter, able to take ownership and initiative, with high energy and stamina\nDecisive and action-oriented, able to make rapid decisions even when they are short of information\nHighly motivated, independent and deeply passionate about sustainability and impact\nExcellent oral and written English communication skills (minimum C1 level-proficient user)\n3+ years industry experience in the data analytics and data engineering field\nYou hold a degree in a quantitative field (CS, Engineer, Math, Physics, \u2026 ) and have great coding skills.\nYou possess very strong SQL, Python database skills, and can make invocations to APIs\nYou are very familiar with using tools such as SnowFlake, Airflow, AWS Services (ie. S3, Quicksight), and DBT\nExperience in advanced analytics, data engineering and a similar related field of data management\nExperience in collaborating with engineering teams and stakeholders to build key datasets and executing data pipelines using SQL/Python/ETL frameworks\nCan design data models and are familiar with best practices around data design and architecture\nCan work well with the Business and Technology stakeholders to deliver business initiatives on time by understanding the business need\nSelf-starter, able to take ownership and initiative, with high energy and stamina\nDecisive and action-oriented, able to make rapid decisions even when they are short of information\nHighly motivated, independent and deeply passionate about sustainability and impact\nExcellent oral and written English communication skills (minimum C1 level-proficient user)\nNice To Have \u2728\nExperience in the financial sector\nKnowledge of sustainability aspects\nExperience in the financial sector\nKnowledge of sustainability aspects\nWhat we offer \ud83e\udd41\nWhat we offer\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events"
    },
    "4117477824": {
        "title": "Data Engineer - OpenData Clinical ",
        "company": "Veeva Systems",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nVeeva Systems is a mission-driven organization and pioneer in industry cloud, helping life sciences companies bring therapies to patients faster. As one of the fastest-growing SaaS companies in history, we surpassed $2B in revenue in our last fiscal year with extensive growth potential ahead.\n\nAt the heart of Veeva are our values: Do the Right Thing, Customer Success, Employee Success, and Speed. We're not just any public company \u2013 we made history in 2021 by becoming a public benefit corporation (PBC), legally bound to balancing the interests of customers, employees, society, and investors.\n\nAs a Work Anywhere company, we support your flexibility to work from home or in the office, so you can thrive in your ideal environment.\n\nJoin us in transforming the life sciences industry, committed to making a positive impact on its customers, employees, and communities.\n\nThe Role\n\nOpenData Clinical is global reference data on sites and investigators.\n\nAs one of our data engineers, you will be the subject matter expert for questions regarding the OpenData Clinical data pipeline, structure, and quality. You will ensure the data we use meets the product goal and provide comprehensive data analysis for the various stakeholders. You will closely work with product management, data operations, data governance, software engineering, and quality assurance teams.\n\nTo thrive in your role, you need to have hands-on experience managing large databases and complex data sets. You are detail-oriented, have a growth mindset, and can turn data into valuable insights. You communicate effectively with stakeholders and can deliver high-quality results within a tough timeline.\n\nWhat You'll Do\n\nOptimize the existing data tools to streamline data operations\nDevelop a data validation framework to automate quality control processes\nIdentify data gaps and quality issues from large data sets and provide root cause analysis\nCollaborate with cross-functional teams to come up with compelling solutions to complex problems\nPerform ad-hoc analysis and create customized reports for various stakeholders\n\nRequirements\n\n3+ years of working experience as a data analyst or data engineer role, with 1+ years in a senior or leadership position\nProven track record delivering complex data analytics projects that drive business decisions with various stakeholders\nAdvanced SQL skills and proficiency in using Python or R for data manipulation and automation\nSkilled in data visualization tools (e.g., Tableau, Power BI)\nStrong written and verbal communication skills in English\nBased in Spain or Portugal\n\nNice to Have\n\nExperience with life science data\nExperience with cloud data tools\nFamiliarity with machine learning techniques\nAbility to create and maintain automated reporting systems\n\n#RemoteSpain\n\nVeeva\u2019s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world.\n\nAs an equal opportunity employer, Veeva is committed to fostering a culture of inclusion and growing a diverse workforce. Diversity makes us stronger. It comes in many forms. Gender, race, ethnicity, religion, politics, sexual orientation, age, disability and life experience shape us all into unique individuals. We value people for the individuals they are and the contributions they can bring to our teams.\n\nIf you need assistance or accommodation due to a disability or special need when applying for a role or in our recruitment process, please contact us at talent_accommodations@veeva.com.\nThe Role\nWhat You'll Do\nOptimize the existing data tools to streamline data operations\nDevelop a data validation framework to automate quality control processes\nIdentify data gaps and quality issues from large data sets and provide root cause analysis\nCollaborate with cross-functional teams to come up with compelling solutions to complex problems\nPerform ad-hoc analysis and create customized reports for various stakeholders\nOptimize the existing data tools to streamline data operations\nDevelop a data validation framework to automate quality control processes\nIdentify data gaps and quality issues from large data sets and provide root cause analysis\nCollaborate with cross-functional teams to come up with compelling solutions to complex problems\nPerform ad-hoc analysis and create customized reports for various stakeholders\nRequirements\n3+ years of working experience as a data analyst or data engineer role, with 1+ years in a senior or leadership position\nProven track record delivering complex data analytics projects that drive business decisions with various stakeholders\nAdvanced SQL skills and proficiency in using Python or R for data manipulation and automation\nSkilled in data visualization tools (e.g., Tableau, Power BI)\nStrong written and verbal communication skills in English\nBased in Spain or Portugal\n3+ years of working experience as a data analyst or data engineer role, with 1+ years in a senior or leadership position\nProven track record delivering complex data analytics projects that drive business decisions with various stakeholders\nAdvanced SQL skills and proficiency in using Python or R for data manipulation and automation\nSkilled in data visualization tools (e.g., Tableau, Power BI)\nStrong written and verbal communication skills in English\nBased in Spain or Portugal\nNice to Have\nExperience with life science data\nExperience with cloud data tools\nFamiliarity with machine learning techniques\nAbility to create and maintain automated reporting systems\nExperience with life science data\nExperience with cloud data tools\nFamiliarity with machine learning techniques\nAbility to create and maintain automated reporting systems"
    },
    "4057245303": {
        "title": "Lead Data Engineer (m/f/d) ",
        "company": "Thoughtworks",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nLead data engineers at Thoughtworks develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. They might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On projects, they will be leading the design of technical solutions, or perhaps overseeing a program inception to build a new product. Alongside hands-on coding, they are leading the team to implement the solution.\n\nJob responsibilities\n\nYou will lead and manage data engineering projects from inception to completion, including goal-setting, scope definition and ensuring on-time delivery with cross team collaboration.\nYou will collaborate with stakeholders to understand their strategic objectives and identify opportunities to leverage data and data quality.\nYou will design, develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions.\nYou will be responsible to create, design and develop intricate data processing pipelines, addressing clients' most challenging problems.\nYou will collaborate with data scientists to design scalable implementations of their models.\nYou write clean and iterative code based on TDD and leverage various continuous delivery practices to deploy, support and operate data pipelines.\nYou will lead and advise clients on how to use different distributed storage and computing technologies from the plethora of options available.\nYou will develop data models by selecting from a variety of modeling techniques and implementing the chosen data model using the appropriate technology stack.\nYou will be responsible for data governance, data security and data privacy to support business and compliance requirements.\nYou will define the strategy for and incorporate data quality into your day-to-day work.\n\n\nJob Qualifications\n\nTechnical Skills\n\nYou have experience in leading the system design and implementation of technical solutions.\nWorking with data excites you; You have created Big Data architecture, can build and operate data pipelines, and maintain data storage, all within distributed systems.\nYou have a deep understanding of data modeling and experience with modern data engineering tools and platforms.\nYou have experience in writing clean, high-quality code using the preferred programming language.\nYou have built and deployed large-scale data pipelines and data-centric applications using any of the distributed storage platforms and distributed processing platforms in a production setting.\nYou have experience with data visualization techniques and can communicate the insights as per the audience.\nYou have experience with data-driven approaches and can apply data security and privacy strategy to solve business problems.\nYou have experience with different types of databases (i.e.: SQL, NoSQL, data lake, data schemas, etc.).\n\n\nProfessional Skills\n\nYou understand the importance of stakeholder management and can easily liaise between clients and other key stakeholders throughout projects, ensuring buy-in and gaining trust along the way.\nYou are resilient in ambiguous situations and can adapt your role to approach challenges from multiple perspectives.\nYou don\u2019t shy away from risks or conflicts, instead you take them on and skillfully manage them.\nYou coach, mentor and motivate others and you aspire to influence teammates to take positive action and accountability for their work.\nYou enjoy influencing others and always advocate for technical excellence while being open to change when needed.\nYou are a proven leader with a track record of encouraging teammates in their professional development and relationships.\nCultivating strong partnerships comes naturally to you; You understand the importance of relationship building and how it can bring new opportunities to our business.\n\n\nOther things to know\n\nLearning & Development\n\nThere is no one-size-fits-all career path at Thoughtworks: however you want to develop your career is entirely up to you. But we also balance autonomy with the strength of our cultivation culture. This means your career is supported by interactive tools, numerous development programs and teammates who want to help you grow. We see value in helping each other be our best and that extends to empowering our employees in their career journeys.\n\nAbout Thoughtworks\n\nThoughtworks is a global technology consultancy that integrates strategy, design and engineering to drive digital innovation. For 30+ years, our clients have trusted our autonomous teams to build solutions that look past the obvious. Here, computer science grads come together with seasoned technologists, self-taught developers, midlife career changers and more to learn from and challenge each other. Career journeys flourish with the strength of our cultivation culture, which has won numerous awards around the world.\n\nJoin Thoughtworks and thrive. Together, our extra curiosity, innovation, passion and dedication overcomes ordinary.\nYou will lead and manage data engineering projects from inception to completion, including goal-setting, scope definition and ensuring on-time delivery with cross team collaboration.\nYou will collaborate with stakeholders to understand their strategic objectives and identify opportunities to leverage data and data quality.\nYou will design, develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions.\nYou will be responsible to create, design and develop intricate data processing pipelines, addressing clients' most challenging problems.\nYou will collaborate with data scientists to design scalable implementations of their models.\nYou write clean and iterative code based on TDD and leverage various continuous delivery practices to deploy, support and operate data pipelines.\nYou will lead and advise clients on how to use different distributed storage and computing technologies from the plethora of options available.\nYou will develop data models by selecting from a variety of modeling techniques and implementing the chosen data model using the appropriate technology stack.\nYou will be responsible for data governance, data security and data privacy to support business and compliance requirements.\nYou will define the strategy for and incorporate data quality into your day-to-day work.\nYou will lead and manage data engineering projects from inception to completion, including goal-setting, scope definition and ensuring on-time delivery with cross team collaboration.\nYou will collaborate with stakeholders to understand their strategic objectives and identify opportunities to leverage data and data quality.\nYou will design, develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions.\nYou will be responsible to create, design and develop intricate data processing pipelines, addressing clients' most challenging problems.\nYou will collaborate with data scientists to design scalable implementations of their models.\nYou write clean and iterative code based on TDD and leverage various continuous delivery practices to deploy, support and operate data pipelines.\nYou will lead and advise clients on how to use different distributed storage and computing technologies from the plethora of options available.\nYou will develop data models by selecting from a variety of modeling techniques and implementing the chosen data model using the appropriate technology stack.\nYou will be responsible for data governance, data security and data privacy to support business and compliance requirements.\nYou will define the strategy for and incorporate data quality into your day-to-day work.\nJob Qualifications\nYou have experience in leading the system design and implementation of technical solutions.\nWorking with data excites you; You have created Big Data architecture, can build and operate data pipelines, and maintain data storage, all within distributed systems.\nYou have a deep understanding of data modeling and experience with modern data engineering tools and platforms.\nYou have experience in writing clean, high-quality code using the preferred programming language.\nYou have built and deployed large-scale data pipelines and data-centric applications using any of the distributed storage platforms and distributed processing platforms in a production setting.\nYou have experience with data visualization techniques and can communicate the insights as per the audience.\nYou have experience with data-driven approaches and can apply data security and privacy strategy to solve business problems.\nYou have experience with different types of databases (i.e.: SQL, NoSQL, data lake, data schemas, etc.).\nYou have experience in leading the system design and implementation of technical solutions.\nWorking with data excites you; You have created Big Data architecture, can build and operate data pipelines, and maintain data storage, all within distributed systems.\nYou have a deep understanding of data modeling and experience with modern data engineering tools and platforms.\nYou have experience in writing clean, high-quality code using the preferred programming language.\nYou have built and deployed large-scale data pipelines and data-centric applications using any of the distributed storage platforms and distributed processing platforms in a production setting.\nYou have experience with data visualization techniques and can communicate the insights as per the audience.\nYou have experience with data-driven approaches and can apply data security and privacy strategy to solve business problems.\nYou have experience with different types of databases (i.e.: SQL, NoSQL, data lake, data schemas, etc.).\nYou understand the importance of stakeholder management and can easily liaise between clients and other key stakeholders throughout projects, ensuring buy-in and gaining trust along the way.\nYou are resilient in ambiguous situations and can adapt your role to approach challenges from multiple perspectives.\nYou don\u2019t shy away from risks or conflicts, instead you take them on and skillfully manage them.\nYou coach, mentor and motivate others and you aspire to influence teammates to take positive action and accountability for their work.\nYou enjoy influencing others and always advocate for technical excellence while being open to change when needed.\nYou are a proven leader with a track record of encouraging teammates in their professional development and relationships.\nCultivating strong partnerships comes naturally to you; You understand the importance of relationship building and how it can bring new opportunities to our business.\nYou understand the importance of stakeholder management and can easily liaise between clients and other key stakeholders throughout projects, ensuring buy-in and gaining trust along the way.\nYou are resilient in ambiguous situations and can adapt your role to approach challenges from multiple perspectives.\nYou don\u2019t shy away from risks or conflicts, instead you take them on and skillfully manage them.\nYou coach, mentor and motivate others and you aspire to influence teammates to take positive action and accountability for their work.\nYou enjoy influencing others and always advocate for technical excellence while being open to change when needed.\nYou are a proven leader with a track record of encouraging teammates in their professional development and relationships.\nCultivating strong partnerships comes naturally to you; You understand the importance of relationship building and how it can bring new opportunities to our business.\nOther things to know\nAbout Thoughtworks"
    },
    "4130523789": {
        "title": "Junior Business Process Data Engineer Spe ",
        "company": "IFF",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nJob Summary\n\nEntry-level position focused on building and maintaining data pipelines. Works under close supervision to ensure data is collected, stored, and processed efficiently. Assists in the development of data integration solutions, supports data quality initiatives, and performs basic cloud data services and infrastructure tasks. Also assists in application deployment and basic DevOps tasks. Emphasis is on learning and development, handling foundational data engineering tasks, and working under close supervision.\n\n Build and maintain data pipelines under supervision.\n Ensure data is collected, stored, and processed efficiently.\n Assist in the development of data - integration solutions.\n Support data quality initiatives.\n Document processes and results.\n Collaborate with team members to understand project requirements.\n Support basic cloud data services and infrastructure.\n Assist in application deployment and basic DevOps tasks.\n\nWe are a global leader in taste, scent, and nutrition, offering our customers a broader range of natural solutions and accelerating our growth strategy. At IFF, we believe that your uniqueness unleashes our potential. We value the diverse mosaic of the ethnicity, national origin, race, age, sex, or veteran status. We strive for an inclusive workplace that allows each of our colleagues to bring their authentic self to work regardless of their religion, gender identity & expression, sexual orientation, or disability.\n\nVisit IFF.com/careers/workplace-diversity-and-inclusion to learn more\nJob Summary\nBuild and maintain data pipelines under supervision.\n Ensure data is collected, stored, and processed efficiently.\n Assist in the development of data - integration solutions.\n Support data quality initiatives.\n Document processes and results.\n Collaborate with team members to understand project requirements.\n Support basic cloud data services and infrastructure.\n Assist in application deployment and basic DevOps tasks.\nBuild and maintain data pipelines under supervision.\nEnsure data is collected, stored, and processed efficiently.\nAssist in the development of data - integration solutions.\nSupport data quality initiatives.\nDocument processes and results.\nCollaborate with team members to understand project requirements.\nSupport basic cloud data services and infrastructure.\nAssist in application deployment and basic DevOps tasks."
    },
    "4176146900": {
        "title": "Senior Data Engineer (Integration) ",
        "company": "Robert Walters",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Contract",
        "description": "About the job\nFor one of the biggest global engineering companies based in Madrid, I am currently and urgently looking for the following profile to develop the data integration solutions across various sources and systems within the company's Asset Management organization.\nMain tasks:\n\n* Development of the data integration processes. \n* Design ETL pipelines \n* Work closely with the data quality manager in designing and implementing data quality checks to ensure high data accuracy and integrity. \n* Help the data governance team to adhere to and promote data governance standards and policies.\n* Collaborate with business analysts, IT teams, and other stakeholders to understand their data needs and ensure the data integration patterns are defined correctly to meet these requirements. \n* Ensure data integration pipelines for issues, perform root cause analysis, and implement fixes in a timely manner.\nRequirements:\n\n\u2714 Bachelor degree in Computer Science / Information Technology / Accounting / Finance, or equivalent\n\u2714 3+ years experience as Data Engineer \n\u2714 Solid experience with Python, SQL, Data Factory, Databricks \n\u2714 Strong experience in data integration and ETL development \n\u2714 Experience with Airflow and Collibra is a plus \n\u2714 English on a fluent proficiency level\n\nOffer:\n\n-> Perm contract, full time, hybrid mode (2 days on site)\n-> Attractive salary (in line with your experience) + bonus + package\n-> A dynamic, international and challenging work environment\n If you are interested, please don't hesitate to send me your CV (English version) and/or write me to have more information about the vacancy.\nFor one of the biggest global engineering companies based in Madrid, I am currently and urgently looking for the following profile to develop the data integration solutions across various sources and systems within the company's Asset Management organization.\nMain tasks:\n* Development of the data integration processes. \n* Design ETL pipelines \n* Work closely with the data quality manager in designing and implementing data quality checks to ensure high data accuracy and integrity. \n* Help the data governance team to adhere to and promote data governance standards and policies.\n* Collaborate with business analysts, IT teams, and other stakeholders to understand their data needs and ensure the data integration patterns are defined correctly to meet these requirements. \n* Ensure data integration pipelines for issues, perform root cause analysis, and implement fixes in a timely manner.\nRequirements:\n\u2714 Bachelor degree in Computer Science / Information Technology / Accounting / Finance, or equivalent\n\u2714 3+ years experience as Data Engineer \n\u2714 Solid experience with Python, SQL, Data Factory, Databricks \n\u2714 Strong experience in data integration and ETL development \n\u2714 Experience with Airflow and Collibra is a plus \n\u2714 English on a fluent proficiency level\nOffer:\n-> Perm contract, full time, hybrid mode (2 days on site)\n-> Attractive salary (in line with your experience) + bonus + package\n-> A dynamic, international and challenging work environment\nIf you are interested, please don't hesitate to send me your CV (English version) and/or write me to have more information about the vacancy."
    },
    "4171635319": {
        "title": "Data Engineer - Databricks - Mid Level ",
        "company": "Lumenalta",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nExperience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\n\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\n\nRequirements\n3+ years experience in a data engineering role using Python; ideally, you have delivered business-critical software to large enterprises\nYou are comfortable manipulating large data sets and handling raw SQL\nExperience using technologies such as Pyspark/AWS/Databricks is essential\nExperience creating ETL Pipeline from scratch\nE-commerce and Financial Services industry experience preferred\nEnglish fluency, verbal and written\nPersonality traits: Professional, problem solver, proactive, passionate, team player.\n\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\nBe 100% dedicated to one project at a time so that you can innovate and grow.\nBe a part of a team of talented and friendly senior-level developers.\nWork on projects that allow you to use leading tech.\n\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\n\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\n\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\nExperience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\nRequirements\n3+ years experience in a data engineering role using Python; ideally, you have delivered business-critical software to large enterprises\nYou are comfortable manipulating large data sets and handling raw SQL\nExperience using technologies such as Pyspark/AWS/Databricks is essential\nExperience creating ETL Pipeline from scratch\nE-commerce and Financial Services industry experience preferred\nEnglish fluency, verbal and written\nPersonality traits: Professional, problem solver, proactive, passionate, team player.\n3+ years experience in a data engineering role using Python; ideally, you have delivered business-critical software to large enterprises\nYou are comfortable manipulating large data sets and handling raw SQL\nExperience using technologies such as Pyspark/AWS/Databricks is essential\nExperience creating ETL Pipeline from scratch\nE-commerce and Financial Services industry experience preferred\nEnglish fluency, verbal and written\nPersonality traits: Professional, problem solver, proactive, passionate, team player.\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\nBe 100% dedicated to one project at a time so that you can innovate and grow.\nBe a part of a team of talented and friendly senior-level developers.\nWork on projects that allow you to use leading tech.\nBe 100% dedicated to one project at a time so that you can innovate and grow.\nBe a part of a team of talented and friendly senior-level developers.\nWork on projects that allow you to use leading tech.\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply"
    },
    "4076668001": {
        "title": "Senior Data Engineer ",
        "company": "Clarity AI",
        "location": "Baleares, Balearic Islands, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nAbout Clarity AI \ud83e\udeb4\nClarity AI is a global tech company founded in 2017 committed to bringing societal impact to markets. We leverage AI and machine learning technologies to provide investors, governments, companies, and consumers with the right data, methodologies, and tools to make more informed decisions.\nWe are now a team of more than 300 highly passionate individuals from all over the world, with offices in New York, Madrid, London, Paris, and Abu Dhabi. Together, we have established Clarity AI as a leading sustainability tech company backed by investors and strategic partners such as BlackRock, SoftBank, and Deutsche B\u00f6rse, who believe in us and share our goals. We are dedicated to cultivating an exceptional workplace environment. We take pride in our culture, defined by our commitment to being fact-based, diverse, transparent, meritocratic, and flexible. \nWe have plans to continue growing our teams globally, so if you would like to join us on this rocket ship, keep reading! Your work will shape and guide the sustainable decisions of investors, companies and consumers worldwide.\nAbout The Role \ud83d\udcbb\n\nThe Senior Data Engineer will lead the development, and implementation of data architectures that support our data-driven products and services. This role involves high-level technical blueprints skills, ensuring efficient data flows, and optimizing data systems for scalability and reliability. The ideal candidate combines technical expertise in data engineering with a strategic mindset, capable of assessing when and why to apply various technologies for scalability and performance, helping shape our data infrastructure to meet the demands of a fast-paced, innovative environment.\nWhat You\u2019ll Be Doing \ud83d\ude80\n1. Data Pipeline and Modeling\nTransform raw data into intuitive, high-quality data models that support analytical and reporting needs across the organization. Apply scalable design patterns but avoid over-engineering, ensuring solutions remain efficient and fit-for-purpose.\n2. Data Quality Assurance\nImplement and monitor data quality checks to ensure the accuracy, completeness, and reliability of data across all systems.\n3. Cross-functional Collaboration\nWork closely with engineering and product teams to understand business requirements, translating them into scalable data solutions.\nAct as a bridge between technical and non-technical stakeholders, ensuring alignment with strategic goals and effective communication of technical designs.\n4. Technical Leadership\nLead initiatives to improve data practices, from schema design to data governance, ensuring data quality, consistency, and security.\nGuide the team in experimenting with new tools and technologies thoughtfully, with a focus on understanding both the benefits and limitations of each option.\n5. Performance Optimization\nContinuously evolve the data architecture for optimal performance, balancing scalability with cost-efficiency and reliability.\nApply a pragmatic approach to performance metrics and scaling decisions, ensuring that the system remains performant without unnecessary complexity.\nImplement performance metrics to monitor system health, proposing improvements where necessary.\n6. Documentation\nMaintain comprehensive documentation of data systems, processes, and best practices to facilitate knowledge sharing and compliance.\n\nRequirements\n\nWhat You\u2019ll Need \ud83d\udc40\nExperience:\n5+ years in data architecture, data engineering, or a related role, with hands-on experience in data modeling, schema design, and cloud-based data systems.\nStrong Python and SQL skills and proficiency with distributed data stores.\nTechnical Skills:\nProficiency in schema design, data modeling, and building data products.\nProficient in at least one major programming language, preferably Python, with a software engineering mindset for writing clean, maintainable code.\nDeep understanding of architectural principles in microservices, distributed systems, and data pipeline design.\nFamiliarity with containerized environments, public/private API integrations, and security best practices.\nLeadership Skills:\nStrong communication and interpersonal skills, with experience working cross-functionally.\nProven ability to guide teams and drive complex projects to successful completion.\n\nNice to Have \u2728\nExperience with Domain-Driven Design (DDD) and Bounded Contexts to help shape and organize our data products around clear, meaningful domains.\nFamiliarity with Team Topologies to drive team structures that optimize flow and collaboration, ensuring alignment between teams and business goals.\nStrong understanding of Lean and Agile methodologies, with a passion for continuous improvement, applying Lean principles to reduce waste and iteratively delivering high-value outcomes.\nWhat we offer \ud83e\udd41\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events\n\nMore About Clarity AI \u2b50\n\nClarity AI\u2019s Founder and CEO, Rebeca Minguela, is a successful entrepreneur who has been recognised by prestigious institutions like the World Economic Forum as one of the most distinguished leaders under 40.\nThe leadership team has an international presence and is composed of professionals from leading tech, consulting, and banking firms, entrepreneurs, PhDs from top research institutions, and MBA graduates from top business schools.\nClarity AI has received several awards:\nThe Forrester New Wave - ESG Ratings, Data, and Analytics - Leaders for 2022-2024\nInvestment Week - Best Sustainable Investment Research & Ratings Provider 2023\nFast Company - Most Innovative Companies 2023\nEuropean Commission | EU Seal of Excellence 2020 \nWorld Economic Forum - Technology Pioneer 2020\nWorld Economic Forum, Young Global Leader - Rebeca Minguela\n\nBy submitting an application to Clarity AI for this position you agree to Clarity AI\u2019s Privacy Policy.\nClarity AI is committed to fostering a diverse and inclusive workplace. Clarity AI is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, ancestry, religion, sex, sexual orientation, gender identity, national origin, age, disability status, protected veteran status, or any other characteristic protected by law. If you are interested in applying for employment with Clarity AI and need special assistance or an accommodation to use our website or to apply for a position, please email people@clarity.ai. Determinations on requests for reasonable accommodation are made on a case-by-case basis.\nAbout Clarity AI \ud83e\udeb4\nAbout Clarity AI\nClarity AI is a global tech company founded in 2017 committed to bringing societal impact to markets. We leverage AI and machine learning technologies to provide investors, governments, companies, and consumers with the right data, methodologies, and tools to make more informed decisions.\nbringing societal impact to markets.\ninvestors, governments, companies, and consumers\nWe are now a team of more than 300 highly passionate individuals from all over the world, with offices in New York, Madrid, London, Paris, and Abu Dhabi. Together, we have established Clarity AI as a leading sustainability tech company backed by investors and strategic partners such as BlackRock, SoftBank, and Deutsche B\u00f6rse, who believe in us and share our goals. We are dedicated to cultivating an exceptional workplace environment. We take pride in our culture, defined by our commitment to being fact-based, diverse, transparent, meritocratic, and flexible.\n300 highly passionate individuals\nleading sustainability tech company\nBlackRock, SoftBank, and Deutsche B\u00f6rse\nfact-based, diverse, transparent, meritocratic, and flexible.\nWe have plans to continue growing our teams globally, so if you would like to join us on this rocket ship, keep reading! Your work will shape and guide the sustainable decisions of investors, companies and consumers worldwide.\nAbout The Role \ud83d\udcbb\nAbout The Role\nThe Senior Data Engineer will lead the development, and implementation of data architectures that support our data-driven products and services. This role involves high-level technical blueprints skills, ensuring efficient data flows, and optimizing data systems for scalability and reliability. The ideal candidate combines technical expertise in data engineering with a strategic mindset, capable of assessing when and why to apply various technologies for scalability and performance, helping shape our data infrastructure to meet the demands of a fast-paced, innovative environment.\nWhat You\u2019ll Be Doing \ud83d\ude80\nWhat You\u2019ll Be Doing\n1. Data Pipeline and Modeling\nTransform raw data into intuitive, high-quality data models that support analytical and reporting needs across the organization. Apply scalable design patterns but avoid over-engineering, ensuring solutions remain efficient and fit-for-purpose.\n2. Data Quality Assurance\nImplement and monitor data quality checks to ensure the accuracy, completeness, and reliability of data across all systems.\n3. Cross-functional Collaboration\nWork closely with engineering and product teams to understand business requirements, translating them into scalable data solutions.\nAct as a bridge between technical and non-technical stakeholders, ensuring alignment with strategic goals and effective communication of technical designs.\nWork closely with engineering and product teams to understand business requirements, translating them into scalable data solutions.\nAct as a bridge between technical and non-technical stakeholders, ensuring alignment with strategic goals and effective communication of technical designs.\n4. Technical Leadership\nLead initiatives to improve data practices, from schema design to data governance, ensuring data quality, consistency, and security.\nGuide the team in experimenting with new tools and technologies thoughtfully, with a focus on understanding both the benefits and limitations of each option.\nLead initiatives to improve data practices, from schema design to data governance, ensuring data quality, consistency, and security.\nGuide the team in experimenting with new tools and technologies thoughtfully, with a focus on understanding both the benefits and limitations of each option.\n5. Performance Optimization\nContinuously evolve the data architecture for optimal performance, balancing scalability with cost-efficiency and reliability.\nApply a pragmatic approach to performance metrics and scaling decisions, ensuring that the system remains performant without unnecessary complexity.\nImplement performance metrics to monitor system health, proposing improvements where necessary.\nContinuously evolve the data architecture for optimal performance, balancing scalability with cost-efficiency and reliability.\nApply a pragmatic approach to performance metrics and scaling decisions, ensuring that the system remains performant without unnecessary complexity.\nImplement performance metrics to monitor system health, proposing improvements where necessary.\n6. Documentation\nMaintain comprehensive documentation of data systems, processes, and best practices to facilitate knowledge sharing and compliance.\nRequirements\nWhat You\u2019ll Need \ud83d\udc40\nExperience:\n5+ years in data architecture, data engineering, or a related role, with hands-on experience in data modeling, schema design, and cloud-based data systems.\nStrong Python and SQL skills and proficiency with distributed data stores.\nTechnical Skills:\nProficiency in schema design, data modeling, and building data products.\nProficient in at least one major programming language, preferably Python, with a software engineering mindset for writing clean, maintainable code.\nDeep understanding of architectural principles in microservices, distributed systems, and data pipeline design.\nFamiliarity with containerized environments, public/private API integrations, and security best practices.\nLeadership Skills:\nStrong communication and interpersonal skills, with experience working cross-functionally.\nProven ability to guide teams and drive complex projects to successful completion.\n5+ years in data architecture, data engineering, or a related role, with hands-on experience in data modeling, schema design, and cloud-based data systems.\nStrong Python and SQL skills and proficiency with distributed data stores.\nTechnical Skills:\nProficiency in schema design, data modeling, and building data products.\nProficient in at least one major programming language, preferably Python, with a software engineering mindset for writing clean, maintainable code.\nDeep understanding of architectural principles in microservices, distributed systems, and data pipeline design.\nFamiliarity with containerized environments, public/private API integrations, and security best practices.\nLeadership Skills:\nStrong communication and interpersonal skills, with experience working cross-functionally.\nProven ability to guide teams and drive complex projects to successful completion.\nNice to Have \u2728\nExperience with Domain-Driven Design (DDD) and Bounded Contexts to help shape and organize our data products around clear, meaningful domains.\nFamiliarity with Team Topologies to drive team structures that optimize flow and collaboration, ensuring alignment between teams and business goals.\nStrong understanding of Lean and Agile methodologies, with a passion for continuous improvement, applying Lean principles to reduce waste and iteratively delivering high-value outcomes.\nExperience with Domain-Driven Design (DDD) and Bounded Contexts to help shape and organize our data products around clear, meaningful domains.\nFamiliarity with Team Topologies to drive team structures that optimize flow and collaboration, ensuring alignment between teams and business goals.\nStrong understanding of Lean and Agile methodologies, with a passion for continuous improvement, applying Lean principles to reduce waste and iteratively delivering high-value outcomes.\nWhat we offer \ud83e\udd41\nWhat we offer\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events\nCompetitive compensation\nEquity Plans, ESOP/Phantom Stock, to share our success\nFlexibility in terms of schedules and location, whether you prefer to work from home, the office, or abroad with access to a global network of co-working spaces\nGenerous paid time off schemes that include vacation, sabbatical, religious observance and compensation days\nPrivate Healthcare coverage that can be extended to your family members\nFitness and Wellness benefits covered through Wellhub\nWorking from home allowances to help you set up your home office and cover monthly expenses\nAnnual training budget to support your professional growth\nRegular team activities and events\nMore About Clarity AI \u2b50\nClarity AI\u2019s Founder and CEO, Rebeca Minguela, is a successful entrepreneur who has been recognised by prestigious institutions like the World Economic Forum as one of the most distinguished leaders under 40.\nFounder and CEO, Rebeca Minguela\nThe leadership team has an international presence and is composed of professionals from leading tech, consulting, and banking firms, entrepreneurs, PhDs from top research institutions, and MBA graduates from top business schools.\nClarity AI has received several awards:\nThe Forrester New Wave - ESG Ratings, Data, and Analytics - Leaders for 2022-2024\nInvestment Week - Best Sustainable Investment Research & Ratings Provider 2023\nFast Company - Most Innovative Companies 2023\nEuropean Commission | EU Seal of Excellence 2020 \nWorld Economic Forum - Technology Pioneer 2020\nWorld Economic Forum, Young Global Leader - Rebeca Minguela\nThe Forrester New Wave - ESG Ratings, Data, and Analytics - Leaders for 2022-2024\nThe Forrester New Wave\nInvestment Week - Best Sustainable Investment Research & Ratings Provider 2023\nInvestment Week\nFast Company - Most Innovative Companies 2023\nFast Company\nEuropean Commission | EU Seal of Excellence 2020\nWorld Economic Forum - Technology Pioneer 2020\nWorld Economic Forum\nWorld Economic Forum, Young Global Leader - Rebeca Minguela\nBy submitting an application to Clarity AI for this position you agree to Clarity AI\u2019s Privacy Policy.\nClarity AI is committed to fostering a diverse and inclusive workplace. Clarity AI is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, ancestry, religion, sex, sexual orientation, gender identity, national origin, age, disability status, protected veteran status, or any other characteristic protected by law. If you are interested in applying for employment with Clarity AI and need special assistance or an accommodation to use our website or to apply for a position, please email people@clarity.ai. Determinations on requests for reasonable accommodation are made on a case-by-case basis."
    },
    "3931418978": {
        "title": "Data Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Galicia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\n\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca \n\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \u270d\n\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n? \ud83d\udc47\n\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada semestre, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario competitivo seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\n\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \u270d\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\ndesarrollo y ejecuci\u00f3n de diversos proyectos\ndefinici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nSQL\nETL\ndatawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nformaci\u00f3n interna y externa en las herramientas relevantes para el equipo y\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\ntendencias tecnol\u00f3gicas\nData & Analytics\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\ncultura de SDG y su unidad de trabajo\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\ndin\u00e1micas de compa\u00f1\u00eda y de equipo\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\nctividades operativas de la compa\u00f1\u00eda\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n? \ud83d\udc47\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nSQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\n\u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\nvisualizaci\u00f3n con foco en Tableau y PowerBI.\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada semestre, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario competitivo seg\u00fan tu formaci\u00f3n y experiencia\nSalario competitivo\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5"
    },
    "4103095486": {
        "title": "Data Engineer ",
        "company": "Playson",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nPlayson is seeking a Data Engineer to join our growing team. In this role, you\u2019ll design, build, and maintain data infrastructure to ensure smooth and scalable data operations. If you\u2019re passionate about robust data pipelines and leveraging technology to empower decision-making, we\u2019d love to hear from you.\n\nWhat You'll Be Doing\n\nDesign, develop and maintain efficient, scalable data pipelines to support BI and analytics.\nEnsure data accuracy, integrity, and security across all systems.\nCollaborate with the BI team to provide the infrastructure needed for actionable insights.\nOptimize ETL processes for speed and reliability.\nProactively evaluate and integrate new tools and technologies into existing systems, focusing on scalability and innovation.\nTroubleshoot, debug, and resolve complex technical challenges in a fast-paced environment.\n\nWhat You\u2019ll Bring\n\n4+ years of experience in data engineering or a similar role.\nExpertise in ETL processes, data pipelines, and managing large-scale data systems.\nProficiency in SQL, Python, and microservice architectures.\nExperience building OLAP databases and integrating them with BI tools like Power BI, Tableau, or similar platforms.\nStrong understanding of data ingestion, database design, data warehouse, and data lake concepts, as well as best practices for data management.\nProblem-solving mindset with the ability to resolve complex technical challenges.\n\nNice-to-Have Skills\n\nExperience with real-time data streaming platforms such as Apache Kafka.\nExperience building data pipelines on AWS. \nExperience with Redash and Clickhouse.\nUnderstanding of AI/ML workflows for data science.\nFamiliarity with containerization and orchestration tools like Docker and Kubernetes.\nUnderstanding of iGaming market, game provider business model and relevant metrics\nEntrepreneurial mindset to thrive in a fast-paced environment.\n\nWhat We Offer\n\nCompetitive compensation with transparent bonus structures.\nComprehensive medical insurance and life event financial support.\nUnlimited vacation and sick leave policies.\nOpportunities to attend industry-leading events and grow professionally.\nAccess to cutting-edge technology and tools in a dynamic industry.\nData Engineer\nWhat You'll Be Doing\nDesign, develop and maintain efficient, scalable data pipelines to support BI and analytics.\nEnsure data accuracy, integrity, and security across all systems.\nCollaborate with the BI team to provide the infrastructure needed for actionable insights.\nOptimize ETL processes for speed and reliability.\nProactively evaluate and integrate new tools and technologies into existing systems, focusing on scalability and innovation.\nTroubleshoot, debug, and resolve complex technical challenges in a fast-paced environment.\nDesign, develop and maintain efficient, scalable data pipelines to support BI and analytics.\nEnsure data accuracy, integrity, and security across all systems.\nCollaborate with the BI team to provide the infrastructure needed for actionable insights.\nOptimize ETL processes for speed and reliability.\nProactively evaluate and integrate new tools and technologies into existing systems, focusing on scalability and innovation.\nTroubleshoot, debug, and resolve complex technical challenges in a fast-paced environment.\nWhat You\u2019ll Bring\n4+ years of experience in data engineering or a similar role.\nExpertise in ETL processes, data pipelines, and managing large-scale data systems.\nProficiency in SQL, Python, and microservice architectures.\nExperience building OLAP databases and integrating them with BI tools like Power BI, Tableau, or similar platforms.\nStrong understanding of data ingestion, database design, data warehouse, and data lake concepts, as well as best practices for data management.\nProblem-solving mindset with the ability to resolve complex technical challenges.\n4+ years of experience in data engineering or a similar role.\nExpertise in ETL processes, data pipelines, and managing large-scale data systems.\nProficiency in SQL, Python, and microservice architectures.\nExperience building OLAP databases and integrating them with BI tools like Power BI, Tableau, or similar platforms.\nStrong understanding of data ingestion, database design, data warehouse, and data lake concepts, as well as best practices for data management.\nProblem-solving mindset with the ability to resolve complex technical challenges.\nNice-to-Have Skills\nExperience with real-time data streaming platforms such as Apache Kafka.\nExperience building data pipelines on AWS. \nExperience with Redash and Clickhouse.\nUnderstanding of AI/ML workflows for data science.\nFamiliarity with containerization and orchestration tools like Docker and Kubernetes.\nUnderstanding of iGaming market, game provider business model and relevant metrics\nEntrepreneurial mindset to thrive in a fast-paced environment.\nExperience with real-time data streaming platforms such as Apache Kafka.\nExperience building data pipelines on AWS.\nExperience with Redash and Clickhouse.\nUnderstanding of AI/ML workflows for data science.\nFamiliarity with containerization and orchestration tools like Docker and Kubernetes.\nUnderstanding of iGaming market, game provider business model and relevant metrics\nEntrepreneurial mindset to thrive in a fast-paced environment.\nWhat We Offer\nCompetitive compensation with transparent bonus structures.\nComprehensive medical insurance and life event financial support.\nUnlimited vacation and sick leave policies.\nOpportunities to attend industry-leading events and grow professionally.\nAccess to cutting-edge technology and tools in a dynamic industry.\nCompetitive compensation with transparent bonus structures.\nComprehensive medical insurance and life event financial support.\nUnlimited vacation and sick leave policies.\nOpportunities to attend industry-leading events and grow professionally.\nAccess to cutting-edge technology and tools in a dynamic industry."
    },
    "4058036262": {
        "title": "Lead Data Engineer (m/f/d) ",
        "company": "Thoughtworks",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nLead data engineers at Thoughtworks develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. They might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On projects, they will be leading the design of technical solutions, or perhaps overseeing a program inception to build a new product. Alongside hands-on coding, they are leading the team to implement the solution.\n\nJob responsibilities\n\nYou will lead and manage data engineering projects from inception to completion, including goal-setting, scope definition and ensuring on-time delivery with cross team collaboration.\nYou will collaborate with stakeholders to understand their strategic objectives and identify opportunities to leverage data and data quality.\nYou will design, develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions.\nYou will be responsible to create, design and develop intricate data processing pipelines, addressing clients' most challenging problems.\nYou will collaborate with data scientists to design scalable implementations of their models.\nYou write clean and iterative code based on TDD and leverage various continuous delivery practices to deploy, support and operate data pipelines.\nYou will lead and advise clients on how to use different distributed storage and computing technologies from the plethora of options available.\nYou will develop data models by selecting from a variety of modeling techniques and implementing the chosen data model using the appropriate technology stack.\nYou will be responsible for data governance, data security and data privacy to support business and compliance requirements.\nYou will define the strategy for and incorporate data quality into your day-to-day work.\n\n\nJob Qualifications\n\nTechnical Skills\n\nYou have experience in leading the system design and implementation of technical solutions.\nWorking with data excites you; You have created Big Data architecture, can build and operate data pipelines, and maintain data storage, all within distributed systems.\nYou have a deep understanding of data modeling and experience with modern data engineering tools and platforms.\nYou have experience in writing clean, high-quality code using the preferred programming language.\nYou have built and deployed large-scale data pipelines and data-centric applications using any of the distributed storage platforms and distributed processing platforms in a production setting.\nYou have experience with data visualization techniques and can communicate the insights as per the audience.\nYou have experience with data-driven approaches and can apply data security and privacy strategy to solve business problems.\nYou have experience with different types of databases (i.e.: SQL, NoSQL, data lake, data schemas, etc.).\n\n\nProfessional Skills\n\nYou understand the importance of stakeholder management and can easily liaise between clients and other key stakeholders throughout projects, ensuring buy-in and gaining trust along the way.\nYou are resilient in ambiguous situations and can adapt your role to approach challenges from multiple perspectives.\nYou don\u2019t shy away from risks or conflicts, instead you take them on and skillfully manage them.\nYou coach, mentor and motivate others and you aspire to influence teammates to take positive action and accountability for their work.\nYou enjoy influencing others and always advocate for technical excellence while being open to change when needed.\nYou are a proven leader with a track record of encouraging teammates in their professional development and relationships.\nCultivating strong partnerships comes naturally to you; You understand the importance of relationship building and how it can bring new opportunities to our business.\n\n\nOther things to know\n\nLearning & Development\n\nThere is no one-size-fits-all career path at Thoughtworks: however you want to develop your career is entirely up to you. But we also balance autonomy with the strength of our cultivation culture. This means your career is supported by interactive tools, numerous development programs and teammates who want to help you grow. We see value in helping each other be our best and that extends to empowering our employees in their career journeys.\n\nAbout Thoughtworks\n\nThoughtworks is a global technology consultancy that integrates strategy, design and engineering to drive digital innovation. For 30+ years, our clients have trusted our autonomous teams to build solutions that look past the obvious. Here, computer science grads come together with seasoned technologists, self-taught developers, midlife career changers and more to learn from and challenge each other. Career journeys flourish with the strength of our cultivation culture, which has won numerous awards around the world.\n\nJoin Thoughtworks and thrive. Together, our extra curiosity, innovation, passion and dedication overcomes ordinary.\nYou will lead and manage data engineering projects from inception to completion, including goal-setting, scope definition and ensuring on-time delivery with cross team collaboration.\nYou will collaborate with stakeholders to understand their strategic objectives and identify opportunities to leverage data and data quality.\nYou will design, develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions.\nYou will be responsible to create, design and develop intricate data processing pipelines, addressing clients' most challenging problems.\nYou will collaborate with data scientists to design scalable implementations of their models.\nYou write clean and iterative code based on TDD and leverage various continuous delivery practices to deploy, support and operate data pipelines.\nYou will lead and advise clients on how to use different distributed storage and computing technologies from the plethora of options available.\nYou will develop data models by selecting from a variety of modeling techniques and implementing the chosen data model using the appropriate technology stack.\nYou will be responsible for data governance, data security and data privacy to support business and compliance requirements.\nYou will define the strategy for and incorporate data quality into your day-to-day work.\nYou will lead and manage data engineering projects from inception to completion, including goal-setting, scope definition and ensuring on-time delivery with cross team collaboration.\nYou will collaborate with stakeholders to understand their strategic objectives and identify opportunities to leverage data and data quality.\nYou will design, develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions.\nYou will be responsible to create, design and develop intricate data processing pipelines, addressing clients' most challenging problems.\nYou will collaborate with data scientists to design scalable implementations of their models.\nYou write clean and iterative code based on TDD and leverage various continuous delivery practices to deploy, support and operate data pipelines.\nYou will lead and advise clients on how to use different distributed storage and computing technologies from the plethora of options available.\nYou will develop data models by selecting from a variety of modeling techniques and implementing the chosen data model using the appropriate technology stack.\nYou will be responsible for data governance, data security and data privacy to support business and compliance requirements.\nYou will define the strategy for and incorporate data quality into your day-to-day work.\nJob Qualifications\nYou have experience in leading the system design and implementation of technical solutions.\nWorking with data excites you; You have created Big Data architecture, can build and operate data pipelines, and maintain data storage, all within distributed systems.\nYou have a deep understanding of data modeling and experience with modern data engineering tools and platforms.\nYou have experience in writing clean, high-quality code using the preferred programming language.\nYou have built and deployed large-scale data pipelines and data-centric applications using any of the distributed storage platforms and distributed processing platforms in a production setting.\nYou have experience with data visualization techniques and can communicate the insights as per the audience.\nYou have experience with data-driven approaches and can apply data security and privacy strategy to solve business problems.\nYou have experience with different types of databases (i.e.: SQL, NoSQL, data lake, data schemas, etc.).\nYou have experience in leading the system design and implementation of technical solutions.\nWorking with data excites you; You have created Big Data architecture, can build and operate data pipelines, and maintain data storage, all within distributed systems.\nYou have a deep understanding of data modeling and experience with modern data engineering tools and platforms.\nYou have experience in writing clean, high-quality code using the preferred programming language.\nYou have built and deployed large-scale data pipelines and data-centric applications using any of the distributed storage platforms and distributed processing platforms in a production setting.\nYou have experience with data visualization techniques and can communicate the insights as per the audience.\nYou have experience with data-driven approaches and can apply data security and privacy strategy to solve business problems.\nYou have experience with different types of databases (i.e.: SQL, NoSQL, data lake, data schemas, etc.).\nYou understand the importance of stakeholder management and can easily liaise between clients and other key stakeholders throughout projects, ensuring buy-in and gaining trust along the way.\nYou are resilient in ambiguous situations and can adapt your role to approach challenges from multiple perspectives.\nYou don\u2019t shy away from risks or conflicts, instead you take them on and skillfully manage them.\nYou coach, mentor and motivate others and you aspire to influence teammates to take positive action and accountability for their work.\nYou enjoy influencing others and always advocate for technical excellence while being open to change when needed.\nYou are a proven leader with a track record of encouraging teammates in their professional development and relationships.\nCultivating strong partnerships comes naturally to you; You understand the importance of relationship building and how it can bring new opportunities to our business.\nYou understand the importance of stakeholder management and can easily liaise between clients and other key stakeholders throughout projects, ensuring buy-in and gaining trust along the way.\nYou are resilient in ambiguous situations and can adapt your role to approach challenges from multiple perspectives.\nYou don\u2019t shy away from risks or conflicts, instead you take them on and skillfully manage them.\nYou coach, mentor and motivate others and you aspire to influence teammates to take positive action and accountability for their work.\nYou enjoy influencing others and always advocate for technical excellence while being open to change when needed.\nYou are a proven leader with a track record of encouraging teammates in their professional development and relationships.\nCultivating strong partnerships comes naturally to you; You understand the importance of relationship building and how it can bring new opportunities to our business.\nOther things to know\nAbout Thoughtworks"
    },
    "4169924356": {
        "title": "Data Platform Engineer ",
        "company": "Paradigma Digital",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEl futuro se construye con talento, pasi\u00f3n e innovaci\u00f3n. \ud83d\ude80 En Paradigma, sabemos que la tecnolog\u00eda es la clave para transformar el mundo, pero son las personas quienes hacen la diferencia.\n\n\ud83d\udd0e Estamos buscando un Data Platform Engineer para incorporarse a nuestro equipo de Data.\n\n\ud83d\udcbc Un Data Platform Engineer en Paradigma, combina habilidades de Data Engineering y DevOps para construir y mantener infraestructuras de datos robustas y escalables. Este rol se centra en la implementaci\u00f3n de infraestructura como c\u00f3digo (IaC), pipelines de CI/CD y la gesti\u00f3n de plataformas en la nube. Adem\u00e1s, asegura la ingesta, tratamiento y disponibilidad de datos, apoyando tanto la anal\u00edtica de negocio como las aplicaciones de Machine Learning y BI.\n\n\ud83d\udccc Principales responsabilidades del puesto:\nImplementaci\u00f3n de IaC: Dise\u00f1ar e implementar infraestructura como c\u00f3digo usando herramientas como Terraform y AWS CDK.\nDesarrollo de pipelines CI/CD: Configurar y mantener pipelines de integraci\u00f3n y despliegue continuo utilizando Jenkins, AWS Code Pipeline, AWS Code Build y Azure Artifacts.\nGesti\u00f3n de plataformas en la nube: Desplegar y administrar infraestructuras de datos en AWS, GCP y Azure.\nSeguridad y gobierno del dato: Implementar pol\u00edticas y procedimientos para asegurar la integridad y seguridad de los datos.\nDesarrollo de ETLs: Dise\u00f1ar y desarrollar procesos de extracci\u00f3n, transformaci\u00f3n y carga de datos.\nGesti\u00f3n del ciclo de vida del dato: Manejar el ciclo completo de los datos desde la ingesta y tratamiento hasta su disponibilizaci\u00f3n.\nProductos de datos: Crear y mantener productos de datos, incluyendo datamarts y data warehouses (DWH).\nImplementaci\u00f3n de BI: Desarrollar y gestionar soluciones de business intelligence utilizando herramientas como Power BI.\nConocimiento b\u00e1sico de flujos de ML.\n\n\ud83c\udfaf Experiencia y conocimientos necesarios:\nExperiencia en herramientas de IaC.\nExperiencia en implementaci\u00f3n y mantenimiento de pipelines de CI/CD.\nExperiencia en automatizaci\u00f3n y despliegue de infraestructuras para\nplataformas de datos en entornos Cloud.\nConocimientos de seguridad y ciclo de vida del dato.\nExperiencia en el desarrollo de ETLs.\nConocimientos de flujos de ML y su integraci\u00f3n con plataformas de datos.\n\n\ud83d\udca1 \u00bfQu\u00e9 encontrar\u00e1s en Paradigma?\n\u2705 Paradigma Everywhere: Modelo de trabajo 100% flexible. T\u00fa eliges desde d\u00f3nde trabajar.\n\u2705 Flexibilidad real: Conciliaci\u00f3n entre vida personal y laboral con horarios adaptables.\n\u2705 Innovaci\u00f3n: Proyectos retadores con tecnolog\u00edas de vanguardia.\n\u2705 El dominio t\u00e9cnico son parte de nuestra identidad.\n\u2705 Formaci\u00f3n continua: Cursos, meetups, clases de ingl\u00e9s, ayudas para formaci\u00f3n y eventos.\n\u2705 Beneficios sociales: Seguro m\u00e9dico 100% cubierto.\n\ud83d\udd17 Con\u00f3cenos mejor aqu\u00ed: YouTube Paradigma Digital\n\n\ud83c\udf0d Nuestro compromiso con la diversidad e inclusi\u00f3n:\nEn Paradigma, la igualdad es clave. Creamos un entorno donde todas las personas son bienvenidas, independientemente de g\u00e9nero, edad, religi\u00f3n, orientaci\u00f3n sexual o cualquier otra condici\u00f3n. Trabajamos para garantizar la igualdad salarial y de oportunidades.\nSe valorar\u00e1 certificado de discapacidad.\n\nEn Paradigma, creamos el cambio. \u00bfTe sumas? \ud83d\udc99\nEl futuro se construye con talento, pasi\u00f3n e innovaci\u00f3n. \ud83d\ude80 En Paradigma, sabemos que la tecnolog\u00eda es la clave para transformar el mundo, pero son las personas quienes hacen la diferencia.\n\ud83d\udd0e Estamos buscando un Data Platform Engineer para incorporarse a nuestro equipo de Data.\nData Platform Engineer\n\ud83d\udcbc Un Data Platform Engineer en Paradigma, combina habilidades de Data Engineering y DevOps para construir y mantener infraestructuras de datos robustas y escalables. Este rol se centra en la implementaci\u00f3n de infraestructura como c\u00f3digo (IaC), pipelines de CI/CD y la gesti\u00f3n de plataformas en la nube. Adem\u00e1s, asegura la ingesta, tratamiento y disponibilidad de datos, apoyando tanto la anal\u00edtica de negocio como las aplicaciones de Machine Learning y BI.\n\ud83d\udcbc\n\ud83d\udccc Principales responsabilidades del puesto:\nPrincipales responsabilidades del puesto:\nImplementaci\u00f3n de IaC: Dise\u00f1ar e implementar infraestructura como c\u00f3digo usando herramientas como Terraform y AWS CDK.\nDesarrollo de pipelines CI/CD: Configurar y mantener pipelines de integraci\u00f3n y despliegue continuo utilizando Jenkins, AWS Code Pipeline, AWS Code Build y Azure Artifacts.\nGesti\u00f3n de plataformas en la nube: Desplegar y administrar infraestructuras de datos en AWS, GCP y Azure.\nSeguridad y gobierno del dato: Implementar pol\u00edticas y procedimientos para asegurar la integridad y seguridad de los datos.\nDesarrollo de ETLs: Dise\u00f1ar y desarrollar procesos de extracci\u00f3n, transformaci\u00f3n y carga de datos.\nGesti\u00f3n del ciclo de vida del dato: Manejar el ciclo completo de los datos desde la ingesta y tratamiento hasta su disponibilizaci\u00f3n.\nProductos de datos: Crear y mantener productos de datos, incluyendo datamarts y data warehouses (DWH).\nImplementaci\u00f3n de BI: Desarrollar y gestionar soluciones de business intelligence utilizando herramientas como Power BI.\nConocimiento b\u00e1sico de flujos de ML.\nImplementaci\u00f3n de IaC: Dise\u00f1ar e implementar infraestructura como c\u00f3digo usando herramientas como Terraform y AWS CDK.\nDesarrollo de pipelines CI/CD: Configurar y mantener pipelines de integraci\u00f3n y despliegue continuo utilizando Jenkins, AWS Code Pipeline, AWS Code Build y Azure Artifacts.\nGesti\u00f3n de plataformas en la nube: Desplegar y administrar infraestructuras de datos en AWS, GCP y Azure.\nSeguridad y gobierno del dato: Implementar pol\u00edticas y procedimientos para asegurar la integridad y seguridad de los datos.\nDesarrollo de ETLs: Dise\u00f1ar y desarrollar procesos de extracci\u00f3n, transformaci\u00f3n y carga de datos.\nGesti\u00f3n del ciclo de vida del dato: Manejar el ciclo completo de los datos desde la ingesta y tratamiento hasta su disponibilizaci\u00f3n.\nProductos de datos: Crear y mantener productos de datos, incluyendo datamarts y data warehouses (DWH).\nImplementaci\u00f3n de BI: Desarrollar y gestionar soluciones de business intelligence utilizando herramientas como Power BI.\nConocimiento b\u00e1sico de flujos de ML.\n\ud83c\udfaf Experiencia y conocimientos necesarios:\nExperiencia en herramientas de IaC.\nExperiencia en implementaci\u00f3n y mantenimiento de pipelines de CI/CD.\nExperiencia en automatizaci\u00f3n y despliegue de infraestructuras para\nplataformas de datos en entornos Cloud.\nConocimientos de seguridad y ciclo de vida del dato.\nExperiencia en el desarrollo de ETLs.\nConocimientos de flujos de ML y su integraci\u00f3n con plataformas de datos.\nExperiencia en herramientas de IaC.\nExperiencia en implementaci\u00f3n y mantenimiento de pipelines de CI/CD.\nExperiencia en automatizaci\u00f3n y despliegue de infraestructuras para\nplataformas de datos en entornos Cloud.\nConocimientos de seguridad y ciclo de vida del dato.\nExperiencia en el desarrollo de ETLs.\nConocimientos de flujos de ML y su integraci\u00f3n con plataformas de datos.\n\ud83d\udca1 \u00bfQu\u00e9 encontrar\u00e1s en Paradigma?\n\u2705 Paradigma Everywhere: Modelo de trabajo 100% flexible. T\u00fa eliges desde d\u00f3nde trabajar.\nParadigma Everywhere:\n\u2705 Flexibilidad real: Conciliaci\u00f3n entre vida personal y laboral con horarios adaptables.\nFlexibilidad real:\n\u2705 Innovaci\u00f3n: Proyectos retadores con tecnolog\u00edas de vanguardia.\nInnovaci\u00f3n:\n\u2705 El dominio t\u00e9cnico son parte de nuestra identidad.\nEl dominio t\u00e9cnico\n\u2705 Formaci\u00f3n continua: Cursos, meetups, clases de ingl\u00e9s, ayudas para formaci\u00f3n y eventos.\nFormaci\u00f3n continua:\n\u2705 Beneficios sociales: Seguro m\u00e9dico 100% cubierto.\nBeneficios sociales:\n\ud83d\udd17 Con\u00f3cenos mejor aqu\u00ed: YouTube Paradigma Digital\n\ud83c\udf0d Nuestro compromiso con la diversidad e inclusi\u00f3n:\nEn Paradigma, la igualdad es clave. Creamos un entorno donde todas las personas son bienvenidas, independientemente de g\u00e9nero, edad, religi\u00f3n, orientaci\u00f3n sexual o cualquier otra condici\u00f3n. Trabajamos para garantizar la igualdad salarial y de oportunidades.\nSe valorar\u00e1 certificado de discapacidad.\nEn Paradigma, creamos el cambio. \u00bfTe sumas? \ud83d\udc99"
    },
    "4168021689": {
        "title": "Senior Data Engineer ",
        "company": "Pleo",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nWe're looking for a Senior Data Engineer to join Pleo and help us in our journey in our Business Analytics team. This team is responsible for delivering and enhancing high-quality, robust data solutions that drive commercial performance, revenue operations, and financial planning across Pleo.\n\nYou'll collaborate extensively with Product, Engineering, and Business teams to ensure our data solutions directly drive product innovation and business outcomes. You\u2019ll help shape the future of Pleo\u2019s data landscape, supporting multiple teams, driving strategic projects, and staying hands-on with cutting-edge technology to deliver impactful solutions.\n\nYou are a trailblazer, ready to lead innovation, mentor others, and tackle strategic challenges while maintaining a strong focus on execution. You\u2019ll partner with engineers, analysts, data scientists, and stakeholders across the company to unlock the potential of data in driving Pleo\u2019s mission forward.\n\nYou will thrive in this role if you\n\nAre excited by the opportunity to work across diverse teams, aligning initiatives and driving domain-wide impact with a \u2018data as a product\u2019 mindset \nLove being hands-on with modern data tools and technology while also contributing to strategic decision-making \nEnjoy the balance of diving into challenging projects while mentoring and coaching others to succeed \nHave a strong background in building and managing data infrastructure at scale, with expertise in Python, SQL, BigQuery, AWS, dbt and, Airflow \nHave a strong background in data modelling and building scalable data pipelines. \nAre naturally curious and enthusiastic about experimenting with new tech to solve complex problems \nCan effectively translate business and product requirements into technical data solutions \nTake pride in fostering a culture of collaboration, innovation, and learning across an organisation \n\nYour colleagues say you\n\nAre an effective bridge-builder who creates strong partnerships across engineering, product, and business teams \nBalance innovation and pragmatism, knowing when to go deep and when to deliver quickly \nAre a trusted mentor who uplifts and develops those around you \nActively seek and share knowledge, contributing to a culture of continuous learning \nHave a knack for bringing people together and aligning them towards shared goals \nAre someone they\u2019d love to have coffee with (virtual or not) \n\nYou will work on\n\nPartnering with product and business teams to develop data strategies that enable new features and improve user experience \nDriving key strategic projects across the domain, dipping in and out as needed to provide leadership and hands-on support \nSupporting multiple teams across the domain in delivering impactful data and analytics solutions \nBuilding data products that directly support domain\u2019s roadmap and business goals \nCollaborating with the leadership team and other data leaders to set the vision for Pleo\u2019s data strategy and ensure alignment with company objectives \nEnhancing our data infrastructure and pipelines to improve scalability, performance, and data quality \nExperimenting with and implementing innovative technologies to keep Pleo\u2019s data stack at the forefront of the industry \nMentoring engineers, analysts, and data scientists to foster growth and build a world-class data team \n\nShow me the benefits!\n\nYour own Pleo card (no more out-of-pocket spending!)\nLunch is on us - with catering in our Lisbon, and London offices or a monthly lunch allowance paid directly together with your salary in other markets \ud83c\udf5c\nPrivate health insurance to ensure you\u2019re fit in body and mind to do your best work\nWe offer 25 days of holiday + your public holidays\nFlexibility/remote working options\nOption to purchase 5 additional days of holiday through a salary sacrifice\nWe\u2019re trialing MyndUp to give our employees access to free mental health and wellbeing support with great success so far \u2764\ufe0f\u200d\ud83e\ude79\nAccess to LinkedIn Learning - acquire new skills, stay abreast of industry trends and fuel your personal and professional development continuously \nPaid parental leave - we want to make sure that we're supportive of families and help you feel that you don't have to compromise your family due to work \ud83d\udc76\n\nWhy join us?\n\nWorking at Pleo means you're working on something very exciting: the future of work. Our mission is to help every company go beyond the books. Pleo itself means \u2018more than you\u2019d expect\u2019, and it\u2019s been the secret to our success over the last 8 years. So it\u2019s only fitting that we\u2019d pass this philosophy onto our customers to help them make the most of their finances.\n\nWe think company spending should be delegated to all employees and teams, that it should be as automated as possible, and that it should drive a culture of responsible spending. Finance teams shouldn\u2019t be siloed from the rest of the organisation \u2013 they should work in unity with marketing, sales, IT and everyone else.\n\nSpeaking of working in unity, our values tell the story of how we work at Pleo. We have four core values, the first of which is \u2018champion the customer\u2019, which means we address real pain points that businesses face. Next up is \u2018succeed as a team\u2019, which highlights how our strength lies in our diversity and trust in each other. We also \u2018make it happen\u2019 by taking bold decisions and following through to deliver results. Last but not least, we \u2018build to scale\u2019, creating lasting solutions that address today\u2019s challenges and anticipate tomorrow\u2019s needs.\n\nSo, in a nutshell, that's Pleo. Today we are a 850+ team, from over 100 nations, sitting in our Copenhagen HQ, London, Stockholm, Berlin, Madrid, Montreal and Lisbon offices \u2014and quite a few full-time remotes in 35 other countries! Being HQ'd out of Copenhagen means we're inspired by things like a good work-life balance. If you don't work in the office with us, we'll help you set up the best remote setup possible and make sure you still have time to connect with your team.\n\nAbout Your Application\n\nPlease submit your application in English; it\u2019s our company language so you\u2019ll be speaking lots of it if you join \ud83d\udc95\nWe treat all candidates equally: If you are interested please apply through our application system - any correspondence should come from there! Our lovely support isn't able to pass on any calls/ emails our way - and this makes sure that the candidate experience is smooth and fair to everyone \ud83d\ude0a\nWe\u2019re on a mission to make everyone feel valued at work. That\u2019s only achievable if our team reflects the diversity of the world around us - and that starts with you, hitting apply, even if you are worried you might not tick all the boxes! We embrace and encourage people from all backgrounds to apply - regardless of race/ethnicity, colour, religion, nationality, gender, sex, sexual orientation, age, marital status, disability, neurodiversity, socio-economic status, culture or beliefs \nWhen you submit an application we process your personal data as a data processor. Find out more about how your data is used in the FAQs section at the bottom of our jobs page\nYou will thrive in this role if you\nAre excited by the opportunity to work across diverse teams, aligning initiatives and driving domain-wide impact with a \u2018data as a product\u2019 mindset \nLove being hands-on with modern data tools and technology while also contributing to strategic decision-making \nEnjoy the balance of diving into challenging projects while mentoring and coaching others to succeed \nHave a strong background in building and managing data infrastructure at scale, with expertise in Python, SQL, BigQuery, AWS, dbt and, Airflow \nHave a strong background in data modelling and building scalable data pipelines. \nAre naturally curious and enthusiastic about experimenting with new tech to solve complex problems \nCan effectively translate business and product requirements into technical data solutions \nTake pride in fostering a culture of collaboration, innovation, and learning across an organisation\nAre excited by the opportunity to work across diverse teams, aligning initiatives and driving domain-wide impact with a \u2018data as a product\u2019 mindset\nLove being hands-on with modern data tools and technology while also contributing to strategic decision-making\nEnjoy the balance of diving into challenging projects while mentoring and coaching others to succeed\nHave a strong background in building and managing data infrastructure at scale, with expertise in Python, SQL, BigQuery, AWS, dbt and, Airflow\nHave a strong background in data modelling and building scalable data pipelines.\nAre naturally curious and enthusiastic about experimenting with new tech to solve complex problems\nCan effectively translate business and product requirements into technical data solutions\nTake pride in fostering a culture of collaboration, innovation, and learning across an organisation\nYour colleagues say you\nAre an effective bridge-builder who creates strong partnerships across engineering, product, and business teams \nBalance innovation and pragmatism, knowing when to go deep and when to deliver quickly \nAre a trusted mentor who uplifts and develops those around you \nActively seek and share knowledge, contributing to a culture of continuous learning \nHave a knack for bringing people together and aligning them towards shared goals \nAre someone they\u2019d love to have coffee with (virtual or not)\nAre an effective bridge-builder who creates strong partnerships across engineering, product, and business teams\nBalance innovation and pragmatism, knowing when to go deep and when to deliver quickly\nAre a trusted mentor who uplifts and develops those around you\nActively seek and share knowledge, contributing to a culture of continuous learning\nHave a knack for bringing people together and aligning them towards shared goals\nAre someone they\u2019d love to have coffee with (virtual or not)\nYou will work on\nPartnering with product and business teams to develop data strategies that enable new features and improve user experience \nDriving key strategic projects across the domain, dipping in and out as needed to provide leadership and hands-on support \nSupporting multiple teams across the domain in delivering impactful data and analytics solutions \nBuilding data products that directly support domain\u2019s roadmap and business goals \nCollaborating with the leadership team and other data leaders to set the vision for Pleo\u2019s data strategy and ensure alignment with company objectives \nEnhancing our data infrastructure and pipelines to improve scalability, performance, and data quality \nExperimenting with and implementing innovative technologies to keep Pleo\u2019s data stack at the forefront of the industry \nMentoring engineers, analysts, and data scientists to foster growth and build a world-class data team\nPartnering with product and business teams to develop data strategies that enable new features and improve user experience\nDriving key strategic projects across the domain, dipping in and out as needed to provide leadership and hands-on support\nSupporting multiple teams across the domain in delivering impactful data and analytics solutions\nBuilding data products that directly support domain\u2019s roadmap and business goals\nCollaborating with the leadership team and other data leaders to set the vision for Pleo\u2019s data strategy and ensure alignment with company objectives\nEnhancing our data infrastructure and pipelines to improve scalability, performance, and data quality\nExperimenting with and implementing innovative technologies to keep Pleo\u2019s data stack at the forefront of the industry\nMentoring engineers, analysts, and data scientists to foster growth and build a world-class data team\nShow me the benefits!\nYour own Pleo card (no more out-of-pocket spending!)\nLunch is on us - with catering in our Lisbon, and London offices or a monthly lunch allowance paid directly together with your salary in other markets \ud83c\udf5c\nPrivate health insurance to ensure you\u2019re fit in body and mind to do your best work\nWe offer 25 days of holiday + your public holidays\nFlexibility/remote working options\nOption to purchase 5 additional days of holiday through a salary sacrifice\nWe\u2019re trialing MyndUp to give our employees access to free mental health and wellbeing support with great success so far \u2764\ufe0f\u200d\ud83e\ude79\nAccess to LinkedIn Learning - acquire new skills, stay abreast of industry trends and fuel your personal and professional development continuously \nPaid parental leave - we want to make sure that we're supportive of families and help you feel that you don't have to compromise your family due to work \ud83d\udc76\nYour own Pleo card (no more out-of-pocket spending!)\nLunch is on us - with catering in our Lisbon, and London offices or a monthly lunch allowance paid directly together with your salary in other markets \ud83c\udf5c\nPrivate health insurance to ensure you\u2019re fit in body and mind to do your best work\nWe offer 25 days of holiday + your public holidays\nFlexibility/remote working options\nOption to purchase 5 additional days of holiday through a salary sacrifice\nWe\u2019re trialing MyndUp to give our employees access to free mental health and wellbeing support with great success so far \u2764\ufe0f\u200d\ud83e\ude79\nAccess to LinkedIn Learning - acquire new skills, stay abreast of industry trends and fuel your personal and professional development continuously\nPaid parental leave - we want to make sure that we're supportive of families and help you feel that you don't have to compromise your family due to work \ud83d\udc76\nWhy join us?\nand\nAbout Your Application\nPlease submit your application in English; it\u2019s our company language so you\u2019ll be speaking lots of it if you join \ud83d\udc95\nWe treat all candidates equally: If you are interested please apply through our application system - any correspondence should come from there! Our lovely support isn't able to pass on any calls/ emails our way - and this makes sure that the candidate experience is smooth and fair to everyone \ud83d\ude0a\nWe\u2019re on a mission to make everyone feel valued at work. That\u2019s only achievable if our team reflects the diversity of the world around us - and that starts with you, hitting apply, even if you are worried you might not tick all the boxes! We embrace and encourage people from all backgrounds to apply - regardless of race/ethnicity, colour, religion, nationality, gender, sex, sexual orientation, age, marital status, disability, neurodiversity, socio-economic status, culture or beliefs \nWhen you submit an application we process your personal data as a data processor. Find out more about how your data is used in the FAQs section at the bottom of our jobs page\nPlease submit your application in English; it\u2019s our company language so you\u2019ll be speaking lots of it if you join \ud83d\udc95\nWe treat all candidates equally: If you are interested please apply through our application system - any correspondence should come from there! Our lovely support isn't able to pass on any calls/ emails our way - and this makes sure that the candidate experience is smooth and fair to everyone \ud83d\ude0a\nWe\u2019re on a mission to make everyone feel valued at work. That\u2019s only achievable if our team reflects the diversity of the world around us - and that starts with you, hitting apply, even if you are worried you might not tick all the boxes! We embrace and encourage people from all backgrounds to apply - regardless of race/ethnicity, colour, religion, nationality, gender, sex, sexual orientation, age, marital status, disability, neurodiversity, socio-economic status, culture or beliefs\nWhen you submit an application we process your personal data as a data processor. Find out more about how your data is used in the FAQs section at the bottom of our jobs page"
    },
    "4152500769": {
        "title": "Data Engineer (Snowflake/DBT) - Remote",
        "company": "UST Espa\u00f1a & Latam",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\ud83d\ude80 We are still looking for the very Top Talent\u2026and we would be delighted if you were to join our team!\nMore in details, UST is a multinational company based in North America, certified as a Top Employer and Great Place to Work company with over 35.000 employees all over the world and presence in more than 35 countries. We are leaders on digital technology services, and we provide large-scale technologic solutions to big companies.\n\n\ud83d\udd0e What we look for?\nFor one of those projects, we are seeking a dynamic and experienced Data Engineer (Snowflake) with strong SQL & Python skills. \nWe\u2019re in the process of transferring many of our data sources and pipelines to Snowflake cloud Databases, so experience with Snowflake Data Engineering or Cloud databases in general will really help you to hit the ground running. \nWe also use Airflow and DBT heavily, so real world experience of these or adjacent technologies, would be advantageous. \n\n100% remote position for people located in Spain.\n\n\u2699 Key Responsibilities:\nYou must have hands on ETL / ELT coding experience and be able to quickly pick up and run with new technologies to meet the varying needs of the team while maintaining high quality, well documented code. Experience of constructing tests and deploying via automated release pipelines is required. \nYou will have a solid understanding of how to apply data modelling theory and ideally be from a computer science or applied science/engineering background, although relevant industry experience is equally valuable and will be taken into consideration \n\n\ud83d\udca1 Mandatory requirements:\nDesign, develop, and maintain data pipelines and ETL processes using Snowflake. \nOptimize Snowflake data warehouse performance and ensure data accuracy and integrity. \nExperience in python, specifically within Data Engineering, in a commercial setting. \nExperience with DBT. \nExcellent understanding of ETL/ELT patterns, idempotency and other data engineering best practices \nExperience with data modelling (3rd normal form, star schemas, wide/tall projections) \nExcellent SQL knowledge, including an understanding of how to write optimised SQL code, good general knowledge of different SQL engines \nWorking knowledge of agile methodology, and capable of following the framework, contributing to team success through participation in ceremonies and occasionally assisting with scrum-master duties, owning of retro actions and maintaining scrum artefacts. \nKnowledgeable in how to build data pipelines that robustly handle different possible modes of failure and maintain Metadata. \n\nOptional \nSnowflake or DBT Certifications \nDemonstrable experience of Engineering in a cloud-based environment, especially with AWS or Azure \nAny experience of meta data driven Engineering \nAny experience of integrating Data Warehouse/Data pipelines with Data Governance tools like Collibra\n\n\ud83d\udcb0 \u00bfWhat can we offer?\n-\u2708\ufe0f 23 days of Annual Leave plus the 24th and 31st of December as discretionary days!\n-\u2764\ufe0f Numerous benefits (Heath Care Plan, Internet Connectivity, Life and Accident Insurances).\n-\ud83c\udf74 `Retribuci\u00f3n Flexible\u00b4 Program: (Meals, Kinder Garden, Transport, online English lessons, Heath Care Plan\u2026)\n-\ud83c\udf93 Free access to several training platforms\n-\ud83d\ude80 Professional stability and career plans\n-\ud83d\udeb6 UST also, compensates referrals from which you could benefit when you refer professionals.\n-\ud83d\udd17 The option to pick between 12 or 14 payments along the year.\n-\ud83c\udfe1 Real Work Life Balance measures (flexibility, WFH or remote work policy, compacted hours during summertime\u2026)\n-\ud83c\udf81 UST Club Platform discounts and gym Access discounts\n\n\ud83d\udce7 If you would like to know more, do not hesitate to apply and we\u2019ll get in touch to fill you in details. UST is waiting for you!\nIn UST we are committed to equal opportunities in our selection processes and do not discriminate based on race, gender, disability, age, religion, sexual orientation or nationality. We have a special commitment to Disability & Inclusion, so we are interested in hiring people with disability certificate.\n\ud83d\ude80 We are still looking for the very Top Talent\u2026and we would be delighted if you were to join our team!\nMore in details, UST is a multinational company based in North America, certified as a Top Employer and Great Place to Work company with over 35.000 employees all over the world and presence in more than 35 countries. We are leaders on digital technology services, and we provide large-scale technologic solutions to big companies.\n\ud83d\udd0e What we look for?\nWhat we look for?\nFor one of those projects, we are seeking a dynamic and experienced Data Engineer (Snowflake) with strong SQL & Python skills.\nData Engineer\nSnowflake\nWe\u2019re in the process of transferring many of our data sources and pipelines to Snowflake cloud Databases, so experience with Snowflake Data Engineering or Cloud databases in general will really help you to hit the ground running.\nWe also use Airflow and DBT heavily, so real world experience of these or adjacent technologies, would be advantageous.\n100% remote position for people located in Spain.\n\u2699 Key Responsibilities:\nKey Responsibilities:\nYou must have hands on ETL / ELT coding experience and be able to quickly pick up and run with new technologies to meet the varying needs of the team while maintaining high quality, well documented code. Experience of constructing tests and deploying via automated release pipelines is required.\nYou will have a solid understanding of how to apply data modelling theory and ideally be from a computer science or applied science/engineering background, although relevant industry experience is equally valuable and will be taken into consideration\n\ud83d\udca1 Mandatory requirements:\nMandatory requirements:\nDesign, develop, and maintain data pipelines and ETL processes using Snowflake. \nOptimize Snowflake data warehouse performance and ensure data accuracy and integrity. \nExperience in python, specifically within Data Engineering, in a commercial setting. \nExperience with DBT. \nExcellent understanding of ETL/ELT patterns, idempotency and other data engineering best practices \nExperience with data modelling (3rd normal form, star schemas, wide/tall projections) \nExcellent SQL knowledge, including an understanding of how to write optimised SQL code, good general knowledge of different SQL engines \nWorking knowledge of agile methodology, and capable of following the framework, contributing to team success through participation in ceremonies and occasionally assisting with scrum-master duties, owning of retro actions and maintaining scrum artefacts. \nKnowledgeable in how to build data pipelines that robustly handle different possible modes of failure and maintain Metadata.\nDesign, develop, and maintain data pipelines and ETL processes using Snowflake.\nOptimize Snowflake data warehouse performance and ensure data accuracy and integrity.\nExperience in python, specifically within Data Engineering, in a commercial setting.\nExperience with DBT.\nExcellent understanding of ETL/ELT patterns, idempotency and other data engineering best practices\nExperience with data modelling (3rd normal form, star schemas, wide/tall projections)\nExcellent SQL knowledge, including an understanding of how to write optimised SQL code, good general knowledge of different SQL engines\nWorking knowledge of agile methodology, and capable of following the framework, contributing to team success through participation in ceremonies and occasionally assisting with scrum-master duties, owning of retro actions and maintaining scrum artefacts.\nKnowledgeable in how to build data pipelines that robustly handle different possible modes of failure and maintain Metadata.\nOptional\nSnowflake or DBT Certifications \nDemonstrable experience of Engineering in a cloud-based environment, especially with AWS or Azure \nAny experience of meta data driven Engineering \nAny experience of integrating Data Warehouse/Data pipelines with Data Governance tools like Collibra\nSnowflake or DBT Certifications\nDemonstrable experience of Engineering in a cloud-based environment, especially with AWS or Azure\nAny experience of meta data driven Engineering\nAny experience of integrating Data Warehouse/Data pipelines with Data Governance tools like Collibra\n\ud83d\udcb0 \u00bfWhat can we offer?\n\u00bfWhat can we offer?\n-\u2708\ufe0f 23 days of Annual Leave plus the 24th and 31st of December as discretionary days!\n-\u2764\ufe0f Numerous benefits (Heath Care Plan, Internet Connectivity, Life and Accident Insurances).\n-\ud83c\udf74 `Retribuci\u00f3n Flexible\u00b4 Program: (Meals, Kinder Garden, Transport, online English lessons, Heath Care Plan\u2026)\n-\ud83c\udf93 Free access to several training platforms\n-\ud83d\ude80 Professional stability and career plans\n-\ud83d\udeb6 UST also, compensates referrals from which you could benefit when you refer professionals.\n-\ud83d\udd17 The option to pick between 12 or 14 payments along the year.\n-\ud83c\udfe1 Real Work Life Balance measures (flexibility, WFH or remote work policy, compacted hours during summertime\u2026)\n-\ud83c\udf81 UST Club Platform discounts and gym Access discounts\n\ud83d\udce7 If you would like to know more, do not hesitate to apply and we\u2019ll get in touch to fill you in details. UST is waiting for you!\nIn UST we are committed to equal opportunities in our selection processes and do not discriminate based on race, gender, disability, age, religion, sexual orientation or nationality. We have a special commitment to Disability & Inclusion, so we are interested in hiring people with disability certificate."
    },
    "3982415625": {
        "title": "Azure Data Solutions Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\n\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\n\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\n\ud83d\udd27 Dise\u00f1ar soluciones de software enfocadas en la gesti\u00f3n de datos, asegurando calidad, seguridad y trazabilidad\n\n\ud83d\udcca Dise\u00f1ar e implementar soluciones impulsadas por metadatos para optimizar y automatizar procesos a lo largo del ciclo de vida del dato.\n\n\u2601\ufe0f Implementar soluciones cloud (Azure, AWS y GCP) asegurando el uso \u00f3ptimo de sus componentes.\n\n\ud83d\udcbb Programar y optimizar procesos de tratamiento de datos, utilizando Python, para el manejo eficiente del ciclo vida del dato.\n\n\ud83d\udcc8 Coordinar el dise\u00f1o y desarrollo de soluciones para la gesti\u00f3n del ciclo del dato, garantizando eficiencia y escalabilidad.\n\n\ud83e\udd1d Colaborar con equipos t\u00e9cnicos para definir estrategias y buenas pr\u00e1cticas en entornos cloud.\n\n\ud83d\udd0d Evaluar y probar servicios cloud innovadores que puedan aportar valor a los proyectos\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n?\n\n\ud83c\udf93 Titulaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o formaci\u00f3n equivalente.\n\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Software Engineer o roles similares dentro del mundo del dato.\n\n\ud83d\udd04 +2 a\u00f1os en integraci\u00f3n y procesamiento de datos, desarrollando pipelines de ingesta y transformaci\u00f3n.\n\n\ud83d\udcbb Conocimiento s\u00f3lido en Python.\n\n\ud83d\udcc2 Bases de datos relacionales y no relacionales, con capacidad para optimizar consultas y estructuras de almacenamiento.\n\n\u2601\ufe0f Experiencia en entornos cloud, principalmente Azure, AWS y GCP.\n\n\ud83c\udfd7\ufe0f Conocimientos en arquitectura de datos, orquestaci\u00f3n de procesos y observabilidad.\n\n\ud83d\udcc8 Desarrollo de soluciones escalables y eficientes para la gesti\u00f3n del ciclo de vida del dato.\n\n\ud83d\ude80 Habilidad para trabajar en entornos de integraci\u00f3n continua y despliegue en la nube.\n\n\ud83e\uddd0 Experiencia en liderazgo t\u00e9cnico.\n\nValoramos tambi\u00e9n:\n\n\ud83d\udd27 Dise\u00f1o de soluciones end-to-end y comprensi\u00f3n de su base t\u00e9cnica.\n\n\ud83d\udcbb Experiencia en programaci\u00f3n para integraci\u00f3n de datos, con conocimiento de frameworks y herramientas relevantes.\n\n\u2601\ufe0f Certificaci\u00f3n cloud o experiencia con servicios en la nube.\n\n\ud83c\udf10 Conocimientos en DataOps, Data Observability y Data Mesh.\n\n\ud83c\udf93 Formaci\u00f3n complementaria como un M\u00e1ster en Big Data y Analytics es un plus.\n\n\ud83d\udde3\ufe0f Ingl\u00e9s para trabajar en un entorno global.\n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\u2705 Acceso a formaciones y certificaciones.\n\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\n\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\ud83d\udd27 Dise\u00f1ar soluciones de software enfocadas en la gesti\u00f3n de datos, asegurando calidad, seguridad y trazabilidad\nDise\u00f1ar soluciones\ngesti\u00f3n de datos\n\ud83d\udcca Dise\u00f1ar e implementar soluciones impulsadas por metadatos para optimizar y automatizar procesos a lo largo del ciclo de vida del dato.\nDise\u00f1ar e implementar\nimpulsadas por metadatos\n\u2601\ufe0f Implementar soluciones cloud (Azure, AWS y GCP) asegurando el uso \u00f3ptimo de sus componentes.\nImplementar soluciones cloud\n\ud83d\udcbb Programar y optimizar procesos de tratamiento de datos, utilizando Python, para el manejo eficiente del ciclo vida del dato.\nProgramar y optimizar\nPython\n\ud83d\udcc8 Coordinar el dise\u00f1o y desarrollo de soluciones para la gesti\u00f3n del ciclo del dato, garantizando eficiencia y escalabilidad.\nCoordinar el dise\u00f1o y desarrollo\n\ud83e\udd1d Colaborar con equipos t\u00e9cnicos para definir estrategias y buenas pr\u00e1cticas en entornos cloud.\nColaborar con equipos t\u00e9cnicos\n\ud83d\udd0d Evaluar y probar servicios cloud innovadores que puedan aportar valor a los proyectos\nEvaluar y probar\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n?\n\ud83c\udf93 Titulaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o formaci\u00f3n equivalente.\nTitulaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Software Engineer o roles similares dentro del mundo del dato.\n+4 a\u00f1os de experiencia\nData Software Engineer\n\ud83d\udd04 +2 a\u00f1os en integraci\u00f3n y procesamiento de datos, desarrollando pipelines de ingesta y transformaci\u00f3n.\n+2 a\u00f1os en integraci\u00f3n y procesamiento de datos\n\ud83d\udcbb Conocimiento s\u00f3lido en Python.\nConocimiento s\u00f3lido en Python.\n\ud83d\udcc2 Bases de datos relacionales y no relacionales, con capacidad para optimizar consultas y estructuras de almacenamiento.\nBases de datos relacionales y no relacionales\n\u2601\ufe0f Experiencia en entornos cloud, principalmente Azure, AWS y GCP.\nExperiencia en entornos cloud\nAzure, AWS y GCP\n\ud83c\udfd7\ufe0f Conocimientos en arquitectura de datos, orquestaci\u00f3n de procesos y observabilidad.\nConocimientos en arquitectura de datos, orquestaci\u00f3n de procesos y observabilidad\n\ud83d\udcc8 Desarrollo de soluciones escalables y eficientes para la gesti\u00f3n del ciclo de vida del dato.\nDesarrollo de soluciones escalables y eficientes\n\ud83d\ude80 Habilidad para trabajar en entornos de integraci\u00f3n continua y despliegue en la nube.\nHabilidad para trabajar en entornos de integraci\u00f3n continua\n\ud83e\uddd0 Experiencia en liderazgo t\u00e9cnico.\nliderazgo t\u00e9cnico.\nValoramos tambi\u00e9n:\n\ud83d\udd27 Dise\u00f1o de soluciones end-to-end y comprensi\u00f3n de su base t\u00e9cnica.\nDise\u00f1o de soluciones end-to-end\n\ud83d\udcbb Experiencia en programaci\u00f3n para integraci\u00f3n de datos, con conocimiento de frameworks y herramientas relevantes.\nExperiencia en programaci\u00f3n\n\u2601\ufe0f Certificaci\u00f3n cloud o experiencia con servicios en la nube.\nCertificaci\u00f3n cloud\nservicios en la nube\n\ud83c\udf10 Conocimientos en DataOps, Data Observability y Data Mesh.\nDataOps, Data Observability y Data Mesh\n\ud83c\udf93 Formaci\u00f3n complementaria como un M\u00e1ster en Big Data y Analytics es un plus.\nFormaci\u00f3n complementaria\nM\u00e1ster en Big Data y Analytics\n\ud83d\udde3\ufe0f Ingl\u00e9s para trabajar en un entorno global.\nIngl\u00e9s\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\nSalario\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\n\u2705 Acceso a formaciones y certificaciones.\nAcceso a formaciones y certificaciones.\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group"
    },
    "4154781745": {
        "title": "Data Engineer",
        "company": "Remobi",
        "location": "European Economic Area",
        "work_mode": "Remote",
        "job_type": "Contract",
        "description": "About the job\nData Engineer\nLocation: Fully Remote within the EU\nStart Date: Within 1 Week\nContract Duration: Until December 2025\nInterview Process: 1 Stage\n\nAbout the Role:\n\nWe are seeking a highly skilled Data Engineer to join our team on a fully remote basis. This is an exciting opportunity to work on cutting-edge data infrastructure and analytics solutions for a long-term contract until December 2025. With a swift hiring process and a single-stage interview, you can start within a week!\n\nResponsibilities:\n\nDesign, build, and maintain scalable data pipelines and ETL processes.\nDevelop and optimize analytical data models using tools like dbt.\nWrite clean, efficient, and maintainable Python and SQL code.\nWork with multiple data architecture paradigms to optimize performance and scalability.\nManage and improve database design, tuning, and maintenance (e.g., PostgreSQL).\nUtilize Azure Data Stack (Azure Data Lake, Data Factory, etc.) for cloud-based data solutions.\nCollaborate with cross-functional teams to support data-driven decision-making.\nMaintain microservices architectures, leveraging Kubernetes, Docker, and REST APIs.\n\nRequirements:\n\n3 - 8 years of experience as a Data Engineer, Data Platform Engineer, or ML Engineer.\nStrong expertise in Python and SQL.\nHands-on experience with data modeling and ETL development.\nFamiliarity with Kubernetes, Docker, and microservice architectures.\nProven track record in designing and optimizing analytical workflows.\n\nPreferred Skills (Nice to Have):\n\nApache Kafka proficiency.\n1-2 years of Snowflake experience, including query optimization and cost monitoring.\nDevOps knowledge: Kubernetes, Terraform, cloud security, CI/CD.\nExperience with AI/ML model deployment and API development (FastAPI, Flask, etc.).\nData Engineer\nLocation: Fully Remote within the EU\nLocation:\nStart Date: Within 1 Week\nStart Date:\nContract Duration: Until December 2025\nContract Duration:\nInterview Process: 1 Stage\nInterview Process:\nAbout the Role:\nWe are seeking a highly skilled Data Engineer to join our team on a fully remote basis. This is an exciting opportunity to work on cutting-edge data infrastructure and analytics solutions for a long-term contract until December 2025. With a swift hiring process and a single-stage interview, you can start within a week!\nResponsibilities:\nDesign, build, and maintain scalable data pipelines and ETL processes.\nDevelop and optimize analytical data models using tools like dbt.\nWrite clean, efficient, and maintainable Python and SQL code.\nWork with multiple data architecture paradigms to optimize performance and scalability.\nManage and improve database design, tuning, and maintenance (e.g., PostgreSQL).\nUtilize Azure Data Stack (Azure Data Lake, Data Factory, etc.) for cloud-based data solutions.\nCollaborate with cross-functional teams to support data-driven decision-making.\nMaintain microservices architectures, leveraging Kubernetes, Docker, and REST APIs.\nDesign, build, and maintain scalable data pipelines and ETL processes.\ndata pipelines\nETL processes\nDevelop and optimize analytical data models using tools like dbt.\nanalytical data models\ndbt\nWrite clean, efficient, and maintainable Python and SQL code.\nPython\nSQL\nWork with multiple data architecture paradigms to optimize performance and scalability.\nmultiple data architecture paradigms\nManage and improve database design, tuning, and maintenance (e.g., PostgreSQL).\ndatabase design, tuning, and maintenance\nUtilize Azure Data Stack (Azure Data Lake, Data Factory, etc.) for cloud-based data solutions.\nAzure Data Stack\nCollaborate with cross-functional teams to support data-driven decision-making.\ndata-driven decision-making\nMaintain microservices architectures, leveraging Kubernetes, Docker, and REST APIs.\nmicroservices architectures\nKubernetes, Docker, and REST APIs\nRequirements:\n3 - 8 years of experience as a Data Engineer, Data Platform Engineer, or ML Engineer.\nStrong expertise in Python and SQL.\nHands-on experience with data modeling and ETL development.\nFamiliarity with Kubernetes, Docker, and microservice architectures.\nProven track record in designing and optimizing analytical workflows.\n3 - 8 years of experience as a Data Engineer, Data Platform Engineer, or ML Engineer.\n3 - 8 years\nData Engineer, Data Platform Engineer, or ML Engineer\nStrong expertise in Python and SQL.\nHands-on experience with data modeling and ETL development.\ndata modeling\nETL development\nFamiliarity with Kubernetes, Docker, and microservice architectures.\nKubernetes, Docker, and microservice architectures\nProven track record in designing and optimizing analytical workflows.\nanalytical workflows\nPreferred Skills (Nice to Have):\nApache Kafka proficiency.\n1-2 years of Snowflake experience, including query optimization and cost monitoring.\nDevOps knowledge: Kubernetes, Terraform, cloud security, CI/CD.\nExperience with AI/ML model deployment and API development (FastAPI, Flask, etc.).\nApache Kafka proficiency.\nApache Kafka\n1-2 years of Snowflake experience, including query optimization and cost monitoring.\n1-2 years of Snowflake\nDevOps knowledge: Kubernetes, Terraform, cloud security, CI/CD.\nDevOps\nExperience with AI/ML model deployment and API development (FastAPI, Flask, etc.).\nAI/ML model deployment\nAPI development"
    },
    "4170837228": {
        "title": "GCP Data Engineer - International Growth Projects ",
        "company": "Bluetab, an IBM Company",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nBluetab an IBM Company.\n\nAre you ready to take your next professional step?\n\nWe are looking for passionate Data & GCP Engineers who want to level up their careers working in international projects according to our EMEA expansion.\n\nBy working at Bluetab EMEA, you can have the best of both worlds; amazing Data and Cloud EMEA (International) projects while maintaining your life in Spain (you will not have to move abroad ;).\n\nLast but not least: we are \u00a1Great Place to Work & Best Work to Place!, known for being one of the best companies to work for in Spain.\n\nWho are we?:\n\nWe are a tech-driven company which led us to be by 2021, IBM\u2019s powerful arm in Data, Cloud, ML & AI services. We help key companies in multiple industries to accelerate and optimize their strategies in data and cloud platforms, taking full advantage of their architecture and development environments.\n\nWe offer the opportunity to work in high levels of technical expertise environments within the Data & Cloud world; where you will be able to develop yourself personally and professionally: learn, train and share technological experiences in a culture fostered by teamwork and collaboration.\n\nWe are bluetabers: we love technology, challenges and most of all people.\nWe are restless, non-conformists, eager to learn and passionate about new technological challenges in all shapes and forms, which boost fantastic teams of professionals.\n\n\ud83d\udd0d What We\u2019re Looking For\n\u2705 5+ years of experience in Data Projects, designing and implementing scalable solutions.\n\ud83d\udc0d Strong programming skills in Python, building efficient and maintainable data pipelines.\n\u26a1 Big Data expertise with Spark, handling massive datasets and optimizing performance.\n\u2601\ufe0f Experience in Big Data development on Google Cloud Platform (GCP), leveraging services like BigQuery, Dataflow, and Dataproc.\n\ud83d\udd27 Hands-on experience with CI/CD for infrastructure services (IaC) using Terraform, Jenkins, or CloudWatch.\n\ud83c\udf0d Advanced English (C1/C2) \u2013 You'll be working on international projects!\n\n\ud83c\udfaf Why Join Bluetab?\n\ud83c\udf0e Work on exciting international projects with top-tier clients.\n\ud83d\udca1 Be part of an innovative and collaborative team of data experts.\n\ud83d\udcc8 Continuous learning and career growth in Cloud and Big Data technologies.\n\ud83c\udfc6 Join Bluetab, an IBM Company, and take your career to the next level!\n\nAnd now for the Good bit: Being a Bluetaber\u2026 What can we offer?\n\nPermanent contract and competitive salary according to role and experience.\n\nThe sixty four million dollar question in the room. Working from home? \u201cFlexible remote working company\u201d. But you will also have access to our offices located in the center of Madrid, Bilbao, Barcelona and Alicante with great transport links.\n\nFlexible timetable from start to finish, to suit your personal needs and reduced timetable on Fridays and during the entire summer (July and August)\n23 days holiday\n\nTicket Restaurant (11 euros per 8 hour working day) as a benefit over and above salary.\nMedical Insurance and dental policy with extensive coverage as a benefit over and above salary\nEntitlement to tax free transport and kindergarten care.\n\nOngoing training plan based on platform gamification with rewards, up to 800 courses and official certifications\nIn addition, you will have access to our Career Coach program, designed to follow you through your professional development and growth, with ongoing assessment and feedback. \u00a1You will be in charge of your own growth!\n\nBirthday gift card valued at 50 euros to spend in more than twenty shops online like Amazon, Decathlon, PlayStation, XBox, Mango,H&M, ToysrUS and a lot more to make that day extra special.\n\n\nAt Bluetab, we believe that diversity is enriching and as such, we wish to safeguard both inclusi\u00f3n and equal opportunities, for this reason Bluetab has an Equality Plan and Ethical Code which includes the above principles in order to guarantee non discrimination of our employees for questions of race, color, nationality, social background, age, sex, marital status, sexual orientation, ideology, political leaning, religi\u00f3n or any other personal aspect whether it be physical or social.\n\nIf you are mad about technology and be part of an innovative environment, 100% techie, then Bluetab is for you.\nWhat are you waiting for?\n#wearehiring #bluetabanibmcompany #bluetabers\nBluetab an IBM Company.\nAre you ready to take your next professional step?\nWe are looking for passionate Data & GCP Engineers who want to level up their careers working in international projects according to our EMEA expansion.\nData & GCP Engineers\nEMEA expansion.\nBy working at Bluetab EMEA, you can have the best of both worlds; amazing Data and Cloud EMEA (International) projects while maintaining your life in Spain (you will not have to move abroad ;).\nLast but not least: we are \u00a1Great Place to Work & Best Work to Place!, known for being one of the best companies to work for in Spain.\nWho are we?:\nWe are a tech-driven company which led us to be by 2021, IBM\u2019s powerful arm in Data, Cloud, ML & AI services. We help key companies in multiple industries to accelerate and optimize their strategies in data and cloud platforms, taking full advantage of their architecture and development environments.\ntech-driven company\nWe offer the opportunity to work in high levels of technical expertise environments within the Data & Cloud world; where you will be able to develop yourself personally and professionally: learn, train and share technological experiences in a culture fostered by teamwork and collaboration.\nWe are bluetabers: we love technology, challenges and most of all people.\nWe are restless, non-conformists, eager to learn and passionate about new technological challenges in all shapes and forms, which boost fantastic teams of professionals.\n\ud83d\udd0d What We\u2019re Looking For\nWhat We\u2019re Looking For\n\u2705 5+ years of experience in Data Projects, designing and implementing scalable solutions.\n\ud83d\udc0d Strong programming skills in Python, building efficient and maintainable data pipelines.\n\u26a1 Big Data expertise with Spark, handling massive datasets and optimizing performance.\n\u2601\ufe0f Experience in Big Data development on Google Cloud Platform (GCP), leveraging services like BigQuery, Dataflow, and Dataproc.\n\ud83d\udd27 Hands-on experience with CI/CD for infrastructure services (IaC) using Terraform, Jenkins, or CloudWatch.\n\ud83c\udf0d Advanced English (C1/C2) \u2013 You'll be working on international projects!\n\u2705 5+ years of experience in Data Projects, designing and implementing scalable solutions.\n5+ years of experience\n\ud83d\udc0d Strong programming skills in Python, building efficient and maintainable data pipelines.\nStrong programming skills in Python\n\u26a1 Big Data expertise with Spark, handling massive datasets and optimizing performance.\nBig Data expertise with Spark\n\u2601\ufe0f Experience in Big Data development on Google Cloud Platform (GCP), leveraging services like BigQuery, Dataflow, and Dataproc.\nExperience in Big Data development on Google Cloud Platform (GCP)\nBigQuery, Dataflow, and Dataproc\n\ud83d\udd27 Hands-on experience with CI/CD for infrastructure services (IaC) using Terraform, Jenkins, or CloudWatch.\nHands-on experience with CI/CD\nIaC\nTerraform, Jenkins, or CloudWatch\n\ud83c\udf0d Advanced English (C1/C2) \u2013 You'll be working on international projects!\nAdvanced English (C1/C2)\n\ud83c\udfaf Why Join Bluetab?\nWhy Join Bluetab?\n\ud83c\udf0e Work on exciting international projects with top-tier clients.\n\ud83d\udca1 Be part of an innovative and collaborative team of data experts.\n\ud83d\udcc8 Continuous learning and career growth in Cloud and Big Data technologies.\n\ud83c\udfc6 Join Bluetab, an IBM Company, and take your career to the next level!\n\ud83c\udf0e Work on exciting international projects with top-tier clients.\nWork on exciting international projects\n\ud83d\udca1 Be part of an innovative and collaborative team of data experts.\nBe part of an innovative and collaborative team\n\ud83d\udcc8 Continuous learning and career growth in Cloud and Big Data technologies.\nContinuous learning and career growth\n\ud83c\udfc6 Join Bluetab, an IBM Company, and take your career to the next level!\nJoin Bluetab, an IBM Company\nAnd now for the Good bit: Being a Bluetaber\u2026 What can we offer?\nPermanent contract and competitive salary according to role and experience.\nThe sixty four million dollar question in the room. Working from home? \u201cFlexible remote working company\u201d. But you will also have access to our offices located in the center of Madrid, Bilbao, Barcelona and Alicante with great transport links.\n\u201cFlexible remote working company\u201d.\nFlexible timetable from start to finish, to suit your personal needs and reduced timetable on Fridays and during the entire summer (July and August)\n23 days holiday\nFlexible timetable from start to finish, to suit your personal needs and reduced timetable on Fridays and during the entire summer (July and August)\nFlexible timetable\nreduced timetable on Fridays and during the entire summer (July and August)\n23 days holiday\nTicket Restaurant (11 euros per 8 hour working day) as a benefit over and above salary.\nMedical Insurance and dental policy with extensive coverage as a benefit over and above salary\nEntitlement to tax free transport and kindergarten care.\nTicket Restaurant (11 euros per 8 hour working day) as a benefit over and above salary.\nTicket Restaurant\nMedical Insurance and dental policy with extensive coverage as a benefit over and above salary\nMedical Insurance and dental policy\nEntitlement to tax free transport and kindergarten care.\nOngoing training plan based on platform gamification with rewards, up to 800 courses and official certifications\nIn addition, you will have access to our Career Coach program, designed to follow you through your professional development and growth, with ongoing assessment and feedback. \u00a1You will be in charge of your own growth!\nOngoing training plan based on platform gamification with rewards, up to 800 courses and official certifications\nOngoing training plan based on platform gamification\nIn addition, you will have access to our Career Coach program, designed to follow you through your professional development and growth, with ongoing assessment and feedback. \u00a1You will be in charge of your own growth!\nCareer Coach program,\nBirthday gift card valued at 50 euros to spend in more than twenty shops online like Amazon, Decathlon, PlayStation, XBox, Mango,H&M, ToysrUS and a lot more to make that day extra special.\nBirthday gift card valued at 50 euros\nAt Bluetab, we believe that diversity is enriching and as such, we wish to safeguard both inclusi\u00f3n and equal opportunities, for this reason Bluetab has an Equality Plan and Ethical Code which includes the above principles in order to guarantee non discrimination of our employees for questions of race, color, nationality, social background, age, sex, marital status, sexual orientation, ideology, political leaning, religi\u00f3n or any other personal aspect whether it be physical or social.\nIf you are mad about technology and be part of an innovative environment, 100% techie, then Bluetab is for you.\nWhat are you waiting for?\n#wearehiring #bluetabanibmcompany #bluetabers"
    },
    "4173911100": {
        "title": "Big Data Engineer ",
        "company": "Neotalent Conclusion",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfQui\u00e9nes somos?:\nNeotalent Conclusion es una de las mayores especialistas del mercado ib\u00e9rico en captaci\u00f3n y gesti\u00f3n de talento inform\u00e1tico y de ingenier\u00eda. Como parte del grupo neerland\u00e9s Conclusion, que se compone por un ecosistema de 25 empresas y 3.600 empleados, nos centramos en aumentar la capacidad tecnol\u00f3gica de nuestros clientes y acelerar la transformaci\u00f3n digital de los mismos.\nEn Neotalent Conclusion, somos especialistas del talento. Atraemos a los mejores profesionales y los combinamos con los proyectos m\u00e1s ambiciosos, en el momento justo. Nuestro \u00e9xito radica en la inversi\u00f3n en innovaci\u00f3n, orientaci\u00f3n a resultados y, por encima de todo, en la garant\u00eda de que las personas tengan carreras felices y realizadoras.\n\nCon m\u00e1s de 25 a\u00f1os de experiencia, Neotalent Conclusion cuenta con oficinas en Madrid, Lisboa y Oporto. Nuestra compa\u00f1\u00eda est\u00e1 formada por un equipo de m\u00e1s de 850 profesionales entre sus diferentes \u00e1reas de operaci\u00f3n.\n\nNeotalent, the responsive people.\n\nM\u00e1s informaci\u00f3n en www.neotalent.pt/es y https://neotalent.pt/es/oportunidades-de-carrera/.\n\nDescripci\u00f3n del puesto:\n\nEstamos buscando un Big Data Engineer con al menos 4 a\u00f1os de experiencia en entorno AWS y con tecnolog\u00edas como Tableau, Hadoop, SQL y Python, para trabajar en uno de nuestros clientes, en modalidad h\u00edbrida en sus oficinas de Madrid.\n\nQu\u00e9 perfil buscamos:\n\nConocimiento muy profundo de Tableau.\nExperiencia en SQL, Hive, Hadoop.\nExperiencia con Python, Pyspark, Pandas, JupyterLab (trabajando con notebooks).\nExperiencia utilizando la plataforma AWS.\nExperiencia con herramientas de integraci\u00f3n continua y entrega continua como Git, Jenkins, etc.\nDesarrollo \u00e1gil/Ciclo de vida del software.\nExcelentes habilidades interpersonales y de comunicaci\u00f3n en ingl\u00e9s (B2+).\n\nSe valorar\u00e1: \nExperiencia con Kafka.\nEspec\u00edficamente, experiencia utilizando EMR (Elastic Map Reduce) en AWS para ejecutar cl\u00fasteres de Spark.\nConocimiento de Terraform.\nExperiencia con Ansible, Bash scripting, boto3.\nExperiencia configurando herramientas de integraci\u00f3n continua y entrega continua.\n\nQu\u00e9 har\u00e1s:\n\nAsumir la responsabilidad de la entrega del software asegurando que se cumplan las expectativas de calidad y alcance.\nIngesta de grandes datos y grandes conjuntos de datos en general. \nCrear y mantener ETLs de alta calidad.\nContribuir y asumir la responsabilidad del dise\u00f1o t\u00e9cnico, asegurando que todos los aspectos de la arquitectura del sistema est\u00e9n bien documentados.\nTrabajar estrechamente con los equipos tecnol\u00f3gicos asociados y colaborar de manera efectiva.\n\nPor qu\u00e9 unirte a nosotros:\n\nTrabajar en proyectos desafiantes para grandes clientes\nAprender m\u00e1s sobre tecnolog\u00edas emergentes\nAcceso a formaci\u00f3n continua y certificaciones\nDisfrutar de un plan de compensaci\u00f3n flexible\n... \u00a1y mucho m\u00e1s!\n\u00bfQui\u00e9nes somos?:\nNeotalent Conclusion es una de las mayores especialistas del mercado ib\u00e9rico en captaci\u00f3n y gesti\u00f3n de talento inform\u00e1tico y de ingenier\u00eda. Como parte del grupo neerland\u00e9s Conclusion, que se compone por un ecosistema de 25 empresas y 3.600 empleados, nos centramos en aumentar la capacidad tecnol\u00f3gica de nuestros clientes y acelerar la transformaci\u00f3n digital de los mismos.\nEn Neotalent Conclusion, somos especialistas del talento. Atraemos a los mejores profesionales y los combinamos con los proyectos m\u00e1s ambiciosos, en el momento justo. Nuestro \u00e9xito radica en la inversi\u00f3n en innovaci\u00f3n, orientaci\u00f3n a resultados y, por encima de todo, en la garant\u00eda de que las personas tengan carreras felices y realizadoras.\nCon m\u00e1s de 25 a\u00f1os de experiencia, Neotalent Conclusion cuenta con oficinas en Madrid, Lisboa y Oporto. Nuestra compa\u00f1\u00eda est\u00e1 formada por un equipo de m\u00e1s de 850 profesionales entre sus diferentes \u00e1reas de operaci\u00f3n.\nNeotalent, the responsive people.\nM\u00e1s informaci\u00f3n en www.neotalent.pt/es y https://neotalent.pt/es/oportunidades-de-carrera/.\nDescripci\u00f3n del puesto:\nEstamos buscando un Big Data Engineer con al menos 4 a\u00f1os de experiencia en entorno AWS y con tecnolog\u00edas como Tableau, Hadoop, SQL y Python, para trabajar en uno de nuestros clientes, en modalidad h\u00edbrida en sus oficinas de Madrid.\nBig Data Engineer\nal menos 4 a\u00f1os\nde experiencia en entorno AWS y con tecnolog\u00edas como Tableau, Hadoop, SQL y Python,\nmodalidad h\u00edbrida\nMadrid.\nQu\u00e9 perfil buscamos:\nConocimiento muy profundo de Tableau.\nExperiencia en SQL, Hive, Hadoop.\nExperiencia con Python, Pyspark, Pandas, JupyterLab (trabajando con notebooks).\nExperiencia utilizando la plataforma AWS.\nExperiencia con herramientas de integraci\u00f3n continua y entrega continua como Git, Jenkins, etc.\nDesarrollo \u00e1gil/Ciclo de vida del software.\nExcelentes habilidades interpersonales y de comunicaci\u00f3n en ingl\u00e9s (B2+).\nConocimiento muy profundo de Tableau.\nTableau.\nExperiencia en SQL, Hive, Hadoop.\nSQL, Hive, Hadoop.\nExperiencia con Python, Pyspark, Pandas, JupyterLab (trabajando con notebooks).\nPython\nExperiencia utilizando la plataforma AWS.\nAWS.\nExperiencia con herramientas de integraci\u00f3n continua y entrega continua como Git, Jenkins, etc.\nDesarrollo \u00e1gil/Ciclo de vida del software.\nExcelentes habilidades interpersonales y de comunicaci\u00f3n en ingl\u00e9s (B2+).\nSe valorar\u00e1:\nExperiencia con Kafka.\nEspec\u00edficamente, experiencia utilizando EMR (Elastic Map Reduce) en AWS para ejecutar cl\u00fasteres de Spark.\nConocimiento de Terraform.\nExperiencia con Ansible, Bash scripting, boto3.\nExperiencia configurando herramientas de integraci\u00f3n continua y entrega continua.\nExperiencia con Kafka.\nEspec\u00edficamente, experiencia utilizando EMR (Elastic Map Reduce) en AWS para ejecutar cl\u00fasteres de Spark.\nConocimiento de Terraform.\nExperiencia con Ansible, Bash scripting, boto3.\nExperiencia configurando herramientas de integraci\u00f3n continua y entrega continua.\nQu\u00e9 har\u00e1s:\nAsumir la responsabilidad de la entrega del software asegurando que se cumplan las expectativas de calidad y alcance.\nIngesta de grandes datos y grandes conjuntos de datos en general. \nCrear y mantener ETLs de alta calidad.\nContribuir y asumir la responsabilidad del dise\u00f1o t\u00e9cnico, asegurando que todos los aspectos de la arquitectura del sistema est\u00e9n bien documentados.\nTrabajar estrechamente con los equipos tecnol\u00f3gicos asociados y colaborar de manera efectiva.\nAsumir la responsabilidad de la entrega del software asegurando que se cumplan las expectativas de calidad y alcance.\nIngesta de grandes datos y grandes conjuntos de datos en general.\nCrear y mantener ETLs de alta calidad.\nContribuir y asumir la responsabilidad del dise\u00f1o t\u00e9cnico, asegurando que todos los aspectos de la arquitectura del sistema est\u00e9n bien documentados.\nTrabajar estrechamente con los equipos tecnol\u00f3gicos asociados y colaborar de manera efectiva.\nPor qu\u00e9 unirte a nosotros:\nTrabajar en proyectos desafiantes para grandes clientes\nAprender m\u00e1s sobre tecnolog\u00edas emergentes\nAcceso a formaci\u00f3n continua y certificaciones\nDisfrutar de un plan de compensaci\u00f3n flexible\n... \u00a1y mucho m\u00e1s!\nTrabajar en proyectos desafiantes para grandes clientes\nAprender m\u00e1s sobre tecnolog\u00edas emergentes\nAcceso a formaci\u00f3n continua y certificaciones\nDisfrutar de un plan de compensaci\u00f3n flexible\n... \u00a1y mucho m\u00e1s!"
    },
    "4163554876": {
        "title": "Data Engineer - Pharma ",
        "company": "RED Global",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Contract",
        "description": "About the job\n12 months contract - Remote - Data Engineer - Pharmaceuticals \n\nOur client, global pharmaceutical company is looking for a Data engineer to work on a contract basis. You\u2019ll build and optimize scalable data pipelines, integrating clinical, genomics, and commercial data to power analytics and AI-driven insights. You\u2019ll collaborate with scientists, analysts, and IT teams to ensure seamless data flow across R&D, clinical trials, and regulatory operations.\n\nKey Responsibilities:\n\nDesign & develop ETL/ELT pipelines for pharma datasets.\nWork with AWS, Azure, GCP, SQL, NoSQL, and big data tools (Spark, Databricks).\nEnsure data integrity, security, and compliance (HIPAA, GxP, GDPR).\nSupport AI/ML models and real-time analytics for drug development.\n\nRequirements:\n\nData Engineer with pharma experience\nExpertise in SQL, Python, Spark, and cloud platforms.\nExperience with clinical & biomedical data (FHIR, HL7, OMOP a plus).\nStrong understanding of regulatory and compliance frameworks.\nAWS or AZURE\n12 months contract - Remote - Data Engineer - Pharmaceuticals\nOur client, global pharmaceutical company is looking for a Data engineer to work on a contract basis. You\u2019ll build and optimize scalable data pipelines, integrating clinical, genomics, and commercial data to power analytics and AI-driven insights. You\u2019ll collaborate with scientists, analysts, and IT teams to ensure seamless data flow across R&D, clinical trials, and regulatory operations.\nData engineer\nclinical, genomics, and commercial data\nR&D, clinical trials, and regulatory operations\nKey Responsibilities:\nDesign & develop ETL/ELT pipelines for pharma datasets.\nWork with AWS, Azure, GCP, SQL, NoSQL, and big data tools (Spark, Databricks).\nEnsure data integrity, security, and compliance (HIPAA, GxP, GDPR).\nSupport AI/ML models and real-time analytics for drug development.\nDesign & develop ETL/ELT pipelines for pharma datasets.\nETL/ELT pipelines\nWork with AWS, Azure, GCP, SQL, NoSQL, and big data tools (Spark, Databricks).\nAWS, Azure, GCP, SQL, NoSQL, and big data tools\nEnsure data integrity, security, and compliance (HIPAA, GxP, GDPR).\ndata integrity, security, and compliance\nSupport AI/ML models and real-time analytics for drug development.\nAI/ML models\nRequirements:\nData Engineer with pharma experience\nExpertise in SQL, Python, Spark, and cloud platforms.\nExperience with clinical & biomedical data (FHIR, HL7, OMOP a plus).\nStrong understanding of regulatory and compliance frameworks.\nAWS or AZURE\nData Engineer with pharma experience\nExpertise in SQL, Python, Spark, and cloud platforms.\nSQL, Python, Spark, and cloud platforms\nExperience with clinical & biomedical data (FHIR, HL7, OMOP a plus).\nclinical & biomedical data\nStrong understanding of regulatory and compliance frameworks.\nregulatory and compliance frameworks\nAWS or AZURE"
    },
    "4138573103": {
        "title": "Data Engineer",
        "company": "Luxevision Consulting LLC",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nAbout Us\n\nLuxe Vision Consulting is a Wisconsin-based consulting firm dedicated to connecting top global talent with leading U.S. companies. We specialize in sourcing skilled professionals, conducting rigorous screening and technical assessments, and preparing candidates for opportunities in the U.S. job market.\n\nOur mission is to bridge the gap between exceptional talent and top-tier businesses, ensuring the right fit for both candidates and clients. With a focus on quality, efficiency, and innovation, we help professionals unlock their potential and secure rewarding careers with prestigious U.S. organizations.\n\nAt Luxe Vision Consulting, we believe in integrity, excellence, and results-driven recruitment. Whether you're a company looking for the best talent or a professional seeking your next big opportunity, we are committed to making the connection that drives success.\n\nThe Role\n\nJob Description\n\nWe are looking for a skilled Data Engineer to join our team. The ideal candidate will have strong experience in designing, building, and maintaining scalable data pipelines and architectures. You will play a critical role in managing data workflows, ensuring data integrity, and optimizing data processing.\n\nResponsibilities\n\nData Pipeline Development: Design, build, and maintain scalable and efficient data pipelines to process and transform large datasets.\nETL & Data Integration: Develop and optimize ETL (Extract, Transform, Load) workflows for structured and unstructured data sources.\nBig Data Processing: Work with PySpark and Pandas to handle large-scale data processing tasks.\nDatabase Management: Design, implement, and manage relational (SQL) and non-relational databases for data storage and retrieval.\nCloud Technologies: Leverage cloud platforms such as AWS, GCP, or Azure to deploy and manage data infrastructure.\nCollaboration: Work closely with data scientists, analysts, and software engineers to support analytical and machine learning projects.\nData Quality & Performance Optimization: Ensure data accuracy, consistency, and security while optimizing performance.\nMonitoring & Troubleshooting: Identify and resolve data pipeline performance bottlenecks and failures.\n\nIdeal Profile\n\nRequired Work Experience\n\n2+ years of experience in data engineering or a related field.\nProven experience developing ETL pipelines and data processing workflows.\nHands-on experience with PySpark, Pandas, and SQL.\nExperience working with big data technologies such as Apache Spark, Hadoop, or Kafka (preferred).\nFamiliarity with cloud data solutions (AWS, GCP, or Azure).\n\nRequired Skills\n\nProgramming: Strong proficiency in Python (PySpark, Pandas) or Scala.\nData Modeling & Storage: Experience with relational databases (PostgreSQL, MySQL, SQL Server) and NoSQL databases (MongoDB, Cassandra).\nBig Data & Distributed Computing: Knowledge of Apache Spark, Hadoop, or Kafka.\nETL & Data Integration: Ability to develop efficient ETL processes and manage data pipelines.\nCloud Computing: Experience with AWS (S3, Redshift, Glue), GCP (BigQuery), or Azure (Data Factory, Synapse).\nData Warehousing: Understanding of data warehousing concepts and best practices.\nProblem-Solving: Strong analytical skills to troubleshoot and optimize data pipelines.\nCommunication: Must be proficient in spoken English to collaborate with US-based teams.\n\nEducation Requirements\n\nBachelor\u2019s degree in Computer Science, Data Engineering, Information Technology, or a related field (preferred).\nEquivalent work experience in data engineering will also be considered.\n\nFill out the application form here: https://forms.gle/nefgwYRFYE7mffdA7\n\nAlternatively, feel free to message us directly here on the page or email your resume to hiring@ luxevisionconsulting com\n\n\ud83d\udcd1Subject: Position - First and Last Name\n\nOnly shortlisted candidates will be contacted. We look forward to hearing from you!\n\nWhat's on Offer?\n\nWork within a company with a solid track record of success\nAttractive salary & benefits\nExcellent career development opportunities\nAbout Us\nThe Role\nJob Description\nResponsibilities\nData Pipeline Development: Design, build, and maintain scalable and efficient data pipelines to process and transform large datasets.\nETL & Data Integration: Develop and optimize ETL (Extract, Transform, Load) workflows for structured and unstructured data sources.\nBig Data Processing: Work with PySpark and Pandas to handle large-scale data processing tasks.\nDatabase Management: Design, implement, and manage relational (SQL) and non-relational databases for data storage and retrieval.\nCloud Technologies: Leverage cloud platforms such as AWS, GCP, or Azure to deploy and manage data infrastructure.\nCollaboration: Work closely with data scientists, analysts, and software engineers to support analytical and machine learning projects.\nData Quality & Performance Optimization: Ensure data accuracy, consistency, and security while optimizing performance.\nMonitoring & Troubleshooting: Identify and resolve data pipeline performance bottlenecks and failures.\nData Pipeline Development: Design, build, and maintain scalable and efficient data pipelines to process and transform large datasets.\nETL & Data Integration: Develop and optimize ETL (Extract, Transform, Load) workflows for structured and unstructured data sources.\nBig Data Processing: Work with PySpark and Pandas to handle large-scale data processing tasks.\nDatabase Management: Design, implement, and manage relational (SQL) and non-relational databases for data storage and retrieval.\nCloud Technologies: Leverage cloud platforms such as AWS, GCP, or Azure to deploy and manage data infrastructure.\nCollaboration: Work closely with data scientists, analysts, and software engineers to support analytical and machine learning projects.\nData Quality & Performance Optimization: Ensure data accuracy, consistency, and security while optimizing performance.\nMonitoring & Troubleshooting: Identify and resolve data pipeline performance bottlenecks and failures.\nIdeal Profile\nRequired Work Experience\n2+ years of experience in data engineering or a related field.\nProven experience developing ETL pipelines and data processing workflows.\nHands-on experience with PySpark, Pandas, and SQL.\nExperience working with big data technologies such as Apache Spark, Hadoop, or Kafka (preferred).\nFamiliarity with cloud data solutions (AWS, GCP, or Azure).\n2+ years of experience in data engineering or a related field.\nProven experience developing ETL pipelines and data processing workflows.\nHands-on experience with PySpark, Pandas, and SQL.\nExperience working with big data technologies such as Apache Spark, Hadoop, or Kafka (preferred).\nFamiliarity with cloud data solutions (AWS, GCP, or Azure).\nRequired Skills\nProgramming: Strong proficiency in Python (PySpark, Pandas) or Scala.\nData Modeling & Storage: Experience with relational databases (PostgreSQL, MySQL, SQL Server) and NoSQL databases (MongoDB, Cassandra).\nBig Data & Distributed Computing: Knowledge of Apache Spark, Hadoop, or Kafka.\nETL & Data Integration: Ability to develop efficient ETL processes and manage data pipelines.\nCloud Computing: Experience with AWS (S3, Redshift, Glue), GCP (BigQuery), or Azure (Data Factory, Synapse).\nData Warehousing: Understanding of data warehousing concepts and best practices.\nProblem-Solving: Strong analytical skills to troubleshoot and optimize data pipelines.\nCommunication: Must be proficient in spoken English to collaborate with US-based teams.\nProgramming: Strong proficiency in Python (PySpark, Pandas) or Scala.\nData Modeling & Storage: Experience with relational databases (PostgreSQL, MySQL, SQL Server) and NoSQL databases (MongoDB, Cassandra).\nBig Data & Distributed Computing: Knowledge of Apache Spark, Hadoop, or Kafka.\nETL & Data Integration: Ability to develop efficient ETL processes and manage data pipelines.\nCloud Computing: Experience with AWS (S3, Redshift, Glue), GCP (BigQuery), or Azure (Data Factory, Synapse).\nData Warehousing: Understanding of data warehousing concepts and best practices.\nProblem-Solving: Strong analytical skills to troubleshoot and optimize data pipelines.\nCommunication: Must be proficient in spoken English to collaborate with US-based teams.\nEducation Requirements\nBachelor\u2019s degree in Computer Science, Data Engineering, Information Technology, or a related field (preferred).\nEquivalent work experience in data engineering will also be considered.\nBachelor\u2019s degree in Computer Science, Data Engineering, Information Technology, or a related field (preferred).\nEquivalent work experience in data engineering will also be considered.\nWhat's on Offer?\nWork within a company with a solid track record of success\nAttractive salary & benefits\nExcellent career development opportunities\nWork within a company with a solid track record of success\nAttractive salary & benefits\nExcellent career development opportunities"
    },
    "4083634099": {
        "title": "Data Lakehouse Engineer (m/f/d)",
        "company": "Linde Europe",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nLinde Portugal, LDA\n\nData Lakehouse Engineer (m/f/d)\n\nPorto, Portugal or other EMEA hubs\uff5creq17985\n\nWhat You Will Enjoy Doing\n\nAs a data engineer you are focussing on building data pipelines and data lakehouses to best support the needs of our Data science and Data Analytics teams as well as our internal business partners\nYou will combine an understanding of business processes and related data with your extraordinary capability in designing and developing targeted and reusable data models and lakehouses\nFurthermore you will play an important role in improving the overall data architecture and platform strategy\nSupporting in establishing and introducing data governance and lineage principles within the wider Linde organization will be part of the role\n\nWhat Makes You Great\n\nYou posses very strong skills in programming and data engineering (SQL /Python /R ) which you combine with a very good understanding of data modelling and data management processes as well as experience in creating data lakehouses or similar concepts preferred\nFurthermore you have a good understanding of platform concepts and system infrastructure and experience in gathering and understanding business requirements and turning them into data models\nYou combine interest and motivation to continuously learn and develop with excellent spoken and written skills in English\nWillingness to travel to collaborate with the wider team and business (< 10%) is a business need in this role\n\nWhy you will love working with us\n\nThinking outside of the box, working on new topics and concepts, deliver great solutions to our internal customers is essential to bring success to our business. Our company will offer you a variety of possibilities to excel in your area of expertise. You will contribute towards making Linde an inclusive and diverse organization, creating innovative practices and fostering an environment that welcomes all differences \u2013 making Linde an employer of choice.\n\nWhat we offer you!\n\nIn addition to an attractive compensation package, we offer you many social benefits, such as an excellent pension plan, flexible working hours and a wide range of health care options. Furthermore, we support you with various offers to help you combine family and career and to support your professional and personal development. A good infrastructure and our company canteen, which has won several awards, are further benefits.\n\nAt Linde, the sky is not the limit. If you\u2019re looking to build a career where your work reaches beyond your job description and betters the people with whom you work, the communities we serve, and the world in which we all live, at Linde, your opportunities are limitless. Be Linde. Be Limitless.\n\nHave we inspired you? Let\u00b4s talk about it\n\nWe are looking forward to receiving your complete application (motivation letter, CV, certificates) via our online job market.\n\nYour contact person\n\nLinde Portugal, LDA\n\nJan Dietrich\n\nAny designations used of course apply to persons of all genders. The form of speech used here is for simplicity only.\n\nLinde Portugal, LDA acts responsibly towards its shareholders, business partners, employees, society and the environment in every one of its business areas, regions and locations across the globe. The company is committed to technologies and products that unite the goals of customer value and sustainable development.\nData Lakehouse Engineer (m/f/d)\nWhat You Will Enjoy Doing\nAs a data engineer you are focussing on building data pipelines and data lakehouses to best support the needs of our Data science and Data Analytics teams as well as our internal business partners\nYou will combine an understanding of business processes and related data with your extraordinary capability in designing and developing targeted and reusable data models and lakehouses\nFurthermore you will play an important role in improving the overall data architecture and platform strategy\nSupporting in establishing and introducing data governance and lineage principles within the wider Linde organization will be part of the role\nAs a data engineer you are focussing on building data pipelines and data lakehouses to best support the needs of our Data science and Data Analytics teams as well as our internal business partners\nYou will combine an understanding of business processes and related data with your extraordinary capability in designing and developing targeted and reusable data models and lakehouses\nFurthermore you will play an important role in improving the overall data architecture and platform strategy\nSupporting in establishing and introducing data governance and lineage principles within the wider Linde organization will be part of the role\nWhat Makes You Great\nYou posses very strong skills in programming and data engineering (SQL /Python /R ) which you combine with a very good understanding of data modelling and data management processes as well as experience in creating data lakehouses or similar concepts preferred\nFurthermore you have a good understanding of platform concepts and system infrastructure and experience in gathering and understanding business requirements and turning them into data models\nYou combine interest and motivation to continuously learn and develop with excellent spoken and written skills in English\nWhy you will love working with us\nWhat we offer you!\nHave we inspired you? Let\u00b4s talk about it\nYour contact person"
    },
    "4142656097": {
        "title": "Analytics Data Engineer (m/f/d) ",
        "company": "BSH Electrodom\u00e9sticos Espa\u00f1a, S.A.",
        "location": "Zaragoza, Aragon, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nTomorrow is our home.\n\n\u00a1Qu\u00e9 f\u00e1cil es explorar nuevas ideas en BSH Electrodom\u00e9sticos! Como grupo l\u00edder en la fabricaci\u00f3n de electrodom\u00e9sticos, ideamos soluciones para mejorar la calidad de vida de las personas. Con nuestras marcas Bosch, Siemens y Balay exploramos la tecnolog\u00eda m\u00e1s innovadora, desde el reconocimiento de voz a la inteligencia artificial. As\u00ed creamos nuevas experiencias de usuario. Pensamos, prototipamos, construimos y no dejamos de aprender. \u00a1\u00danete a nuestro equipo y dale a tu carrera un hogar!\n\nBSH Electrodom\u00e9sticos Espa\u00f1a, S.A. | Jornada completa |\n\nZaragoza\n\nTus funciones\n\n\u00bfCrees Que Ha Llegado El Momento De Hacer Algo Diferente? \u00a1Entonces Est\u00e1s En El Lugar Adecuado! Trabajar\u00e1s En Un Entorno Internacional Apasionante y Contribuir\u00e1s Personalmente Al \u00c9xito De La Empresa. Te Esperan Proyectos Apasionantes y Tareas Diferentes, Como\n\nDise\u00f1ar, crear y manter pipelines de datos escalables para procesar grandes conjuntos de datos de manera eficiente.\nImplementar y administrar soluciones de virtualizaci\u00f3n y data warehouse con AWS Redshift, Denodo y SAP Datasphere.\nApoyar el desarrollo y la implementaci\u00f3n de modelos de aprendizaje autom\u00e1tico con herramientas como AWS EMR, AWS Airflow y Alteryx.\nColaboraci\u00f3n con equipos de an\u00e1lisis y data scientists para comprender las necesidades de datos, garantizar su precisi\u00f3n y proporcionar soluciones.\nEvaluar peri\u00f3dicamente el rendimiento del sistema, resolver problemas e implementar mejoras para aumentar la eficacia.\n\nTu perfil\n\nGrado en Ingenier\u00eda o similar.\nAl menos 3 a\u00f1os de experiencia en ingenier\u00eda de datos o campos relacionados.\nExperiencia demostrada en servicios de AWS como Redshift, Data Lake, EMR y Airflow.\nConocimientos en SAP HANA, SAP Datasphere. Se valorar\u00e1 estar familiarizado con machine learning. \nNivel avanzado de ingl\u00e9s.\n\nBeneficios\n\nFlexibilidad horaria y posibilidad de teletrabajo (50%).\nDescuentos en electrodom\u00e9sticos BSH y otros beneficios.\nAcceso gratuito a nuestra plataforma de aprendizaje online, incluyendo idiomas.\nComida subvencionada.\nAutob\u00fas de empresa.\n\nM\u00e1s informaci\u00f3n\n\nPor favor visita bsh-group.com/career. \u00a1Te esperamos con los brazos abiertos!\n\nEncuentra Un Nuevo Hogar Para Tu Futuro Profesional\n\nBSH Electrodom\u00e9sticos Espa\u00f1a, S.A es el responsable del tratamiento y procesa tus datos personales con el \u00fanico fin de gestionar tu solicitud de empleo. Si deseas ejercer tus derechos de protecci\u00f3n de datos, env\u00eda un correo electr\u00f3nico a Data-Protection-ES@bshg.com.\n\nYour application will also be forwarded to individual employees of the following company for the purpose of filling the position: BSH Ev Aletleri Sanayi ve Ticaret A.\u015e. (Turkey).\nTus funciones\nDise\u00f1ar, crear y manter pipelines de datos escalables para procesar grandes conjuntos de datos de manera eficiente.\nImplementar y administrar soluciones de virtualizaci\u00f3n y data warehouse con AWS Redshift, Denodo y SAP Datasphere.\nApoyar el desarrollo y la implementaci\u00f3n de modelos de aprendizaje autom\u00e1tico con herramientas como AWS EMR, AWS Airflow y Alteryx.\nColaboraci\u00f3n con equipos de an\u00e1lisis y data scientists para comprender las necesidades de datos, garantizar su precisi\u00f3n y proporcionar soluciones.\nEvaluar peri\u00f3dicamente el rendimiento del sistema, resolver problemas e implementar mejoras para aumentar la eficacia.\nDise\u00f1ar, crear y manter pipelines de datos escalables para procesar grandes conjuntos de datos de manera eficiente.\nImplementar y administrar soluciones de virtualizaci\u00f3n y data warehouse con AWS Redshift, Denodo y SAP Datasphere.\nApoyar el desarrollo y la implementaci\u00f3n de modelos de aprendizaje autom\u00e1tico con herramientas como AWS EMR, AWS Airflow y Alteryx.\nColaboraci\u00f3n con equipos de an\u00e1lisis y data scientists para comprender las necesidades de datos, garantizar su precisi\u00f3n y proporcionar soluciones.\nEvaluar peri\u00f3dicamente el rendimiento del sistema, resolver problemas e implementar mejoras para aumentar la eficacia.\nTu perfil\nGrado en Ingenier\u00eda o similar.\nAl menos 3 a\u00f1os de experiencia en ingenier\u00eda de datos o campos relacionados.\nExperiencia demostrada en servicios de AWS como Redshift, Data Lake, EMR y Airflow.\nConocimientos en SAP HANA, SAP Datasphere. Se valorar\u00e1 estar familiarizado con machine learning. \nNivel avanzado de ingl\u00e9s.\nGrado en Ingenier\u00eda o similar.\nAl menos 3 a\u00f1os de experiencia en ingenier\u00eda de datos o campos relacionados.\nExperiencia demostrada en servicios de AWS como Redshift, Data Lake, EMR y Airflow.\nConocimientos en SAP HANA, SAP Datasphere. Se valorar\u00e1 estar familiarizado con machine learning.\nNivel avanzado de ingl\u00e9s.\nBeneficios\nFlexibilidad horaria y posibilidad de teletrabajo (50%).\nDescuentos en electrodom\u00e9sticos BSH y otros beneficios.\nAcceso gratuito a nuestra plataforma de aprendizaje online, incluyendo idiomas.\nComida subvencionada.\nAutob\u00fas de empresa.\nFlexibilidad horaria y posibilidad de teletrabajo (50%).\nDescuentos en electrodom\u00e9sticos BSH y otros beneficios.\nAcceso gratuito a nuestra plataforma de aprendizaje online, incluyendo idiomas.\nComida subvencionada.\nAutob\u00fas de empresa.\nM\u00e1s informaci\u00f3n\nEncuentra Un Nuevo Hogar Para Tu Futuro Profesional"
    },
    "3931420873": {
        "title": "Data Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Valencia, Valencian Community, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\n\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca \n\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \u270d\n\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n? \ud83d\udc47\n\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada semestre, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario competitivo seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n? \u270d\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\nSer\u00e1s el/la responsable integral en el desarrollo y ejecuci\u00f3n de diversos proyectos a cargo de un equipo, asegurando el cumplimiento exitoso de todas las fases de los proyectos, desde la toma de requisitos hasta su puesta en producci\u00f3n. Incluyendo los siguientes aspectos: definici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\ndesarrollo y ejecuci\u00f3n de diversos proyectos\ndefinici\u00f3n t\u00e9cnico-funcional, desarrollo de la ETL y modelado de datos, visualizaci\u00f3n, as\u00ed como la realizaci\u00f3n de formaciones y el roll out entre otros aspectos claves.\nTrabajar\u00e1s d\u00eda a d\u00eda con Base de datos SQL, modelado de datos con herramientas ETL y con entornos datawarehouse.\nSQL\nETL\ndatawarehouse.\nTe encargar\u00e1s de la formaci\u00f3n interna y externa en las herramientas relevantes para el equipo y detectar\u00e1s formaciones y certificaciones necesarias para el crecimiento del equipo.\nformaci\u00f3n interna y externa en las herramientas relevantes para el equipo y\nPermanecer\u00e1s al d\u00eda en las \u00faltimas tendencias tecnol\u00f3gicas, con especial \u00e9nfasis en el campo de Data & Analytics, mediante un compromiso continuo con la formaci\u00f3n y la exploraci\u00f3n proactiva de nuevas innovaciones en el sector.\ntendencias tecnol\u00f3gicas\nData & Analytics\nTe integrar\u00e1s en la cultura de SDG y su unidad de trabajo, contribuyendo a un entorno colaborativo y eficiente.\ncultura de SDG y su unidad de trabajo\nParticipar\u00e1s en el d\u00eda a d\u00eda de las din\u00e1micas de compa\u00f1\u00eda y de equipo.\ndin\u00e1micas de compa\u00f1\u00eda y de equipo\nColaborar\u00e1s estrechamente con su Manager en actividades operativas de la compa\u00f1\u00eda, que incluyen temas como la gesti\u00f3n de la imputaci\u00f3n de horas del equipo, el an\u00e1lisis de forecast para los siguientes meses, actividades de team building, etc.\nctividades operativas de la compa\u00f1\u00eda\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n? \ud83d\udc47\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior u otra salida acad\u00e9mica que te haya dotado de capacidad de desarrollo y pensamiento cr\u00edtico en este ambiente.\nEstudios en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencia de datos, Industriales, o contar con un Ciclo Superior\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end.\nExperiencia m\u00ednima de 3 a\u00f1os trabajando como consultor/desarrollador en plataformas de datos participando en proyectos de Data & Analytics end-to-end\nDeseable haber trabajado en empresas de Consultor\u00eda para entender las necesidades y la din\u00e1mica del d\u00eda a d\u00eda de nuestra organizaci\u00f3n.\nS\u00f3lido conocimiento en tecnolog\u00edas de Data & Analytics, con experiencia demostrada en desarrollos en SQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nSQL, tecnolog\u00edas de ETL y conocimientos en modelado de datos, y especial relevancia a la adaptaci\u00f3n a entornos Cloud (AWS, Azure, Google).\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio, para entender el contexto de los proyectos y ayudar a la toma de decisiones de nuestros clientes. Es\nHabilidades comunicativas, anal\u00edticas y con orientaci\u00f3n a negocio\nimportante demostrar una actitud proactiva y participativa.\nComo trabajamos en un entorno global, \u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\n\u00a1ser\u00e1 un plus que puedas comunicarte en ingl\u00e9s!\nValoraremos positivamente que cuentes con experiencia en visualizaci\u00f3n con foco en Tableau y PowerBI.\nvisualizaci\u00f3n con foco en Tableau y PowerBI.\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada semestre, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario competitivo seg\u00fan tu formaci\u00f3n y experiencia\nSalario competitivo\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5"
    },
    "4170823631": {
        "title": "Senior Data Engineer",
        "company": "Holded",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nJoin the team. Make an impact.\nAt Holded, we believe that daily admin should never stop a great idea from becoming a success. That's why we create intuitive software to empower anyone who dares to start their own business. Long story short: we want to make business simple.\nIn order to create cutting-edge products that meet the needs of the sector, talent is essential. We are looking for passion, creativity, and commitment. In return, we offer the same.\n\nThe Role\nWe are seeking a highly skilled Data Engineer to join our team. You will play a crucial role in enhancing and administering our data infrastructure and implementing solutions that drive business intelligence and analytics. You will implement scalable data solutions that support business intelligence and analytics, working collaboratively with engineering teams and stakeholders to ensure data quality, compliance, and accessibility.\n\nKey Responsibilities:\nOptimize and enhance data workflows and processes to improve efficiency and effectiveness.\nImplement solutions to ensure compliance with relevant data protection standards and regulations.\nCollaborate with cross-functional teams to establish robust data governance practices.\nDevelop and maintain complex data models to support strategic business initiatives.\nOversee and enhance the data infrastructure.\nManage the data platform.\nDefine and execute a long-term strategy for data architecture.\nFacilitate data ingestion and transformation processes to enable seamless data integration.\nProvide support and resources to enable team members to create comprehensive reports and analytics.\nAssist in delivering essential data for advanced analytics and machine learning initiatives.\nIndependently deliver medium to large-scale features.\nEstablish and promote best practices for code maintainability and performance.\n\nAbout you:\nYou have 5+ years of work experience as a Data Engineer\nYou understand, follow and implement data engineering best practices\nYou have expertise in Python and SQL\nYou are proficient in Google Cloud Platform (GCP), particularly with BigQuery and Apache Beam/Dataflow\nYou have demonstrated experience with Apache Airflow for workflow orchestration\nYou have a strong understanding of Docker\nYou have experience with implementing data governance policies\nYou have experience integrating data in streaming and in batch from multiple sources including DBs, product tracking, and APIs\nYou are fluent in English\n\nNice to haves:\nFamiliarity with Terraform and Google Tag Manager\nKnowledge of GDPR compliance\nExperience with MongoDB, Kubernetes and Golang\nPrevious work with DBT (Data Build Tool)\n\nWhat it's like to work with us\nPermanent contract\n26 paid days-off\nFlexible working hours\nShort work-day on Fridays\nFree catered lunch\nContinuous Training: annual budget for training for each employee\nFully equipped kitchen with snacks, drinks, and fresh fruit.\nTop-notch work equipment\nOffice in front of the sea with ping pong, pool table, PlayStation\u2026\nInteresting projects and a great work environment\nAn excellent opportunity to grow with the company\nDiscounts on a Gym membership\nJoin the team. Make an impact.\nAt Holded, we believe that daily admin should never stop a great idea from becoming a success. That's why we create intuitive software to empower anyone who dares to start their own business. Long story short: we want to make business simple.\nHolded\nIn order to create cutting-edge products that meet the needs of the sector, talent is essential. We are looking for passion, creativity, and commitment. In return, we offer the same.\nWe are looking for passion, creativity, and commitment\nThe Role\nWe are seeking a highly skilled Data Engineer to join our team. You will play a crucial role in enhancing and administering our data infrastructure and implementing solutions that drive business intelligence and analytics. You will implement scalable data solutions that support business intelligence and analytics, working collaboratively with engineering teams and stakeholders to ensure data quality, compliance, and accessibility.\nKey Responsibilities:\nOptimize and enhance data workflows and processes to improve efficiency and effectiveness.\nImplement solutions to ensure compliance with relevant data protection standards and regulations.\nCollaborate with cross-functional teams to establish robust data governance practices.\nDevelop and maintain complex data models to support strategic business initiatives.\nOversee and enhance the data infrastructure.\nManage the data platform.\nDefine and execute a long-term strategy for data architecture.\nFacilitate data ingestion and transformation processes to enable seamless data integration.\nProvide support and resources to enable team members to create comprehensive reports and analytics.\nAssist in delivering essential data for advanced analytics and machine learning initiatives.\nIndependently deliver medium to large-scale features.\nEstablish and promote best practices for code maintainability and performance.\nOptimize and enhance data workflows and processes to improve efficiency and effectiveness.\nImplement solutions to ensure compliance with relevant data protection standards and regulations.\nCollaborate with cross-functional teams to establish robust data governance practices.\nDevelop and maintain complex data models to support strategic business initiatives.\nOversee and enhance the data infrastructure.\nManage the data platform.\nDefine and execute a long-term strategy for data architecture.\nFacilitate data ingestion and transformation processes to enable seamless data integration.\nProvide support and resources to enable team members to create comprehensive reports and analytics.\nAssist in delivering essential data for advanced analytics and machine learning initiatives.\nIndependently deliver medium to large-scale features.\nEstablish and promote best practices for code maintainability and performance.\nAbout you:\nYou have 5+ years of work experience as a Data Engineer\nYou understand, follow and implement data engineering best practices\nYou have expertise in Python and SQL\nYou are proficient in Google Cloud Platform (GCP), particularly with BigQuery and Apache Beam/Dataflow\nYou have demonstrated experience with Apache Airflow for workflow orchestration\nYou have a strong understanding of Docker\nYou have experience with implementing data governance policies\nYou have experience integrating data in streaming and in batch from multiple sources including DBs, product tracking, and APIs\nYou are fluent in English\nYou have 5+ years of work experience as a Data Engineer\nYou understand, follow and implement data engineering best practices\nYou have expertise in Python and SQL\nYou are proficient in Google Cloud Platform (GCP), particularly with BigQuery and Apache Beam/Dataflow\nYou have demonstrated experience with Apache Airflow for workflow orchestration\nYou have a strong understanding of Docker\nYou have experience with implementing data governance policies\nYou have experience integrating data in streaming and in batch from multiple sources including DBs, product tracking, and APIs\nYou are fluent in English\nNice to haves:\nFamiliarity with Terraform and Google Tag Manager\nKnowledge of GDPR compliance\nExperience with MongoDB, Kubernetes and Golang\nPrevious work with DBT (Data Build Tool)\nFamiliarity with Terraform and Google Tag Manager\nKnowledge of GDPR compliance\nExperience with MongoDB, Kubernetes and Golang\nPrevious work with DBT (Data Build Tool)\nWhat it's like to work with us\nPermanent contract\n26 paid days-off\nFlexible working hours\nShort work-day on Fridays\nFree catered lunch\nContinuous Training: annual budget for training for each employee\nFully equipped kitchen with snacks, drinks, and fresh fruit.\nTop-notch work equipment\nOffice in front of the sea with ping pong, pool table, PlayStation\u2026\nInteresting projects and a great work environment\nAn excellent opportunity to grow with the company\nDiscounts on a Gym membership\nPermanent contract\n26 paid days-off\nFlexible working hours\nShort work-day on Fridays\nFree catered lunch\nContinuous Training: annual budget for training for each employee\nFully equipped kitchen with snacks, drinks, and fresh fruit.\nTop-notch work equipment\nOffice in front of the sea with ping pong, pool table, PlayStation\u2026\nInteresting projects and a great work environment\nAn excellent opportunity to grow with the company\nDiscounts on a Gym membership"
    },
    "4147188503": {
        "title": "Data Engineer",
        "company": "Daus Data",
        "location": "Greater Madrid Metropolitan Area",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfTe apasiona el BigData? \u00bfQuieres formar parte de un equipo con una de las trayectorias m\u00e1s solventes en el mundo del Machine Learning, Analytics y BigData? \u00bfTe apetece trabajar en una compa\u00f1\u00eda joven, participativa y especializada?\n\nPara ello solo exigimos un requisito, que tus respuestas a las preguntas anteriores hayan sido: \u00a1S\u00cd!\n\nEn Daus Data, trabajamos continuamente para mantenernos a la vanguardia tecnol\u00f3gica, y adem\u00e1s somos una empresa People Centric, donde ponemos al trabajador en el centro de todo lo que hacemos. Creemos en una cultura horizontal, sin egos, en un entorno donde se valoran tanto las ideas como las personas.\n\nBuscamos 1 Data Engineer con al menos 3 a\u00f1os de experiencia, habiendo trabajado en todas las etapas del ciclo de vida de proyectos de Data: desde la preparaci\u00f3n de datos, transformaci\u00f3n y hasta su puesta en producci\u00f3n. Queremos destacar especialmente este \u00faltimo punto: buscamos personas que, al igual que nosotros, no se conformen con las pruebas de concepto, sino que se impliquen en la puesta en producci\u00f3n real de los procesos de Data.\n\n\u00bfQu\u00e9 buscamos?\n\nA nivel t\u00e9cnico:\n1.- Muy buen nivel de PysPark.\n2 - Experiencia en la realizaci\u00f3n de ETL con Spark.\n3.- Experiencia trabajando con cliente y recopilando requisitos de negocio en buenas habilidades de comunicaci\u00f3n.\n4 - Conocimiento en el uso de servicios de AWS como AWS Glue, AWS Lambda, EMR, S3, Step Functions.\n\nSer\u00e1 un plus:\n1.- Tener experiencia en DevOps.\n2 - Tener conocimientos de gesti\u00f3n en AWS como AWS VPC, AWS IAM.\n\nTambi\u00e9n es importante tener un buen nivel de ingl\u00e9s, ya que muchos de nuestros proyectos ser\u00e1n para clientes internacionales.\n\n\u00bfQu\u00e9 ofrecemos?\nContrato indefinido y un salario competitivo, en este caso entre los 36.000 y 42.000 euros brutos/a\u00f1o de entrada, ajustado seg\u00fan tu experiencia (nuestro plan de carrera incluye 2 revisiones anuales). Tambi\u00e9n tendr\u00e1s a tu disposici\u00f3n un plan de formaci\u00f3n especializado, participaci\u00f3n en la propiedad y los beneficios de Grupo M\u00e1tica: Wetaca gratuito en la oficina dos d\u00edas a la semana, horario flexible con facilidades para trabajar desde donde quieras, seguro de salud y un divertido welcome kit con port\u00e1til (podr\u00e1s escoger entre varios modelos Mac o Windows), m\u00f3vil, botella personalizada, auriculares bluetooth y golosinas.\nCuando est\u00e9s en nuestra oficina, en el centro de Barcelona o Madrid, la fruta, los caf\u00e9s y los refrescos corren a cuenta de M\u00e1tica. \u00a1Ah! Y si eres amante de los animales, te gustar\u00e1 saber esto: \u00a1nuestras oficinas son Pet-friendly!\n\nSi te interesa, escr\u00edbenos por LinkedIn o env\u00eda tu CV a seleccion@maticapartners.com\n\nDo you want to be M\u00e1tica? Be M\u00e1tica!\n\u00bfTe apasiona el BigData? \u00bfQuieres formar parte de un equipo con una de las trayectorias m\u00e1s solventes en el mundo del Machine Learning, Analytics y BigData? \u00bfTe apetece trabajar en una compa\u00f1\u00eda joven, participativa y especializada?\nPara ello solo exigimos un requisito, que tus respuestas a las preguntas anteriores hayan sido: \u00a1S\u00cd!\nEn Daus Data, trabajamos continuamente para mantenernos a la vanguardia tecnol\u00f3gica, y adem\u00e1s somos una empresa People Centric, donde ponemos al trabajador en el centro de todo lo que hacemos. Creemos en una cultura horizontal, sin egos, en un entorno donde se valoran tanto las ideas como las personas.\nDaus Data\nPeople Centric\nBuscamos 1 Data Engineer con al menos 3 a\u00f1os de experiencia, habiendo trabajado en todas las etapas del ciclo de vida de proyectos de Data: desde la preparaci\u00f3n de datos, transformaci\u00f3n y hasta su puesta en producci\u00f3n. Queremos destacar especialmente este \u00faltimo punto: buscamos personas que, al igual que nosotros, no se conformen con las pruebas de concepto, sino que se impliquen en la puesta en producci\u00f3n real de los procesos de Data.\n1 Data Engineer\n3 a\u00f1os de experiencia\npreparaci\u00f3n de datos, transformaci\u00f3n y hasta su puesta en producci\u00f3n\n\u00bfQu\u00e9 buscamos?\nA nivel t\u00e9cnico:\n1.- Muy buen nivel de PysPark.\nPysPark\n2 - Experiencia en la realizaci\u00f3n de ETL con Spark.\nETL con Spark\n3.- Experiencia trabajando con cliente y recopilando requisitos de negocio en buenas habilidades de comunicaci\u00f3n.\n4 - Conocimiento en el uso de servicios de AWS como AWS Glue, AWS Lambda, EMR, S3, Step Functions.\nAWS\nAWS Glue, AWS Lambda, EMR, S3, Step Functions\nSer\u00e1 un plus:\n1.- Tener experiencia en DevOps.\n2 - Tener conocimientos de gesti\u00f3n en AWS como AWS VPC, AWS IAM.\nAWS VPC, AWS IAM\nTambi\u00e9n es importante tener un buen nivel de ingl\u00e9s, ya que muchos de nuestros proyectos ser\u00e1n para clientes internacionales.\nbuen nivel de ingl\u00e9s\n\u00bfQu\u00e9 ofrecemos?\nContrato indefinido y un salario competitivo, en este caso entre los 36.000 y 42.000 euros brutos/a\u00f1o de entrada, ajustado seg\u00fan tu experiencia (nuestro plan de carrera incluye 2 revisiones anuales). Tambi\u00e9n tendr\u00e1s a tu disposici\u00f3n un plan de formaci\u00f3n especializado, participaci\u00f3n en la propiedad y los beneficios de Grupo M\u00e1tica: Wetaca gratuito en la oficina dos d\u00edas a la semana, horario flexible con facilidades para trabajar desde donde quieras, seguro de salud y un divertido welcome kit con port\u00e1til (podr\u00e1s escoger entre varios modelos Mac o Windows), m\u00f3vil, botella personalizada, auriculares bluetooth y golosinas.\n36.000 y 42.000 euros brutos/a\u00f1o de entrada\nplan de formaci\u00f3n especializado\nparticipaci\u00f3n en la propiedad\nWetaca\nhorario flexible\nseguro de salud\nwelcome kit\nMac o Windows\nCuando est\u00e9s en nuestra oficina, en el centro de Barcelona o Madrid, la fruta, los caf\u00e9s y los refrescos corren a cuenta de M\u00e1tica. \u00a1Ah! Y si eres amante de los animales, te gustar\u00e1 saber esto: \u00a1nuestras oficinas son Pet-friendly!\nBarcelona o Madrid\nPet-friendly\nSi te interesa, escr\u00edbenos por LinkedIn o env\u00eda tu CV a seleccion@maticapartners.com\nDo you want to be M\u00e1tica? Be M\u00e1tica!\nBe M\u00e1tica!"
    },
    "4166585573": {
        "title": "Data Engineer ",
        "company": "Merkle",
        "location": "Greater Gij\u00f3n Metropolitan Area",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nEl prop\u00f3sito de esta funci\u00f3n es desarrollar las funciones de Data Management requeridas, logrando una entrega oportuna de acuerdo con los est\u00e1ndares de rendimiento y calidad de la empresa.\n\nJob Description:\n\n\u00a1Hola, somos Merkle! Una compa\u00f1\u00eda especializada en Customer Experience Management, que forma parte del grupo global de comunicaci\u00f3n dentsu.\n\nAyudamos a empresas IBEX35, grandes anunciantes y compa\u00f1\u00edas Fortune 500 a desarrollar experiencias de clientes \u00fanicas y personales a trav\u00e9s de los datos, la tecnolog\u00eda y la creativida\n\nd.Poseemos capacidades \u00fanicas en Customer Experience, Data Management e Intelligence Analytics. Al tiempo que incorporamos la visi\u00f3n estrat\u00e9gica a nuestro alto grado de especializaci\u00f3n creando una manera de trabajar \u00fanica con nuestros clientes.\n\nCon sede en Madrid, Barcelona y Gij\u00f3n, la base del liderazgo de Merkle lo forman las personas. Un EQUIPO de TALENTO DIGITAL formado por profesionales con gran pasi\u00f3n, ilusi\u00f3n y potencial en su trabajo.\n\nJob Description\n\nEn MERKLE apostamos por aquellas personas a las que les gusta salir de zona de confort, que aspiran a encontrarse y superar nuevos retos y que quieren trabajar en un ambiente amigable y donde todos somos un equipo.\n\nBuscamos personas motivadas, que se propongan retos y que se sientan c\u00f3modas trabajando en equipo y proponiendo nuevas formas de hacer las cosas. Personas que disfruten del trato con el cliente en un ambiente donde los datos nos rodean y la interpretaci\u00f3n de los mismos es nuestro arte.\n\nEl perfil que estamos buscando tendr\u00e1 como principales funciones:\n\nDise\u00f1ar e implementar la arquitectura de soluciones en plataformas Cloud\nImplementar y mantener procesos ETL\nViabilizar y poner a disposici\u00f3n de los equipos del cliente tablas y bases de datos a partir de necesidades definidas\nConfigurar servicios/m\u00f3dulos de la soluci\u00f3n en base a la arquitectura proporcionada\nFormaci\u00f3n continua en las tecnolog\u00edas y disciplinas digitales de la industria\n\n\nTe ofrecemos un puesto de trabajo estable, de futuro, con posibilidad de promoci\u00f3n interna.\n\nTrabajamos con clientes de primer nivel, empresas l\u00edderes en su sector con un alto grado de madurez digital.\n\nQueremos que te sientas a gusto por lo que fomentamos actividades de cohesi\u00f3n interna para sentirnos m\u00e1s equipo.\n\nTe daremos formaci\u00f3n inicial y formaci\u00f3n continua, aunque tu esp\u00edritu de buscar por ti mismo y tus ganas de aprender debe ser algo que marque tu desarrollo profesional con nosotros.\n\nFomentamos los h\u00e1bitos de vida saludables y por ello tendr\u00e1s seguro m\u00e9dico privado pagado \u00edntegramente por la empresa.\u00bfTe unes al equipo? \u00a1\u00a1Te esperamos!!\n\nQualifications\n\nSi tienes una carrera t\u00e9cnica (Inform\u00e1tica, Telecomunicaciones, etc) o una formaci\u00f3n de CFGS en inform\u00e1tica o similar ser\u00eda muy interesante.\n\nLo ideal es que tengas conocimientos de:\n\nSistemas LINUX\nLenguajes de programaci\u00f3n orientados al procesamiento de datos (Python, JavaScript, Scala\u2026)\nBases de datos\nManejo de APIs\nConocimiento de alg\u00fan sistema de control de versiones (i.e. git)\nContenerizaci\u00f3n (Docker)\n\n\nSe valorar\u00e1 que tengas experiencia en alguno de estos \u00e1mbitos:\n\nPlataformas de Cloud Computing. Lo ideal es que tuvieras experiencia en Google Cloud, Azure , AWS... \nDevOpsBack-end development\nSysOps\nHadoop, Spark\n\n\nAdem\u00e1s buscamos una persona con dotes comunicativas y habituada al trato con el cliente, inter\u00e9s y curiosidad en dato digital.\n\nTe ofrecemos:\n\nContrato indefinido\nExcelente ambiente de trabajo\nSeguro m\u00e9dico privado\nAmplio n\u00famero de d\u00edas de vacaciones\nHorario flexible\nHorario de verano (3 meses)\nFormato h\u00edbrido teletrabajo\nFormaci\u00f3n continua (Linkedin Learning, Merkle University, etc)\nClases de ingl\u00e9s\n\n\nMerkle does not discriminate against job applicants on the basis of age, disability, gender reassignment, marital or civil partner status, pregnancy or maternity, race, colour, nationality, ethnic or national origin, religion or belief, sex or sexual orientation. Experience stipulated in this job description serves as a guide only and all applications will be considered on their merits, irrespective of experience. At the point of application, the candidate must have the legal right to work in Spain as we are unable to sponsor visas at this time.\n\nAs part of our Diversity and Inclusion agenda, and as an Equal Opportunities employer, if you require reasonable adjustments during the selection process please engage directly with your Recruiter.\n\nLocation:\n\nGijon\n\nBrand:\n\nMerkle\n\nTime Type:\n\nFull time\n\nContract Type:\n\nPermanent\nJob Description:\ngrupo global de comunicaci\u00f3n dentsu.\nJob Description\nEl perfil que estamos buscando tendr\u00e1 como principales funciones:\nDise\u00f1ar e implementar la arquitectura de soluciones en plataformas Cloud\nImplementar y mantener procesos ETL\nViabilizar y poner a disposici\u00f3n de los equipos del cliente tablas y bases de datos a partir de necesidades definidas\nConfigurar servicios/m\u00f3dulos de la soluci\u00f3n en base a la arquitectura proporcionada\nFormaci\u00f3n continua en las tecnolog\u00edas y disciplinas digitales de la industria\nDise\u00f1ar e implementar la arquitectura de soluciones en plataformas Cloud\nImplementar y mantener procesos ETL\nViabilizar y poner a disposici\u00f3n de los equipos del cliente tablas y bases de datos a partir de necesidades definidas\nConfigurar servicios/m\u00f3dulos de la soluci\u00f3n en base a la arquitectura proporcionada\nFormaci\u00f3n continua en las tecnolog\u00edas y disciplinas digitales de la industria\ne esperamos!!\nQualifications\nSistemas LINUX\nLenguajes de programaci\u00f3n orientados al procesamiento de datos (Python, JavaScript, Scala\u2026)\nBases de datos\nManejo de APIs\nConocimiento de alg\u00fan sistema de control de versiones (i.e. git)\nContenerizaci\u00f3n (Docker)\nSistemas LINUX\nLenguajes de programaci\u00f3n orientados al procesamiento de datos (Python, JavaScript, Scala\u2026)\nBases de datos\nManejo de APIs\nConocimiento de alg\u00fan sistema de control de versiones (i.e. git)\nContenerizaci\u00f3n (Docker)\nPlataformas de Cloud Computing. Lo ideal es que tuvieras experiencia en Google Cloud, Azure , AWS... \nDevOpsBack-end development\nSysOps\nHadoop, Spark\nPlataformas de Cloud Computing. Lo ideal es que tuvieras experiencia en Google Cloud, Azure , AWS...\nDevOpsBack-end development\nSysOps\nHadoop, Spark\nTe ofrecemos:\nContrato indefinido\nExcelente ambiente de trabajo\nSeguro m\u00e9dico privado\nAmplio n\u00famero de d\u00edas de vacaciones\nHorario flexible\nHorario de verano (3 meses)\nFormato h\u00edbrido teletrabajo\nFormaci\u00f3n continua (Linkedin Learning, Merkle University, etc)\nClases de ingl\u00e9s\nContrato indefinido\nExcelente ambiente de trabajo\nSeguro m\u00e9dico privado\nAmplio n\u00famero de d\u00edas de vacaciones\nHorario flexible\nHorario de verano (3 meses)\nFormato h\u00edbrido teletrabajo\nFormaci\u00f3n continua (Linkedin Learning, Merkle University, etc)\nClases de ingl\u00e9s\nMerkle does not discriminate against job applicants on the basis of age, disability, gender reassignment, marital or civil partner status, pregnancy or maternity, race, colour, nationality, ethnic or national origin, religion or belief, sex or sexual orientation. Experience stipulated in this job description serves as a guide only and all applications will be considered on their merits, irrespective of experience. At the point of application, the candidate must have the legal right to work in Spain as we are unable to sponsor visas at this time.\nAs part of our Diversity and Inclusion agenda, and as an Equal Opportunities employer, if you require reasonable adjustments during the selection process please engage direct\nLocation:\nBrand:\nTime Type:\nContract Type:"
    },
    "4150275242": {
        "title": "Data Engineer",
        "company": "Remobi",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Contract",
        "description": "About the job\nWe are building the world's greatest community of remote technologists!\n\nToday, organizations that understand the value of remote working will reap the rewards. Because it doesn\u2019t just provide team members with a healthier work-life balance, it gives you the opportunity to access the brightest minds in the world.\n\nOur clients access our community to build or extend their existing teams. All are made up of remote, distributed software engineering experts - the best-in-class. Rapidly deployed without compromising on quality.\n\nJoin our Remobi community to have access to meaningful, innovative freelance projects and play a key role in shaping how companies operate.\n\nDuration: 6 months initial\nLocation: Remote\nContract Type: Freelance / B2B / Contract (Candidates must be based in EU)\n\nSummary:\n\nWe are seeking an experienced Data Engineer with strong expertise in NestJS to design, implement, and optimize scalable data solutions. The ideal candidate should have a deep understanding of backend development, data pipeline architecture, and cloud technologies.\n\nWhat You\u2019ll Do:\n\nDesign, build, and maintain scalable data pipelines and architectures.\nDevelop, optimize, and maintain ETL processes.\nOwn, maintain, and improve analytical data modeling practices using tools like dbt.\nWork with large datasets, ensuring high availability, accuracy, and performance.\nImplement and manage databases, ensuring best practices in design, tuning, and maintenance (e.g., PostgreSQL).\nDevelop and maintain backend services and APIs using NestJS.\nCollaborate with cross-functional teams to integrate data solutions with microservices and APIs.\nUtilize Python and SQL to develop and maintain data solutions.\nWork with cloud-based data solutions, particularly within the Azure ecosystem (Azure Data Lake, Data Factory, etc.).\nImplement and maintain CI/CD pipelines and DevOps best practices.\n\nWhat You Bring:\n\n3 - 8 years of experience as a Data Engineer.\nStrong proficiency in Python and SQL.\nExpertise in backend development using NestJS.\nExperience with ETL processes and data pipeline optimization.\nFamiliarity with microservices, Kubernetes, Docker, and REST APIs.\nHands-on experience with database design, tuning, and maintenance.\nPractical knowledge of Azure data stack tools (Azure Data Lake, Data Factory, etc.).\n\nWhat Will Make You Stand Out:\n\nExpertise in Apache Kafka.\n1-2 years of hands-on, production-grade Snowflake experience (architecture design, query optimization, cost monitoring).\nStrong DevOps skills, including Kubernetes, Terraform, and cloud security principles.\nExperience developing and deploying machine learning and AI models.\nAPI development experience using FastAPI, Flask, or similar frameworks.\n\nWhy Join Us?\n\nBe part of an innovative community working with the latest technologies.\nAccess to top-tier remote freelance opportunities.\nCompetitive compensation and flexible working arrangements.\nA collaborative and inclusive work environment.\nWe are building the world's greatest community of remote technologists!\nToday, organizations that understand the value of remote working will reap the rewards. Because it doesn\u2019t just provide team members with a healthier work-life balance, it gives you the opportunity to access the brightest minds in the world.\nOur clients access our community to build or extend their existing teams. All are made up of remote, distributed software engineering experts - the best-in-class. Rapidly deployed without compromising on quality.\nJoin our Remobi community to have access to meaningful, innovative freelance projects and play a key role in shaping how companies operate.\nDuration: 6 months initial\nDuration:\nLocation: Remote\nLocation:\nContract Type: Freelance / B2B / Contract (Candidates must be based in EU)\nContract Type:\nSummary:\nWe are seeking an experienced Data Engineer with strong expertise in NestJS to design, implement, and optimize scalable data solutions. The ideal candidate should have a deep understanding of backend development, data pipeline architecture, and cloud technologies.\nWhat You\u2019ll Do:\nDesign, build, and maintain scalable data pipelines and architectures.\nDevelop, optimize, and maintain ETL processes.\nOwn, maintain, and improve analytical data modeling practices using tools like dbt.\nWork with large datasets, ensuring high availability, accuracy, and performance.\nImplement and manage databases, ensuring best practices in design, tuning, and maintenance (e.g., PostgreSQL).\nDevelop and maintain backend services and APIs using NestJS.\nCollaborate with cross-functional teams to integrate data solutions with microservices and APIs.\nUtilize Python and SQL to develop and maintain data solutions.\nWork with cloud-based data solutions, particularly within the Azure ecosystem (Azure Data Lake, Data Factory, etc.).\nImplement and maintain CI/CD pipelines and DevOps best practices.\nDesign, build, and maintain scalable data pipelines and architectures.\nDevelop, optimize, and maintain ETL processes.\nOwn, maintain, and improve analytical data modeling practices using tools like dbt.\nWork with large datasets, ensuring high availability, accuracy, and performance.\nImplement and manage databases, ensuring best practices in design, tuning, and maintenance (e.g., PostgreSQL).\nDevelop and maintain backend services and APIs using NestJS.\nCollaborate with cross-functional teams to integrate data solutions with microservices and APIs.\nUtilize Python and SQL to develop and maintain data solutions.\nWork with cloud-based data solutions, particularly within the Azure ecosystem (Azure Data Lake, Data Factory, etc.).\nImplement and maintain CI/CD pipelines and DevOps best practices.\nWhat You Bring:\n3 - 8 years of experience as a Data Engineer.\nStrong proficiency in Python and SQL.\nExpertise in backend development using NestJS.\nExperience with ETL processes and data pipeline optimization.\nFamiliarity with microservices, Kubernetes, Docker, and REST APIs.\nHands-on experience with database design, tuning, and maintenance.\nPractical knowledge of Azure data stack tools (Azure Data Lake, Data Factory, etc.).\n3 - 8 years of experience as a Data Engineer.\nStrong proficiency in Python and SQL.\nExpertise in backend development using NestJS.\nExperience with ETL processes and data pipeline optimization.\nFamiliarity with microservices, Kubernetes, Docker, and REST APIs.\nHands-on experience with database design, tuning, and maintenance.\nPractical knowledge of Azure data stack tools (Azure Data Lake, Data Factory, etc.).\nWhat Will Make You Stand Out:\nExpertise in Apache Kafka.\n1-2 years of hands-on, production-grade Snowflake experience (architecture design, query optimization, cost monitoring).\nStrong DevOps skills, including Kubernetes, Terraform, and cloud security principles.\nExperience developing and deploying machine learning and AI models.\nAPI development experience using FastAPI, Flask, or similar frameworks.\nExpertise in Apache Kafka.\n1-2 years of hands-on, production-grade Snowflake experience (architecture design, query optimization, cost monitoring).\nStrong DevOps skills, including Kubernetes, Terraform, and cloud security principles.\nExperience developing and deploying machine learning and AI models.\nAPI development experience using FastAPI, Flask, or similar frameworks.\nWhy Join Us?\nBe part of an innovative community working with the latest technologies.\nAccess to top-tier remote freelance opportunities.\nCompetitive compensation and flexible working arrangements.\nA collaborative and inclusive work environment.\nBe part of an innovative community working with the latest technologies.\nAccess to top-tier remote freelance opportunities.\nCompetitive compensation and flexible working arrangements.\nA collaborative and inclusive work environment."
    },
    "4141618245": {
        "title": "Data Engineer",
        "company": "Otto Group one.O",
        "location": "Valencia, Valencian Community, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nWhat will you do?\n\nBe part of the team responsible for the migration of Data and Processes for BAUR and UNITO from the legacy store data platform for BI and ERP to the Google Cloud. \nWe want to significantly increase the usability and impact of our data platform for real-time and batch processing within our customers - with your expertise!\nTogether with your colleagues, you would be responsible for the design and implementation of our new, high-performance and robust BI platform in the Google Cloud for big, fast & smart data. 100% Cloud only. The methodology for migrating all remaining processes to the Cloud is based on Lift & Shift basis.\nYou would also be responsible for working in the migration of data for the Legacy ERP into the new one. \nProviding stability into the provided solution is also part and implies participation in the maintenance of the data.\nWith a strong customer focus, we migrate the data for our customer BI analysis applications, through understanding of data and processes, with transparency along the data processing and meeting all legal and regulatory requirements. \n\nWhat's your story?\n\nYour passion: Large amounts of data, their high-performance processing and technical innovations.\nYou have excellent conceptual skills and a quick mind.\nYou are eager for knowledge in BI processes and data analysis, transforming data into knowledge to inspire customers and have a basic technical understanding.\nYou excel at designing models that reduce data redundancy and improve enterprise information management.\nVery good developer skills in SQL.\nConfident handling of relational databases (e.g. BigQuery) and concepts of data warehousing and data modelling (e.g. DataVault beneficial).\nExperience in DBT. It could optionally be beneficial experience in Python.\nBasic experience in the development of scalable cloud solutions based on Google or other cloud providers (e.g. Terraform, Google Composer, etc.).\nExperience in the development of methodical approach: a clean object-oriented or functional design, tests and monitorability characterize your software. Clean build and deploy processes are important to you.\nYou are characterized by team spirit, initiative and self-organization and strong communication skills.\nGood English writen and spoken.\n\nBenefits\n\nPermanent contract \nFlexible working hours (you decide how to organize your day to day!) \nHybrid work mode (work from home up to 80% of the time, having the rest to meet you colleagues at the office!) \nYou will be part of a fast growing company, being part of a great team \nCompetitive salary\nFlexible retribution\nMedical insurance \nAgile working methods\nTraining and Development: we have a clear focus on technical innovation, but we don't forget the personal growth!\n\n\u00bfQui\u00e9nes somos?\n\nMore than just IT\n\nOSP (Otto Group Solution Provider) is an IT service provider for retail and logistics with headquarters in Dresden. We live our passion for IT in an appreciative, trusting work environment. What drives us is the idea of working together as a team to achieve great things for our customers.\n\nWe rely on the personal responsibility and willingness to learn of our employees, on modern technologies and high quality in software development. Agile working, transparent decisions, a lively feedback culture and collegial cooperation make us successful.\n\nWe are active for customers inside and outside the Otto Group. With over 450 employees at several German and international locations, we have been developing flexible software and BI solutions since 1991 and are thus shaping the shopping worlds of tomorrow. We are part of the Otto Group and share a value system that focuses on responsible and sustainable action.\nWhat will you do?\nBe part of the team responsible for the migration of Data and Processes for BAUR and UNITO from the legacy store data platform for BI and ERP to the Google Cloud. \nWe want to significantly increase the usability and impact of our data platform for real-time and batch processing within our customers - with your expertise!\nTogether with your colleagues, you would be responsible for the design and implementation of our new, high-performance and robust BI platform in the Google Cloud for big, fast & smart data. 100% Cloud only. The methodology for migrating all remaining processes to the Cloud is based on Lift & Shift basis.\nYou would also be responsible for working in the migration of data for the Legacy ERP into the new one. \nProviding stability into the provided solution is also part and implies participation in the maintenance of the data.\nWith a strong customer focus, we migrate the data for our customer BI analysis applications, through understanding of data and processes, with transparency along the data processing and meeting all legal and regulatory requirements.\nBe part of the team responsible for the migration of Data and Processes for BAUR and UNITO from the legacy store data platform for BI and ERP to the Google Cloud.\nWe want to significantly increase the usability and impact of our data platform for real-time and batch processing within our customers - with your expertise!\nTogether with your colleagues, you would be responsible for the design and implementation of our new, high-performance and robust BI platform in the Google Cloud for big, fast & smart data. 100% Cloud only. The methodology for migrating all remaining processes to the Cloud is based on Lift & Shift basis.\nYou would also be responsible for working in the migration of data for the Legacy ERP into the new one.\nProviding stability into the provided solution is also part and implies participation in the maintenance of the data.\nWith a strong customer focus, we migrate the data for our customer BI analysis applications, through understanding of data and processes, with transparency along the data processing and meeting all legal and regulatory requirements.\nWhat's your story?\nYour passion: Large amounts of data, their high-performance processing and technical innovations.\nYou have excellent conceptual skills and a quick mind.\nYou are eager for knowledge in BI processes and data analysis, transforming data into knowledge to inspire customers and have a basic technical understanding.\nYou excel at designing models that reduce data redundancy and improve enterprise information management.\nVery good developer skills in SQL.\nConfident handling of relational databases (e.g. BigQuery) and concepts of data warehousing and data modelling (e.g. DataVault beneficial).\nExperience in DBT. It could optionally be beneficial experience in Python.\nBasic experience in the development of scalable cloud solutions based on Google or other cloud providers (e.g. Terraform, Google Composer, etc.).\nExperience in the development of methodical approach: a clean object-oriented or functional design, tests and monitorability characterize your software. Clean build and deploy processes are important to you.\nYou are characterized by team spirit, initiative and self-organization and strong communication skills.\nGood English writen and spoken.\nYour passion: Large amounts of data, their high-performance processing and technical innovations.\nYou have excellent conceptual skills and a quick mind.\nYou are eager for knowledge in BI processes and data analysis, transforming data into knowledge to inspire customers and have a basic technical understanding.\nYou excel at designing models that reduce data redundancy and improve enterprise information management.\nVery good developer skills in SQL.\nConfident handling of relational databases (e.g. BigQuery) and concepts of data warehousing and data modelling (e.g. DataVault beneficial).\nExperience in DBT. It could optionally be beneficial experience in Python.\nBasic experience in the development of scalable cloud solutions based on Google or other cloud providers (e.g. Terraform, Google Composer, etc.).\nExperience in the development of methodical approach: a clean object-oriented or functional design, tests and monitorability characterize your software. Clean build and deploy processes are important to you.\nYou are characterized by team spirit, initiative and self-organization and strong communication skills.\nGood English writen and spoken.\nBenefits\nPermanent contract \nFlexible working hours (you decide how to organize your day to day!) \nHybrid work mode (work from home up to 80% of the time, having the rest to meet you colleagues at the office!) \nYou will be part of a fast growing company, being part of a great team \nCompetitive salary\nFlexible retribution\nMedical insurance \nAgile working methods\nTraining and Development: we have a clear focus on technical innovation, but we don't forget the personal growth!\nPermanent contract\nFlexible working hours (you decide how to organize your day to day!)\nHybrid work mode (work from home up to 80% of the time, having the rest to meet you colleagues at the office!)\nYou will be part of a fast growing company, being part of a great team\nCompetitive salary\nFlexible retribution\nMedical insurance\nAgile working methods\nTraining and Development: we have a clear focus on technical innovation, but we don't forget the personal growth!\n\u00bfQui\u00e9nes somos?\nMore than just IT"
    },
    "4112952765": {
        "title": "Data Engineer ",
        "company": "Deel",
        "location": "EMEA",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nWho We Are Is What We Do.\n\nDeel and our family of growing companies are made up of global teams dedicated to helping businesses hire anyone, anywhere, easily.\n\nThe team comprises over three thousand self-driven individuals spanning over 100 countries, and our unified yet diverse culture keeps us continually learning and innovating the platform and products for customers.\n\nCompanies should be able to hire the best talent anywhere in the world, so we are building the best platform to make that a reality. Our market-leading technology, expertise, and global team are crucial to the platform\u2019s success. We deliver the best products and features in our space, enabling millions of jobs worldwide and connecting the global workforce with the best companies and opportunities.\n\nWhy should you be part of our success story?\n\nA 30-mile hiring radius should no longer dictate how companies hire because exceptional talent lives everywhere. Deel sees a world without hiring borders and endless talent that pairs perfect candidates with great companies.\n\nWe offer global teams all the tools they need to hire, onboard, manage, pay, and scale at full speed. We aim to foster a diverse global economy by building a generational platform that seamlessly connects companies with talent worldwide.\n\nAfter our successful Series D in 2021, we raised another $50M in 2023, doubling our valuation to $12B. There\u2019s never been a more exciting time to join Deel \u2014 the international payroll and compliance market leader.\n\nThe Team\n\nThe Data Platform team at Deel is dedicated to enhancing data quality, optimizing pipeline performance, building robust platform tools, and managing costs across the entire data stack\u2014from ingestion to outbound integrations and everything in between. As a Data Engineer on this team, you\u2019ll play a critical role in shaping the future of Deel\u2019s data infrastructure, ensuring it scales effectively with 30+ Analytics Engineers and 100+ data professionals embedded across the organization.\n\nOur team collaborates cross-functionally with analysts, analytics engineers, data scientists, software engineers, and leadership to achieve these goals.\n\nYour Role as a Data Engineer\n\nAs a Data Engineer on the Data Platform team, you will:\n\nDesign, implement, and manage scalable and efficient data pipelines using tools like Snowflake, Airflow, dbt, and Fivetran. Experience with similar technologies is also valued as we continuously evolve our stack.\nCollaborate with cross-functional teams to understand data requirements and deliver solutions that align with business needs.\nImplement and enforce robust data governance policies to maintain data integrity, quality, and security.\nDevelop and maintain ELT processes to extract, load, and transform data from diverse sources into the data warehouse in both batch and streaming fashion.\nOptimize SQL queries to enhance data analysis and reporting.\nPartner with data analysts, data scientists, analytics engineers, and stakeholders to understand data workflows and provide technical support.\nDiagnose and resolve data-related issues promptly.\nBuild a cutting-edge development environment and CI/CD system to ensure scalability and efficiency.\nIngest data from various external systems via APIs.\nStay informed on industry trends and emerging technologies to drive continuous improvement in data engineering practices.\n\n\nQualifications\n\nExperience: Minimum 3 years as a Data Engineer or in a similar role.\nTechnical Skills: Strong proficiency in Python and SQL; experience with Golang or JVM-based languages is a plus.\nData Warehousing: Hands-on experience with cloud-based data warehouses.\nData Modeling: Proficiency in designing efficient database schemas.\nWorkflow Orchestration: Familiarity with tools like Apache Airflow.\nData Streaming: Experience with data streaming and Change Data Capture (CDC).\nInfrastructure: Proficiency in Terraform and GitHub Actions.\nCompliance: Experience in setting up PII anonymization and RBAC.\nCollaboration: Strong ability to work with cross-functional teams and communicate technical concepts to non-technical stakeholders.\nProblem-Solving: Excellent analytical skills with a keen attention to detail.\n\n\nPreferred Skills\n\nKubernetes: Experience with Kubernetes is a plus.\nData Warehouse Administration: Prior experience managing data warehouses is advantageous.\nRegulatory Compliance: Familiarity with GDPR or similar data privacy regulations is beneficial.\n\n\nJoin us in building a world-class data platform that drives actionable insights and scales with Deel\u2019s rapidly growing business.\n\nTotal Rewards\n\nOur workforce deserves fair and competitive pay that meets them where they are. With scalable benefits, rewards, and perks, our total rewards programs reflect our commitment to inclusivity and access for all.\n\nSome things you\u2019ll enjoy\n\nProvided computer equipment tailored to your role\nStock grant opportunities dependent on your role, employment status and location\nAdditional perks and benefits based on your employment status and country\nThe flexibility of remote work, including WeWork access where available\n\n\nDISCLOSURE \n\nWe use Covey as part of our hiring and/or promotional processes. As part of the evaluation process, we provide Covey with job requirements and candidate-submitted applications.Certain features of the platform may qualify it as an Automated Employment Decision Tool (AEDT) under applicable regulations. For positions in New York City, our use of Covey complies with NYC Local Law 144.\n\nWe began using Covey Scout for Inbound on the 19th of February, 2025.\n\nFor more information about our data protection practices, please visit our Privacy Policy.\n\nYou can review the independent bias audit report covering our use of Covey here: https://getcovey.com/nyc-local-law-144.\n\nAt Deel, we\u2019re an equal-opportunity employer that values diversity and positively encourage applications from suitably qualified and eligible candidates regardless of race, religion, sex, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, pregnancy or maternity or other applicable legally protected characteristics.\n\nUnless otherwise agreed, we will communicate with job applicants using Deel-specific emails, which include @deel.com and other acquired company emails like @payspace.com and @paygroup.com. You can view the most up-to-date job listings at Deel by visiting our careers page.\n\nDeel is an equal-opportunity employer and is committed to cultivating a diverse and inclusive workplace that reflects different abilities, backgrounds, beliefs, experiences, identities and perspectives.\n\nDeel will provide accommodation on request throughout the recruitment, selection and assessment process for applicants with disabilities. If you require accommodation, please inform our Talent Acquisition Team at recruiting@deel.com of the nature of the accommodation that you may require, to ensure your equal participation.\nWho We Are Is What We Do.\nWhy should you be part of our success story?\nThe Team\nYour Role as a Data Engineer\nDesign, implement, and manage scalable and efficient data pipelines using tools like Snowflake, Airflow, dbt, and Fivetran. Experience with similar technologies is also valued as we continuously evolve our stack.\nCollaborate with cross-functional teams to understand data requirements and deliver solutions that align with business needs.\nImplement and enforce robust data governance policies to maintain data integrity, quality, and security.\nDevelop and maintain ELT processes to extract, load, and transform data from diverse sources into the data warehouse in both batch and streaming fashion.\nOptimize SQL queries to enhance data analysis and reporting.\nPartner with data analysts, data scientists, analytics engineers, and stakeholders to understand data workflows and provide technical support.\nDiagnose and resolve data-related issues promptly.\nBuild a cutting-edge development environment and CI/CD system to ensure scalability and efficiency.\nIngest data from various external systems via APIs.\nStay informed on industry trends and emerging technologies to drive continuous improvement in data engineering practices.\nDesign, implement, and manage scalable and efficient data pipelines using tools like Snowflake, Airflow, dbt, and Fivetran. Experience with similar technologies is also valued as we continuously evolve our stack.\nCollaborate with cross-functional teams to understand data requirements and deliver solutions that align with business needs.\nImplement and enforce robust data governance policies to maintain data integrity, quality, and security.\nDevelop and maintain ELT processes to extract, load, and transform data from diverse sources into the data warehouse in both batch and streaming fashion.\nOptimize SQL queries to enhance data analysis and reporting.\nPartner with data analysts, data scientists, analytics engineers, and stakeholders to understand data workflows and provide technical support.\nDiagnose and resolve data-related issues promptly.\nBuild a cutting-edge development environment and CI/CD system to ensure scalability and efficiency.\nIngest data from various external systems via APIs.\nStay informed on industry trends and emerging technologies to drive continuous improvement in data engineering practices.\nQualifications\nExperience: Minimum 3 years as a Data Engineer or in a similar role.\nTechnical Skills: Strong proficiency in Python and SQL; experience with Golang or JVM-based languages is a plus.\nData Warehousing: Hands-on experience with cloud-based data warehouses.\nData Modeling: Proficiency in designing efficient database schemas.\nWorkflow Orchestration: Familiarity with tools like Apache Airflow.\nData Streaming: Experience with data streaming and Change Data Capture (CDC).\nInfrastructure: Proficiency in Terraform and GitHub Actions.\nCompliance: Experience in setting up PII anonymization and RBAC.\nCollaboration: Strong ability to work with cross-functional teams and communicate technical concepts to non-technical stakeholders.\nProblem-Solving: Excellent analytical skills with a keen attention to detail.\nExperience: Minimum 3 years as a Data Engineer or in a similar role.\nTechnical Skills: Strong proficiency in Python and SQL; experience with Golang or JVM-based languages is a plus.\nData Warehousing: Hands-on experience with cloud-based data warehouses.\nData Modeling: Proficiency in designing efficient database schemas.\nWorkflow Orchestration: Familiarity with tools like Apache Airflow.\nData Streaming: Experience with data streaming and Change Data Capture (CDC).\nInfrastructure: Proficiency in Terraform and GitHub Actions.\nCompliance: Experience in setting up PII anonymization and RBAC.\nCollaboration: Strong ability to work with cross-functional teams and communicate technical concepts to non-technical stakeholders.\nProblem-Solving: Excellent analytical skills with a keen attention to detail.\nPreferred Skills\nKubernetes: Experience with Kubernetes is a plus.\nData Warehouse Administration: Prior experience managing data warehouses is advantageous.\nRegulatory Compliance: Familiarity with GDPR or similar data privacy regulations is beneficial.\nKubernetes: Experience with Kubernetes is a plus.\nData Warehouse Administration: Prior experience managing data warehouses is advantageous.\nRegulatory Compliance: Familiarity with GDPR or similar data privacy regulations is beneficial.\nTotal Rewards\nSome things you\u2019ll enjoy\nProvided computer equipment tailored to your role\nStock grant opportunities dependent on your role, employment status and location\nAdditional perks and benefits based on your employment status and country\nThe flexibility of remote work, including WeWork access where available\nProvided computer equipment tailored to your role\nStock grant opportunities dependent on your role, employment status and location\nAdditional perks and benefits based on your employment status and country\nThe flexibility of remote work, including WeWork access where available\nDISCLOSURE\nUnless otherwise agreed, we will communicate with job applicants using Deel-specific emails, which include @\ndeel.com\nand other acquired company emails like @\npayspace.com\nand @\npaygroup.com\n. You can view the most up-to-date job listings at Deel by visiting\nour careers page\n.\nDeel is an equal-opportunity employer and is committed to cultivating a diverse and inclusive workplace that reflects different abilities, backgrounds, beliefs, experiences, identities and perspectives.\n\nDeel will provide accommodation on request throughout the recruitment, selection and assessment process for applicants with disabilities. If you require accommodation, please inform our Talent Acquisition Team at recruiting@deel.com of the nature of the accommodation that you may require, to ensure your equal participation."
    },
    "3508040830": {
        "title": "Big Data Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\n\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda? \n\n\ud83d\udd04 Desarrollar\u00e1s soluciones de datos end-to-end, desde el entendimiento del problema hasta la puesta en producci\u00f3n.\n\n\ud83d\udca1 Aportar\u00e1s tu visi\u00f3n t\u00e9cnica en el dise\u00f1o de arquitecturas y procesos, participando en debates y decisiones clave.\n\n\ud83d\udccc Definir\u00e1s e implementar\u00e1s estrategias de modelado y transformaci\u00f3n de datos seg\u00fan las necesidades del proyecto.\n\n\ud83d\udee0\ufe0f Colaborar\u00e1s en la toma de requerimientos para arquitecturas y pipelines de datos.\n\n\ud83e\uddea Probar\u00e1s nuevas tecnolog\u00edas y servicios cloud, aportando innovaci\u00f3n a nuestros proyectos.\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n? \n\n\ud83d\udcda Formaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o cualquier titulaci\u00f3n t\u00e9cnica relacionada con la ingenier\u00eda del software.\n\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Engineer, desarrollando soluciones de ingesta y transformaci\u00f3n de datos en entornos on-premise o cloud.\n\n\ud83c\udfd7\ufe0f Experiencia en arquitectura de datos, trabajando con estructuras, particionado, modelado y ciclo de vida en Data Lake.\n\n\ud83c\udf0d Ingl\u00e9s para trabajar en un entorno global.\n\n\nValoramos tambi\u00e9n: \n\n\u2601\ufe0f Plataforma: Experiencia en plataformas modernas como Databricks, Dataproc o EMR; o en plataformas tradicionales como Cloudera o Stratio.\n\n\u2699\ufe0f Procesos dirigidos por metadatos.\n\n\ud83d\udd04 Integraci\u00f3n de datos: Frameworks de procesamiento Spark o Beam y lenguajes Python, Scala o Java.\n\n\ud83d\udee0\ufe0f Orquestaci\u00f3n: Experiencia con Apache Airflow o Databricks Workflows.\n\n\ud83d\udcdd Gesti\u00f3n del c\u00f3digo: Buenas pr\u00e1cticas y metodolog\u00edas.\n\n\ud83d\udcca Familiaridad con conceptos como DataOps, Data Observability y Data Mesh.\n\n\ud83d\udcbc Experiencia en consultor\u00eda dentro del mundo Data & Analytics.\n\n\ud83c\udf93 Formaci\u00f3n extra como un M\u00e1ster en Big Data & Analytics u otras certificaciones que potencien tus habilidades.\n\n\ud83d\udee0\ufe0f Conocimiento en tecnolog\u00edas como SQL, Snowflake, dbt, Azure Data Factory o Microsoft Fabric.\n\n\ud83d\udce6 Experiencia con contenedores y orquestaci\u00f3n.\n\n\u2601\ufe0f Cloud & Automatizaci\u00f3n: Infraestructura, pipelines de datos, operaci\u00f3n y monitorizaci\u00f3n. \n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\u2705 Acceso a formaciones y certificaciones\n \nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\ud83d\udd04 Desarrollar\u00e1s soluciones de datos end-to-end, desde el entendimiento del problema hasta la puesta en producci\u00f3n.\nDesarrollar\u00e1s soluciones de datos end-to-end\n\ud83d\udca1 Aportar\u00e1s tu visi\u00f3n t\u00e9cnica en el dise\u00f1o de arquitecturas y procesos, participando en debates y decisiones clave.\nAportar\u00e1s tu visi\u00f3n t\u00e9cnica\n\ud83d\udccc Definir\u00e1s e implementar\u00e1s estrategias de modelado y transformaci\u00f3n de datos seg\u00fan las necesidades del proyecto.\nDefinir\u00e1s e implementar\u00e1s estrategias\n\ud83d\udee0\ufe0f Colaborar\u00e1s en la toma de requerimientos para arquitecturas y pipelines de datos.\nColaborar\u00e1s en la toma de requerimientos\n\ud83e\uddea Probar\u00e1s nuevas tecnolog\u00edas y servicios cloud, aportando innovaci\u00f3n a nuestros proyectos.\nProbar\u00e1s nuevas tecnolog\u00edas y servicios cloud\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n?\n\ud83d\udcda Formaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o cualquier titulaci\u00f3n t\u00e9cnica relacionada con la ingenier\u00eda del software.\nFormaci\u00f3n\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Engineer, desarrollando soluciones de ingesta y transformaci\u00f3n de datos en entornos on-premise o cloud.\n+4 a\u00f1os de experiencia\nData Engineer\ningesta y transformaci\u00f3n de datos\non-premise o cloud\n\ud83c\udfd7\ufe0f Experiencia en arquitectura de datos, trabajando con estructuras, particionado, modelado y ciclo de vida en Data Lake.\nExperiencia en arquitectura de datos\nData Lake\n\ud83c\udf0d Ingl\u00e9s para trabajar en un entorno global.\nIngl\u00e9s\nValoramos tambi\u00e9n:\n\u2601\ufe0f Plataforma: Experiencia en plataformas modernas como Databricks, Dataproc o EMR; o en plataformas tradicionales como Cloudera o Stratio.\nPlataforma\nDatabricks, Dataproc o EMR\nCloudera o Stratio\n\u2699\ufe0f Procesos dirigidos por metadatos.\nProcesos dirigidos por metadatos\n\ud83d\udd04 Integraci\u00f3n de datos: Frameworks de procesamiento Spark o Beam y lenguajes Python, Scala o Java.\nIntegraci\u00f3n de datos\nSpark o Beam\nPython, Scala o Java.\n\ud83d\udee0\ufe0f Orquestaci\u00f3n: Experiencia con Apache Airflow o Databricks Workflows.\nOrquestaci\u00f3n\nApache Airflow o Databricks Workflows\n\ud83d\udcdd Gesti\u00f3n del c\u00f3digo: Buenas pr\u00e1cticas y metodolog\u00edas.\nGesti\u00f3n del c\u00f3digo\n\ud83d\udcca Familiaridad con conceptos como DataOps, Data Observability y Data Mesh.\nDataOps, Data Observability y Data Mesh\n\ud83d\udcbc Experiencia en consultor\u00eda dentro del mundo Data & Analytics.\nExperiencia en consultor\u00eda\nData & Analytics\n\ud83c\udf93 Formaci\u00f3n extra como un M\u00e1ster en Big Data & Analytics u otras certificaciones que potencien tus habilidades.\nM\u00e1ster en Big Data & Analytics\n\ud83d\udee0\ufe0f Conocimiento en tecnolog\u00edas como SQL, Snowflake, dbt, Azure Data Factory o Microsoft Fabric.\nSQL, Snowflake, dbt, Azure Data Factory o Microsoft Fabric\n\ud83d\udce6 Experiencia con contenedores y orquestaci\u00f3n.\ncontenedores y orquestaci\u00f3n\n\u2601\ufe0f Cloud & Automatizaci\u00f3n: Infraestructura, pipelines de datos, operaci\u00f3n y monitorizaci\u00f3n.\nCloud & Automatizaci\u00f3n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\nSalario\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\n\u2705 Acceso a formaciones y certificaciones\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group"
    },
    "4165432720": {
        "title": "Data Engineer ",
        "company": "Claire Joster",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEn Claire Joster somos especialistas en la b\u00fasqueda y selecci\u00f3n de Perfiles cualificados de Mando Intermedio y Management en el \u00e1mbito tecnol\u00f3gico para las principales \u00e1reas funcionales y t\u00e9cnicas, con un valor a\u00f1adido fundamental: la captaci\u00f3n de talento, basada en valores y ajuste cultural de nuestros clientes y candidatos.\n\nActualmente estamos buscando un perfil de Data Engineer para importante proyecto tecnol\u00f3gico, La persona seleccionada trabajar\u00e1 en estrecha colaboraci\u00f3n con equipos de datos, ingenier\u00eda y DevOps para garantizar la escalabilidad, confiabilidad y automatizaci\u00f3n de los procesos:\n\n Funciones: \n\n- Dise\u00f1o y despliegue de monitorizaci\u00f3n: Definir e implementar estrategias para la observabilidad y el monitoreo de la plataforma de datos, asegurando la disponibilidad y el rendimiento. \n- Gesti\u00f3n de infraestructura como c\u00f3digo (IaC): Implementar y mantener infraestructuras utilizando herramientas como Terraform y Liquibase, garantizando buenas pr\u00e1cticas de automatizaci\u00f3n y versionado. \n- Desarrollo de flujos CI/CD: Dise\u00f1ar, implementar y mantener pipelines de CI/CD con Jenkins y Git, asegurando la integraci\u00f3n y despliegue eficiente de componentes de la plataforma de datos. \n- Gesti\u00f3n y optimizaci\u00f3n de plataformas de datos: Administrar y optimizar bases de datos en Snowflake, asegurando rendimiento, seguridad y eficiencia en la gesti\u00f3n de datos. \n- Cloud & Data Platform Management: Implementar y gestionar servicios en AWS y Aura, optimizando la infraestructura y asegurando la escalabilidad del ecosistema de datos. \n- Colaboraci\u00f3n y documentaci\u00f3n: Trabajar junto con equipos de datos, ingenier\u00eda y operaciones para documentar procesos y mejorar la eficiencia del flujo de trabajo. \nETL \n\nConsideramos imprescindible:\n\n- Experiencia de al menos 3 a\u00f1os en monitorizaci\u00f3n y observabilidad de plataformas de datos. - Experiencia demostrable con herramientas de infraestructura como c\u00f3digo (IaC) como Terraform y Liquibase. \n- Conocimiento de flujos CI/CD, pipelines y automatizaci\u00f3n con Jenkins y Git. - Familiaridad con AWS y plataformas de gesti\u00f3n de datos como Aura. \n- Conocimientos en scripting (Python, Bash) para la automatizaci\u00f3n de tareas. \n- Capacidad para trabajar en entornos \u00e1giles y colaborar con m\u00faltiples equipos\nEn Claire Joster somos especialistas en la b\u00fasqueda y selecci\u00f3n de Perfiles cualificados de Mando Intermedio y Management en el \u00e1mbito tecnol\u00f3gico para las principales \u00e1reas funcionales y t\u00e9cnicas, con un valor a\u00f1adido fundamental: la captaci\u00f3n de talento, basada en valores y ajuste cultural de nuestros clientes y candidatos.\nActualmente estamos buscando un perfil de Data Engineer para importante proyecto tecnol\u00f3gico, La persona seleccionada trabajar\u00e1 en estrecha colaboraci\u00f3n con equipos de datos, ingenier\u00eda y DevOps para garantizar la escalabilidad, confiabilidad y automatizaci\u00f3n de los procesos:\nData Engineer\nFunciones:\n- Dise\u00f1o y despliegue de monitorizaci\u00f3n: Definir e implementar estrategias para la observabilidad y el monitoreo de la plataforma de datos, asegurando la disponibilidad y el rendimiento. \n- Gesti\u00f3n de infraestructura como c\u00f3digo (IaC): Implementar y mantener infraestructuras utilizando herramientas como Terraform y Liquibase, garantizando buenas pr\u00e1cticas de automatizaci\u00f3n y versionado. \n- Desarrollo de flujos CI/CD: Dise\u00f1ar, implementar y mantener pipelines de CI/CD con Jenkins y Git, asegurando la integraci\u00f3n y despliegue eficiente de componentes de la plataforma de datos. \n- Gesti\u00f3n y optimizaci\u00f3n de plataformas de datos: Administrar y optimizar bases de datos en Snowflake, asegurando rendimiento, seguridad y eficiencia en la gesti\u00f3n de datos. \n- Cloud & Data Platform Management: Implementar y gestionar servicios en AWS y Aura, optimizando la infraestructura y asegurando la escalabilidad del ecosistema de datos. \n- Colaboraci\u00f3n y documentaci\u00f3n: Trabajar junto con equipos de datos, ingenier\u00eda y operaciones para documentar procesos y mejorar la eficiencia del flujo de trabajo. \nETL\n- Dise\u00f1o y despliegue de monitorizaci\u00f3n: Definir e implementar estrategias para la observabilidad y el monitoreo de la plataforma de datos, asegurando la disponibilidad y el rendimiento.\n- Gesti\u00f3n de infraestructura como c\u00f3digo (IaC): Implementar y mantener infraestructuras utilizando herramientas como Terraform y Liquibase, garantizando buenas pr\u00e1cticas de automatizaci\u00f3n y versionado.\n- Desarrollo de flujos CI/CD: Dise\u00f1ar, implementar y mantener pipelines de CI/CD con Jenkins y Git, asegurando la integraci\u00f3n y despliegue eficiente de componentes de la plataforma de datos.\n- Gesti\u00f3n y optimizaci\u00f3n de plataformas de datos: Administrar y optimizar bases de datos en Snowflake, asegurando rendimiento, seguridad y eficiencia en la gesti\u00f3n de datos.\n- Cloud & Data Platform Management: Implementar y gestionar servicios en AWS y Aura, optimizando la infraestructura y asegurando la escalabilidad del ecosistema de datos.\n- Colaboraci\u00f3n y documentaci\u00f3n: Trabajar junto con equipos de datos, ingenier\u00eda y operaciones para documentar procesos y mejorar la eficiencia del flujo de trabajo.\nETL\nConsideramos imprescindible:\n- Experiencia de al menos 3 a\u00f1os en monitorizaci\u00f3n y observabilidad de plataformas de datos. - Experiencia demostrable con herramientas de infraestructura como c\u00f3digo (IaC) como Terraform y Liquibase. \n- Conocimiento de flujos CI/CD, pipelines y automatizaci\u00f3n con Jenkins y Git. - Familiaridad con AWS y plataformas de gesti\u00f3n de datos como Aura. \n- Conocimientos en scripting (Python, Bash) para la automatizaci\u00f3n de tareas. \n- Capacidad para trabajar en entornos \u00e1giles y colaborar con m\u00faltiples equipos\n- Experiencia de al menos 3 a\u00f1os en monitorizaci\u00f3n y observabilidad de plataformas de datos. - Experiencia demostrable con herramientas de infraestructura como c\u00f3digo (IaC) como Terraform y Liquibase.\n- Conocimiento de flujos CI/CD, pipelines y automatizaci\u00f3n con Jenkins y Git. - Familiaridad con AWS y plataformas de gesti\u00f3n de datos como Aura.\n- Conocimientos en scripting (Python, Bash) para la automatizaci\u00f3n de tareas.\n- Capacidad para trabajar en entornos \u00e1giles y colaborar con m\u00faltiples equipos"
    },
    "4167559229": {
        "title": "Data Engineer AWS",
        "company": "Logicalis Spain",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEn Logicalis Spain estamos buscando un perfil de Ingeniero de Datos AWS Mid - Senior para integrarse en nuestra BU de Data & Analytics. El equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\n\nREQUISITOS T\u00c9CNICOS\nExperiencia comprobable en ingenier\u00eda de datos, con un enfoque en AWS.\nDominio de herramientas de ETL como AWS Glue, o herramientas similares.\nConocimiento en arquitecturas de datos en la nube y mejores pr\u00e1cticas.\nCompetencia en scripting y automatizaci\u00f3n (Python, SQL, etc.).\nExperiencia en bases de datos SQL, valorable NoSQL.\nHabilidad para trabajar con grandes vol\u00famenes de datos y realizar optimizaci\u00f3n de rendimiento.\nCapacidad para trabajar de forma aut\u00f3noma y en equipo, con excelentes habilidades de comunicaci\u00f3n.\nValorable contar con certificaciones en AWS.\nValorable experiencia en CI/CD para pipelines de datos.\n\nFUNCIONES\nDise\u00f1ar, desarrollar y mantener pipelines de datos robustos utilizando herramientas y servicios en AWS.\nImplementar y gestionar soluciones ETL para procesar grandes vol\u00famenes de datos de manera eficiente.\nColaborar con equipos multidisciplinarios para entender las necesidades de negocio y traducirlas en soluciones t\u00e9cnicas.\nOptimizar el rendimiento y la eficiencia de los pipelines de datos.\nGestionar la seguridad y el cumplimiento de las pol\u00edticas de datos en la nube.\nProporcionar soporte y resoluci\u00f3n de problemas para sistemas de datos en producci\u00f3n.\n\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo 100% remoto, desde cualquier punto de Espa\u00f1a.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados.\n> Buen ambiente de trabajo y entorno muy colaborativo.\nEn Logicalis Spain estamos buscando un perfil de Ingeniero de Datos AWS Mid - Senior para integrarse en nuestra BU de Data & Analytics. El equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\nLogicalis Spain\nIngeniero de Datos AWS Mid - Senior\nData & Analytics\nREQUISITOS T\u00c9CNICOS\nExperiencia comprobable en ingenier\u00eda de datos, con un enfoque en AWS.\nDominio de herramientas de ETL como AWS Glue, o herramientas similares.\nConocimiento en arquitecturas de datos en la nube y mejores pr\u00e1cticas.\nCompetencia en scripting y automatizaci\u00f3n (Python, SQL, etc.).\nExperiencia en bases de datos SQL, valorable NoSQL.\nHabilidad para trabajar con grandes vol\u00famenes de datos y realizar optimizaci\u00f3n de rendimiento.\nCapacidad para trabajar de forma aut\u00f3noma y en equipo, con excelentes habilidades de comunicaci\u00f3n.\nValorable contar con certificaciones en AWS.\nValorable experiencia en CI/CD para pipelines de datos.\nExperiencia comprobable en ingenier\u00eda de datos, con un enfoque en AWS.\nDominio de herramientas de ETL como AWS Glue, o herramientas similares.\nConocimiento en arquitecturas de datos en la nube y mejores pr\u00e1cticas.\nCompetencia en scripting y automatizaci\u00f3n (Python, SQL, etc.).\nExperiencia en bases de datos SQL, valorable NoSQL.\nHabilidad para trabajar con grandes vol\u00famenes de datos y realizar optimizaci\u00f3n de rendimiento.\nCapacidad para trabajar de forma aut\u00f3noma y en equipo, con excelentes habilidades de comunicaci\u00f3n.\nValorable contar con certificaciones en AWS.\nValorable experiencia en CI/CD para pipelines de datos.\nFUNCIONES\nDise\u00f1ar, desarrollar y mantener pipelines de datos robustos utilizando herramientas y servicios en AWS.\nImplementar y gestionar soluciones ETL para procesar grandes vol\u00famenes de datos de manera eficiente.\nColaborar con equipos multidisciplinarios para entender las necesidades de negocio y traducirlas en soluciones t\u00e9cnicas.\nOptimizar el rendimiento y la eficiencia de los pipelines de datos.\nGestionar la seguridad y el cumplimiento de las pol\u00edticas de datos en la nube.\nProporcionar soporte y resoluci\u00f3n de problemas para sistemas de datos en producci\u00f3n.\nDise\u00f1ar, desarrollar y mantener pipelines de datos robustos utilizando herramientas y servicios en AWS.\nImplementar y gestionar soluciones ETL para procesar grandes vol\u00famenes de datos de manera eficiente.\nColaborar con equipos multidisciplinarios para entender las necesidades de negocio y traducirlas en soluciones t\u00e9cnicas.\nOptimizar el rendimiento y la eficiencia de los pipelines de datos.\nGestionar la seguridad y el cumplimiento de las pol\u00edticas de datos en la nube.\nProporcionar soporte y resoluci\u00f3n de problemas para sistemas de datos en producci\u00f3n.\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo 100% remoto, desde cualquier punto de Espa\u00f1a.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados.\n> Buen ambiente de trabajo y entorno muy colaborativo."
    },
    "4172291103": {
        "title": "Senior Data Engineer ",
        "company": "Tripledot Studios",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nDepartment: Engineering\nLocation: Barcelona, ES\n\nTripledot Studios is an award-winning, record breaking games studio with a team circa 400 people. Headquartered in London, we also have offices in Warsaw, Barcelona, Minsk, Jakarta, and Melbourne.\n\nWe specialise in Casual, Free to Play, Mobile games that delight millions of players every day.\n\nFastest Growing Business in Europe in 2022\nTech Growth Business of the Year 2021 and 2023\nPocketGamer Best Mobile Developer 2023\nThe Kings Award for International Enterprise 2024\n\nOur guiding principle as a team is that when people love what they do, what they do will be loved by others.\n\nTake a look at our games: iOS Store + Google Play\n\nAbout the Role\n\nWe are looking for a passionate Senior Data Engineer to join our best team!\n\nKey Responsibilities\n\nBuild and support ETL pipelines\nBuild and support Data Warehouse and Data Lake (Snowflake and S3)\nBuild and support Data Models\nBuild and support Data Infrastructure (Airflow, Terraform, K8s)\n\nRequired Skills, Knowledge and Expertise\n\n4+ years of experience in Data Engineering\n3+ years of experience with Python\nExperience with Airflow and OLAP Databases (Snowflake, Redshift. Greenplum, etc.) or Spark\nStrong working knowledge of SQL\nProficient understanding of distributed computing principles\nExcellent English skills, both written and verbal\n\nIt will be a plus:\n\nExperience in gaming domain or great desire to work in gaming :)\nExperience with AWS\nWorking experience with DBT\nExperience with 3rd-party APIs\n\nWorking at Tripledot\n\n25 days holiday: Enjoy 25 days of paid holiday in addition to bank holidays to relax and refresh throughout the year.\nHybrid Working: Work from home 2 days a week\n20 days fully remote working: Work from anywhere in the world, 20 days of the year.\nRegular company events and rewards: Join in regular events and rewards that celebrate cultural events, our achievements and our team spirit.\nPrivate Medical Cover: Have peace of mind with private medical cover, ensuring your health is in good hands.\nLife & Critical Illness Cover: Protect your future with our life and critical illness cover.\nFamily Forming Support: Receive vital support on your family forming/ fertility journey with our support program [subject to policy]\nEmployee Assistance Program: Access confidential support anytime through our Employee Assistance Program.\nSport Compensation: Stay fit and active with our sport compensation benefit.\nMeal and Transport Vouchers: Save on meals and transport with our convenient vouchers.\nEnglish & Spanish Classes: Enhance your English and Spanish skills with our provided language classes.\nContinuous Professional Development: Propel your career with continuous opportunities for professional development.\nDepartment: Engineering\nDepartment:\nLocation: Barcelona, ES\nLocation:\nTripledot Studios is an award-winning, record breaking games studio with a team circa 400 people. Headquartered in London, we also have offices in Warsaw, Barcelona, Minsk, Jakarta, and Melbourne.\nTripledot Studios\nWe specialise in Casual, Free to Play, Mobile games that delight millions of players every day.\nFastest Growing Business in Europe in 2022\nTech Growth Business of the Year 2021 and 2023\nPocketGamer Best Mobile Developer 2023\nThe Kings Award for International Enterprise 2024\nFastest Growing Business in Europe in 2022\nFastest Growing Business in Europe\nTech Growth Business of the Year 2021 and 2023\nTech Growth Business of the Year\nPocketGamer Best Mobile Developer 2023\nPocketGamer Best Mobile Developer\nThe Kings Award for International Enterprise 2024\nThe Kings Award for International Enterprise\nOur guiding principle as a team is that when people love what they do, what they do will be loved by others.\nTake a look at our games: iOS Store + Google Play\nTake a look at our games:\nAbout the Role\nWe are looking for a passionate Senior Data Engineer to join our best team!\nKey Responsibilities\nBuild and support ETL pipelines\nBuild and support Data Warehouse and Data Lake (Snowflake and S3)\nBuild and support Data Models\nBuild and support Data Infrastructure (Airflow, Terraform, K8s)\nBuild and support ETL pipelines\nBuild and support Data Warehouse and Data Lake (Snowflake and S3)\nBuild and support Data Models\nBuild and support Data Infrastructure (Airflow, Terraform, K8s)\nRequired Skills, Knowledge and Expertise\n4+ years of experience in Data Engineering\n3+ years of experience with Python\nExperience with Airflow and OLAP Databases (Snowflake, Redshift. Greenplum, etc.) or Spark\nStrong working knowledge of SQL\nProficient understanding of distributed computing principles\nExcellent English skills, both written and verbal\n4+ years of experience in Data Engineering\n3+ years of experience with Python\nExperience with Airflow and OLAP Databases (Snowflake, Redshift. Greenplum, etc.) or Spark\nStrong working knowledge of SQL\nProficient understanding of distributed computing principles\nExcellent English skills, both written and verbal\nIt will be a plus:\nExperience in gaming domain or great desire to work in gaming :)\nExperience with AWS\nWorking experience with DBT\nExperience with 3rd-party APIs\nExperience in gaming domain or great desire to work in gaming :)\nExperience with AWS\nWorking experience with DBT\nExperience with 3rd-party APIs\nWorking at Tripledot\n25 days holiday: Enjoy 25 days of paid holiday in addition to bank holidays to relax and refresh throughout the year.\nHybrid Working: Work from home 2 days a week\n20 days fully remote working: Work from anywhere in the world, 20 days of the year.\nRegular company events and rewards: Join in regular events and rewards that celebrate cultural events, our achievements and our team spirit.\nPrivate Medical Cover: Have peace of mind with private medical cover, ensuring your health is in good hands.\nLife & Critical Illness Cover: Protect your future with our life and critical illness cover.\nFamily Forming Support: Receive vital support on your family forming/ fertility journey with our support program [subject to policy]\nEmployee Assistance Program: Access confidential support anytime through our Employee Assistance Program.\nSport Compensation: Stay fit and active with our sport compensation benefit.\nMeal and Transport Vouchers: Save on meals and transport with our convenient vouchers.\nEnglish & Spanish Classes: Enhance your English and Spanish skills with our provided language classes.\nContinuous Professional Development: Propel your career with continuous opportunities for professional development.\n25 days holiday: Enjoy 25 days of paid holiday in addition to bank holidays to relax and refresh throughout the year.\n25 days holiday:\nHybrid Working: Work from home 2 days a week\nHybrid Working:\n20 days fully remote working: Work from anywhere in the world, 20 days of the year.\n20 days fully remote working:\nRegular company events and rewards: Join in regular events and rewards that celebrate cultural events, our achievements and our team spirit.\nRegular company events and rewards:\nPrivate Medical Cover: Have peace of mind with private medical cover, ensuring your health is in good hands.\nPrivate Medical Cover:\nLife & Critical Illness Cover: Protect your future with our life and critical illness cover.\nLife & Critical Illness Cover:\nFamily Forming Support: Receive vital support on your family forming/ fertility journey with our support program [subject to policy]\nFamily Forming Support:\nEmployee Assistance Program: Access confidential support anytime through our Employee Assistance Program.\nEmployee Assistance Program:\nSport Compensation: Stay fit and active with our sport compensation benefit.\nSport Compensation:\nMeal and Transport Vouchers: Save on meals and transport with our convenient vouchers.\nMeal and Transport Vouchers:\nEnglish & Spanish Classes: Enhance your English and Spanish skills with our provided language classes.\nEnglish & Spanish Classes:\nContinuous Professional Development: Propel your career with continuous opportunities for professional development.\nContinuous Professional Development:"
    },
    "4174838688": {
        "title": "Cloud Data Engineer \u2013 AWS & Big Data(H/M/X) ",
        "company": "Experis Espa\u00f1a",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEn Experis buscamos un Cloud Data Engineer con experiencia en entornos de datos en AWS y un s\u00f3lido conocimiento en tecnolog\u00edas Big Data.\nLa persona seleccionada participar\u00e1 en todas las fases del ciclo de vida del data lake, asegurando la correcta integraci\u00f3n de soluciones de negocio en la Cloud Data Platform (CDP).\n\nResponsabilidades:\nParticipar en todo el ciclo de vida del data lake desde la perspectiva de infraestructura.\nMantener y evolucionar la Cloud Data Platform en AWS.\nImplementar e integrar casos de negocio en la CDP, asegurando el cumplimiento de la arquitectura de referencia.\nMonitorear la plataforma, gestionar riesgos y escalar problemas.\nInvestigar y adoptar nuevas tecnolog\u00edas en el mercado.\n\nRequisitos Obligatorios:\nCertificaci\u00f3n AWS Associate (deseable nivel Professional).\nExperiencia en Big Data Technologies Ecosystem.\nConocimientos en Terraform.\nHabilidades Deseadas (Nice to Have):\nShell Scripting\nPython, Scala, Spark\n\n\u2714\ufe0f Experiencia de 3 a 5 a\u00f1os m\u00e1ximo en roles similares.\n\ud83d\udccd Ubicaci\u00f3n: 100% teletrabajo - Espa\u00f1a\n\nExperis, somos una compa\u00f1\u00eda especializada en servicios profesionales y gesti\u00f3n de proyectos IT asociados a nuestras 3 pr\u00e1cticas: Business Transformation, Cloud & Infrastructure y Enterprise Applications.\nEn la actualidad combinamos nuestras soluciones tecnol\u00f3gicas con las habilidades m\u00e1s demandadas del mercado.\nAdem\u00e1s, proporcionamos formaci\u00f3n especializada asociada a las l\u00edneas de servicio antes mencionadas.\nContamos con una plantilla de m\u00e1s de 1.800 profesionales especializados en IT en Espa\u00f1a y presencia internacional en 54 pa\u00edses.\nFormar parte de Experis significa encontrar el desarrollo profesional que necesitas para alcanzar tus objetivos, ofreci\u00e9ndote:\nProyectos y servicios con tecnolog\u00edas punteras.\nAcompa\u00f1amiento a trav\u00e9s de un Mentor para potenciar tus capacidades.\nDesarrollo profesional y plan de formaci\u00f3n a tu medida (cursos tecnol\u00f3gicos, idiomas, soft skills...).\nSalario competitivo acorde a tus capacidades + Retribuci\u00f3n flexible, con revisi\u00f3n en base a evaluaci\u00f3n de desempe\u00f1o.\nEstabilidad laboral y rotaci\u00f3n entre diversos proyectos/servicios para potenciar tu desarrollo.\nEquipos diversos (multiculturales, deslocalizados...).\n\nEncuentra tu pr\u00f3xima oportunidad con nosotros. Pasa al siguiente nivel con Experis!\nEn Experis buscamos un Cloud Data Engineer con experiencia en entornos de datos en AWS y un s\u00f3lido conocimiento en tecnolog\u00edas Big Data.\nCloud Data Engineer\nLa persona seleccionada participar\u00e1 en todas las fases del ciclo de vida del data lake, asegurando la correcta integraci\u00f3n de soluciones de negocio en la Cloud Data Platform (CDP).\nResponsabilidades:\nParticipar en todo el ciclo de vida del data lake desde la perspectiva de infraestructura.\nMantener y evolucionar la Cloud Data Platform en AWS.\nImplementar e integrar casos de negocio en la CDP, asegurando el cumplimiento de la arquitectura de referencia.\nMonitorear la plataforma, gestionar riesgos y escalar problemas.\nInvestigar y adoptar nuevas tecnolog\u00edas en el mercado.\nParticipar en todo el ciclo de vida del data lake desde la perspectiva de infraestructura.\nMantener y evolucionar la Cloud Data Platform en AWS.\nImplementar e integrar casos de negocio en la CDP, asegurando el cumplimiento de la arquitectura de referencia.\nMonitorear la plataforma, gestionar riesgos y escalar problemas.\nInvestigar y adoptar nuevas tecnolog\u00edas en el mercado.\nRequisitos Obligatorios:\nCertificaci\u00f3n AWS Associate (deseable nivel Professional).\nExperiencia en Big Data Technologies Ecosystem.\nConocimientos en Terraform.\nCertificaci\u00f3n AWS Associate (deseable nivel Professional).\nExperiencia en Big Data Technologies Ecosystem.\nBig Data Technologies Ecosystem\nConocimientos en Terraform.\nTerraform\nHabilidades Deseadas (Nice to Have):\nShell Scripting\nPython, Scala, Spark\nShell Scripting\nPython, Scala, Spark\n\u2714\ufe0f Experiencia de 3 a 5 a\u00f1os m\u00e1ximo en roles similares.\n3 a 5 a\u00f1os m\u00e1ximo\n\ud83d\udccd Ubicaci\u00f3n: 100% teletrabajo - Espa\u00f1a\nUbicaci\u00f3n:\nExperis, somos una compa\u00f1\u00eda especializada en servicios profesionales y gesti\u00f3n de proyectos IT asociados a nuestras 3 pr\u00e1cticas: Business Transformation, Cloud & Infrastructure y Enterprise Applications.\nEn la actualidad combinamos nuestras soluciones tecnol\u00f3gicas con las habilidades m\u00e1s demandadas del mercado.\nAdem\u00e1s, proporcionamos formaci\u00f3n especializada asociada a las l\u00edneas de servicio antes mencionadas.\nContamos con una plantilla de m\u00e1s de 1.800 profesionales especializados en IT en Espa\u00f1a y presencia internacional en 54 pa\u00edses.\nFormar parte de Experis significa encontrar el desarrollo profesional que necesitas para alcanzar tus objetivos, ofreci\u00e9ndote:\nProyectos y servicios con tecnolog\u00edas punteras.\nAcompa\u00f1amiento a trav\u00e9s de un Mentor para potenciar tus capacidades.\nDesarrollo profesional y plan de formaci\u00f3n a tu medida (cursos tecnol\u00f3gicos, idiomas, soft skills...).\nSalario competitivo acorde a tus capacidades + Retribuci\u00f3n flexible, con revisi\u00f3n en base a evaluaci\u00f3n de desempe\u00f1o.\nEstabilidad laboral y rotaci\u00f3n entre diversos proyectos/servicios para potenciar tu desarrollo.\nEquipos diversos (multiculturales, deslocalizados...).\nProyectos y servicios con tecnolog\u00edas punteras.\nAcompa\u00f1amiento a trav\u00e9s de un Mentor para potenciar tus capacidades.\nDesarrollo profesional y plan de formaci\u00f3n a tu medida (cursos tecnol\u00f3gicos, idiomas, soft skills...).\nSalario competitivo acorde a tus capacidades + Retribuci\u00f3n flexible, con revisi\u00f3n en base a evaluaci\u00f3n de desempe\u00f1o.\nEstabilidad laboral y rotaci\u00f3n entre diversos proyectos/servicios para potenciar tu desarrollo.\nEquipos diversos (multiculturales, deslocalizados...).\nEncuentra tu pr\u00f3xima oportunidad con nosotros. Pasa al siguiente nivel con Experis!"
    },
    "4171621460": {
        "title": "Senior Data Engineer ",
        "company": "PepsiCo",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nOverview\n\nAre you ready to drive PepsiCo\u2019s digital evolution and accelerate transformation across our global operations? \n\nPepsiCo operates in a fast-changing environment where big data and digital technologies are driving business transformation. Our Data Management and Operations team plays a critical role in building and maintaining high-quality data foundations that power business insights, advanced analytics, and innovation in areas like eCommerce, IoT, and mobile experiences. By ensuring seamless data collection, standardization, and accessibility across the company, we empower business leaders and data scientists with the insights they need to drive impactful decisions.\n\nAs a Data Engineer at PepsiCo, you will be a key technical expert responsible for developing and managing data products that directly impact critical business areas such as revenue management, supply chain, manufacturing, and logistics. You will design and build complex data pipelines, integrate diverse data sources into the PepsiCo Data Lake, and enable analytics, machine learning, and product development at scale. Working in a hybrid environment across cloud and on-premise systems, you\u2019ll collaborate closely with process owners and business users to shape the future of PepsiCo\u2019s data ecosystem.\n\nResponsibilities\n\nYour day to day with us:\n\n Active contributor to code development in projects and services.\n Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n Responsible for implementing best practices around systems integration, security, performance and data management.\n Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n Develop and optimize procedures to \u201cproductionalize\u201d data science models.\n Define and manage SLA\u2019s for data products and processes running in production.\n Support large-scale experimentation done by data scientists.\n Prototype new approaches and build solutions at scale.\n Create documentation for learnings and knowledge transfer.\n Create and audit reusable packages or libraries.\n\nQualifications\n\nWhat you will need to succeed:\n\n 4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n 2+ years in cloud data engineering experience in Azure.\n Fluent with Azure cloud services. Azure Certification is a plus.\n Experience with integration of multi cloud services with on-premises technologies.\n Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.\n Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.\n Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\n Experience with version control systems like Github and deployment & CI tools.\n Working knowledge of agile development, including DevOps and DataOps concepts.\n\nSkills, Abilities, Knowledge:\n\n Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n Proven track record of leading, mentoring data teams.\n Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.\n Ability to understand and translate business requirements into data and technical requirements.\n High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\n Foster a team culture of accountability, communication, and self-management.\n Proactively drives impact and engagement while bringing others along.\n Consistently attain/exceed individual and team goals\n Ability to lead others without direct authority in a matrixed environment.\n\nWhat makes us different? \n\nHybrid work model: combination of remote and collaborative office experience to enable innovation \nEntrepreneurial environment in leading international company \nProfessional growth possibilities & learning opportunities \nVariety of benefits to support your physical, emotional and financial wellbeing \nVolunteering opportunities to help external communities \nDiverse team with more than 30% of female representation & over 30 nationalities \nHave a stake in D&I strategy and bring your whole self to work \n\nAbout PepsiCo\n\nWe believe that culture should be at the cornerstone of everything we do at PepsiCo. We are agile, innovative and not afraid of failure. We want our team to come to work every day excited to explore new ways to bring enjoyment, refreshment and fun to the world.\n\nPepsiCo Positive (pep+) is the future of our organization - a strategic end-to-end transformation, with sustainability at the center of how we will create growth and value by operating within planetary boundaries and inspiring positive change for the planet and people.\n\nSo, if you\u2019re ready to be a part of a playground for those who think big, we\u2019d love to chat.\n\nWe encourage the diversity of applicants across gender, age, ethnicity, nationality, sexual orientation, social background, religion or belief and disability.\nOverview\nAre you ready to drive PepsiCo\u2019s digital evolution and accelerate transformation across our global operations?\nData Engineer at PepsiCo\nResponsibilities\nYour day to day with us:\nActive contributor to code development in projects and services.\n Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\n Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\n Responsible for implementing best practices around systems integration, security, performance and data management.\n Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\n Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\n Develop and optimize procedures to \u201cproductionalize\u201d data science models.\n Define and manage SLA\u2019s for data products and processes running in production.\n Support large-scale experimentation done by data scientists.\n Prototype new approaches and build solutions at scale.\n Create documentation for learnings and knowledge transfer.\n Create and audit reusable packages or libraries.\nActive contributor to code development in projects and services.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\nResponsible for implementing best practices around systems integration, security, performance and data management.\nEmpower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\nCollaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\nDevelop and optimize procedures to \u201cproductionalize\u201d data science models.\nDefine and manage SLA\u2019s for data products and processes running in production.\nSupport large-scale experimentation done by data scientists.\nPrototype new approaches and build solutions at scale.\nCreate documentation for learnings and knowledge transfer.\nCreate and audit reusable packages or libraries.\nQualifications\nWhat you will need to succeed:\n4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n 3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n 2+ years in cloud data engineering experience in Azure.\n Fluent with Azure cloud services. Azure Certification is a plus.\n Experience with integration of multi cloud services with on-premises technologies.\n Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.\n Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.\n Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\n Experience with version control systems like Github and deployment & CI tools.\n Working knowledge of agile development, including DevOps and DataOps concepts.\n4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like Github and deployment & CI tools.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\n Proven track record of leading, mentoring data teams.\n Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.\n Ability to understand and translate business requirements into data and technical requirements.\n High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\n Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\n Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\n Foster a team culture of accountability, communication, and self-management.\n Proactively drives impact and engagement while bringing others along.\n Consistently attain/exceed individual and team goals\n Ability to lead others without direct authority in a matrixed environment.\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals\nAbility to lead others without direct authority in a matrixed environment.\nWhat makes us different?\nHybrid work model: combination of remote and collaborative office experience to enable innovation \nEntrepreneurial environment in leading international company \nProfessional growth possibilities & learning opportunities \nVariety of benefits to support your physical, emotional and financial wellbeing \nVolunteering opportunities to help external communities \nDiverse team with more than 30% of female representation & over 30 nationalities \nHave a stake in D&I strategy and bring your whole self to work\nHybrid work model: combination of remote and collaborative office experience to enable innovation\nEntrepreneurial environment in leading international company\nProfessional growth possibilities & learning opportunities\nVariety of benefits to support your physical, emotional and financial wellbeing\nVolunteering opportunities to help external communities\nDiverse team with more than 30% of female representation & over 30 nationalities\nHave a stake in D&I strategy and bring your whole self to work\nAbout PepsiCo\nWe encourage the diversity of applicants across gender, age, ethnicity, nationality, sexual orientation, social background, religion or belief and disability."
    },
    "4173576730": {
        "title": "Streaming Data Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\n\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\n\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\n\ud83d\udd04 Desarrollar\u00e1s soluciones de datos end-to-end, desde el entendimiento del problema hasta la puesta en producci\u00f3n.\n\n\ud83d\udca1 Aportar\u00e1s tu visi\u00f3n t\u00e9cnica en el dise\u00f1o de arquitecturas y procesos, participando en debates y decisiones clave.\n\n\ud83d\udccc Definir\u00e1s e implementar\u00e1s estrategias de modelado y transformaci\u00f3n de datos seg\u00fan las necesidades del proyecto.\n\n\ud83d\udee0\ufe0f Colaborar\u00e1s en la toma de requerimientos para arquitecturas y pipelines de datos.\n\n\ud83e\uddea Probar\u00e1s nuevas tecnolog\u00edas y servicios cloud, aportando innovaci\u00f3n a nuestros proyectos.\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n? \n\n\ud83d\udcda Formaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o cualquier titulaci\u00f3n t\u00e9cnica relacionada con la ingenier\u00eda del software.\n\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Engineer, desarrollando soluciones de ingesta y transformaci\u00f3n de datos en entornos on-premise o cloud.\n\n\ud83c\udfd7\ufe0f Experiencia en arquitectura de datos, trabajando con estructuras, particionado, modelado y ciclo de vida en Kafka y Data Lake.\n\n\ud83c\udf0d Ingl\u00e9s para trabajar en un entorno global.\n\n\u2601\ufe0f Plataforma: Experiencia en Kafka / Confluent.\n\n\u2699\ufe0f Procesos dirigidos por metadatos.\n\n\ud83d\udd04 Integraci\u00f3n de datos: Frameworks de procesamiento en streaming Kafka Streams, Spark Streaming o Flink.\n\n\ud83d\udee0\ufe0f Orquestaci\u00f3n: Experiencia con orquestaci\u00f3n dirigida por eventos. Orquestaci\u00f3n de procesos s\u00edncrona con Apache Airflow.\n\n\ud83d\udcdd Gesti\u00f3n del c\u00f3digo: Buenas pr\u00e1cticas y metodolog\u00edas.\n\n\nValoramos tambi\u00e9n:\n\n\ud83d\udcca Familiaridad con conceptos como DataOps, Data Observability y Data Mesh.\n\ud83d\udcbc Experiencia en consultor\u00eda dentro del mundo Data & Analytics.\n\ud83c\udf93 Formaci\u00f3n extra como un M\u00e1ster en Big Data & Analytics u otras certificaciones que potencien tus habilidades.\n\ud83d\udee0\ufe0f Conocimiento en tecnolog\u00edas como Spark, SQL, Snowflake, dbt, Azure Data Factory o Microsoft Fabric.\n\ud83d\udce6 Experiencia con contenedores y orquestaci\u00f3n.\n\u2601\ufe0f Cloud & Automatizaci\u00f3n: Infraestructura, pipelines de datos, operaci\u00f3n y monitorizaci\u00f3n.\n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\u2705 Acceso a formaciones y certificaciones\n \nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\ud83d\udd04 Desarrollar\u00e1s soluciones de datos end-to-end, desde el entendimiento del problema hasta la puesta en producci\u00f3n.\nDesarrollar\u00e1s soluciones de datos end-to-end\n\ud83d\udca1 Aportar\u00e1s tu visi\u00f3n t\u00e9cnica en el dise\u00f1o de arquitecturas y procesos, participando en debates y decisiones clave.\nAportar\u00e1s tu visi\u00f3n t\u00e9cnica\n\ud83d\udccc Definir\u00e1s e implementar\u00e1s estrategias de modelado y transformaci\u00f3n de datos seg\u00fan las necesidades del proyecto.\nDefinir\u00e1s e implementar\u00e1s estrategias\n\ud83d\udee0\ufe0f Colaborar\u00e1s en la toma de requerimientos para arquitecturas y pipelines de datos.\nColaborar\u00e1s en la toma de requerimientos\n\ud83e\uddea Probar\u00e1s nuevas tecnolog\u00edas y servicios cloud, aportando innovaci\u00f3n a nuestros proyectos.\nProbar\u00e1s nuevas tecnolog\u00edas y servicios cloud\n\u00bfQu\u00e9 te har\u00e1 triunfar en esta posici\u00f3n?\n\ud83d\udcda Formaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o cualquier titulaci\u00f3n t\u00e9cnica relacionada con la ingenier\u00eda del software.\nFormaci\u00f3n\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Engineer, desarrollando soluciones de ingesta y transformaci\u00f3n de datos en entornos on-premise o cloud.\n+4 a\u00f1os de experiencia\nData Engineer\ningesta y transformaci\u00f3n de datos\non-premise o cloud\n\ud83c\udfd7\ufe0f Experiencia en arquitectura de datos, trabajando con estructuras, particionado, modelado y ciclo de vida en Kafka y Data Lake.\nExperiencia en arquitectura de datos\nKafka y Data Lake\n\ud83c\udf0d Ingl\u00e9s para trabajar en un entorno global.\nIngl\u00e9s\n\u2601\ufe0f Plataforma: Experiencia en Kafka / Confluent.\nPlataforma\nKafka / Confluent\n\u2699\ufe0f Procesos dirigidos por metadatos.\nProcesos dirigidos por metadatos\n\ud83d\udd04 Integraci\u00f3n de datos: Frameworks de procesamiento en streaming Kafka Streams, Spark Streaming o Flink.\nIntegraci\u00f3n de datos\nKafka Streams, Spark Streaming o Flink.\n\ud83d\udee0\ufe0f Orquestaci\u00f3n: Experiencia con orquestaci\u00f3n dirigida por eventos. Orquestaci\u00f3n de procesos s\u00edncrona con Apache Airflow.\nOrquestaci\u00f3n\nApache Airflow\n\ud83d\udcdd Gesti\u00f3n del c\u00f3digo: Buenas pr\u00e1cticas y metodolog\u00edas.\nGesti\u00f3n del c\u00f3digo\nValoramos tambi\u00e9n:\n\ud83d\udcca Familiaridad con conceptos como DataOps, Data Observability y Data Mesh.\nDataOps, Data Observability y Data Mesh\n\ud83d\udcbc Experiencia en consultor\u00eda dentro del mundo Data & Analytics.\nExperiencia en consultor\u00eda\nData & Analytics\n\ud83c\udf93 Formaci\u00f3n extra como un M\u00e1ster en Big Data & Analytics u otras certificaciones que potencien tus habilidades.\nM\u00e1ster en Big Data & Analytics\n\ud83d\udee0\ufe0f Conocimiento en tecnolog\u00edas como Spark, SQL, Snowflake, dbt, Azure Data Factory o Microsoft Fabric.\nSpark, SQL,\nSnowflake, dbt, Azure Data Factory o Microsoft Fabric\n\ud83d\udce6 Experiencia con contenedores y orquestaci\u00f3n.\ncontenedores y orquestaci\u00f3n\n\u2601\ufe0f Cloud & Automatizaci\u00f3n: Infraestructura, pipelines de datos, operaci\u00f3n y monitorizaci\u00f3n.\nCloud & Automatizaci\u00f3n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\nSalario\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\n\u2705 Acceso a formaciones y certificaciones\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group\nhttps://medium.com/sdg-group"
    },
    "4119123700": {
        "title": "Mid Data Engineer (30-40K)",
        "company": "zenital",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nzenital offers quality data services that empower brand new solutions. We are currently looking for a Data Engineer to help us to achieve the next level in our goal to transform how people understands and works with data.\n\nWe are working in mid-market customers such as airlines, logistic operators and consumer products distributors with modern data architectures and honest long-term relationships allowing us to excel at our work.\n\nWhat do we need from you? \ud83e\uddd0\n\nBe a nice person, honest and empathetic\nStrong technical skills and data engineering fundamentals\nMaster SQL\nBI knowledge and dimensional modeling (dimensions, facts, etc.)\n2+ years in data integration \n2+ years working in technical teams\nA proactive, adaptative and detail-oriented mindset.\n\nExtra points \u26a1\n\nExperience with distributed computing platforms and environments such as Databricks, Azure data suite and/or AWS data suite.\n\nExperience programming with Scala or Python in a production environment\n\nWhat will you get from this opportunity?\n\nSalary from 30-40k euros (depending on the experience), and yearly performance and salary reviews.\nStarting with 23 vacation days and additional 2 days for each year worked, up to 28.\n2 extra days off during the Christmas holidays.\nFlexible schedule and the option to work remotely or from our offices in Barcelona.\nOffsite events throughout the year, fostering a great working environment. (If you are wondering how is the zenital offsite style you can imagine Barcelona, Bilbao, Porto, and activities like surfing, winery, partying\u2026 \ud83d\ude09)\nSpecific training and certifications according to your needs.\n\nWe believe in innovation and pursuing great ideas! You will be able to give your opinion and make your ideas come to life.\nWhat do we need from you? \ud83e\uddd0\nBe a nice person, honest and empathetic\nStrong technical skills and data engineering fundamentals\nMaster SQL\nBI knowledge and dimensional modeling (dimensions, facts, etc.)\n2+ years in data integration \n2+ years working in technical teams\nA proactive, adaptative and detail-oriented mindset.\nBe a nice person, honest and empathetic\nStrong technical skills and data engineering fundamentals\nMaster SQL\nBI knowledge and dimensional modeling (dimensions, facts, etc.)\n2+ years in data integration\n2+ years working in technical teams\nA proactive, adaptative and detail-oriented mindset.\nExtra points \u26a1\nWhat will you get from this opportunity?\nSalary from 30-40k euros (depending on the experience), and yearly performance and salary reviews.\nStarting with 23 vacation days and additional 2 days for each year worked, up to 28.\n2 extra days off during the Christmas holidays.\nFlexible schedule and the option to work remotely or from our offices in Barcelona.\nOffsite events throughout the year, fostering a great working environment. (If you are wondering how is the zenital offsite style you can imagine Barcelona, Bilbao, Porto, and activities like surfing, winery, partying\u2026 \ud83d\ude09)\nSpecific training and certifications according to your needs.\nSalary from 30-40k euros (depending on the experience), and yearly performance and salary reviews.\nStarting with 23 vacation days and additional 2 days for each year worked, up to 28.\n2 extra days off during the Christmas holidays.\nFlexible schedule and the option to work remotely or from our offices in Barcelona.\nOffsite events throughout the year, fostering a great working environment. (If you are wondering how is the zenital offsite style you can imagine Barcelona, Bilbao, Porto, and activities like surfing, winery, partying\u2026 \ud83d\ude09)\nSpecific training and certifications according to your needs."
    },
    "4167800605": {
        "title": "Data Engineer ",
        "company": "Transition Technologies MS",
        "location": "European Economic Area",
        "work_mode": "Remote",
        "job_type": "Contract",
        "description": "About the job\nNote! Only candidates from the EU will be taken into consideration!\n\nCompany Description\nTransition Technologies MS is a fast-growing IT company with a global presence, specializing in end-to-end service delivery and staffing with industry-leading and niche technology expertise. At the moment TTMS is looking for a senior Data engineer for one of its pharma clients.\n\nSenior Data Engineer\n\nResponsibilities:\n\nApply data engineering practices and standards to develop robust and maintainable data pipelines\nAnalyze and organize raw data ingestion pipelines\nEvaluate business needs and objectives\nSupport senior business stakeholders in defining new data product use cases and their value\nTake ownership of data product pipelines and their maintenance\nExplore ways to enhance data quality and reliability, be the \"Quality Gatekeeper\" for developed Data Products\nAdapt and apply best practices from the Data One community\nBe constantly on the lookout for ways to improve best practices and efficiencies and make concrete proposals\nTake leadership and collaborate with other teams proactively to keep things moving\nBe flexible and take on other responsibilities within the scope of the Agile Team\n\n\nQualifications:\n\nPrevious experience as a data engineer\nTechnical expertise with data modeling techniques (Data Vault)\nAdvanced expertise with ETL tools (Talend, Alteryx etc.)\nAdvanced SQL programming experience, Python is a plus\nPrevious experience with agile methodologies in Software Development\nPrevious experience working with Data Transformation Tools like DBT\nPrevious experience with DevOps, DataOps practices (CI/CD, GitLab, DataOps.live)\nHand-on experience with Snowflake, certification is a plus\nExperience with the lifecycle management of data products\nKnowledge of Data Mesh and FAIR principles\nAt least 2-3 years of DB/ETL development experience - Talend and DBT preferred\n\n\nRequired Skills:\n\n4+ years of working with programming language focused on data pipelines, e.g. Python or R\n4+ years of experience working with SQL\n3+ years of experience in data pipelines maintenance\n3+ years of experience with different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)\n3+ years of experience in working in data architecture concepts (in any of following areas data modeling, metadata mng., workflow management, ETL/ELT, real-time streaming, data quality, distributed systems)\n3+ years of cloud technologies with emphasis on data pipelines (Airflow, Glue, Dataflow - but also other smart solutions of handling data in the cloud - elastic, redshift, bigquery, lambda, s3, EBS etc.)\n4+ years of experience working with SQL\n1+ years of experience in Java and/or Scala\nVery good knowledge of relational databases (optional)\nVery good knowledge of data serialization languages such as JSON, XML, YAML\nExcellent knowledge of Git, Gitflow and DevOps tools (e.g. Docker, Bamboo, Jenkins, Terraform\nCapability to conduct performance analysis, troubleshooting and remediation (optional)\nExcellent knowledge of Unix\nPharma data formats is a big plus (SDTM)\nNote! Only candidates from the EU will be taken into consideration!\nCompany Description\nTransition Technologies MS is a fast-growing IT company with a global presence, specializing in end-to-end service delivery and staffing with industry-leading and niche technology expertise. At the moment TTMS is looking for a senior Data engineer for one of its pharma clients.\nSenior Data Engineer\nResponsibilities:\nApply data engineering practices and standards to develop robust and maintainable data pipelines\nAnalyze and organize raw data ingestion pipelines\nEvaluate business needs and objectives\nSupport senior business stakeholders in defining new data product use cases and their value\nTake ownership of data product pipelines and their maintenance\nExplore ways to enhance data quality and reliability, be the \"Quality Gatekeeper\" for developed Data Products\nAdapt and apply best practices from the Data One community\nBe constantly on the lookout for ways to improve best practices and efficiencies and make concrete proposals\nTake leadership and collaborate with other teams proactively to keep things moving\nBe flexible and take on other responsibilities within the scope of the Agile Team\nApply data engineering practices and standards to develop robust and maintainable data pipelines\nAnalyze and organize raw data ingestion pipelines\nEvaluate business needs and objectives\nSupport senior business stakeholders in defining new data product use cases and their value\nTake ownership of data product pipelines and their maintenance\nExplore ways to enhance data quality and reliability, be the \"Quality Gatekeeper\" for developed Data Products\nAdapt and apply best practices from the Data One community\nBe constantly on the lookout for ways to improve best practices and efficiencies and make concrete proposals\nTake leadership and collaborate with other teams proactively to keep things moving\nBe flexible and take on other responsibilities within the scope of the Agile Team\nQualifications:\nPrevious experience as a data engineer\nTechnical expertise with data modeling techniques (Data Vault)\nAdvanced expertise with ETL tools (Talend, Alteryx etc.)\nAdvanced SQL programming experience, Python is a plus\nPrevious experience with agile methodologies in Software Development\nPrevious experience working with Data Transformation Tools like DBT\nPrevious experience with DevOps, DataOps practices (CI/CD, GitLab, DataOps.live)\nHand-on experience with Snowflake, certification is a plus\nExperience with the lifecycle management of data products\nKnowledge of Data Mesh and FAIR principles\nAt least 2-3 years of DB/ETL development experience - Talend and DBT preferred\nPrevious experience as a data engineer\nTechnical expertise with data modeling techniques (Data Vault)\nAdvanced expertise with ETL tools (Talend, Alteryx etc.)\nAdvanced SQL programming experience, Python is a plus\nPrevious experience with agile methodologies in Software Development\nPrevious experience working with Data Transformation Tools like DBT\nPrevious experience with DevOps, DataOps practices (CI/CD, GitLab, DataOps.live)\nHand-on experience with Snowflake, certification is a plus\nExperience with the lifecycle management of data products\nKnowledge of Data Mesh and FAIR principles\nAt least 2-3 years of DB/ETL development experience - Talend and DBT preferred\nRequired Skills:\n4+ years of working with programming language focused on data pipelines, e.g. Python or R\n4+ years of experience working with SQL\n3+ years of experience in data pipelines maintenance\n3+ years of experience with different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)\n3+ years of experience in working in data architecture concepts (in any of following areas data modeling, metadata mng., workflow management, ETL/ELT, real-time streaming, data quality, distributed systems)\n3+ years of cloud technologies with emphasis on data pipelines (Airflow, Glue, Dataflow - but also other smart solutions of handling data in the cloud - elastic, redshift, bigquery, lambda, s3, EBS etc.)\n4+ years of experience working with SQL\n1+ years of experience in Java and/or Scala\nVery good knowledge of relational databases (optional)\nVery good knowledge of data serialization languages such as JSON, XML, YAML\nExcellent knowledge of Git, Gitflow and DevOps tools (e.g. Docker, Bamboo, Jenkins, Terraform\nCapability to conduct performance analysis, troubleshooting and remediation (optional)\nExcellent knowledge of Unix\nPharma data formats is a big plus (SDTM)\n4+ years of working with programming language focused on data pipelines, e.g. Python or R\n4+ years of experience working with SQL\n3+ years of experience in data pipelines maintenance\n3+ years of experience with different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)\n3+ years of experience in working in data architecture concepts (in any of following areas data modeling, metadata mng., workflow management, ETL/ELT, real-time streaming, data quality, distributed systems)\n3+ years of cloud technologies with emphasis on data pipelines (Airflow, Glue, Dataflow - but also other smart solutions of handling data in the cloud - elastic, redshift, bigquery, lambda, s3, EBS etc.)\n1+ years of experience in Java and/or Scala\nVery good knowledge of relational databases (optional)\nVery good knowledge of data serialization languages such as JSON, XML, YAML\nExcellent knowledge of Git, Gitflow and DevOps tools (e.g. Docker, Bamboo, Jenkins, Terraform\nCapability to conduct performance analysis, troubleshooting and remediation (optional)\nExcellent knowledge of Unix\nPharma data formats is a big plus (SDTM)"
    },
    "4137594764": {
        "title": "Data Engineer Junior",
        "company": "Logicalis Spain",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEn Logicalis Spain estamos buscando un perfil de Ingeniero de Datos Junior para integrarse en nuestra BU de Data & Analytics. El equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\n\nREQUISITOS T\u00c9CNICOS\nM\u00ednimo 1 a\u00f1o de experiencia profesional trabajando con ecosistemas Big Data en entorno AWS: \nExperiencia contrastada en desarrollo en entornos Scala-Spark, Python. \nValorable conocer el uso de la herramienta de automatizaci\u00f3n Airflow / Docker / Jenkins.\nValorable experiencia trabajando con metodolog\u00edas \u00c1giles \n\nFUNCIONES\nDesarrollo de procesos ETL en AWS (EMR, Step functions, BATCH, ECS, Lambdas, Fargate). \nDar soluci\u00f3n a incidencias en producci\u00f3n relacionadas con ETL y los componentes de la soluci\u00f3n (Problemas escalabilidad, fallos memoria, time-out\u2026).\nEjecuci\u00f3n de guardias.\n\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo 100% remoto, desde cualquier punto de Espa\u00f1a.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados.\n> Buen ambiente de trabajo y entorno muy colaborativo.\nEn Logicalis Spain estamos buscando un perfil de Ingeniero de Datos Junior para integrarse en nuestra BU de Data & Analytics. El equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\nLogicalis Spain\nIngeniero de Datos Junior\nData & Analytics\nREQUISITOS T\u00c9CNICOS\nM\u00ednimo 1 a\u00f1o de experiencia profesional trabajando con ecosistemas Big Data en entorno AWS: \nExperiencia contrastada en desarrollo en entornos Scala-Spark, Python. \nValorable conocer el uso de la herramienta de automatizaci\u00f3n Airflow / Docker / Jenkins.\nValorable experiencia trabajando con metodolog\u00edas \u00c1giles\nM\u00ednimo 1 a\u00f1o de experiencia profesional trabajando con ecosistemas Big Data en entorno AWS:\nExperiencia contrastada en desarrollo en entornos Scala-Spark, Python.\nValorable conocer el uso de la herramienta de automatizaci\u00f3n Airflow / Docker / Jenkins.\nValorable experiencia trabajando con metodolog\u00edas \u00c1giles\nFUNCIONES\nDesarrollo de procesos ETL en AWS (EMR, Step functions, BATCH, ECS, Lambdas, Fargate). \nDar soluci\u00f3n a incidencias en producci\u00f3n relacionadas con ETL y los componentes de la soluci\u00f3n (Problemas escalabilidad, fallos memoria, time-out\u2026).\nEjecuci\u00f3n de guardias.\nDesarrollo de procesos ETL en AWS (EMR, Step functions, BATCH, ECS, Lambdas, Fargate).\nDar soluci\u00f3n a incidencias en producci\u00f3n relacionadas con ETL y los componentes de la soluci\u00f3n (Problemas escalabilidad, fallos memoria, time-out\u2026).\nEjecuci\u00f3n de guardias.\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo 100% remoto, desde cualquier punto de Espa\u00f1a.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados.\n> Buen ambiente de trabajo y entorno muy colaborativo."
    },
    "4174135755": {
        "title": "Principal Data Engineer ",
        "company": "Intellias",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nOver 20 years of market experience, Intellias brings together technologists, creators and innovators in Europe, North and Latin America, and the Middle East. Join our international team and take the mission to solve the advanced tech challenges of tomorrow!\n\nWe are seeking an experienced Principal Data Engineer to lead a team in developing and maintaining robust, scalable data pipelines, bridging on-premises and cloud environments, and delivering real-time analytics systems. This role requires deep expertise in data engineering and streaming technologies, combined with strong leadership skills to drive the team towards achieving business objectives. The manager will collaborate with cross-functional teams including architecture, product, and software engineering to ensure the delivery of high-quality data solutions aligned with company goals.\n\nRequirements: \n5+ years of hands-on experience in data engineering, including expertise in Python, Scala, or Java.\nDeep understanding of Apache Kafka for stream processing workflows (required)\nProficiency in managing and optimizing databases such as PostgreSQL, MySQL, MSSQL, and Oracle.\nFamiliarity with analytical databases like ClickHouse or similar.\nFamiliarity with both cloud solutions (AWS, Azure, Google Cloud) and on-premises environments as part of cost-optimization efforts.\nKnowledge of additional data tools and frameworks such as Redis, RabbitMQ, Superset, Cube.js, Minio, and Grafana (optional but beneficial).\nStrong leadership and mentoring skills, with the ability to guide a team and provide technical direction.\nExperience ensuring system reliability, scalability, and data integrity through best practices.\nFamiliarity with iGaming industry terminology and challenges is highly preferred.\n\nResponsibilities:\nProvide technical leadership, including making key decisions on solution design, architecture, and implementation strategies.\nLead and mentor a team of data engineers, serving as the primary point of contact for technical guidance.\nDesign and oversee the implementation of scalable, efficient data pipelines and architectures, with a strong focus on stream processing.\nDevelop and maintain robust data storage and processing solutions, leveraging tools like Apache Kafka, Redis (ClickHouse is a strong plus).\nGuide the migration of selected cloud-based solutions to on-premises tools, optimizing costs while maintaining performance and reliability.\nCollaborate with stakeholders to gather requirements, propose designs, and align data strategies with business objectives.\nEnsure system reliability and scalability, with a focus on high availability and robust data transfer mechanisms (e.g., \"at least once\" delivery).\nStay up-to-date with emerging technologies and evaluate their potential application to improve the overall data ecosystem.\n\nAt Intellias, where technology takes center stage, people always come before processes. We're dedicated to cultivating a tech-savvy environment that empowers individuals to unlock their true potential and achieve extraordinary results. Our customized benefits not only prioritize your well-being but also charge your professional growth, making this opportunity an ideal match for tech enthusiasts like you.\nOver 20 years of market experience, Intellias brings together technologists, creators and innovators in Europe, North and Latin America, and the Middle East. Join our international team and take the mission to solve the advanced tech challenges of tomorrow!\nWe are seeking an experienced Principal Data Engineer to lead a team in developing and maintaining robust, scalable data pipelines, bridging on-premises and cloud environments, and delivering real-time analytics systems. This role requires deep expertise in data engineering and streaming technologies, combined with strong leadership skills to drive the team towards achieving business objectives. The manager will collaborate with cross-functional teams including architecture, product, and software engineering to ensure the delivery of high-quality data solutions aligned with company goals.\nRequirements:\n5+ years of hands-on experience in data engineering, including expertise in Python, Scala, or Java.\nDeep understanding of Apache Kafka for stream processing workflows (required)\nProficiency in managing and optimizing databases such as PostgreSQL, MySQL, MSSQL, and Oracle.\nFamiliarity with analytical databases like ClickHouse or similar.\nFamiliarity with both cloud solutions (AWS, Azure, Google Cloud) and on-premises environments as part of cost-optimization efforts.\nKnowledge of additional data tools and frameworks such as Redis, RabbitMQ, Superset, Cube.js, Minio, and Grafana (optional but beneficial).\nStrong leadership and mentoring skills, with the ability to guide a team and provide technical direction.\nExperience ensuring system reliability, scalability, and data integrity through best practices.\nFamiliarity with iGaming industry terminology and challenges is highly preferred.\n5+ years of hands-on experience in data engineering, including expertise in Python, Scala, or Java.\nDeep understanding of Apache Kafka for stream processing workflows (required)\nProficiency in managing and optimizing databases such as PostgreSQL, MySQL, MSSQL, and Oracle.\nFamiliarity with analytical databases like ClickHouse or similar.\nFamiliarity with both cloud solutions (AWS, Azure, Google Cloud) and on-premises environments as part of cost-optimization efforts.\nKnowledge of additional data tools and frameworks such as Redis, RabbitMQ, Superset, Cube.js, Minio, and Grafana (optional but beneficial).\nStrong leadership and mentoring skills, with the ability to guide a team and provide technical direction.\nExperience ensuring system reliability, scalability, and data integrity through best practices.\nFamiliarity with iGaming industry terminology and challenges is highly preferred.\nResponsibilities:\nProvide technical leadership, including making key decisions on solution design, architecture, and implementation strategies.\nLead and mentor a team of data engineers, serving as the primary point of contact for technical guidance.\nDesign and oversee the implementation of scalable, efficient data pipelines and architectures, with a strong focus on stream processing.\nDevelop and maintain robust data storage and processing solutions, leveraging tools like Apache Kafka, Redis (ClickHouse is a strong plus).\nGuide the migration of selected cloud-based solutions to on-premises tools, optimizing costs while maintaining performance and reliability.\nCollaborate with stakeholders to gather requirements, propose designs, and align data strategies with business objectives.\nEnsure system reliability and scalability, with a focus on high availability and robust data transfer mechanisms (e.g., \"at least once\" delivery).\nStay up-to-date with emerging technologies and evaluate their potential application to improve the overall data ecosystem.\nProvide technical leadership, including making key decisions on solution design, architecture, and implementation strategies.\nLead and mentor a team of data engineers, serving as the primary point of contact for technical guidance.\nDesign and oversee the implementation of scalable, efficient data pipelines and architectures, with a strong focus on stream processing.\nDevelop and maintain robust data storage and processing solutions, leveraging tools like Apache Kafka, Redis (ClickHouse is a strong plus).\nGuide the migration of selected cloud-based solutions to on-premises tools, optimizing costs while maintaining performance and reliability.\nCollaborate with stakeholders to gather requirements, propose designs, and align data strategies with business objectives.\nEnsure system reliability and scalability, with a focus on high availability and robust data transfer mechanisms (e.g., \"at least once\" delivery).\nStay up-to-date with emerging technologies and evaluate their potential application to improve the overall data ecosystem.\nAt Intellias, where technology takes center stage, people always come before processes. We're dedicated to cultivating a tech-savvy environment that empowers individuals to unlock their true potential and achieve extraordinary results. Our customized benefits not only prioritize your well-being but also charge your professional growth, making this opportunity an ideal match for tech enthusiasts like you."
    },
    "4161441671": {
        "title": "Senior Data Engineer | Up to 65K | Madrid",
        "company": "Talent-R",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u27a1\ufe0f We\u2019re looking for a Data Engineer willing to play a pivotal role in developing a regulatory-grade data solution for our client, a boutique consultancy firm with a start-up ethos, specializing in data solutions for top-tier clients.\n\ud83d\udccd Location: Hybrid in Madrid (3 days of remote / 2 days in the office)\n\ud83d\udcb0 Salary: Up to 65K\n\ud83d\udcbc Project: Join the Data Delivery Team of a leading automobile leasing company in Europe\n\nRESPONSIBILITIES \u2692\ufe0f\n\ud83d\udcd0Data Pipeline Development: Design, build, and maintain efficient, scalable, and reliable data pipelines.\n\ud83c\udf10 CI/CD & Infrastructure Optimization: Maintenance and enhancement of CI/CD architecture, design of improvements to the data infrastructure.\n\ud83d\udcd1 Data Governance & Best Practices \u2013 Implementation of best practices for data management, security, and governance.\n\ud83d\udd01Cross-Functional Collaboration: Collaborate with analysts and other stakeholders to understand data requirements and deliver solutions\n\ud83d\udca1Technical Expertise: Guide and mentor junior data engineers, framing projects and assigning priorities to key deliverables.\n\nQUALIFICATIONS \ud83d\udca1\n+4 years of experience in Data Engineering or a similar role\nProficiency with Azure Cloud, Azure Data Factory and Databricks\nExpertise in PySpark, SQL and experience with relational databases (e.g., PostgreSQL, MySQL)\nFamiliarity with cloud platforms and data warehousing solutions\n\nABOUT YOU \ud83d\udde3\ufe0f\nExcellent problem-solving skills and attention to detail\nGood verbal and written communication skills in English and Spanish\nAbility to collaborate effectively with cross-functional teams\n\nBENEFITS \ud83c\udf31\n\ud83c\udf4eMeal allowance (10\u20ac per day)\n\ud83c\udf3430 days of holidays per year\n\ud83c\udfe5 Private medical insurance (Up to 25.50\u20ac/month)\n\ud83c\udfd7\ufe0f Support for training and certification\n\ud83d\udcc8 Performance bonus , a bonus for your contribution to the development of the company\u2019s billing and referral bonus of 1,500\u20ac for each candidate recruited through your recommendation.\n\u27a1\ufe0f We\u2019re looking for a Data Engineer willing to play a pivotal role in developing a regulatory-grade data solution for our client, a boutique consultancy firm with a start-up ethos, specializing in data solutions for top-tier clients.\nData Engineer\n\ud83d\udccd Location: Hybrid in Madrid (3 days of remote / 2 days in the office)\n\ud83d\udcb0 Salary: Up to 65K\n\ud83d\udcbc Project: Join the Data Delivery Team of a leading automobile leasing company in Europe\n\ud83d\udccd Location: Hybrid in Madrid (3 days of remote / 2 days in the office)\n\ud83d\udccd Location:\n\ud83d\udcb0 Salary: Up to 65K\n\ud83d\udcb0 Salary:\n\ud83d\udcbc Project: Join the Data Delivery Team of a leading automobile leasing company in Europe\nProject:\nRESPONSIBILITIES \u2692\ufe0f\nRESPONSIBILITIES\n\ud83d\udcd0Data Pipeline Development: Design, build, and maintain efficient, scalable, and reliable data pipelines.\n\ud83c\udf10 CI/CD & Infrastructure Optimization: Maintenance and enhancement of CI/CD architecture, design of improvements to the data infrastructure.\n\ud83d\udcd1 Data Governance & Best Practices \u2013 Implementation of best practices for data management, security, and governance.\n\ud83d\udd01Cross-Functional Collaboration: Collaborate with analysts and other stakeholders to understand data requirements and deliver solutions\n\ud83d\udca1Technical Expertise: Guide and mentor junior data engineers, framing projects and assigning priorities to key deliverables.\n\ud83d\udcd0Data Pipeline Development: Design, build, and maintain efficient, scalable, and reliable data pipelines.\nData Pipeline Development:\n\ud83c\udf10 CI/CD & Infrastructure Optimization: Maintenance and enhancement of CI/CD architecture, design of improvements to the data infrastructure.\nCI/CD & Infrastructure Optimization:\n\ud83d\udcd1 Data Governance & Best Practices \u2013 Implementation of best practices for data management, security, and governance.\nData Governance & Best Practices\n\ud83d\udd01Cross-Functional Collaboration: Collaborate with analysts and other stakeholders to understand data requirements and deliver solutions\nCross-Functional Collaboration:\n\ud83d\udca1Technical Expertise: Guide and mentor junior data engineers, framing projects and assigning priorities to key deliverables.\nTechnical Expertise:\nQUALIFICATIONS \ud83d\udca1\nQUALIFICATIONS\n+4 years of experience in Data Engineering or a similar role\nProficiency with Azure Cloud, Azure Data Factory and Databricks\nExpertise in PySpark, SQL and experience with relational databases (e.g., PostgreSQL, MySQL)\nFamiliarity with cloud platforms and data warehousing solutions\n+4 years of experience in Data Engineering or a similar role\n+4 years of experience\nProficiency with Azure Cloud, Azure Data Factory and Databricks\nAzure Cloud, Azure Data Factory and Databricks\nExpertise in PySpark, SQL and experience with relational databases (e.g., PostgreSQL, MySQL)\nPySpark, SQL\nFamiliarity with cloud platforms and data warehousing solutions\ncloud platforms\ndata warehousing solutions\nABOUT YOU \ud83d\udde3\ufe0f\nABOUT YOU\nExcellent problem-solving skills and attention to detail\nGood verbal and written communication skills in English and Spanish\nAbility to collaborate effectively with cross-functional teams\nExcellent problem-solving skills and attention to detail\nGood verbal and written communication skills in English and Spanish\nEnglish and Spanish\nAbility to collaborate effectively with cross-functional teams\nBENEFITS \ud83c\udf31\nBENEFITS\n\ud83c\udf4eMeal allowance (10\u20ac per day)\n\ud83c\udf3430 days of holidays per year\n\ud83c\udfe5 Private medical insurance (Up to 25.50\u20ac/month)\n\ud83c\udfd7\ufe0f Support for training and certification\n\ud83d\udcc8 Performance bonus , a bonus for your contribution to the development of the company\u2019s billing and referral bonus of 1,500\u20ac for each candidate recruited through your recommendation.\n\ud83c\udf4eMeal allowance (10\u20ac per day)\n\ud83c\udf3430 days of holidays per year\n\ud83c\udfe5 Private medical insurance (Up to 25.50\u20ac/month)\n\ud83c\udfd7\ufe0f Support for training and certification\n\ud83d\udcc8 Performance bonus , a bonus for your contribution to the development of the company\u2019s billing and referral bonus of 1,500\u20ac for each candidate recruited through your recommendation."
    },
    "4088150922": {
        "title": "Data Engineer",
        "company": "Allianz Technology",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nAt Allianz Technology, our Information Security Operations team is the driving force behind securing our digital world. If you're passionate about cybersecurity and eager to make an impact, this is your chance to be at the forefront of shaping and strengthening our global security initiatives. You will help develop solutions, services, and capabilities that protect Allianz from the ever-evolving landscape of cyber threats.\n\nAllianz Vulnerability Management (AVM) is a key player in this mission, focused on detecting, analyzing, and reporting vulnerabilities across various business and technological systems. If you're excited about identifying risks and fortifying digital defenses, AVM, as part of the Security Services group and core Information Security Operations pillar, offers the perfect opportunity to grow and contribute.\n\nWhat you can find at Allianz Technology:\n\nWork Life - Balance: We offer flexible working hours so you can start your working day according to your needs. In addition, during the summer and every Friday of the year, you have the option of intensive working hours, which will allow you to enjoy more free time for yourself.\nInternational and Multicultural Environment: We value the diversity and richness that comes from working in an international and multicultural environment. With more than 52 different nationalities represented in our company, we offer our employees the opportunity to collaborate with people from all over the world, enriching the work environment and providing the opportunity to learn and grow in a truly global setting. This diversity allows us to have a broader perspective, foster creativity and innovation, and promote an inclusive work environment where every voice is heard and valued.\nProfessional Growth Opportunities: We are committed to the professional development of our employees and provide numerous opportunities for growth within the company. We value internal talent and promote internal advancement, offering training programs, mentoring, and skills development that allow our employees to advance in their careers. \nCompensation and Benefits Package: It includes a company bonus plan, pension, employee stock program, and multiple employee discounts. \n\nWhat will make you succeed in this position?\n\n 5-10 years of experience as a Database Administrator/Architect, with specific experience in PostgreSQL and MongoDB along with good Python skills. \n Proficiency in PostgreSQL and MongoDB. \n Strong skills in Python and Ansible for development and automation. \n Familiarity with cloud-based database management (AWS, Azure, Google Cloud Platform). \n Experience in creating and managing databases, tables, and indexes. \n Strong understanding of database performance monitoring and optimization. \n Knowledge of data integrity and security best practices. \n Proficient in developing backup and recovery procedures. \n Ability to analyze and organize raw data and build data systems and pipelines. \n Experience in conducting complex data analysis and building ETL solutions. \n Knowledge of Agile Methodology \n Good to have knowledge about the Vulnerability Management and broader knowledge of cybersecurity (ISO27001 ISMS framework) \n Bachelor\u2019s degree in Computer Science, Information Technology, or a related field. \n Excellent problem-solving and troubleshooting skills. \n Strong collaboration and communication skills. \n\nYour mission in the role will be:\n\nData Engineering:\n\n Design, build, and maintain scalable and robust data pipelines to support data integration and data warehousing. \n Develop and optimize ETL processes to ingest, clean, and transform data from various sources like Qualys, CMDB etc \n Ensure the reliability, availability, and performance of data systems. \n\n Data Management \n\nManage and maintain data architecture, data models, and data schemas. \n Implement and maintain data governance and data quality standards. \n Work with relational and NoSQL databases, ensuring data integrity and security. \n\n Performance Monitoring and Optimization: \n\nMonitor database performance and optimize query execution for maximum efficiency. \n Troubleshoot and resolve database-related issues. \n\nData Integrity and Security \n\nEnsure data integrity and security, including managing user access and permissions. \n Develop and implement backup and recovery procedures to minimize data loss in the event of hardware or software failure. \n\n Cloud-Based Database Management \n\nManage cloud-based databases on platforms like AWS, Azure, and Google Cloud Platform. \n Keep up-to-date with the latest PostgreSQL/MongoDB releases, features, and patches. \n\n Collaboration and Documentation \n\nCollaborate with developers and other IT staff to ensure database systems meet business requirements. \n Document database processes and procedures. \n\nAbout Allianz Technology\n\nAllianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 12,000 employees located in 51 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry.\n\nWe oversee the full digitalization spectrum \u2013 from one of the industry\u2019s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n\nD&I statement\n\nAllianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, ethnicity and cultural background, age, nationality, religion, disability, or philosophy of life.\n\nJoin us. Let\u00b4s care for tomorrow.\n\nYou. IT\n\n61258 | Ingenier\u00eda inform\u00e1tica y tecnol\u00f3gica | Profesional / Senior | Non-Executive | Allianz Technology | Jornada completa | Indefinido\nWhat you can find at Allianz Technology:\nWork Life - Balance: We offer flexible working hours so you can start your working day according to your needs. In addition, during the summer and every Friday of the year, you have the option of intensive working hours, which will allow you to enjoy more free time for yourself.\nInternational and Multicultural Environment: We value the diversity and richness that comes from working in an international and multicultural environment. With more than 52 different nationalities represented in our company, we offer our employees the opportunity to collaborate with people from all over the world, enriching the work environment and providing the opportunity to learn and grow in a truly global setting. This diversity allows us to have a broader perspective, foster creativity and innovation, and promote an inclusive work environment where every voice is heard and valued.\nProfessional Growth Opportunities: We are committed to the professional development of our employees and provide numerous opportunities for growth within the company. We value internal talent and promote internal advancement, offering training programs, mentoring, and skills development that allow our employees to advance in their careers. \nCompensation and Benefits Package: It includes a company bonus plan, pension, employee stock program, and multiple employee discounts.\nWork Life - Balance: We offer flexible working hours so you can start your working day according to your needs. In addition, during the summer and every Friday of the year, you have the option of intensive working hours, which will allow you to enjoy more free time for yourself.\nInternational and Multicultural Environment: We value the diversity and richness that comes from working in an international and multicultural environment. With more than 52 different nationalities represented in our company, we offer our employees the opportunity to collaborate with people from all over the world, enriching the work environment and providing the opportunity to learn and grow in a truly global setting. This diversity allows us to have a broader perspective, foster creativity and innovation, and promote an inclusive work environment where every voice is heard and valued.\nProfessional Growth Opportunities: We are committed to the professional development of our employees and provide numerous opportunities for growth within the company. We value internal talent and promote internal advancement, offering training programs, mentoring, and skills development that allow our employees to advance in their careers.\nCompensation and Benefits Package: It includes a company bonus plan, pension, employee stock program, and multiple employee discounts.\nWhat will make you succeed in this position?\n5-10 years of experience as a Database Administrator/Architect, with specific experience in PostgreSQL and MongoDB along with good Python skills. \n Proficiency in PostgreSQL and MongoDB. \n Strong skills in Python and Ansible for development and automation. \n Familiarity with cloud-based database management (AWS, Azure, Google Cloud Platform). \n Experience in creating and managing databases, tables, and indexes. \n Strong understanding of database performance monitoring and optimization. \n Knowledge of data integrity and security best practices. \n Proficient in developing backup and recovery procedures. \n Ability to analyze and organize raw data and build data systems and pipelines. \n Experience in conducting complex data analysis and building ETL solutions. \n Knowledge of Agile Methodology \n Good to have knowledge about the Vulnerability Management and broader knowledge of cybersecurity (ISO27001 ISMS framework) \n Bachelor\u2019s degree in Computer Science, Information Technology, or a related field. \n Excellent problem-solving and troubleshooting skills. \n Strong collaboration and communication skills.\n5-10 years of experience as a Database Administrator/Architect, with specific experience in PostgreSQL and MongoDB along with good Python skills.\nProficiency in PostgreSQL and MongoDB.\nStrong skills in Python and Ansible for development and automation.\nFamiliarity with cloud-based database management (AWS, Azure, Google Cloud Platform).\nExperience in creating and managing databases, tables, and indexes.\nStrong understanding of database performance monitoring and optimization.\nKnowledge of data integrity and security best practices.\nProficient in developing backup and recovery procedures.\nAbility to analyze and organize raw data and build data systems and pipelines.\nExperience in conducting complex data analysis and building ETL solutions.\nKnowledge of Agile Methodology\nGood to have knowledge about the Vulnerability Management and broader knowledge of cybersecurity (ISO27001 ISMS framework)\nBachelor\u2019s degree in Computer Science, Information Technology, or a related field.\nExcellent problem-solving and troubleshooting skills.\nStrong collaboration and communication skills.\nYour mission in the role will be:\nData Engineering:\nDesign, build, and maintain scalable and robust data pipelines to support data integration and data warehousing. \n Develop and optimize ETL processes to ingest, clean, and transform data from various sources like Qualys, CMDB etc \n Ensure the reliability, availability, and performance of data systems.\nDesign, build, and maintain scalable and robust data pipelines to support data integration and data warehousing.\nDevelop and optimize ETL processes to ingest, clean, and transform data from various sources like Qualys, CMDB etc\nEnsure the reliability, availability, and performance of data systems.\nData Management\nManage and maintain data architecture, data models, and data schemas. \n Implement and maintain data governance and data quality standards. \n Work with relational and NoSQL databases, ensuring data integrity and security.\nManage and maintain data architecture, data models, and data schemas.\nImplement and maintain data governance and data quality standards.\nWork with relational and NoSQL databases, ensuring data integrity and security.\nPerformance Monitoring and Optimization:\nMonitor database performance and optimize query execution for maximum efficiency. \n Troubleshoot and resolve database-related issues.\nMonitor database performance and optimize query execution for maximum efficiency.\nTroubleshoot and resolve database-related issues.\nData Integrity and Security\nEnsure data integrity and security, including managing user access and permissions. \n Develop and implement backup and recovery procedures to minimize data loss in the event of hardware or software failure.\nEnsure data integrity and security, including managing user access and permissions.\nDevelop and implement backup and recovery procedures to minimize data loss in the event of hardware or software failure.\nCloud-Based Database Management\nManage cloud-based databases on platforms like AWS, Azure, and Google Cloud Platform. \n Keep up-to-date with the latest PostgreSQL/MongoDB releases, features, and patches.\nManage cloud-based databases on platforms like AWS, Azure, and Google Cloud Platform.\nKeep up-to-date with the latest PostgreSQL/MongoDB releases, features, and patches.\nCollaboration and Documentation\nCollaborate with developers and other IT staff to ensure database systems meet business requirements. \n Document database processes and procedures.\nCollaborate with developers and other IT staff to ensure database systems meet business requirements.\nDocument database processes and procedures.\nAbout Allianz Technology\nD&I statement"
    },
    "4172639382": {
        "title": "Data Engineer",
        "company": "JSS ASSOCIATES",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Contract",
        "description": "About the job\nJob Description:\nWe are seeking a skilled Data Engineer with expertise in AWS, Python, PySpark, and Databricks to join our team. The ideal candidate will have hands-on experience in building data pipelines and transformations for both batch and streaming data. As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions that support our business objectives.\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS services, Python, PySpark, and Databricks.\nWork with both batch and streaming data, ensuring efficient data processing and transformation.\nUtilize AWS tools such as EMR, EC2, Airflow, Lambda, and Step Functions to build scalable data solutions.\nCollaborate with cross-functional teams to understand data requirements and deliver solutions that meet business needs.\nMaintain and improve existing data pipelines, ensuring high performance and reliability.\nDevelop and enforce best practices for data engineering within the team.\nEssential Skills:\nProven experience in Data Engineering with a strong focus on AWS cloud services.\nHands-on experience with Python, PySpark, and Databricks for data processing and transformation.\nProficiency in building and managing data pipelines in AWS environments.\nExperience with AWS tools such as EMR, EC2, Airflow, Lambda, and Step Functions.\nStrong understanding of cloud PaaS environments, particularly AWS.\nAbility to work effectively in a collaborative team environment.\nExcellent problem-solving skills and attention to detail.\nDesirable Skills:\nExperience in designing data architectures and implementing data warehousing solutions.\nFamiliarity with additional AWS services and cloud platforms.\nKnowledge of data security and compliance standards.\nStrong communication skills with the ability to present technical concepts to non-technical stakeholders.\nJob Description:\nWe are seeking a skilled Data Engineer with expertise in AWS, Python, PySpark, and Databricks to join our team. The ideal candidate will have hands-on experience in building data pipelines and transformations for both batch and streaming data. As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions that support our business objectives.\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS services, Python, PySpark, and Databricks.\nWork with both batch and streaming data, ensuring efficient data processing and transformation.\nUtilize AWS tools such as EMR, EC2, Airflow, Lambda, and Step Functions to build scalable data solutions.\nCollaborate with cross-functional teams to understand data requirements and deliver solutions that meet business needs.\nMaintain and improve existing data pipelines, ensuring high performance and reliability.\nDevelop and enforce best practices for data engineering within the team.\nDesign, develop, and optimize data pipelines using AWS services, Python, PySpark, and Databricks.\nWork with both batch and streaming data, ensuring efficient data processing and transformation.\nUtilize AWS tools such as EMR, EC2, Airflow, Lambda, and Step Functions to build scalable data solutions.\nCollaborate with cross-functional teams to understand data requirements and deliver solutions that meet business needs.\nMaintain and improve existing data pipelines, ensuring high performance and reliability.\nDevelop and enforce best practices for data engineering within the team.\nEssential Skills:\nProven experience in Data Engineering with a strong focus on AWS cloud services.\nHands-on experience with Python, PySpark, and Databricks for data processing and transformation.\nProficiency in building and managing data pipelines in AWS environments.\nExperience with AWS tools such as EMR, EC2, Airflow, Lambda, and Step Functions.\nStrong understanding of cloud PaaS environments, particularly AWS.\nAbility to work effectively in a collaborative team environment.\nExcellent problem-solving skills and attention to detail.\nProven experience in Data Engineering with a strong focus on AWS cloud services.\nHands-on experience with Python, PySpark, and Databricks for data processing and transformation.\nProficiency in building and managing data pipelines in AWS environments.\nExperience with AWS tools such as EMR, EC2, Airflow, Lambda, and Step Functions.\nStrong understanding of cloud PaaS environments, particularly AWS.\nAbility to work effectively in a collaborative team environment.\nExcellent problem-solving skills and attention to detail.\nDesirable Skills:\nExperience in designing data architectures and implementing data warehousing solutions.\nFamiliarity with additional AWS services and cloud platforms.\nKnowledge of data security and compliance standards.\nStrong communication skills with the ability to present technical concepts to non-technical stakeholders.\nExperience in designing data architectures and implementing data warehousing solutions.\nFamiliarity with additional AWS services and cloud platforms.\nKnowledge of data security and compliance standards.\nStrong communication skills with the ability to present technical concepts to non-technical stakeholders."
    },
    "4173783803": {
        "title": "Data Engineer",
        "company": "Mitiga Solutions",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nDescription\n\nOur mission\n\nClimate change is causing deep uncertainty at all levels of society, with climate volatility reducing the security of all land-based assets and posing a multi-level security threat for the functioning of enterprises, societies, and governments.\n\nMitiga is a spinoff of the Barcelona Supercomputing Center, which hosts MareNostrum, one of the most powerful supercomputers in Europe.\n\nMitiga's mission is firmly rooted in employing cutting-edge science and technology to mitigate the multifaceted risks posed by climate-driven perils. Our approach spans a spectrum of temporal scales, from real-time and annual forecasts to projections spanning the next century. This comprehensive perspective on risk, coupled with our capacity to transfer risks to capital markets via catastrophe bonds, positions us to significantly contribute to enhanced climate mitigation, resilience, and equity.\n\nAnd we need you.\n\nWhat You Will Do\n\nIf you build it, data will come: you'll help us put in place a system where data can travel easily from the capture point to the warehouse and through transformation processes to become useful information. Your superpowers will be put to use in designing, developing, and maintaining scalable data pipelines and ETL processes to ingest, transform, and store large volumes of data. In addition, you'll build and optimise data models, databases, and data catalogs to support retrieval needs.\n\nA sum of many parts: you'll constantly be on the lookout for ways to make cleaner, more accurate data. This includes fixing any broken or outdated parts of the pipeline, as well as identifying and resolving performance and scalability issues in data processing and storage systems. But you won't be alone in troubleshooting; you'll collaborate with great data scientists, developers, and scientists at Mitiga to understand data requirements and translate them into technical solutions.\n\nYou're picky about quality: you understand that quality is paramount when it comes to using and scaling the use of data. You'll implement data governance and data quality frameworks to ensure the accuracy, integrity, and security of our data. Additionally, you'll stay up to date with industry best practices, emerging technologies, and trends in data engineering and analytics to continuously push the envelope at Mitiga.\n\nSharing the knowledge: you'll generously share your wealth of knowledge and experience by mentoring and providing technical guidance to junior members of the data engineering team. You'll also actively participate in code reviews and promote coding standards and best practices within the team.\n\nRequirements\n\nYour expertise stems from a B.S. in Computer Science, Computer Engineering, or a related data field. You're a pro at orchestration, including containers like Docker and Kubernetes, as well as tools like Terraform, GitHub CI/CD, Git, and Jenkins. Plus, your impressive experience extends to pipeline tools like Argo, Airflow, Prefect or CVD. You have been working with APIs and offering data to clients and partners.\n\nPython and multidimensional data processing libraries like xarray, along with parallelisation libraries such as dask and mpi, are second nature to you. And of course, you're fluent in English!\n\nYour love for learning drives you to embrace new and challenging areas. You're a team player who thrives on collaborating and building cool stuff with your teammates. The fast-paced, results-oriented culture of a startup excites you, and you're no stranger to agile methodologies (hello, Jira). You share our mission goals and always strive to deliver top-notch solutions of the highest quality and on time.\n\nIf you're ready to bring your skills and attitude to our team, we'd love to hear from you!\n\nDiversity\n\nA recent study shows that one in every 10 tech workers in Spain are women. Only 8% of all contracts in 2019 with people with disabilities were in technology and science. Meanwhile, 30% of LGTBIQ people in Spain do not come out in the workplace for fear of discrimination.\n\nThis is neither the world we want to live in nor the workplace we\u2019re building.\n\nMitiga is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, colour, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.\n\nIn fact, we strongly encourage applicants from minority groups to apply.\n\nOur offer\n\nOur vision is to be the place where a diverse mix of talented people want to come, to stay and do their best work. Some of our benefits, like unlimited personal time or flexible working arrangements, help work adjust to your life \u2013 and not the other way around. You can work from home or come twice a week to our office in the heart of Barcelona. Dogs and other furry friends are welcome!\n\nOur compressed work schedule allows you to work half a day on Fridays during the year (so you can start the weekend early). During the summer, it also allows you to work 4 days a week, so you spend time with your loved ones (or work on your tan). Shadow our leadership and participate in our structured feedback and mentorship programs. If you grow, we grow.\n\nDon't see a fitting open role? Fill out this form and we will get back to you if an opportunity matching your profile comes up!\nDescription\nOur mission\nWhat You Will Do\nIf you build it, data will come:\nA sum of many parts\nYou're picky about quality\nSharing the knowledge:\nRequirements\nDiversity\nOur offer\nDon't see a fitting open role? Fill out\nthis form\nand we will get back to you if an opportunity matching your profile comes up!"
    },
    "3982414505": {
        "title": "Cloud Data Solutions Software Engineer ",
        "company": "SDG Group Espa\u00f1a",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola, Data Lover! \ud83d\udc99\n\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\n\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\n\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\n\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\n\ud83d\udd27 Dise\u00f1ar soluciones de software enfocadas en la gesti\u00f3n de datos, asegurando calidad, seguridad y trazabilidad\n\n\ud83d\udcca Dise\u00f1ar e implementar soluciones impulsadas por metadatos para optimizar y automatizar procesos a lo largo del ciclo de vida del dato.\n\n\u2601\ufe0f Implementar soluciones cloud (Azure, AWS y GCP) asegurando el uso \u00f3ptimo de sus componentes.\n\n\ud83d\udcbb Programar y optimizar procesos de tratamiento de datos, utilizando Python, para el manejo eficiente del ciclo vida del dato.\n\n\ud83d\udcc8 Coordinar el dise\u00f1o y desarrollo de soluciones para la gesti\u00f3n del ciclo del dato, garantizando eficiencia y escalabilidad.\n\n\ud83e\udd1d Colaborar con equipos t\u00e9cnicos para definir estrategias y buenas pr\u00e1cticas en entornos cloud.\n\n\ud83d\udd0d Evaluar y probar servicios cloud innovadores que puedan aportar valor a los proyectos\n\n\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n?\n\n\ud83c\udf93 Titulaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o formaci\u00f3n equivalente.\n\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Software Engineer o roles similares dentro del mundo del dato.\n\n\ud83d\udd04 +2 a\u00f1os en integraci\u00f3n y procesamiento de datos, desarrollando pipelines de ingesta y transformaci\u00f3n.\n\n\ud83d\udcbb Conocimiento s\u00f3lido en Python.\n\n\ud83d\udcc2 Bases de datos relacionales y no relacionales, con capacidad para optimizar consultas y estructuras de almacenamiento.\n\n\u2601\ufe0f Experiencia en entornos cloud, principalmente Azure, AWS y GCP.\n\n\ud83c\udfd7\ufe0f Conocimientos en arquitectura de datos, orquestaci\u00f3n de procesos y observabilidad.\n\n\ud83d\udcc8 Desarrollo de soluciones escalables y eficientes para la gesti\u00f3n del ciclo de vida del dato.\n\n\ud83d\ude80 Habilidad para trabajar en entornos de integraci\u00f3n continua y despliegue en la nube.\n\n\ud83e\uddd0 Experiencia en liderazgo t\u00e9cnico.\n\n\nValoramos tambi\u00e9n:\n\n\ud83d\udd27 Dise\u00f1o de soluciones end-to-end y comprensi\u00f3n de su base t\u00e9cnica.\n\n\ud83d\udcbb Experiencia en programaci\u00f3n para integraci\u00f3n de datos, con conocimiento de frameworks y herramientas relevantes.\n\n\u2601\ufe0f Certificaci\u00f3n cloud o experiencia con servicios en la nube.\n\n\ud83c\udf10 Conocimientos en DataOps, Data Observability y Data Mesh.\n\n\ud83c\udf93 Formaci\u00f3n complementaria como un M\u00e1ster en Big Data y Analytics es un plus.\n\n\ud83d\udde3\ufe0f Ingl\u00e9s para trabajar en un entorno global.\n\n\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\n\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\n\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\n\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\n\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\n\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\n\n\n\ud83d\udccc Y, por si fuera poco\u2026\n\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\n\u2705 Acceso a formaciones y certificaciones.\n\n\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\n\n\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group\n\u00a1Hola, Data Lover! \ud83d\udc99\n\u00bfQuieres llevar tus habilidades en Data, Analytics & IA al siguiente nivel? \ud83d\ude80.\nData, Analytics & IA\nSomos l\u00edderes globales, trabajamos con la mitad del IBEX-35 y no paramos de crecer desde 1994.\nSi te apasionan los datos, \u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00a1\u00fanete a nosotrxs! \u2728\ud83d\udcca\n\u00bfC\u00f3mo ser\u00e1 tu d\u00eda a d\u00eda?\n\ud83d\udd27 Dise\u00f1ar soluciones de software enfocadas en la gesti\u00f3n de datos, asegurando calidad, seguridad y trazabilidad\nDise\u00f1ar soluciones\ngesti\u00f3n de datos\n\ud83d\udcca Dise\u00f1ar e implementar soluciones impulsadas por metadatos para optimizar y automatizar procesos a lo largo del ciclo de vida del dato.\nDise\u00f1ar e implementar\nimpulsadas por metadatos\n\u2601\ufe0f Implementar soluciones cloud (Azure, AWS y GCP) asegurando el uso \u00f3ptimo de sus componentes.\nImplementar soluciones cloud\n\ud83d\udcbb Programar y optimizar procesos de tratamiento de datos, utilizando Python, para el manejo eficiente del ciclo vida del dato.\nProgramar y optimizar\nPython\n\ud83d\udcc8 Coordinar el dise\u00f1o y desarrollo de soluciones para la gesti\u00f3n del ciclo del dato, garantizando eficiencia y escalabilidad.\nCoordinar el dise\u00f1o y desarrollo\n\ud83e\udd1d Colaborar con equipos t\u00e9cnicos para definir estrategias y buenas pr\u00e1cticas en entornos cloud.\nColaborar con equipos t\u00e9cnicos\n\ud83d\udd0d Evaluar y probar servicios cloud innovadores que puedan aportar valor a los proyectos\nEvaluar y probar\n\u00bfQu\u00e9 te har\u00e1 triunfar en est\u00e1 posici\u00f3n?\n\ud83c\udf93 Titulaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones o formaci\u00f3n equivalente.\nTitulaci\u00f3n en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones\n\ud83d\udcca +4 a\u00f1os de experiencia como Data Software Engineer o roles similares dentro del mundo del dato.\n+4 a\u00f1os de experiencia\nData Software Engineer\n\ud83d\udd04 +2 a\u00f1os en integraci\u00f3n y procesamiento de datos, desarrollando pipelines de ingesta y transformaci\u00f3n.\n+2 a\u00f1os en integraci\u00f3n y procesamiento de datos\n\ud83d\udcbb Conocimiento s\u00f3lido en Python.\nConocimiento s\u00f3lido en Python.\n\ud83d\udcc2 Bases de datos relacionales y no relacionales, con capacidad para optimizar consultas y estructuras de almacenamiento.\nBases de datos relacionales y no relacionales\n\u2601\ufe0f Experiencia en entornos cloud, principalmente Azure, AWS y GCP.\nExperiencia en entornos cloud\nAzure, AWS y GCP\n\ud83c\udfd7\ufe0f Conocimientos en arquitectura de datos, orquestaci\u00f3n de procesos y observabilidad.\nConocimientos en arquitectura de datos, orquestaci\u00f3n de procesos y observabilidad\n\ud83d\udcc8 Desarrollo de soluciones escalables y eficientes para la gesti\u00f3n del ciclo de vida del dato.\nDesarrollo de soluciones escalables y eficientes\n\ud83d\ude80 Habilidad para trabajar en entornos de integraci\u00f3n continua y despliegue en la nube.\nHabilidad para trabajar en entornos de integraci\u00f3n continua\n\ud83e\uddd0 Experiencia en liderazgo t\u00e9cnico.\nliderazgo t\u00e9cnico.\nValoramos tambi\u00e9n:\n\ud83d\udd27 Dise\u00f1o de soluciones end-to-end y comprensi\u00f3n de su base t\u00e9cnica.\nDise\u00f1o de soluciones end-to-end\n\ud83d\udcbb Experiencia en programaci\u00f3n para integraci\u00f3n de datos, con conocimiento de frameworks y herramientas relevantes.\nExperiencia en programaci\u00f3n\n\u2601\ufe0f Certificaci\u00f3n cloud o experiencia con servicios en la nube.\nCertificaci\u00f3n cloud\nservicios en la nube\n\ud83c\udf10 Conocimientos en DataOps, Data Observability y Data Mesh.\nDataOps, Data Observability y Data Mesh\n\ud83c\udf93 Formaci\u00f3n complementaria como un M\u00e1ster en Big Data y Analytics es un plus.\nFormaci\u00f3n complementaria\nM\u00e1ster en Big Data y Analytics\n\ud83d\udde3\ufe0f Ingl\u00e9s para trabajar en un entorno global.\nIngl\u00e9s\n\ud83d\ude80 \u00bfQu\u00e9 nos hace \u00fanicxs?\nAqu\u00ed no somos una empresa m\u00e1s... \u00a1somos un equipo que va a por todas! \ud83d\udca1\u2728\n\u00a1somos un equipo que va a por todas!\n\ud83d\udd39 La tecnolog\u00eda y los datos son nuestra esencia. No solo trabajamos con lo \u00faltimo del mercado, sino que te formaremos para que domines las herramientas m\u00e1s innovadoras. Contamos con un \u00e1rea de innovaci\u00f3n donde exploramos nuevas alianzas con gigantes tecnol\u00f3gicos y centros de I+D.\nLa tecnolog\u00eda y los datos son nuestra esencia.\nI+D.\n\ud83d\udd39 T\u00fa marcas el ritmo. Olv\u00eddate de estructuras r\u00edgidas. Cada cierto tiempo, te sentar\u00e1s con tu responsable para hablar de tus logros, nuevos retos y\u2026 \u00a1ajustes salariales en base a resultados!\nT\u00fa marcas el ritmo.\n\u00a1ajustes salariales en base a resultados!\n\ud83d\udd39 Desde el minuto 1, ser\u00e1s parte de nuestra familia. Aqu\u00ed nadie camina solx. Siempre habr\u00e1 un compa\u00f1erx dispuesto a ayudarte. \ud83e\udd1d\ud83d\ude03\nDesde el minuto 1, ser\u00e1s parte de nuestra familia.\n\ud83d\udd39 Aprender\u00e1s de lxs mejores. Te codear\u00e1s con cracks del sector (tanto t\u00e9cnicos como de negocio), que estar\u00e1n encantadxs de compartir su conocimiento y celebrar tu crecimiento. \ud83d\udcc8\ud83d\udd25\nAprender\u00e1s de lxs mejores.\n\ud83d\udd39 Somos una empresa con prop\u00f3sito. Creemos en la diversidad y la inclusi\u00f3n como motores de la creatividad, lo que nos permite superar cualquier desaf\u00edo. \ud83c\udf0d\ud83d\udc9c\nSomos una empresa con prop\u00f3sito.\n\ud83d\udccc Y, por si fuera poco\u2026\n\u2705 Contrato indefinido (\u00a1queremos que te quedes mucho tiempo!)\nContrato indefinido\n\u2705 Jornada intensiva en julio y agosto \ud83c\udfd6\ufe0f\nJornada intensiva en julio y agosto\n\u2705 Salario seg\u00fan tu formaci\u00f3n y experiencia\nSalario\n\u2705 Apoyo econ\u00f3mico para el teletrabajo \ud83d\udcbb\ud83c\udfe1\nApoyo econ\u00f3mico para el teletrabajo\n\u2705 Acceso a formaciones y certificaciones.\nAcceso a formaciones y certificaciones.\nSi buscas un lugar donde crecer, innovar y sentirte parte de algo grande\u2026 \u00a1este es tu sitio! \ud83d\ude80\ud83d\udca5\nMientras tanto, ent\u00e9rate de m\u00e1s: https://medium.com/sdg-group"
    },
    "4138069095": {
        "title": "Senior Data Engineer / BI",
        "company": "Logicalis Spain",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEn Logicalis Spain estamos buscando un perfil experto en iniciativas de Data Engineering / BI sobre entornos Cloud (Azure y AWS; deseable GCP, Snowflake y/o Databricks) dentro de nuestra unidad de negocio de Data & Analytics, ubicada en las oficinas de Madrid o Barcelona.\n\nEl equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\n\nFunciones y responsabilidades:\nComprender las necesidades y requisitos de los clientes y elaborar \u00edntegramente propuestas asociadas a proyectos de Data Engineering / BI para cubrir el ciclo de vida de los datos (Ingesta, Almacenamiento, Transformaci\u00f3n, Explotaci\u00f3n) bajo los paradigmas actuales del mercado. Deseable haber trabajado en entornos de datos con metodolog\u00edas de modelado DataVault y arquitecturas DataMesh.\nResponsabilizarse del enfoque de la soluci\u00f3n que mejor puede dar respuesta a las problem\u00e1ticas identificadas y mayores beneficios puede proporcionar, en oportunidades relativas a Integraci\u00f3n de Datos, Modelizaci\u00f3n y Explotaci\u00f3n.\nAcompa\u00f1ar en el dise\u00f1o de propuestas de migraci\u00f3n/modernizaci\u00f3n desde entornos on-premise a entornos Cloud, de la plataforma de datos que pueda tener el cliente.\nDefensa ante los clientes de propuestas y planteamientos.\nPlanteamiento de artefactos y componentes que industrialicen los trabajos de Data Engineering / BI (algoritmos reutilizables, c\u00f3digo en base a metadatos, procesos autom\u00e1ticos de verificaci\u00f3n de calidad, etc).\nAcompa\u00f1ar en la evoluci\u00f3n el Demo Center para contemplar prototipos y demos sobre aplicaciones de datos en cloud.\nAcompa\u00f1ar a los proyectos en las fases iniciales para garantizar y certificar el planteamiento de los proyectos que se realiz\u00f3 en fase de preventa, y ayudar en la resoluci\u00f3n de riesgos o problemas.\n\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo remoto, con flexibilidad para ir puntualmente a nuestras oficinas/clientes ubicados en Madrid o Barcelona.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados.\nEn Logicalis Spain estamos buscando un perfil experto en iniciativas de Data Engineering / BI sobre entornos Cloud (Azure y AWS; deseable GCP, Snowflake y/o Databricks) dentro de nuestra unidad de negocio de Data & Analytics, ubicada en las oficinas de Madrid o Barcelona.\nLogicalis Spain\nexperto en iniciativas de Data Engineering / BI\nEl equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\nData & Analytics de Logicalis\nFunciones y responsabilidades:\nComprender las necesidades y requisitos de los clientes y elaborar \u00edntegramente propuestas asociadas a proyectos de Data Engineering / BI para cubrir el ciclo de vida de los datos (Ingesta, Almacenamiento, Transformaci\u00f3n, Explotaci\u00f3n) bajo los paradigmas actuales del mercado. Deseable haber trabajado en entornos de datos con metodolog\u00edas de modelado DataVault y arquitecturas DataMesh.\nResponsabilizarse del enfoque de la soluci\u00f3n que mejor puede dar respuesta a las problem\u00e1ticas identificadas y mayores beneficios puede proporcionar, en oportunidades relativas a Integraci\u00f3n de Datos, Modelizaci\u00f3n y Explotaci\u00f3n.\nAcompa\u00f1ar en el dise\u00f1o de propuestas de migraci\u00f3n/modernizaci\u00f3n desde entornos on-premise a entornos Cloud, de la plataforma de datos que pueda tener el cliente.\nDefensa ante los clientes de propuestas y planteamientos.\nPlanteamiento de artefactos y componentes que industrialicen los trabajos de Data Engineering / BI (algoritmos reutilizables, c\u00f3digo en base a metadatos, procesos autom\u00e1ticos de verificaci\u00f3n de calidad, etc).\nAcompa\u00f1ar en la evoluci\u00f3n el Demo Center para contemplar prototipos y demos sobre aplicaciones de datos en cloud.\nAcompa\u00f1ar a los proyectos en las fases iniciales para garantizar y certificar el planteamiento de los proyectos que se realiz\u00f3 en fase de preventa, y ayudar en la resoluci\u00f3n de riesgos o problemas.\nComprender las necesidades y requisitos de los clientes y elaborar \u00edntegramente propuestas asociadas a proyectos de Data Engineering / BI para cubrir el ciclo de vida de los datos (Ingesta, Almacenamiento, Transformaci\u00f3n, Explotaci\u00f3n) bajo los paradigmas actuales del mercado. Deseable haber trabajado en entornos de datos con metodolog\u00edas de modelado DataVault y arquitecturas DataMesh.\nResponsabilizarse del enfoque de la soluci\u00f3n que mejor puede dar respuesta a las problem\u00e1ticas identificadas y mayores beneficios puede proporcionar, en oportunidades relativas a Integraci\u00f3n de Datos, Modelizaci\u00f3n y Explotaci\u00f3n.\nAcompa\u00f1ar en el dise\u00f1o de propuestas de migraci\u00f3n/modernizaci\u00f3n desde entornos on-premise a entornos Cloud, de la plataforma de datos que pueda tener el cliente.\nDefensa ante los clientes de propuestas y planteamientos.\nPlanteamiento de artefactos y componentes que industrialicen los trabajos de Data Engineering / BI (algoritmos reutilizables, c\u00f3digo en base a metadatos, procesos autom\u00e1ticos de verificaci\u00f3n de calidad, etc).\nAcompa\u00f1ar en la evoluci\u00f3n el Demo Center para contemplar prototipos y demos sobre aplicaciones de datos en cloud.\nAcompa\u00f1ar a los proyectos en las fases iniciales para garantizar y certificar el planteamiento de los proyectos que se realiz\u00f3 en fase de preventa, y ayudar en la resoluci\u00f3n de riesgos o problemas.\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo remoto, con flexibilidad para ir puntualmente a nuestras oficinas/clientes ubicados en Madrid o Barcelona.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados."
    },
    "4124949664": {
        "title": "Senior/Data Engineer ",
        "company": "Gameplay Galaxy",
        "location": "European Economic Area",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nDepartment: Data\n\nEmployment Type: Full Time\n\nLocation: Remote\n\nDescription\n\nWe are looking for a talented Senior / Data Engineer to join our dynamic analytics team. You'll work closely with our game teams, building the data infrastructure that powers player experiences. By designing scalable systems and ensuring robust data pipelines, you'll enable insights that drive player retention, enhance monetization strategies, and optimize user experience. If you're excited by big data challenges and have a passion for games, this is the role for you.\n\nAll The Responsibilities We'll Trust You With\n\nYou'll design and implement scalable systems: you'll develop systems capable of handling hundreds of thousands of events per second and manage databases with terabytes of data.\nYou'll build and maintain our data warehouse: you'll create ETLs that integrate data from various sources, optimizing it for analysis.\nYou'll ensure data integrity and robustness: you'll establish a single source of truth for our key performance indicators (KPIs) and make them accessible to non-technical users through advanced data visualization tools.\nYou'll develop data ingestion systems: you'll design systems to ingest streams of events from our games and data from third-party APIs.\nYou'll implement data quality checks: you'll establish rigorous data integrity protocols to ensure our data is reliable and to identify any technical issues within our games.\nYou'll deploy and maintain orchestration tools: you'll manage tools for job scheduling and workflow orchestration.\nYou'll collaborate across teams: you'll work closely with developers and data scientists to enhance our business intelligence tools and architecture.\n\nWhat You'll Bring To The Team\n\n Programming Skills: Solid experience with programming or scripting languages such as Python. \n Expertise in SQL: Proficient in optimizing queries and understanding the capabilities of both open-source and commercial databases.\n Data Warehouse Experience: Proven experience in designing and implementing large-scale Data Warehouse architectures using big data technologies like Redshift, Snowflake, Vertica, ClickHouse or BigQuery.\n ETL Architecture: Extensive experience in implementing large-scale ETL processes, supporting multiple data sources and use cases including product analytics, marketing, and executive dashboards.\n Messaging Queues: Experience with messaging systems such as RabbitMQ.\n Cloud Platform Experience: Familiarity with Google Cloud Platform, including Google Functions and Google Cloud Storage.\n Dimensional Modeling: Strong skills in designing data models tailored to specific use cases.\n Adaptability and growth: Ability to thrive in a fast-paced environment, quickly adjust to changing priorities and maintain a strong drive to learn new technologies and take on challenges.\n Detail-Oriented and Business-Minded: Focus on delivering high quality work with strong attention to detail, while keeping a clear understanding of how solutions align with broader business goals.\n Creative Thinking: Smart, creative, and pragmatic, with the ability to integrate new advancements into practical solutions.\n Communication Skills: Excellent communication skills, both written and verbal. Collaborative and proactive approach.\n\nWhy You'll Love Working With Us\n\nWe\u2019re 100% remote, work wherever you want forever!\nWe offer unlimited holidays and sick days. Take as much time off as you need, to spend time with your family and friends or just because you feel like playing games all day. We trust our team and want to offer a modern work life balance that allows you to thrive both at work and in your personal life.\nYou\u2019ll be able to make an impact, to work closely with the rest of the team and its leads to work on one of the most promising upcoming games in the mobile gaming industry and the Web3 space.\nYou\u2019ll have a comfortable work schedule and stable workload. With a variety of tasks that will allow your constant professional development.\nWe want our team to grow personally and professionally and support learning opportunities, especially with a focus around new AI tools and methods. \nWe have a culture that puts our people first. We believe in open communication, creativity and the value of unique contributions from everyone in the team.\nWe offer a competitive salary in addition to equity option plans.\nDepartment:\nEmployment Type:\nLocation:\nDescription\nSenior / Data Engineer\nAll The Responsibilities We'll Trust You With\nYou'll design and implement scalable systems: you'll develop systems capable of handling hundreds of thousands of events per second and manage databases with terabytes of data.\nYou'll build and maintain our data warehouse: you'll create ETLs that integrate data from various sources, optimizing it for analysis.\nYou'll ensure data integrity and robustness: you'll establish a single source of truth for our key performance indicators (KPIs) and make them accessible to non-technical users through advanced data visualization tools.\nYou'll develop data ingestion systems: you'll design systems to ingest streams of events from our games and data from third-party APIs.\nYou'll implement data quality checks: you'll establish rigorous data integrity protocols to ensure our data is reliable and to identify any technical issues within our games.\nYou'll deploy and maintain orchestration tools: you'll manage tools for job scheduling and workflow orchestration.\nYou'll collaborate across teams: you'll work closely with developers and data scientists to enhance our business intelligence tools and architecture.\nYou'll design and implement scalable systems: you'll develop systems capable of handling hundreds of thousands of events per second and manage databases with terabytes of data.\nYou'll build and maintain our data warehouse: you'll create ETLs that integrate data from various sources, optimizing it for analysis.\nYou'll ensure data integrity and robustness: you'll establish a single source of truth for our key performance indicators (KPIs) and make them accessible to non-technical users through advanced data visualization tools.\nYou'll develop data ingestion systems: you'll design systems to ingest streams of events from our games and data from third-party APIs.\nYou'll implement data quality checks: you'll establish rigorous data integrity protocols to ensure our data is reliable and to identify any technical issues within our games.\nYou'll deploy and maintain orchestration tools: you'll manage tools for job scheduling and workflow orchestration.\nYou'll collaborate across teams: you'll work closely with developers and data scientists to enhance our business intelligence tools and architecture.\nWhat You'll Bring To The Team\nProgramming Skills: Solid experience with programming or scripting languages such as Python. \n Expertise in SQL: Proficient in optimizing queries and understanding the capabilities of both open-source and commercial databases.\n Data Warehouse Experience: Proven experience in designing and implementing large-scale Data Warehouse architectures using big data technologies like Redshift, Snowflake, Vertica, ClickHouse or BigQuery.\n ETL Architecture: Extensive experience in implementing large-scale ETL processes, supporting multiple data sources and use cases including product analytics, marketing, and executive dashboards.\n Messaging Queues: Experience with messaging systems such as RabbitMQ.\n Cloud Platform Experience: Familiarity with Google Cloud Platform, including Google Functions and Google Cloud Storage.\n Dimensional Modeling: Strong skills in designing data models tailored to specific use cases.\n Adaptability and growth: Ability to thrive in a fast-paced environment, quickly adjust to changing priorities and maintain a strong drive to learn new technologies and take on challenges.\n Detail-Oriented and Business-Minded: Focus on delivering high quality work with strong attention to detail, while keeping a clear understanding of how solutions align with broader business goals.\n Creative Thinking: Smart, creative, and pragmatic, with the ability to integrate new advancements into practical solutions.\n Communication Skills: Excellent communication skills, both written and verbal. Collaborative and proactive approach.\nProgramming Skills: Solid experience with programming or scripting languages such as Python.\nExpertise in SQL: Proficient in optimizing queries and understanding the capabilities of both open-source and commercial databases.\nData Warehouse Experience: Proven experience in designing and implementing large-scale Data Warehouse architectures using big data technologies like Redshift, Snowflake, Vertica, ClickHouse or BigQuery.\nETL Architecture: Extensive experience in implementing large-scale ETL processes, supporting multiple data sources and use cases including product analytics, marketing, and executive dashboards.\nMessaging Queues: Experience with messaging systems such as RabbitMQ.\nCloud Platform Experience: Familiarity with Google Cloud Platform, including Google Functions and Google Cloud Storage.\nDimensional Modeling: Strong skills in designing data models tailored to specific use cases.\nAdaptability and growth: Ability to thrive in a fast-paced environment, quickly adjust to changing priorities and maintain a strong drive to learn new technologies and take on challenges.\nDetail-Oriented and Business-Minded: Focus on delivering high quality work with strong attention to detail, while keeping a clear understanding of how solutions align with broader business goals.\nCreative Thinking: Smart, creative, and pragmatic, with the ability to integrate new advancements into practical solutions.\nCommunication Skills: Excellent communication skills, both written and verbal. Collaborative and proactive approach.\nWhy You'll Love Working With Us\nWe\u2019re 100% remote, work wherever you want forever!\nWe offer unlimited holidays and sick days. Take as much time off as you need, to spend time with your family and friends or just because you feel like playing games all day. We trust our team and want to offer a modern work life balance that allows you to thrive both at work and in your personal life.\nYou\u2019ll be able to make an impact, to work closely with the rest of the team and its leads to work on one of the most promising upcoming games in the mobile gaming industry and the Web3 space.\nYou\u2019ll have a comfortable work schedule and stable workload. With a variety of tasks that will allow your constant professional development.\nWe want our team to grow personally and professionally and support learning opportunities, especially with a focus around new AI tools and methods. \nWe have a culture that puts our people first. We believe in open communication, creativity and the value of unique contributions from everyone in the team.\nWe offer a competitive salary in addition to equity option plans.\nWe\u2019re 100% remote, work wherever you want forever!\nWe offer unlimited holidays and sick days. Take as much time off as you need, to spend time with your family and friends or just because you feel like playing games all day. We trust our team and want to offer a modern work life balance that allows you to thrive both at work and in your personal life.\nYou\u2019ll be able to make an impact, to work closely with the rest of the team and its leads to work on one of the most promising upcoming games in the mobile gaming industry and the Web3 space.\nYou\u2019ll have a comfortable work schedule and stable workload. With a variety of tasks that will allow your constant professional development.\nWe want our team to grow personally and professionally and support learning opportunities, especially with a focus around new AI tools and methods.\nWe have a culture that puts our people first. We believe in open communication, creativity and the value of unique contributions from everyone in the team.\nWe offer a competitive salary in addition to equity option plans."
    },
    "4145646120": {
        "title": "Data Engineer Intern - Data Engineering Community ",
        "company": "Zurich Insurance",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Part-time",
        "description": "About the job\nOur opportunity\n\nLife is too short to just tick the box. Reimagine the box with Zurich! \n\nWe are looking for a Data Engineer Intern to join the Data Engineering team as part of the Data & Business Intelligence (DBI) department! This is an exciting opportunity as we offer:\n\nInteresting and challenging tasks in a friendly environment.\nSignificant learning experiences: a hands-on introduction to real world data processes working directly with senior data engineers.\nOpportunities to work on real world data gathering and processing techniques.\n\nDuring this experience not only you will learn and boost your career but also you will build relationships and have fun with teams all over the world!\n\nWith us you will have a real opportunity to make a difference and be part of the team!\n\nYour role\n\nAs a Data Engineer Intern, you will assist Data Engineering team with:\n\nDevelop Backend and Frontend Website for the Data Engineering Community of Practice of Zurich.\nHelp to define best practices and reusable tools and frameworks to increase the productivity of the day-to-day project\u2019s development. \nDevelop project accelerators for different Data Engineering technologies to reduce setup time of new projects.\nYou will learn about data engineering methodologies and best practices (Git, functional programming, deployment process, security topics, etc).\nLearn about data engineering cloud-based projects and services (Azure & AWS).\nLearn about docker containerized applications and Kubernetes platform.\n\nYour Skills And Experience\n\nWe would like to give this opportunity to somebody who is:\n\nPossessing some experience with Python programming language \nA basic understanding of, or a desire to learn, JavaScript and the React framework.\nFluent in English: B2\nActive student status with Spanish University/School is mandatory \nUniversity student interested in developing career in tech field \nLooking for at least 6 months part-time/internship program \nTech savvy approach \nAn enthusiastic, capable, curious person craving to learn \nA good team player with responsible and can-do attitude \n\nAdditional Information\n\nWe offer benefits such as:\n\nPaid Internship \nUp to 30 hours weekly with Flexible schedule \nLinkedIn and internal training opportunities \nSpecial discounts on different brands and products & great banking and insurance conditions. \nWelcome pack that includes amazing gadgets. \nA cool workspace with gaming, gym, and fun area! \nA variety of clubs for you to get to know your colleagues and have fun, e.g. hiking, skiing, reading and many others! \nFree coffee and fruit at the office. \n\nPrimary work location is Barcelona Poblenou.\n\nDoes it look like you have found your future job? Don\u2019t wait anymore and apply sending your CV in English!\n\nWho We Are\n\nLooking for a challenging and inspiring work environment where you can make a difference? At Zurich millions of individuals and businesses place their trust in our products and services every day. Our 53,000 employees worldwide form the basis of our success, enabling, businesses and communities to face a world of risk with confidence. Imagine if you could help people do this all over the world. You\u2019d give them confidence and reassurance by protecting what they love most. It\u2019s a big challenge, but you will be supported by a world-class team who believe in helping you to reach your full potential and deliver on our promises.\n\nDiversity & Inclusion\n\nAt Zurich we are an equal opportunity employer. We attract and retain the best qualified individuals available, without regard to race/ethnicity, religion, gender, sexual orientation, age or disability.\n\nSo be challenged. Be inspired. Help us make a difference.\n\nYou are the heart & soul of Zurich! \n\nAt Zurich, we like to think outside the box and challenge the status quo. We take an optimistic approach by focusing on the positives and constantly asking What can go right?\n\nWe highly value the experience and know-how of our employees and offer a wide range of opportunities across business areas to encourage you to apply for new opportunities within Zurich when you are ready for your next career step.\n\nLet\u2019s continue to grow together!\n\nLocation(s): ES - Barcelona \nSchedule: Part Time\nRecruiter name: Alba Marin Sola\nOur opportunity\nLife is too short to just tick the box. Reimagine the box with Zurich!\nInteresting and challenging tasks in a friendly environment.\nSignificant learning experiences: a hands-on introduction to real world data processes working directly with senior data engineers.\nOpportunities to work on real world data gathering and processing techniques.\nInteresting and challenging tasks in a friendly environment.\nSignificant learning experiences: a hands-on introduction to real world data processes working directly with senior data engineers.\nOpportunities to work on real world data gathering and processing techniques.\nYour role\nAs a Data Engineer Intern, you will assist Data Engineering team with:\nDevelop Backend and Frontend Website for the Data Engineering Community of Practice of Zurich.\nHelp to define best practices and reusable tools and frameworks to increase the productivity of the day-to-day project\u2019s development. \nDevelop project accelerators for different Data Engineering technologies to reduce setup time of new projects.\nYou will learn about data engineering methodologies and best practices (Git, functional programming, deployment process, security topics, etc).\nLearn about data engineering cloud-based projects and services (Azure & AWS).\nLearn about docker containerized applications and Kubernetes platform.\nDevelop Backend and Frontend Website for the Data Engineering Community of Practice of Zurich.\nHelp to define best practices and reusable tools and frameworks to increase the productivity of the day-to-day project\u2019s development.\nDevelop project accelerators for different Data Engineering technologies to reduce setup time of new projects.\nYou will learn about data engineering methodologies and best practices (Git, functional programming, deployment process, security topics, etc).\nLearn about data engineering cloud-based projects and services (Azure & AWS).\nLearn about docker containerized applications and Kubernetes platform.\nYour Skills And Experience\nWe would like to give this opportunity to somebody who is\nPossessing some experience with Python programming language \nA basic understanding of, or a desire to learn, JavaScript and the React framework.\nFluent in English: B2\nActive student status with Spanish University/School is mandatory \nUniversity student interested in developing career in tech field \nLooking for at least 6 months part-time/internship program \nTech savvy approach \nAn enthusiastic, capable, curious person craving to learn \nA good team player with responsible and can-do attitude\nPossessing some experience with Python programming language\nA basic understanding of, or a desire to learn, JavaScript and the React framework.\nFluent in English: B2\nActive student status with Spanish University/School is mandatory\nUniversity student interested in developing career in tech field\nLooking for at least 6 months part-time/internship program\nTech savvy approach\nAn enthusiastic, capable, curious person craving to learn\nA good team player with responsible and can-do attitude\nAdditional Information\nWe offer benefits such as:\nPaid Internship \nUp to 30 hours weekly with Flexible schedule \nLinkedIn and internal training opportunities \nSpecial discounts on different brands and products & great banking and insurance conditions. \nWelcome pack that includes amazing gadgets. \nA cool workspace with gaming, gym, and fun area! \nA variety of clubs for you to get to know your colleagues and have fun, e.g. hiking, skiing, reading and many others! \nFree coffee and fruit at the office.\nPaid Internship\nUp to 30 hours weekly with Flexible schedule\nLinkedIn and internal training opportunities\nSpecial discounts on different brands and products & great banking and insurance conditions.\nWelcome pack that includes amazing gadgets.\nA cool workspace with gaming, gym, and fun area!\nA variety of clubs for you to get to know your colleagues and have fun, e.g. hiking, skiing, reading and many others!\nFree coffee and fruit at the office.\nPrimary work location is Barcelona Poblenou\nWho We Are\nDiversity & Inclusion\nSo be challenged. Be inspired. Help us make a difference.\n.\nYou are the heart & soul of Zurich!\nLocation(s): ES - Barcelona \nSchedule: Part Time\nRecruiter name: Alba Marin Sola\nLocation(s): ES - Barcelona\nSchedule: Part Time\nRecruiter name: Alba Marin Sola"
    },
    "4169621071": {
        "title": "Lead Data & Systems Engineer \u2013 Growth & Lead Acquisition (data scraping & enrichment, Java, Python, React..) ",
        "company": "Groupon",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nGroupon is a marketplace where customers discover new experiences and services everyday and local businesses thrive. To date we have worked with over a million merchant partners worldwide, connecting over 16 million customers with deals across various categories. In a world often dominated by e-commerce giants, we stand out as one of the few platforms uniquely committed to helping local businesses succeed on a performance basis.\n\nGroupon is on a radical journey to transform our business with relentless pursuit of results. Even with thousands of employees spread across multiple continents, we still maintain a culture that inspires innovation, rewards risk-taking and celebrates success. The impact here can be immediate due to our scale and the speed of our transformation. We're a \"best of both worlds\" kind of company. We're big enough to have the resources and scale, but small enough that a single person has a surprising amount of autonomy and can make a meaningful impact.\n\nAbout The Role\n\nWe are searching for a flexible, business-savvy engineer to own our end-to-end LeadGen and Lead Allocation pipelines. You should be comfortable working across a variety of languages (Java, Python, TypeScript, etc.) and adept at rapidly prototyping solutions to meet business needs. Think of this role as \u201cmini-CTO\u201d for our lead acquisition and enrichment processes, where you\u2019ll ensure high-quality leads flow seamlessly from external sources to internal systems like Salesforce.\n\nWhy This Role Is Different:\n\nRather than seeking a narrowly focused Java developer, we need a creative problem-solver who can build and integrate data pipelines, orchestrate workflows, perform large-scale data transformations, and quickly pivot between technologies. If you thrive on learning new APIs, building POCs in days (not weeks), and delivering production-grade systems, this position is for you.\n\nYou'll Spend Time on the Following:\n\n\ud83d\udca1 End-to-End System Design\n\nArchitect, develop, and maintain microservices or stand-alone scripts for lead acquisition and data enrichment. \nFlex between Java/Spring Boot, Python scripting, or TypeScript-based frameworks depending on project needs. \nOwn the entire lifecycle: from initial prototype to scalable, production-ready deployments. \n\n\u2699\ufe0f Workflow Orchestration & Automation\n\nSet up and maintain workflow orchestration using tools like Temporal, Airflow, or similar. \nDesign resilient pipelines for data scraping, transformation, deduplication, and enrichment. \nBuild in safeguards for data consistency and error handling, ensuring minimal downtime. \n\n\ud83d\udd0d Data Acquisition & Enrichment\n\nIntegrate with various external APIs (e.g., Apify, web scraping solutions) and internal data warehouses (EDW, BigQuery). \nEnsure data is enriched, cleaned, and deduplicated, leveraging advanced matching or ML-based algorithms. \nPull in real-time or batch data from multiple sources to create a single, unified lead record. \n\n\ud83d\ude80 Prototype Fast, Scale Effectively\n\nRapidly develop proof-of-concept solutions to validate business ideas. \nGather feedback from stakeholders, iterate quickly, and prepare for production deployment once validated. \nOwn the \u201cfail fast, learn fast\u201d approach\u2014optimizing for both agility and reliability. \n\n\ud83d\udee0 Infrastructure & Deployment\n\nContainerize applications or scripts using Docker; collaborate on orchestration (Kubernetes, Docker Swarm, etc.). \nParticipate in CI/CD processes, ensuring automated builds, tests, and deployments. \nMonitor and troubleshoot system performance, actively proposing optimizations. \n\n\ud83d\udcc8 Business Collaboration & Ownership\n\nTranslate business objectives and lead-gen strategies into technical projects. \nPartner with product managers, marketing, and sales stakeholders to refine requirements and roadmap. \nServe as the \"go-to\" technical advisor for anything related to lead data flows, from concept to final delivery. \n\n\ud83d\udcbe Database & Analytics\n\nWork with PostgreSQL or other relational databases to model entities, handle migrations, and tune performance. \nIntegrate BigQuery for large-scale data processing and analytics. \nExplore advanced queries or analytics to enhance lead scoring and enrichment logic. \n\n\ud83d\udd10 Security & Best Practices\n\nImplement secure coding patterns, data validations, and compliance measures for sensitive data. \nConduct code reviews, encourage testing automation, and champion clean coding standards. \n\n\ud83d\udcbb Frontend (Optional, But Nice to Have)\n\nIf comfortable, contribute to or lead the development of a future UI in React, Angular, or Vue.js. \nCollaborate with product and UX teams to ensure intuitive, seamless user interactions with lead data. \n\nSkills & Experience Required:\n\n\u2705 Technical Breadth:\n\nComfort with multiple programming languages (e.g., Java, Python, React). \nExperience building RESTful APIs, data pipelines, and containerized microservices. \n\n\u2705 Data Mastery:\n\nFamiliarity with data manipulation at scale: scraping, ETL pipelines, schema design, and deduplication strategies. \nKnowledge of workflow orchestration platforms (Temporal, Airflow, or similar). \n\n\u2705 Business Mindset:\n\nAbility to understand growth objectives and translate them into actionable tech solutions. \nExperience working directly with non-technical stakeholders (Sales, Marketing) and turning requirements into specs. \n\n\u2705 Ownership & Leadership:\n\nComfortable making architecture decisions and justifying trade-offs to both technical and business teams. \nAble to take initiative and drive features from concept to completion with minimal oversight. \n\n\ud83c\udfaf Bonus Points If You Have:\n\nExperience with DevOps practices, Kubernetes, or advanced container orchestration. \nFamiliarity with front-end frameworks (React, Angular, or Vue.js). \nProven track record of rapidly creating MVPs or PoCs and iterating into full production systems. \n\nGroupon\u2019s purpose is to build strong communities through thriving small businesses. To learn more about the world\u2019s largest local e-commerce marketplace, click here. You can also find out more about us in the latest Groupon news as well as learning about our DEI approach. If all of this sounds like something that\u2019s a great fit for you, then click apply and join us on a mission to become the ultimate destination for local experiences and services.\n\nBeware of Recruitment Fraud: Groupon follows a merit-based recruitment process without charging job seekers any fees. We've noticed an increase in recruitment fraud, including fake job postings and fraudulent interviews and job offers aimed at stealing personal information or money. Be cautious of individuals falsely representing Groupon's Talent Acquisition team with fake job offers. If you encounter any suspicious job offers or interview calls demanding money, recognize these as scams. Groupon is not responsible for losses from such dealings. For legitimate job openings (and a sneak peek into life at Groupon), always check our official career website at grouponcareers.com.\nAbout The Role\nflexible, business-savvy engineer\n\u201cmini-CTO\u201d\nWhy This Role Is Different:\ncreative problem-solver\nYou'll Spend Time on the Following:\nEnd-to-End System Design\nArchitect, develop, and maintain microservices or stand-alone scripts for lead acquisition and data enrichment. \nFlex between Java/Spring Boot, Python scripting, or TypeScript-based frameworks depending on project needs. \nOwn the entire lifecycle: from initial prototype to scalable, production-ready deployments.\nArchitect, develop, and maintain microservices or stand-alone scripts for lead acquisition and data enrichment.\nFlex between Java/Spring Boot, Python scripting, or TypeScript-based frameworks depending on project needs.\nOwn the entire lifecycle: from initial prototype to scalable, production-ready deployments.\nWorkflow Orchestration & Automation\nSet up and maintain workflow orchestration using tools like Temporal, Airflow, or similar. \nDesign resilient pipelines for data scraping, transformation, deduplication, and enrichment. \nBuild in safeguards for data consistency and error handling, ensuring minimal downtime.\nSet up and maintain workflow orchestration using tools like Temporal, Airflow, or similar.\nDesign resilient pipelines for data scraping, transformation, deduplication, and enrichment.\nBuild in safeguards for data consistency and error handling, ensuring minimal downtime.\nData Acquisition & Enrichment\nIntegrate with various external APIs (e.g., Apify, web scraping solutions) and internal data warehouses (EDW, BigQuery). \nEnsure data is enriched, cleaned, and deduplicated, leveraging advanced matching or ML-based algorithms. \nPull in real-time or batch data from multiple sources to create a single, unified lead record.\nIntegrate with various external APIs (e.g., Apify, web scraping solutions) and internal data warehouses (EDW, BigQuery).\nEnsure data is enriched, cleaned, and deduplicated, leveraging advanced matching or ML-based algorithms.\nPull in real-time or batch data from multiple sources to create a single, unified lead record.\nPrototype Fast, Scale Effectively\nRapidly develop proof-of-concept solutions to validate business ideas. \nGather feedback from stakeholders, iterate quickly, and prepare for production deployment once validated. \nOwn the \u201cfail fast, learn fast\u201d approach\u2014optimizing for both agility and reliability.\nRapidly develop proof-of-concept solutions to validate business ideas.\nGather feedback from stakeholders, iterate quickly, and prepare for production deployment once validated.\nOwn the \u201cfail fast, learn fast\u201d approach\u2014optimizing for both agility and reliability.\nInfrastructure & Deployment\nContainerize applications or scripts using Docker; collaborate on orchestration (Kubernetes, Docker Swarm, etc.). \nParticipate in CI/CD processes, ensuring automated builds, tests, and deployments. \nMonitor and troubleshoot system performance, actively proposing optimizations.\nContainerize applications or scripts using Docker; collaborate on orchestration (Kubernetes, Docker Swarm, etc.).\nParticipate in CI/CD processes, ensuring automated builds, tests, and deployments.\nMonitor and troubleshoot system performance, actively proposing optimizations.\nBusiness Collaboration & Ownership\nTranslate business objectives and lead-gen strategies into technical projects. \nPartner with product managers, marketing, and sales stakeholders to refine requirements and roadmap. \nServe as the \"go-to\" technical advisor for anything related to lead data flows, from concept to final delivery.\nTranslate business objectives and lead-gen strategies into technical projects.\nPartner with product managers, marketing, and sales stakeholders to refine requirements and roadmap.\nServe as the \"go-to\" technical advisor for anything related to lead data flows, from concept to final delivery.\nDatabase & Analytics\nWork with PostgreSQL or other relational databases to model entities, handle migrations, and tune performance. \nIntegrate BigQuery for large-scale data processing and analytics. \nExplore advanced queries or analytics to enhance lead scoring and enrichment logic.\nWork with PostgreSQL or other relational databases to model entities, handle migrations, and tune performance.\nIntegrate BigQuery for large-scale data processing and analytics.\nExplore advanced queries or analytics to enhance lead scoring and enrichment logic.\nSecurity & Best Practices\nImplement secure coding patterns, data validations, and compliance measures for sensitive data. \nConduct code reviews, encourage testing automation, and champion clean coding standards.\nImplement secure coding patterns, data validations, and compliance measures for sensitive data.\nConduct code reviews, encourage testing automation, and champion clean coding standards.\nFrontend (Optional, But Nice to Have)\nIf comfortable, contribute to or lead the development of a future UI in React, Angular, or Vue.js. \nCollaborate with product and UX teams to ensure intuitive, seamless user interactions with lead data.\nIf comfortable, contribute to or lead the development of a future UI in React, Angular, or Vue.js.\nCollaborate with product and UX teams to ensure intuitive, seamless user interactions with lead data.\nSkills & Experience Required:\nTechnical Breadth:\nComfort with multiple programming languages (e.g., Java, Python, React). \nExperience building RESTful APIs, data pipelines, and containerized microservices.\nComfort with multiple programming languages (e.g., Java, Python, React).\nExperience building RESTful APIs, data pipelines, and containerized microservices.\nData Mastery:\nFamiliarity with data manipulation at scale: scraping, ETL pipelines, schema design, and deduplication strategies. \nKnowledge of workflow orchestration platforms (Temporal, Airflow, or similar).\nFamiliarity with data manipulation at scale: scraping, ETL pipelines, schema design, and deduplication strategies.\nKnowledge of workflow orchestration platforms (Temporal, Airflow, or similar).\nBusiness Mindset:\nAbility to understand growth objectives and translate them into actionable tech solutions. \nExperience working directly with non-technical stakeholders (Sales, Marketing) and turning requirements into specs.\nAbility to understand growth objectives and translate them into actionable tech solutions.\nExperience working directly with non-technical stakeholders (Sales, Marketing) and turning requirements into specs.\nOwnership & Leadership:\nComfortable making architecture decisions and justifying trade-offs to both technical and business teams. \nAble to take initiative and drive features from concept to completion with minimal oversight.\nComfortable making architecture decisions and justifying trade-offs to both technical and business teams.\nAble to take initiative and drive features from concept to completion with minimal oversight.\nBonus Points If You Have:\nExperience with DevOps practices, Kubernetes, or advanced container orchestration. \nFamiliarity with front-end frameworks (React, Angular, or Vue.js). \nProven track record of rapidly creating MVPs or PoCs and iterating into full production systems.\nExperience with DevOps practices, Kubernetes, or advanced container orchestration.\nFamiliarity with front-end frameworks (React, Angular, or Vue.js).\nProven track record of rapidly creating MVPs or PoCs and iterating into full production systems.\nGroupon\nBeware of Recruitment Fraud:"
    },
    "4138642067": {
        "title": "Data Engineer Intern - Claims Team ",
        "company": "Zurich Insurance",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Part-time",
        "description": "About the job\nOur opportunity \u2013 Data Engineer intern \u2013 DBI Claims VS\n\nLife is too short to just tick the box. Reimagine the box with Zurich! \n\nWe are excited to offer an internship opportunity within the Data & Business Intelligence team, specifically in the Claims Value Stream. As an intern, you will join a dynamic team of four experienced Data Engineers dedicated to developing innovative data solutions for the Claims area.\n\nThis internship provides a unique chance to gain hands-on experience in the field of data engineering while contributing to impactful projects. You will be directly involved in new developments, assisting the team in creating and optimizing data pipelines, ensuring data quality, and implementing advanced data solutions that drive our Claims operations forward. Additionally, you will play a crucial role in documenting all tables and processes, which is essential for maintaining transparency and facilitating future developments.\n\nDuring this experience not only you will learn and boost your career but also you will build relationships and have fun with teams all over the world!\n\nWith us you will have a real opportunity to make a difference and be part of the team!\n\nYour role\n\nJoining our team means stepping into a fast-paced environment where attention to detail and a proactive mindset are key. You will encounter complex data structures and processes that require analytical thinking and problem-solving skills. The challenge lies in not only supporting daily BAU operations but also in continuously learning and adapting to new technologies and methodologies in the ever-evolving field of data engineering.\n\nAs a Data Engineer intern, you will have the oportunity to:\n\nGain practical experience working with real-world data and advanced data engineering tools.\nDevelop your technical skills in SQL, DBT, data modeling, and ETL processes.\nLearn from and collaborate with experienced Data Engineers in a supportive team environment.\nContribute to meaningful projects that directly impact our Claims operations.\nEnhance your problem-solving and analytical thinking abilities.\nUnderstand the importance of data documentation and how it supports long-term data management.\n\nYour Skills And Experience\n\nWe would like to give this opportunity to somebody who is:\n\nBasic understanding of SQL and programming languages such as Python.\nFamiliarity with data modeling concepts and ETL processes.\nStrong analytical and problem-solving skills.\nExcellent attention to detail and a proactive approach to learning.\nGood communication skills and the ability to work collaboratively in a team environment.\nEnthusiasm for working with data and a desire to develop a career in data engineering.\nActive student status with Spanish University/School is mandatory \nUniversity student interested in developing career in tech field \nLooking for at least 6 months part-time/internship program\nTech savvy approach \nAn enthusiastic, capable, curious person craving to learn \nA good team player with responsible and can-do attitude \n\nAdditional Information\n\nWe offer benefits such as:\n\nPaid Internship \nUp to 30 hours weekly with Flexible schedule \nLinkedIn and internal training opportunities \nSpecial discounts on different brands and products & great banking and insurance conditions. \nWelcome pack that includes amazing gadgets. \nA cool workspace with gaming, gym, and fun area! \nA variety of clubs for you to get to know your colleagues and have fun, e.g. hiking, skiing, reading and many others! \nFree coffee and fruit at the office. \n\nPrimary work location is Barcelona Poblenou.\n\nDoes it look like you have found your future job? Don\u2019t wait anymore and apply sending your CV in English!\n\nWho We Are\n\nLooking for a challenging and inspiring work environment where you can make a difference? At Zurich millions of individuals and businesses place their trust in our products and services every day. Our 53,000 employees worldwide form the basis of our success, enabling, businesses and communities to face a world of risk with confidence. Imagine if you could help people do this all over the world. You\u2019d give them confidence and reassurance by protecting what they love most. It\u2019s a big challenge, but you will be supported by a world-class team who believe in helping you to reach your full potential and deliver on our promises.\n\nDiversity & Inclusion\n\nAt Zurich we are an equal opportunity employer. We attract and retain the best qualified individuals available, without regard to race/ethnicity, religion, gender, sexual orientation, age or disability.\n\nSo be challenged. Be inspired. Help us make a difference.\n\nYou are the heart & soul of Zurich! \n\nAt Zurich, we like to think outside the box and challenge the status quo. We take an optimistic approach by focusing on the positives and constantly asking What can go right?\n\nWe highly value the experience and know-how of our employees and offer a wide range of opportunities across business areas to encourage you to apply for new opportunities within Zurich when you are ready for your next career step.\n\nLet\u2019s continue to grow together!\n\nLocation(s): ES - Barcelona \nSchedule: Part Time\nRecruiter name: Alba Marin Sola\nOur opportunity \u2013 Data Engineer intern \u2013 DBI Claims VS\nLife is too short to just tick the box. Reimagine the box with Zurich!\nYour role\nData Engineer intern\nGain practical experience working with real-world data and advanced data engineering tools.\nDevelop your technical skills in SQL, DBT, data modeling, and ETL processes.\nLearn from and collaborate with experienced Data Engineers in a supportive team environment.\nContribute to meaningful projects that directly impact our Claims operations.\nEnhance your problem-solving and analytical thinking abilities.\nUnderstand the importance of data documentation and how it supports long-term data management.\nGain practical experience working with real-world data and advanced data engineering tools.\nDevelop your technical skills in SQL, DBT, data modeling, and ETL processes.\nLearn from and collaborate with experienced Data Engineers in a supportive team environment.\nContribute to meaningful projects that directly impact our Claims operations.\nEnhance your problem-solving and analytical thinking abilities.\nUnderstand the importance of data documentation and how it supports long-term data management.\nYour Skills And Experience\nWe would like to give this opportunity to somebody who is\nBasic understanding of SQL and programming languages such as Python.\nFamiliarity with data modeling concepts and ETL processes.\nStrong analytical and problem-solving skills.\nExcellent attention to detail and a proactive approach to learning.\nGood communication skills and the ability to work collaboratively in a team environment.\nEnthusiasm for working with data and a desire to develop a career in data engineering.\nActive student status with Spanish University/School is mandatory \nUniversity student interested in developing career in tech field \nLooking for at least 6 months part-time/internship program\nTech savvy approach \nAn enthusiastic, capable, curious person craving to learn \nA good team player with responsible and can-do attitude\nBasic understanding of SQL and programming languages such as Python.\nFamiliarity with data modeling concepts and ETL processes.\nStrong analytical and problem-solving skills.\nExcellent attention to detail and a proactive approach to learning.\nGood communication skills and the ability to work collaboratively in a team environment.\nEnthusiasm for working with data and a desire to develop a career in data engineering.\nActive student status with Spanish University/School is mandatory\nUniversity student interested in developing career in tech field\nLooking for at least 6 months part-time/internship program\nTech savvy approach\nAn enthusiastic, capable, curious person craving to learn\nA good team player with responsible and can-do attitude\nAdditional Information\nWe offer benefits such as:\nPaid Internship \nUp to 30 hours weekly with Flexible schedule \nLinkedIn and internal training opportunities \nSpecial discounts on different brands and products & great banking and insurance conditions. \nWelcome pack that includes amazing gadgets. \nA cool workspace with gaming, gym, and fun area! \nA variety of clubs for you to get to know your colleagues and have fun, e.g. hiking, skiing, reading and many others! \nFree coffee and fruit at the office.\nPaid Internship\nUp to 30 hours weekly with Flexible schedule\nLinkedIn and internal training opportunities\nSpecial discounts on different brands and products & great banking and insurance conditions.\nWelcome pack that includes amazing gadgets.\nA cool workspace with gaming, gym, and fun area!\nA variety of clubs for you to get to know your colleagues and have fun, e.g. hiking, skiing, reading and many others!\nFree coffee and fruit at the office.\nPrimary work location is Barcelona Poblenou\nWho We Are\nDiversity & Inclusion\nSo be challenged. Be inspired. Help us make a difference.\n.\nYou are the heart & soul of Zurich!\nLocation(s): ES - Barcelona \nSchedule: Part Time\nRecruiter name: Alba Marin Sola\nLocation(s): ES - Barcelona\nSchedule: Part Time\nRecruiter name: Alba Marin Sola"
    },
    "4172319265": {
        "title": "Data Engineer Datastage Junior",
        "company": "Logicalis Spain",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEn Logicalis Spain estamos buscando varios perfiles de Data Engineer Datastage Junior para integrarse en nuestra BU de Data & Analytics. El equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\n\nREQUISITOS T\u00c9CNICOS\nExperiencia en el desarrollo de procesos ETL (1-2 a\u00f1os).\nExperiencia en el uso de la herramienta IBM DataStage (1 a\u00f1o).\nExperiencia en el uso de bases de datos relacionales SQL (PostgreSQL, MySQL).\nValorable experiencia en el uso de bases de datos no relacionales NoSQL de tipo documental (MongoDb).\nValorable conocimientos de la herramienta IBM Knowledge Catalog.\nValorable experiencia en la operativizaci\u00f3n de procesos ETL.\nValorable experiencia en programaci\u00f3n en el lenguaje Python.\nMuy valorable contar con alguna certificaci\u00f3n.\n\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo 100% remoto, desde cualquier punto de Espa\u00f1a.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados.\n> Buen ambiente de trabajo y entorno muy colaborativo.\nEn Logicalis Spain estamos buscando varios perfiles de Data Engineer Datastage Junior para integrarse en nuestra BU de Data & Analytics. El equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\nLogicalis Spain\nData Engineer Datastage Junior\nData & Analytics\nREQUISITOS T\u00c9CNICOS\nExperiencia en el desarrollo de procesos ETL (1-2 a\u00f1os).\nExperiencia en el uso de la herramienta IBM DataStage (1 a\u00f1o).\nExperiencia en el uso de bases de datos relacionales SQL (PostgreSQL, MySQL).\nValorable experiencia en el uso de bases de datos no relacionales NoSQL de tipo documental (MongoDb).\nValorable conocimientos de la herramienta IBM Knowledge Catalog.\nValorable experiencia en la operativizaci\u00f3n de procesos ETL.\nValorable experiencia en programaci\u00f3n en el lenguaje Python.\nMuy valorable contar con alguna certificaci\u00f3n.\nExperiencia en el desarrollo de procesos ETL (1-2 a\u00f1os).\nExperiencia en el uso de la herramienta IBM DataStage (1 a\u00f1o).\nExperiencia en el uso de bases de datos relacionales SQL (PostgreSQL, MySQL).\nValorable experiencia en el uso de bases de datos no relacionales NoSQL de tipo documental (MongoDb).\nValorable conocimientos de la herramienta IBM Knowledge Catalog.\nValorable experiencia en la operativizaci\u00f3n de procesos ETL.\nValorable experiencia en programaci\u00f3n en el lenguaje Python.\nMuy valorable contar con alguna certificaci\u00f3n.\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo 100% remoto, desde cualquier punto de Espa\u00f1a.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados.\n> Buen ambiente de trabajo y entorno muy colaborativo."
    },
    "4104386099": {
        "title": "Data Engineer (Barcelona)",
        "company": "SlashMobility",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfDe qu\u00e9 va la empresa en la que trabajar\u00e1s? \n\nSomos Slashmobility una empresa boutique especializada en transformaci\u00f3n digital y comprometida con las personas. Nuestro prop\u00f3sito se centra en acompa\u00f1ar a nuestros clientes en su impulso a la aceleraci\u00f3n digital tecnol\u00f3gica a trav\u00e9s de las apps y el talento IT.\n\nCon +14 a\u00f1os de experiencia, un equipo comprometido, +500 proyectos ejecutados con \u00e9xito, hemos acompa\u00f1ado a centenares de clientes l\u00edderes en su sector desde la confianza, flexibilidad y solidez a enfrentarse a los desaf\u00edos de la aceleraci\u00f3n digital de la sociedad.\n\n\u00bfY qu\u00e9 nos hace especiales? \n\nNos basamos en cuatro valores fundamentales:\n\nInnovaci\u00f3n, ofreciendo soluciones innovadoras de una manera profesional y rentable con un valor diferencial: el conocimiento experto en toda la cadena de valor.\nCompromiso ya que llevamos como propios los objetivos de la organizaci\u00f3n y superamos cualquier obst\u00e1culo para lograrlos.\nProactividad, proveyendo y realizando acciones para mejorar sus servicios y calidad.\nTrabajo en equipo, contando con un equipo motivado y multidisciplinar que trabaja en perfecta coordinaci\u00f3n para lograr los objetivos comunes.\n\n\nRequisitos:\n\nPara proyecto estable se requiere un perfil DATA ENGINEER.\n\n\u00bfCu\u00e1les son los REQUISITOS clave para este puesto?\n\nETL (Extract Transform Load)\nLibrerias Python.\nBBDD SQL\n\n\nIDIOMAS: Ingl\u00e9s intermedio o avanzado.\n\n\u00bfQu\u00e9 podemos ofrecerte?\n\nContrato indefinido.\nModalidad de trabajo: H\u00edbrida (Barcelona)\nProyecto Estable.\nExpectativas salariales: seg\u00fan la experiencia demostrada.\n\n\nSi est\u00e1s interesado/a, \u00a1m\u00e1ndanos tu cv!\n\u00bfDe qu\u00e9 va la empresa en la que trabajar\u00e1s?\nimpulso a la aceleraci\u00f3n digital\n14 a\u00f1os de experiencia\ncomprometido\n\u00e9xito\nclientes l\u00edderes\nconfianza, flexibilidad y solidez\n\u00bfY qu\u00e9 nos hace especiales?\nInnovaci\u00f3n, ofreciendo soluciones innovadoras de una manera profesional y rentable con un valor diferencial: el conocimiento experto en toda la cadena de valor.\nCompromiso ya que llevamos como propios los objetivos de la organizaci\u00f3n y superamos cualquier obst\u00e1culo para lograrlos.\nProactividad, proveyendo y realizando acciones para mejorar sus servicios y calidad.\nTrabajo en equipo, contando con un equipo motivado y multidisciplinar que trabaja en perfecta coordinaci\u00f3n para lograr los objetivos comunes.\nInnovaci\u00f3n, ofreciendo soluciones innovadoras de una manera profesional y rentable con un valor diferencial: el conocimiento experto en toda la cadena de valor.\nCompromiso ya que llevamos como propios los objetivos de la organizaci\u00f3n y superamos cualquier obst\u00e1culo para lograrlos.\nProactividad, proveyendo y realizando acciones para mejorar sus servicios y calidad.\nTrabajo en equipo, contando con un equipo motivado y multidisciplinar que trabaja en perfecta coordinaci\u00f3n para lograr los objetivos comunes.\nETL (Extract Transform Load)\nLibrerias Python.\nBBDD SQL\nETL (Extract Transform Load)\nLibrerias Python.\nBBDD SQL\nContrato indefinido.\nModalidad de trabajo: H\u00edbrida (Barcelona)\nProyecto Estable.\nExpectativas salariales: seg\u00fan la experiencia demostrada.\nContrato indefinido.\nModalidad de trabajo: H\u00edbrida (Barcelona)\nProyecto Estable.\nExpectativas salariales: seg\u00fan la experiencia demostrada."
    },
    "4169733896": {
        "title": "Data Engineer (beca) ",
        "company": "Capitole",
        "location": "Tres Cantos, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nCapitole es una de las mejores consultoras IT y el lugar en el que quieres estar. \u00bfPor qu\u00e9?\n\n\ud83e\udd1d Nuestra gente lo primero. Ponemos a las personas en el centro de todo lo que hacemos\n\ud83d\udc68\u200d\ud83d\udcbb Proyectos interesantes. Tecnolog\u00edas de vanguardia. Metodolog\u00edas \u00e1giles\n\ud83d\ude01 Felicidad y baja tasa de rotaci\u00f3n\n\ud83c\udf93 1200\u20ac anuales de formaci\u00f3n a la carta\n\u231a Horario flexible\n\ud83c\udf0f M\u00e1s de 950 profesionales preparados, con hasta 27 nacionalidades\n\ud83c\udfaf Trayectoria profesional a medida\n\ud83d\udcc6 Seguimientos mensuales. Balance anual con evaluaci\u00f3n 360\u00ba\n\ud83e\ude7a Seguro m\u00e9dico privado\n\ud83d\udcb0 Retribuci\u00f3n flexible\n\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Wellhub: apoyo para la actividad f\u00edsica, el bienestar y la salud mental\n\ud83d\udce2 Comunidades t\u00e9cnicas\n\ud83e\udd73 Un mont\u00f3n de eventos incre\u00edbles\n\nEstamos buscando un/a Data Engineer (beca) con formaci\u00f3n en Ingenier\u00eda, valorable postgrado o especializaci\u00f3n en Data Engineering / Big Data, para formarse en:\n\nIngenier\u00eda de Datos: Experiencia en la construcci\u00f3n y gesti\u00f3n de pipelines de datos (ETL: Extract, Transform, Load) para asegurar la disponibilidad y calidad de los datos.\nBases de Datos y Almacenamiento: Conocimiento del dise\u00f1o, implementaci\u00f3n y mantenimiento de bases de datos, tanto relacionales como no relacionales, y soluciones de almacenamiento como data warehouses y data lakes.\nProgramaci\u00f3n y Scripting: Habilidad en lenguajes de programaci\u00f3n como Python, Java o Scala, y en scripting para automatizar procesos y optimizar flujos de trabajo.\nSistemas Distribuidos y Big Data: Familiaridad con tecnolog\u00edas de procesamiento de grandes vol\u00famenes de datos como Hadoop, Spark y sistemas de mensajer\u00eda como Kafka o MQ.\nGesti\u00f3n de la Calidad de los Datos: Implementaci\u00f3n de mecanismos para asegurar la integridad, consistencia y precisi\u00f3n de los datos a lo largo de su ciclo de vida (CdV).\n\nHorario: Flexible, 40 horas a la semana, jornada intensiva los viernes y en verano\nModalidad: H\u00edbrida, asistencia a oficinas 3 d\u00edas a la semana, 2 d\u00edas en remoto\nUbicaci\u00f3n: Tres Cantos, Madrid\nSalario: 1000\u20ac durante 9 meses, con posibilidad de pasar a contrato indefinido despu\u00e9s de esee periodo\n\nQui\u00e9nes somos \ud83d\udc47\nCapitole Consulting\n\nMira lo que dicen de nosotros \ud83d\udc47\nGlassdoor Reviews\n\nSi\u00e9ntete libre de mandarnos tu perfil. \u00a1Estamos deseando conocerte!\ud83d\udc99\n\nEl empleado/a se adherir\u00e1 a las pol\u00edticas de seguridad de la informaci\u00f3n:\n- Tendr\u00e1 acceso a la informaci\u00f3n confidencial relativa a Capitole y al proyecto en el que trabaja.\n- Tendr\u00e1 que cumplir las pol\u00edticas de seguridad y las pol\u00edticas internas de la empresa y cliente.\n- Tendr\u00e1 que firmar un NDA.\nCapitole es una de las mejores consultoras IT y el lugar en el que quieres estar. \u00bfPor qu\u00e9?\n\ud83e\udd1d Nuestra gente lo primero. Ponemos a las personas en el centro de todo lo que hacemos\n\ud83d\udc68\u200d\ud83d\udcbb Proyectos interesantes. Tecnolog\u00edas de vanguardia. Metodolog\u00edas \u00e1giles\n\ud83d\ude01 Felicidad y baja tasa de rotaci\u00f3n\n\ud83c\udf93 1200\u20ac anuales de formaci\u00f3n a la carta\n\u231a Horario flexible\n\ud83c\udf0f M\u00e1s de 950 profesionales preparados, con hasta 27 nacionalidades\n\ud83c\udfaf Trayectoria profesional a medida\n\ud83d\udcc6 Seguimientos mensuales. Balance anual con evaluaci\u00f3n 360\u00ba\n\ud83e\ude7a Seguro m\u00e9dico privado\n\ud83d\udcb0 Retribuci\u00f3n flexible\n\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Wellhub: apoyo para la actividad f\u00edsica, el bienestar y la salud mental\n\ud83d\udce2 Comunidades t\u00e9cnicas\n\ud83e\udd73 Un mont\u00f3n de eventos incre\u00edbles\nEstamos buscando un/a Data Engineer (beca) con formaci\u00f3n en Ingenier\u00eda, valorable postgrado o especializaci\u00f3n en Data Engineering / Big Data, para formarse en:\nun/a Data Engineer (beca)\nIngenier\u00eda de Datos: Experiencia en la construcci\u00f3n y gesti\u00f3n de pipelines de datos (ETL: Extract, Transform, Load) para asegurar la disponibilidad y calidad de los datos.\nBases de Datos y Almacenamiento: Conocimiento del dise\u00f1o, implementaci\u00f3n y mantenimiento de bases de datos, tanto relacionales como no relacionales, y soluciones de almacenamiento como data warehouses y data lakes.\nProgramaci\u00f3n y Scripting: Habilidad en lenguajes de programaci\u00f3n como Python, Java o Scala, y en scripting para automatizar procesos y optimizar flujos de trabajo.\nSistemas Distribuidos y Big Data: Familiaridad con tecnolog\u00edas de procesamiento de grandes vol\u00famenes de datos como Hadoop, Spark y sistemas de mensajer\u00eda como Kafka o MQ.\nGesti\u00f3n de la Calidad de los Datos: Implementaci\u00f3n de mecanismos para asegurar la integridad, consistencia y precisi\u00f3n de los datos a lo largo de su ciclo de vida (CdV).\nIngenier\u00eda de Datos: Experiencia en la construcci\u00f3n y gesti\u00f3n de pipelines de datos (ETL: Extract, Transform, Load) para asegurar la disponibilidad y calidad de los datos.\nIngenier\u00eda de Datos:\nBases de Datos y Almacenamiento: Conocimiento del dise\u00f1o, implementaci\u00f3n y mantenimiento de bases de datos, tanto relacionales como no relacionales, y soluciones de almacenamiento como data warehouses y data lakes.\nBases de Datos y Almacenamiento:\nProgramaci\u00f3n y Scripting: Habilidad en lenguajes de programaci\u00f3n como Python, Java o Scala, y en scripting para automatizar procesos y optimizar flujos de trabajo.\nProgramaci\u00f3n y Scripting:\nSistemas Distribuidos y Big Data: Familiaridad con tecnolog\u00edas de procesamiento de grandes vol\u00famenes de datos como Hadoop, Spark y sistemas de mensajer\u00eda como Kafka o MQ.\nSistemas Distribuidos y Big Data:\nGesti\u00f3n de la Calidad de los Datos: Implementaci\u00f3n de mecanismos para asegurar la integridad, consistencia y precisi\u00f3n de los datos a lo largo de su ciclo de vida (CdV).\nGesti\u00f3n de la Calidad de los Datos:\nHorario: Flexible, 40 horas a la semana, jornada intensiva los viernes y en verano\nHorario:\nModalidad: H\u00edbrida, asistencia a oficinas 3 d\u00edas a la semana, 2 d\u00edas en remoto\nModalidad:\nUbicaci\u00f3n: Tres Cantos, Madrid\nUbicaci\u00f3n\nSalario: 1000\u20ac durante 9 meses, con posibilidad de pasar a contrato indefinido despu\u00e9s de esee periodo\nSalario\nQui\u00e9nes somos \ud83d\udc47\nCapitole Consulting\nMira lo que dicen de nosotros \ud83d\udc47\nGlassdoor Reviews\nSi\u00e9ntete libre de mandarnos tu perfil. \u00a1Estamos deseando conocerte!\ud83d\udc99\nEl empleado/a se adherir\u00e1 a las pol\u00edticas de seguridad de la informaci\u00f3n:\n- Tendr\u00e1 acceso a la informaci\u00f3n confidencial relativa a Capitole y al proyecto en el que trabaja.\n- Tendr\u00e1 que cumplir las pol\u00edticas de seguridad y las pol\u00edticas internas de la empresa y cliente.\n- Tendr\u00e1 que firmar un NDA."
    },
    "4053370005": {
        "title": "Senior Data Integration Engineer (EDM) ",
        "company": "EPAM Systems",
        "location": "M\u00e1laga, Andalusia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nWe are looking for an EDM Data Engineer based in M\u00e1laga or ready to relocate to Spain. EPAM will provide relocation support to you and your family upon successful interviewing.\n\nWe always hire for a career in EPAM, but the initial client for this hire is a leading UK-based hedge fund with aggressive growth aspirations and a challenging delivery agenda. A successful candidate is expected to work from our M\u00e1laga office 4 days/week and visit the client in their London office as required.\n\nResponsibilities\n\n\nAct as a senior software database developer and data architect for company\u2019s projects\nPerform design, implementation, and unit testing of assigned functionality\nOwn assigned work, elaborate implementation details, keeps close coordination with BAs and QAs to deliver result of a high quality\nParticipate in a wide range of non-project activities within Databases Competency Center, including knowledge sharing and elaborating best practices according to modern industry trends\nCreate and follow personal education plan\n\n\nRequirements\n\n\n3+ years of experience in database development, design and modelling\nReadiness to work from office 4 days/week\nStrong knowledge of SQL language and modern relational database management systems technologies\nExperience with and knowledge of technology stack in use:\nMS SQL Server, SSIS, SSRS\nMarkit EDM\nSQL, T-SQL\nScripting languages\nStrong TDD experience, understanding of the best CI/CD practices, experience with code quality and code review tools, sense of responsibility for the code quality\nStrong UML modelling and design skills\nStrong computer science background\nAbility to implement functionality without supervision and test own work thoroughly using test cases\nAbility to create deliverables in good quality\nExcellent written and verbal communication skills\nEnglish level B2 or higher, ability to read, write, speak and write high-quality unambiguous texts in English\nAbility to read and understand project and requirement documentation, ability to create design technical and project documentation, including documentation describing own code\nAbility to participate in phone conferences, interviews and face-to-face meetings with customers without supervision\nAbility to mentor colleagues in software engineering disciplines\n\n\nNice to have\n\n\nKnowledge of non-relational database management systems\n\n\nWe offer\n\n\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\n\n\nEPAM is a leading digital transformation services and product engineering company with over 52,650 EPAMers in more than 55 countries and regions. Since 1993, our multidisciplinary teams have been helping make the future real for our clients and communities around the world. In 2018, we opened an office in Spain that quickly grew to over 1,450 EPAMers distributed between the offices in M\u00e1laga and Madrid as well as remotely across the country. Here you will collaborate with multinational teams, contribute to numerous innovative projects, and have an opportunity to learn and grow continuously.\n\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nEDM Data Engineer\nResponsibilities\nAct as a senior software database developer and data architect for company\u2019s projects\nPerform design, implementation, and unit testing of assigned functionality\nOwn assigned work, elaborate implementation details, keeps close coordination with BAs and QAs to deliver result of a high quality\nParticipate in a wide range of non-project activities within Databases Competency Center, including knowledge sharing and elaborating best practices according to modern industry trends\nCreate and follow personal education plan\nAct as a senior software database developer and data architect for company\u2019s projects\nPerform design, implementation, and unit testing of assigned functionality\nOwn assigned work, elaborate implementation details, keeps close coordination with BAs and QAs to deliver result of a high quality\nParticipate in a wide range of non-project activities within Databases Competency Center, including knowledge sharing and elaborating best practices according to modern industry trends\nCreate and follow personal education plan\nRequirements\n3+ years of experience in database development, design and modelling\nReadiness to work from office 4 days/week\nStrong knowledge of SQL language and modern relational database management systems technologies\nExperience with and knowledge of technology stack in use:\nMS SQL Server, SSIS, SSRS\nMarkit EDM\nSQL, T-SQL\nScripting languages\nStrong TDD experience, understanding of the best CI/CD practices, experience with code quality and code review tools, sense of responsibility for the code quality\nStrong UML modelling and design skills\nStrong computer science background\nAbility to implement functionality without supervision and test own work thoroughly using test cases\nAbility to create deliverables in good quality\nExcellent written and verbal communication skills\nEnglish level B2 or higher, ability to read, write, speak and write high-quality unambiguous texts in English\nAbility to read and understand project and requirement documentation, ability to create design technical and project documentation, including documentation describing own code\nAbility to participate in phone conferences, interviews and face-to-face meetings with customers without supervision\nAbility to mentor colleagues in software engineering disciplines\n3+ years of experience in database development, design and modelling\nReadiness to work from office 4 days/week\nStrong knowledge of SQL language and modern relational database management systems technologies\nExperience with and knowledge of technology stack in use:\nMS SQL Server, SSIS, SSRS\nMarkit EDM\nSQL, T-SQL\nScripting languages\nStrong TDD experience, understanding of the best CI/CD practices, experience with code quality and code review tools, sense of responsibility for the code quality\nStrong UML modelling and design skills\nStrong computer science background\nAbility to implement functionality without supervision and test own work thoroughly using test cases\nAbility to create deliverables in good quality\nExcellent written and verbal communication skills\nEnglish level B2 or higher, ability to read, write, speak and write high-quality unambiguous texts in English\nAbility to read and understand project and requirement documentation, ability to create design technical and project documentation, including documentation describing own code\nAbility to participate in phone conferences, interviews and face-to-face meetings with customers without supervision\nAbility to mentor colleagues in software engineering disciplines\nNice to have\nKnowledge of non-relational database management systems\nWe offer\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends."
    },
    "4171633756": {
        "title": "Data Engineer - Databricks - Senior ",
        "company": "Lumenalta",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nExperience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\n\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\n\nRequirements\n7+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\nYou are comfortable manipulating large data sets and handling raw SQL\nExperience using technologies such as Pyspark/AWS/Databricks is essential\nExperience creating ETL Pipeline from scratch\nE-commerce and Financial Services industry experience preferred\nEnglish fluency, verbal and written\nPersonality traits: Professional, problem solver, proactive, passionate, team player.\n\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\nBe 100% dedicated to one project at a time so that you can innovate and grow.\nBe a part of a team of talented and friendly senior-level developers.\nWork on projects that allow you to use leading tech.\n\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\n\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\n\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\nExperience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\nRequirements\n7+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\nYou are comfortable manipulating large data sets and handling raw SQL\nExperience using technologies such as Pyspark/AWS/Databricks is essential\nExperience creating ETL Pipeline from scratch\nE-commerce and Financial Services industry experience preferred\nEnglish fluency, verbal and written\nPersonality traits: Professional, problem solver, proactive, passionate, team player.\n7+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\nYou are comfortable manipulating large data sets and handling raw SQL\nExperience using technologies such as Pyspark/AWS/Databricks is essential\nExperience creating ETL Pipeline from scratch\nE-commerce and Financial Services industry experience preferred\nEnglish fluency, verbal and written\nPersonality traits: Professional, problem solver, proactive, passionate, team player.\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\nBe 100% dedicated to one project at a time so that you can innovate and grow.\nBe a part of a team of talented and friendly senior-level developers.\nWork on projects that allow you to use leading tech.\nBe 100% dedicated to one project at a time so that you can innovate and grow.\nBe a part of a team of talented and friendly senior-level developers.\nWork on projects that allow you to use leading tech.\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply"
    },
    "4173620993": {
        "title": "Data Engineer (Talend)",
        "company": "knowmad mood",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nLa comunidad Data de knowmad mood busca incorporar un Data Engineer con amplia experiencia en Talend para colaborar dentro de un cliente del sector seguros.\n\n\u00bfQu\u00e9 ofrecemos?\nHorario flexible \u23f0: Adapta tu jornada a tu ritmo personal, coordinando con tu equipo para disfrutar de un balance ideal entre trabajo y vida personal.\nVacaciones \ud83c\udf34: Disfruta de 22 d\u00edas de vacaciones m\u00e1s 2 d\u00edas de libre disposici\u00f3n, adem\u00e1s de los d\u00edas 24 y 31 de diciembre libres.\nContrato indefinido \ud83d\udcc4: Desde el primer d\u00eda, con la opci\u00f3n de elegir si prefieres recibir tu n\u00f3mina en 12 o 14 pagas.\nRetribuci\u00f3n flexible \ud83d\udcb3: Elige c\u00f3mo destinar parte de tu salario a productos o servicios con ventajas fiscales, como tarjeta restaurante, cheque guarder\u00eda, seguro m\u00e9dico o formaci\u00f3n.\nClub de ventajas: Accede a descuentos exclusivos en tecnolog\u00eda, ocio y formaci\u00f3n para mejorar tu vida personal.\nFormaci\u00f3n continua \ud83d\udcda: Accede a plataformas como Udemy Business, adem\u00e1s de cursos de idiomas, certificaciones oficiales y formaci\u00f3n t\u00e9cnica, adaptados a tus necesidades y ritmo.\nPlan de carrera \ud83c\udf31: Dise\u00f1a tu propio camino de crecimiento, con apoyo para avanzar dentro de la empresa, transitar entre comunidades o asumir roles de liderazgo.\nBienestar integral \ud83e\uddd8: Acceso a nuestro programa de bienestar que incluye nutrici\u00f3n saludable, actividad f\u00edsica y talleres de equilibrio emocional.\nConciliaci\u00f3n +VIDA: Asistencia personal y familiar 24/7, con servicios como consultas m\u00e9dicas, legales, dentales, asistencia personal, renovaci\u00f3n de carnet de conducir y m\u00e1s para ti y tus familiares.\nAmbiente de trabajo inclusivo \ud83c\udf0d: Fomentamos la diversidad generacional y cultural, con pol\u00edticas activas de igualdad y diversidad en un entorno de m\u00e1s de 70 nacionalidades.\nEventos sociales y diversi\u00f3n \ud83c\udf89: Participa en actividades como videojuegos multijugador, escape rooms virtuales y m\u00e1s, para conectar con tus compa\u00f1eros fuera del entorno laboral.\n\n\u00bfCu\u00e1les son los requisitos?\nExperiencia trabajando con Talend.\nS\u00f3lidos conocimientos en BBDD, especialmente Oracle. Tambi\u00e9n se valorar\u00e1 experiencia con otras como Db2, SQL Server y/o PostgreSQL.\nValorable experiencia con Synapse (Azure Cloud).\n\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n en el proyecto?\nRealizar pruebas de calidad para garantizar el correcto funcionamiento de los procesos.\nDise\u00f1ar, construir y montar diferentes ETL utilizando Talend.\nEjecutar las tareas asignadas mediante el uso de Talend, asegurando su entrega en tiempo y forma.\nRealizar an\u00e1lisis t\u00e9cnicos de las tareas a implementar, identificando posibles riesgos y proponiendo soluciones.\nDetectar y resolver errores (bugs) en los flujos de datos.\n\n\u00bfD\u00f3nde trabajaras? \n\n100% remoto.\n\n\u00a1Ap\u00fantate y te daremos m\u00e1s detalles! \ud83d\udc47\nLa comunidad Data de knowmad mood busca incorporar un Data Engineer con amplia experiencia en Talend para colaborar dentro de un cliente del sector seguros.\n\u00bfQu\u00e9 ofrecemos?\nHorario flexible \u23f0: Adapta tu jornada a tu ritmo personal, coordinando con tu equipo para disfrutar de un balance ideal entre trabajo y vida personal.\nVacaciones \ud83c\udf34: Disfruta de 22 d\u00edas de vacaciones m\u00e1s 2 d\u00edas de libre disposici\u00f3n, adem\u00e1s de los d\u00edas 24 y 31 de diciembre libres.\nContrato indefinido \ud83d\udcc4: Desde el primer d\u00eda, con la opci\u00f3n de elegir si prefieres recibir tu n\u00f3mina en 12 o 14 pagas.\nRetribuci\u00f3n flexible \ud83d\udcb3: Elige c\u00f3mo destinar parte de tu salario a productos o servicios con ventajas fiscales, como tarjeta restaurante, cheque guarder\u00eda, seguro m\u00e9dico o formaci\u00f3n.\nClub de ventajas: Accede a descuentos exclusivos en tecnolog\u00eda, ocio y formaci\u00f3n para mejorar tu vida personal.\nFormaci\u00f3n continua \ud83d\udcda: Accede a plataformas como Udemy Business, adem\u00e1s de cursos de idiomas, certificaciones oficiales y formaci\u00f3n t\u00e9cnica, adaptados a tus necesidades y ritmo.\nPlan de carrera \ud83c\udf31: Dise\u00f1a tu propio camino de crecimiento, con apoyo para avanzar dentro de la empresa, transitar entre comunidades o asumir roles de liderazgo.\nBienestar integral \ud83e\uddd8: Acceso a nuestro programa de bienestar que incluye nutrici\u00f3n saludable, actividad f\u00edsica y talleres de equilibrio emocional.\nConciliaci\u00f3n +VIDA: Asistencia personal y familiar 24/7, con servicios como consultas m\u00e9dicas, legales, dentales, asistencia personal, renovaci\u00f3n de carnet de conducir y m\u00e1s para ti y tus familiares.\nAmbiente de trabajo inclusivo \ud83c\udf0d: Fomentamos la diversidad generacional y cultural, con pol\u00edticas activas de igualdad y diversidad en un entorno de m\u00e1s de 70 nacionalidades.\nEventos sociales y diversi\u00f3n \ud83c\udf89: Participa en actividades como videojuegos multijugador, escape rooms virtuales y m\u00e1s, para conectar con tus compa\u00f1eros fuera del entorno laboral.\nHorario flexible \u23f0: Adapta tu jornada a tu ritmo personal, coordinando con tu equipo para disfrutar de un balance ideal entre trabajo y vida personal.\nHorario flexible \u23f0\nVacaciones \ud83c\udf34: Disfruta de 22 d\u00edas de vacaciones m\u00e1s 2 d\u00edas de libre disposici\u00f3n, adem\u00e1s de los d\u00edas 24 y 31 de diciembre libres.\nVacaciones \ud83c\udf34\nContrato indefinido \ud83d\udcc4: Desde el primer d\u00eda, con la opci\u00f3n de elegir si prefieres recibir tu n\u00f3mina en 12 o 14 pagas.\nContrato indefinido \ud83d\udcc4\nRetribuci\u00f3n flexible \ud83d\udcb3: Elige c\u00f3mo destinar parte de tu salario a productos o servicios con ventajas fiscales, como tarjeta restaurante, cheque guarder\u00eda, seguro m\u00e9dico o formaci\u00f3n.\nRetribuci\u00f3n flexible \ud83d\udcb3\nClub de ventajas: Accede a descuentos exclusivos en tecnolog\u00eda, ocio y formaci\u00f3n para mejorar tu vida personal.\nClub de ventajas\nFormaci\u00f3n continua \ud83d\udcda: Accede a plataformas como Udemy Business, adem\u00e1s de cursos de idiomas, certificaciones oficiales y formaci\u00f3n t\u00e9cnica, adaptados a tus necesidades y ritmo.\nFormaci\u00f3n continua \ud83d\udcda\nPlan de carrera \ud83c\udf31: Dise\u00f1a tu propio camino de crecimiento, con apoyo para avanzar dentro de la empresa, transitar entre comunidades o asumir roles de liderazgo.\nPlan de carrera \ud83c\udf31\nBienestar integral \ud83e\uddd8: Acceso a nuestro programa de bienestar que incluye nutrici\u00f3n saludable, actividad f\u00edsica y talleres de equilibrio emocional.\nBienestar integral \ud83e\uddd8\nConciliaci\u00f3n +VIDA: Asistencia personal y familiar 24/7, con servicios como consultas m\u00e9dicas, legales, dentales, asistencia personal, renovaci\u00f3n de carnet de conducir y m\u00e1s para ti y tus familiares.\nConciliaci\u00f3n +VIDA\nAmbiente de trabajo inclusivo \ud83c\udf0d: Fomentamos la diversidad generacional y cultural, con pol\u00edticas activas de igualdad y diversidad en un entorno de m\u00e1s de 70 nacionalidades.\nAmbiente de trabajo inclusivo \ud83c\udf0d\nEventos sociales y diversi\u00f3n \ud83c\udf89: Participa en actividades como videojuegos multijugador, escape rooms virtuales y m\u00e1s, para conectar con tus compa\u00f1eros fuera del entorno laboral.\nEventos sociales y diversi\u00f3n \ud83c\udf89\n\u00bfCu\u00e1les son los requisitos?\nExperiencia trabajando con Talend.\nS\u00f3lidos conocimientos en BBDD, especialmente Oracle. Tambi\u00e9n se valorar\u00e1 experiencia con otras como Db2, SQL Server y/o PostgreSQL.\nValorable experiencia con Synapse (Azure Cloud).\nExperiencia trabajando con Talend.\nS\u00f3lidos conocimientos en BBDD, especialmente Oracle. Tambi\u00e9n se valorar\u00e1 experiencia con otras como Db2, SQL Server y/o PostgreSQL.\nValorable experiencia con Synapse (Azure Cloud).\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n en el proyecto?\nRealizar pruebas de calidad para garantizar el correcto funcionamiento de los procesos.\nDise\u00f1ar, construir y montar diferentes ETL utilizando Talend.\nEjecutar las tareas asignadas mediante el uso de Talend, asegurando su entrega en tiempo y forma.\nRealizar an\u00e1lisis t\u00e9cnicos de las tareas a implementar, identificando posibles riesgos y proponiendo soluciones.\nDetectar y resolver errores (bugs) en los flujos de datos.\nRealizar pruebas de calidad para garantizar el correcto funcionamiento de los procesos.\nDise\u00f1ar, construir y montar diferentes ETL utilizando Talend.\nEjecutar las tareas asignadas mediante el uso de Talend, asegurando su entrega en tiempo y forma.\nRealizar an\u00e1lisis t\u00e9cnicos de las tareas a implementar, identificando posibles riesgos y proponiendo soluciones.\nDetectar y resolver errores (bugs) en los flujos de datos.\n\u00bfD\u00f3nde trabajaras?\n100% remoto.\n\u00a1Ap\u00fantate y te daremos m\u00e1s detalles! \ud83d\udc47"
    },
    "4132066918": {
        "title": "Data Engineer",
        "company": "Apipana.io",
        "location": "Seville, Andalusia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nEn Apipana, creamos soluciones innovadoras con c\u00f3digo de alta calidad, cubriendo todo el ciclo de vida del desarrollo de software (SDLC) trabajando de la mano con nuestro cliente principal dedicado a casino games, betting, Fintech y soluciones de software en un ambiente internacional.\n\nActualmente nos encontramos en pleno crecimiento buscando mentes creativas y entusiastas para unirse a nuestro equipo en nuestras oficinas ubicadas en Sevilla o Granada. Si te apasionan los retos tecnol\u00f3gicos y quieres estar a la \u00faltima, \u00a1\u00fanete al equipo y forma parte de los primeros 50!\n\nTu misi\u00f3n\n\nComo Data Software Engineer ser\u00e1s responsable junto con tu equipo, del dise\u00f1o, desarrollo e implementaci\u00f3n de una plataforma de datos de \u00faltima generaci\u00f3n que integra data streaming, un data lake y una base de datos anal\u00edtica de alto rendimiento. Si buscas un desaf\u00edo emocionante y la oportunidad de trabajar en un equipo comprometido con la excelencia, \u00a1te estamos buscando a ti! \u00a1\u00danete a Apipana y juntos crearemos el futuro de la tecnolog\u00eda!\n\n\u00bfQu\u00e9 har\u00e1s?\n\nDise\u00f1ar y desarrollar pipelines de datos en tiempo real, siguiendo principios SOLID y asegur\u00e1ndote de cumplir con los est\u00e1ndares de calidad establecidos.\nIncorporar nuevos puntos de datos en los modelos ya existentes.\nImplementar transformaciones complejas con el objetivo de enriquecer los pipelines de datos en streaming.\nColaborar con el equipo, compartiendo y aprendiendo con tus compa\u00f1eros.\nOptimizar el rendimiento y la eficiencia de los pipelines y las consultas de datos.\nConfigurar sistemas de monitoring, alertas y m\u00e9tricas de calidad.\n\nPrincipales herramientas y entornos con los que trabajar\u00e1s en tu d\u00eda a d\u00eda: \n\nPOO en Java. \nProcesamiento de datos en Streaming con Apache Flink y Apache Beam.\nBases de datos SQL Operacionales y Anal\u00edticas.\nMessaging (Kafka).\nEcosistema Hadoop.\nGesti\u00f3n de contenedores con Docker y Kubernetes como orquestador.\nHerramienta declarativa gitOps, ArgoCD.\n\n\u00bfQu\u00e9 esperamos de ti?\n\nGrado en Inform\u00e1tica o un campo relacionado, con al menos 3 a\u00f1os de experiencia en data engineering.\nExperiencia pr\u00e1ctica con grandes vol\u00famenes de datos y procesos ETL, trabajando con datos estructurados, semiestructurados y no estructurados.\nS\u00f3lidos conocimientos en dise\u00f1o y desarrollo de bases de datos, adem\u00e1s de modelos de datos multidimensionales.\nExperiencia en data ingestion utilizando Kafka.\nFamiliaridad con tecnolog\u00edas de procesamiento de datos en streaming como Flink, Beam o Spark.\nConocimientos b\u00e1sicos sobre data lakes (como Hadoop y su ecosistema).\nExperiencia trabajando con contenedores mediante Docker.\nAmplios conocimientos de Java y, como plus, experiencia manipulando datos con este lenguaje.\nExperiencia s\u00f3lida con bases de datos relacionales y consultas SQL.\nCapacidad para resolver problemas, habilidades anal\u00edticas y disposici\u00f3n para asumir retos t\u00e9cnicos con entusiasmo.\nBuenas habilidades interpersonales y capacidad para colaborar en equipo.\nExperiencia trabajando con aplicaciones en contenedores mediante Kubernetes.\nConocimientos sobre arquitecturas Data Lakehouse, especialmente el formato de tablas Iceberg.\n\n\u00a1No te preocupes si no las re\u00fanes todos los requisitos, un cambio de empleo tambi\u00e9n significa aprender cosas nuevas!\n\nBeneficios de trabajar en Apipana:\n\n30 d\u00edas laborales de vacaciones\nHorario flexible\nSistema h\u00edbrido, 3 d\u00edas en la oficina y 2 desde casa\nSeguro m\u00e9dico completo con DKV\nClases gratuitas de ingl\u00e9s dentro de tu jornada laboral\nTickets restaurante\nHasta 25 euros al mes de descuento en cualquier actividad deportiva\nPresupuesto para formaci\u00f3n\nRecomienda a un/a amig/a para unirse al equipo y gana hasta 3.000 euros por cada recomendaci\u00f3n exitosa que hagas\nOficina en el centro de Granada, \u00a1ubicaci\u00f3n inmejorable!\n\n\u00a1No pierdas la oportunidad de unirte y ser parte de nuestro futuro!\nEn Apipana, creamos soluciones innovadoras con c\u00f3digo de alta calidad, cubriendo todo el ciclo de vida del desarrollo de software (SDLC) trabajando de la mano con nuestro cliente principal dedicado a casino games, betting, Fintech y soluciones de software en un ambiente internacional.\nActualmente nos encontramos en pleno crecimiento buscando mentes creativas y entusiastas para unirse a nuestro equipo en nuestras oficinas ubicadas en Sevilla o Granada. Si te apasionan los retos tecnol\u00f3gicos y quieres estar a la \u00faltima, \u00a1\u00fanete al equipo y forma parte de los primeros 50!\nTu misi\u00f3n\nComo Data Software Engineer ser\u00e1s responsable junto con tu equipo, del dise\u00f1o, desarrollo e implementaci\u00f3n de una plataforma de datos de \u00faltima generaci\u00f3n que integra data streaming, un data lake y una base de datos anal\u00edtica de alto rendimiento. Si buscas un desaf\u00edo emocionante y la oportunidad de trabajar en un equipo comprometido con la excelencia, \u00a1te estamos buscando a ti! \u00a1\u00danete a Apipana y juntos crearemos el futuro de la tecnolog\u00eda!\n\u00bfQu\u00e9 har\u00e1s?\nDise\u00f1ar y desarrollar pipelines de datos en tiempo real, siguiendo principios SOLID y asegur\u00e1ndote de cumplir con los est\u00e1ndares de calidad establecidos.\nIncorporar nuevos puntos de datos en los modelos ya existentes.\nImplementar transformaciones complejas con el objetivo de enriquecer los pipelines de datos en streaming.\nColaborar con el equipo, compartiendo y aprendiendo con tus compa\u00f1eros.\nOptimizar el rendimiento y la eficiencia de los pipelines y las consultas de datos.\nConfigurar sistemas de monitoring, alertas y m\u00e9tricas de calidad.\nDise\u00f1ar y desarrollar pipelines de datos en tiempo real, siguiendo principios SOLID y asegur\u00e1ndote de cumplir con los est\u00e1ndares de calidad establecidos.\nIncorporar nuevos puntos de datos en los modelos ya existentes.\nImplementar transformaciones complejas con el objetivo de enriquecer los pipelines de datos en streaming.\nColaborar con el equipo, compartiendo y aprendiendo con tus compa\u00f1eros.\nOptimizar el rendimiento y la eficiencia de los pipelines y las consultas de datos.\nConfigurar sistemas de monitoring, alertas y m\u00e9tricas de calidad.\nPrincipales herramientas y entornos con los que trabajar\u00e1s en tu d\u00eda a d\u00eda:\nPOO en Java. \nProcesamiento de datos en Streaming con Apache Flink y Apache Beam.\nBases de datos SQL Operacionales y Anal\u00edticas.\nMessaging (Kafka).\nEcosistema Hadoop.\nGesti\u00f3n de contenedores con Docker y Kubernetes como orquestador.\nHerramienta declarativa gitOps, ArgoCD.\nPOO en Java.\nProcesamiento de datos en Streaming con Apache Flink y Apache Beam.\nBases de datos SQL Operacionales y Anal\u00edticas.\nMessaging (Kafka).\nEcosistema Hadoop.\nGesti\u00f3n de contenedores con Docker y Kubernetes como orquestador.\nHerramienta declarativa gitOps, ArgoCD.\n\u00bfQu\u00e9 esperamos de ti?\nGrado en Inform\u00e1tica o un campo relacionado, con al menos 3 a\u00f1os de experiencia en data engineering.\nExperiencia pr\u00e1ctica con grandes vol\u00famenes de datos y procesos ETL, trabajando con datos estructurados, semiestructurados y no estructurados.\nS\u00f3lidos conocimientos en dise\u00f1o y desarrollo de bases de datos, adem\u00e1s de modelos de datos multidimensionales.\nExperiencia en data ingestion utilizando Kafka.\nFamiliaridad con tecnolog\u00edas de procesamiento de datos en streaming como Flink, Beam o Spark.\nConocimientos b\u00e1sicos sobre data lakes (como Hadoop y su ecosistema).\nExperiencia trabajando con contenedores mediante Docker.\nAmplios conocimientos de Java y, como plus, experiencia manipulando datos con este lenguaje.\nExperiencia s\u00f3lida con bases de datos relacionales y consultas SQL.\nCapacidad para resolver problemas, habilidades anal\u00edticas y disposici\u00f3n para asumir retos t\u00e9cnicos con entusiasmo.\nBuenas habilidades interpersonales y capacidad para colaborar en equipo.\nExperiencia trabajando con aplicaciones en contenedores mediante Kubernetes.\nConocimientos sobre arquitecturas Data Lakehouse, especialmente el formato de tablas Iceberg.\nGrado en Inform\u00e1tica o un campo relacionado, con al menos 3 a\u00f1os de experiencia en data engineering.\nExperiencia pr\u00e1ctica con grandes vol\u00famenes de datos y procesos ETL, trabajando con datos estructurados, semiestructurados y no estructurados.\nS\u00f3lidos conocimientos en dise\u00f1o y desarrollo de bases de datos, adem\u00e1s de modelos de datos multidimensionales.\nExperiencia en data ingestion utilizando Kafka.\nFamiliaridad con tecnolog\u00edas de procesamiento de datos en streaming como Flink, Beam o Spark.\nConocimientos b\u00e1sicos sobre data lakes (como Hadoop y su ecosistema).\nExperiencia trabajando con contenedores mediante Docker.\nAmplios conocimientos de Java y, como plus, experiencia manipulando datos con este lenguaje.\nExperiencia s\u00f3lida con bases de datos relacionales y consultas SQL.\nCapacidad para resolver problemas, habilidades anal\u00edticas y disposici\u00f3n para asumir retos t\u00e9cnicos con entusiasmo.\nBuenas habilidades interpersonales y capacidad para colaborar en equipo.\nExperiencia trabajando con aplicaciones en contenedores mediante Kubernetes.\nConocimientos sobre arquitecturas Data Lakehouse, especialmente el formato de tablas Iceberg.\n\u00a1No te preocupes si no las re\u00fanes todos los requisitos, un cambio de empleo tambi\u00e9n significa aprender cosas nuevas!\nBeneficios de trabajar en Apipana:\n30 d\u00edas laborales de vacaciones\nHorario flexible\nSistema h\u00edbrido, 3 d\u00edas en la oficina y 2 desde casa\nSeguro m\u00e9dico completo con DKV\nClases gratuitas de ingl\u00e9s dentro de tu jornada laboral\nTickets restaurante\nHasta 25 euros al mes de descuento en cualquier actividad deportiva\nPresupuesto para formaci\u00f3n\nRecomienda a un/a amig/a para unirse al equipo y gana hasta 3.000 euros por cada recomendaci\u00f3n exitosa que hagas\nOficina en el centro de Granada, \u00a1ubicaci\u00f3n inmejorable!\n30 d\u00edas laborales de vacaciones\nHorario flexible\nSistema h\u00edbrido, 3 d\u00edas en la oficina y 2 desde casa\nSeguro m\u00e9dico completo con DKV\nClases gratuitas de ingl\u00e9s dentro de tu jornada laboral\nTickets restaurante\nHasta 25 euros al mes de descuento en cualquier actividad deportiva\nPresupuesto para formaci\u00f3n\nRecomienda a un/a amig/a para unirse al equipo y gana hasta 3.000 euros por cada recomendaci\u00f3n exitosa que hagas\nOficina en el centro de Granada, \u00a1ubicaci\u00f3n inmejorable!\n\u00a1No pierdas la oportunidad de unirte y ser parte de nuestro futuro!"
    },
    "4159417620": {
        "title": "Senior Data Engineer",
        "company": "Welaxio Management",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nOur client, is seeking a talented Senior Data Engineer to join our dynamic Data Delivery Team.\n\nRole Overview: As a Senior Data Engineer, you will play a crucial role in designing, developing, and maintaining scalable data pipelines and systems. You will work closely with our team of data experts to deliver high-quality data solutions that drive business insights and decision-making for our client.\nExcellent communication skills are essential, as part of the client's team is in India.\n\nKey Responsibilities:\nDesign, build, and maintain efficient, scalable, and reliable data pipelines.\nCollaborate with analysts and other stakeholders to understand data requirements and deliver solutions.\nProficiency in SQL and PySpark.\nImplement best practices for data management, security, and governance.\nMaintain and enhance our CI/CD architecture, designing improvements to our data infrastructure.\nMentor and guide junior data engineers, framing projects and assigning priorities to key deliverables.\n\nQualifications:\n4+ yearsof experience in data engineering or a similar role.\nProficiency with Azure Cloud (with vast knowledge in Azure Data Factory in particular) and Databricks.\nProficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).\nFamiliarity with cloud platforms(e.g., AWS, Azure, GCP) and data warehousing solutions.\nExcellent problem-solving skills and attention to detail.\nStrong communication and collaboration skills in English (Spanish is desirable).\n\nWhat We Offer:\nBe part of a solid and rapidly growing company that fosters innovation and collaboration.\nOpportunities for continuous training and professional development. \nA positive, team-oriented work environment, promoting growth and knowledge sharing. And a competitive compensation package tailored to your experience and skillset\n\nIf you're passionate about data engineering and eager to grow in an innovative environment, apply now! \ud83d\ude80\nOur client, is seeking a talented Senior Data Engineer to join our dynamic Data Delivery Team.\nSenior Data Engineer\nData Delivery\nRole Overview: As a Senior Data Engineer, you will play a crucial role in designing, developing, and maintaining scalable data pipelines and systems. You will work closely with our team of data experts to deliver high-quality data solutions that drive business insights and decision-making for our client.\nRole Overview:\nExcellent communication skills are essential, as part of the client's team is in India.\nKey Responsibilities:\nDesign, build, and maintain efficient, scalable, and reliable data pipelines.\nCollaborate with analysts and other stakeholders to understand data requirements and deliver solutions.\nProficiency in SQL and PySpark.\nImplement best practices for data management, security, and governance.\nMaintain and enhance our CI/CD architecture, designing improvements to our data infrastructure.\nMentor and guide junior data engineers, framing projects and assigning priorities to key deliverables.\nDesign, build, and maintain efficient, scalable, and reliable data pipelines.\nCollaborate with analysts and other stakeholders to understand data requirements and deliver solutions.\nProficiency in SQL and PySpark.\nImplement best practices for data management, security, and governance.\nMaintain and enhance our CI/CD architecture, designing improvements to our data infrastructure.\nMentor and guide junior data engineers, framing projects and assigning priorities to key deliverables.\nQualifications:\n4+ yearsof experience in data engineering or a similar role.\nProficiency with Azure Cloud (with vast knowledge in Azure Data Factory in particular) and Databricks.\nProficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).\nFamiliarity with cloud platforms(e.g., AWS, Azure, GCP) and data warehousing solutions.\nExcellent problem-solving skills and attention to detail.\nStrong communication and collaboration skills in English (Spanish is desirable).\n4+ yearsof experience in data engineering or a similar role.\nProficiency with Azure Cloud (with vast knowledge in Azure Data Factory in particular) and Databricks.\nProficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).\nFamiliarity with cloud platforms(e.g., AWS, Azure, GCP) and data warehousing solutions.\nExcellent problem-solving skills and attention to detail.\nStrong communication and collaboration skills in English (Spanish is desirable).\nWhat We Offer:\nBe part of a solid and rapidly growing company that fosters innovation and collaboration.\nOpportunities for continuous training and professional development.\ncontinuous training\nA positive, team-oriented work environment, promoting growth and knowledge sharing. And a competitive compensation package tailored to your experience and skillset\nteam-oriented work environment\ncompetitive compensation package\nIf you're passionate about data engineering and eager to grow in an innovative environment, apply now! \ud83d\ude80"
    },
    "4136261724": {
        "title": "Data Engineer Junior",
        "company": "Logicalis Spain",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEn Logicalis Spain estamos buscando un perfil de Ingeniero de Datos Junior para integrarse en nuestra BU de Data & Analytics. El equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\n\nREQUISITOS T\u00c9CNICOS\n\nM\u00ednimo 1 a\u00f1o de experiencia profesional trabajando con ecosistemas Big Data en entorno AWS:\nExperiencia contrastada en desarrollo en entornos Scala-Spark, Python.\nValorable conocer el uso de la herramienta de automatizaci\u00f3n Airflow / Docker / Jenkins.\nValorable experiencia trabajando con metodolog\u00edas \u00c1giles\n\nFUNCIONES\n\nDesarrollo de procesos ETL en AWS (EMR, Step functions, BATCH, ECS, Lambdas, Fargate).\nDar soluci\u00f3n a incidencias en producci\u00f3n relacionadas con ETL y los componentes de la soluci\u00f3n (Problemas escalabilidad, fallos memoria, time-out\u2026).\nEjecuci\u00f3n de guardias.\n\nBENEFICIOS\n\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n\n> Modalidad de trabajo 100% remoto, desde cualquier punto de Espa\u00f1a.\n\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n\n> Seguro m\u00e9dico y GYMPASS.\n\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n\n> Portal de descuentos especiales para empleados.\n\n> Buen ambiente de trabajo y entorno muy colaborativo.\nLogicalis Spain\nIngeniero de Datos Junior\nData & Analytics\nREQUISITOS T\u00c9CNICOS\nM\u00ednimo 1 a\u00f1o de experiencia profesional trabajando con ecosistemas Big Data en entorno AWS:\nExperiencia contrastada en desarrollo en entornos Scala-Spark, Python.\nValorable conocer el uso de la herramienta de automatizaci\u00f3n Airflow / Docker / Jenkins.\nValorable experiencia trabajando con metodolog\u00edas \u00c1giles\nM\u00ednimo 1 a\u00f1o de experiencia profesional trabajando con ecosistemas Big Data en entorno AWS:\nExperiencia contrastada en desarrollo en entornos Scala-Spark, Python.\nValorable conocer el uso de la herramienta de automatizaci\u00f3n Airflow / Docker / Jenkins.\nValorable experiencia trabajando con metodolog\u00edas \u00c1giles\nFUNCIONES\nDesarrollo de procesos ETL en AWS (EMR, Step functions, BATCH, ECS, Lambdas, Fargate).\nDar soluci\u00f3n a incidencias en producci\u00f3n relacionadas con ETL y los componentes de la soluci\u00f3n (Problemas escalabilidad, fallos memoria, time-out\u2026).\nEjecuci\u00f3n de guardias.\nDesarrollo de procesos ETL en AWS (EMR, Step functions, BATCH, ECS, Lambdas, Fargate).\nDar soluci\u00f3n a incidencias en producci\u00f3n relacionadas con ETL y los componentes de la soluci\u00f3n (Problemas escalabilidad, fallos memoria, time-out\u2026).\nEjecuci\u00f3n de guardias.\nBENEFICIOS"
    },
    "4146213892": {
        "title": "Data Architect & Engineer",
        "company": "Brico Dep\u00f4t Iberia (Grupo Kingfisher)",
        "location": "El Prat de Llobregat, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfQuieres unirte a un lugar donde destaca el compa\u00f1erismo y el buen ambiente, con flexibilidad para organizar tu trabajo mientras contribuyes al prop\u00f3sito de hacer la mejora del hogar accesible?\n\n\u00a1\u00danete a Brico Dep\u00f4t!\n\n\u00bfC\u00f3mo es trabajar en Brico Dep\u00f4t?\n\nFlexibilidad y autonom\u00eda: Tendr\u00e1s un modelo de trabajo h\u00edbrido con 3 d\u00edas de teletrabajo, que te da la libertad de dise\u00f1ar tu jornada laboral de manera que maximice tu productividad y bienestar.\n\nAmbiente c\u00e1lido y de apoyo: Descubrir\u00e1s un esp\u00edritu de equipo y apoyo mutuo m\u00e1s all\u00e1 de las relaciones laborales habituales. La energ\u00eda positiva y el buen ambiente son constantes, creando un espacio donde sentirse valorado y conectado es la norma.\n\nCrecimiento continuo: Te ofrecemos formaciones continuas (habilidades, idiomas, etc.) para que crezcas, tanto en lo profesional como en lo personal, y la oportunidad de aprender de cada proyecto y desaf\u00edo.\n\nLiderazgo inclusivo y participativo: Tus ideas y opiniones son valoradas. La comunicaci\u00f3n directa y transparente con los managers te permitir\u00e1 influir en el rumbo de los proyectos y decisiones, reforzando tu sentido de pertenencia y contribuci\u00f3n al equipo.\n\nTu bienestar es nuestra prioridad: Te ofrecemos una gama de beneficios pensados para tu salud y seguridad, incluyendo seguro m\u00e9dico, servicios de fisioterapia, apoyo al bienestar mental, fruta fresca diariamente y acceso a Wellhub.\n\nEn Brico Dep\u00f4t, cada d\u00eda es una oportunidad para crecer, sentirte apoyado y vivir plenamente. Si estas experiencias resuenan contigo, te invitamos a unirte a Brico Dep\u00f4t y comenzar un viaje donde tu carrera y bienestar van de la mano.\n\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n?\n\nEn este puesto, tu misi\u00f3n ser\u00e1 liderar la direcci\u00f3n t\u00e9cnica y el apoyo al equipo de ingenier\u00eda de datos para garantizar la estabilidad, calidad y mejora continua de nuestra plataforma de datos.\n\n\u00bfQu\u00e9 har\u00e1s?\n\n Liderar la ingenier\u00eda de datos, estableciendo y promoviendo las mejores pr\u00e1cticas, procesos de revisi\u00f3n y pruebas.\n Ofrecer soporte t\u00e9cnico al equipo de ingenier\u00eda de datos en los procesos de ingesta, transformaci\u00f3n y arquitectura.\n Asegurar que contamos con las soluciones adecuada para el equipo de ingenier\u00eda de datos y ofrecer mentor\u00eda.\n Ser responsable de las revisiones necesarias para garantizar la integridad y estabilidad de la plataforma de datos.\n Trabajar conjuntamente con los analistas de datos para adecuar la informaci\u00f3n a los requerimientos de negocio.\n Trabajar estrechamente con los arquitectos del Grupo Kingfisher en Londres para implantar nuevas tecnolog\u00edas y crear nuevas estrategias.\n Proporcionar soporte urgente y una gesti\u00f3n \u00e1gil para resolver problemas en el entorno de producci\u00f3n.\n Fomentar un ambiente de equipo positivo, mostrando buenas pr\u00e1cticas en la entrega tecnol\u00f3gica.\n\n\u00bfQu\u00e9 te har\u00e1 destacar?\n\n Experiencia en liderar equipos t\u00e9cnicos y ofrecer mentor\u00eda a ingenieros de datos.\n Experiencia en los Clouds GCP y Azure\n S\u00f3lidos conocimientos de lenguajes como DBT, Airflow, Java, Python, SQL, adem\u00e1s de control de versiones (Git) y procesos CI/CD.\n Experiencia con pruebas automatizadas y monitorizaci\u00f3n de la plataforma de datos en el entorno productivo.\n Habilidades de comunicaci\u00f3n efectiva, colaboraci\u00f3n con POs y equipos \u00e1giles, y orientaci\u00f3n a resultados basados en datos.\n\n\u00bfQu\u00e9 buscamos en ti?\n\n Al menos 5-7 a\u00f1os de experiencia en ingenier\u00eda de datos o roles similares, y 1-3 a\u00f1os liderando equipos o proyectos.\n Capacidad anal\u00edtica, pensamiento estrat\u00e9gico y proactividad.\n Pasi\u00f3n por la tecnolog\u00eda y compromiso con el aprendizaje continuo.\n Nivel de ingl\u00e9s intermedio/alto. Mantener conversaciones t\u00e9cnicas y de negocio.\n\nSi todo esto te encaja y quieres sumarte a una familia donde el aprendizaje y los retos no paran, tu m\u00e1nager te respalda, y puedes ayudar a nuestros clientes a mejorar sus hogares, \u00a1te esperamos en Brico Dep\u00f4t!\n\n\u00a1Inscr\u00edbete ahora!\n\u00bfC\u00f3mo es trabajar en Brico Dep\u00f4t?\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n?\n\u00bfQu\u00e9 har\u00e1s?\nLiderar la ingenier\u00eda de datos, estableciendo y promoviendo las mejores pr\u00e1cticas, procesos de revisi\u00f3n y pruebas.\n Ofrecer soporte t\u00e9cnico al equipo de ingenier\u00eda de datos en los procesos de ingesta, transformaci\u00f3n y arquitectura.\n Asegurar que contamos con las soluciones adecuada para el equipo de ingenier\u00eda de datos y ofrecer mentor\u00eda.\n Ser responsable de las revisiones necesarias para garantizar la integridad y estabilidad de la plataforma de datos.\n Trabajar conjuntamente con los analistas de datos para adecuar la informaci\u00f3n a los requerimientos de negocio.\n Trabajar estrechamente con los arquitectos del Grupo Kingfisher en Londres para implantar nuevas tecnolog\u00edas y crear nuevas estrategias.\n Proporcionar soporte urgente y una gesti\u00f3n \u00e1gil para resolver problemas en el entorno de producci\u00f3n.\n Fomentar un ambiente de equipo positivo, mostrando buenas pr\u00e1cticas en la entrega tecnol\u00f3gica.\nLiderar la ingenier\u00eda de datos, estableciendo y promoviendo las mejores pr\u00e1cticas, procesos de revisi\u00f3n y pruebas.\nOfrecer soporte t\u00e9cnico al equipo de ingenier\u00eda de datos en los procesos de ingesta, transformaci\u00f3n y arquitectura.\nAsegurar que contamos con las soluciones adecuada para el equipo de ingenier\u00eda de datos y ofrecer mentor\u00eda.\nSer responsable de las revisiones necesarias para garantizar la integridad y estabilidad de la plataforma de datos.\nTrabajar conjuntamente con los analistas de datos para adecuar la informaci\u00f3n a los requerimientos de negocio.\nTrabajar estrechamente con los arquitectos del Grupo Kingfisher en Londres para implantar nuevas tecnolog\u00edas y crear nuevas estrategias.\nProporcionar soporte urgente y una gesti\u00f3n \u00e1gil para resolver problemas en el entorno de producci\u00f3n.\nFomentar un ambiente de equipo positivo, mostrando buenas pr\u00e1cticas en la entrega tecnol\u00f3gica.\n\u00bfQu\u00e9 te har\u00e1 destacar?\nExperiencia en liderar equipos t\u00e9cnicos y ofrecer mentor\u00eda a ingenieros de datos.\n Experiencia en los Clouds GCP y Azure\n S\u00f3lidos conocimientos de lenguajes como DBT, Airflow, Java, Python, SQL, adem\u00e1s de control de versiones (Git) y procesos CI/CD.\n Experiencia con pruebas automatizadas y monitorizaci\u00f3n de la plataforma de datos en el entorno productivo.\n Habilidades de comunicaci\u00f3n efectiva, colaboraci\u00f3n con POs y equipos \u00e1giles, y orientaci\u00f3n a resultados basados en datos.\nExperiencia en liderar equipos t\u00e9cnicos y ofrecer mentor\u00eda a ingenieros de datos.\nExperiencia en los Clouds GCP y Azure\nS\u00f3lidos conocimientos de lenguajes como DBT, Airflow, Java, Python, SQL, adem\u00e1s de control de versiones (Git) y procesos CI/CD.\nExperiencia con pruebas automatizadas y monitorizaci\u00f3n de la plataforma de datos en el entorno productivo.\nHabilidades de comunicaci\u00f3n efectiva, colaboraci\u00f3n con POs y equipos \u00e1giles, y orientaci\u00f3n a resultados basados en datos.\n\u00bfQu\u00e9 buscamos en ti?\nAl menos 5-7 a\u00f1os de experiencia en ingenier\u00eda de datos o roles similares, y 1-3 a\u00f1os liderando equipos o proyectos.\n Capacidad anal\u00edtica, pensamiento estrat\u00e9gico y proactividad.\n Pasi\u00f3n por la tecnolog\u00eda y compromiso con el aprendizaje continuo.\n Nivel de ingl\u00e9s intermedio/alto. Mantener conversaciones t\u00e9cnicas y de negocio.\nAl menos 5-7 a\u00f1os de experiencia en ingenier\u00eda de datos o roles similares, y 1-3 a\u00f1os liderando equipos o proyectos.\nCapacidad anal\u00edtica, pensamiento estrat\u00e9gico y proactividad.\nPasi\u00f3n por la tecnolog\u00eda y compromiso con el aprendizaje continuo.\nNivel de ingl\u00e9s intermedio/alto. Mantener conversaciones t\u00e9cnicas y de negocio.\n\u00a1Inscr\u00edbete ahora!"
    },
    "4157386910": {
        "title": "Data Engineer",
        "company": "TETRACE",
        "location": "Pamplona, Chartered Community of Navarre, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nAbout Us\n\nSince 2010, our dedication and expertise have allowed us to grow and make a significant difference in the world of wind and photovoltaic energy.\n\nTetrace was born from the ambition to offer specialized engineering services in the renewable sector. We started with engineering for civil works, access and ports in renewable projects.\n\nOver the years, we have evolved to offer a diversified and tailor-made portfolio of services.\n\nWhy work with us?\n\nIn an ever-changing world, we offer our partners the opportunity to be at the forefront of change. We are firmly committed to a more sustainable future and actively seek those passionate about renewable energy to be part of our vision.\n\nIf you are a born innovator, with a thirst to contribute to a cleaner and more efficient planet, then you have found your niche. We are constantly on the lookout for new talent who, like you, want to make a difference.\n\nMinimum Requirements\n\nTasks / functions:\n\n Design, develop and deploy digital solutions ensuring the software development life cycle in an agile setup.\n Develope solutions on a leading-edge cloud based platform for managing and analysing large datasets.\n Create technical documentation.\n Analyze and decompose business requirements into technical functionalities.\n Produce clean and efficient code based on business requirements and specifications.\n Create Notebooks, pipelines and workflows in SCALA or Python to ingest, process and serve data in our platform.\n Be a technical lead for junior and external developers.\n\nQualification/Training\n\nSuccessfully completed Technical degree in electrical engineering, automation technology or comparable qualification.\n\nExperience or certification in Databricks, SQL data warehousing, Azure Data lakes.\n\nExperience\n\n3+ years of proven experience as a Solution Developer, Software Engineer or similar position with successful track record in complex project environments.\n\nProfound knowledge of Scala, T-SQL. Knowledge of JAVA, PowerShell scripting, RESTful APIs, SQL and NoSQL databases.\n\nExperience in data analytics platforms.\n\nExperience in agile development and its related tools.\n\nOther Skills\n\nLanguages: Fluent English skills (German or Spanish skills are a plus).\nAbout Us\nWhy work with us?\nMinimum Requirements\nTasks / functions:\nDesign, develop and deploy digital solutions ensuring the software development life cycle in an agile setup.\n Develope solutions on a leading-edge cloud based platform for managing and analysing large datasets.\n Create technical documentation.\n Analyze and decompose business requirements into technical functionalities.\n Produce clean and efficient code based on business requirements and specifications.\n Create Notebooks, pipelines and workflows in SCALA or Python to ingest, process and serve data in our platform.\n Be a technical lead for junior and external developers.\nDesign, develop and deploy digital solutions ensuring the software development life cycle in an agile setup.\nDevelope solutions on a leading-edge cloud based platform for managing and analysing large datasets.\nCreate technical documentation.\nAnalyze and decompose business requirements into technical functionalities.\nProduce clean and efficient code based on business requirements and specifications.\nCreate Notebooks, pipelines and workflows in SCALA or Python to ingest, process and serve data in our platform.\nBe a technical lead for junior and external developers.\nQualification/Training\nExperience\nOther Skills\nDesired Skills and Experience\nTrabajo en equipo, Perf\u00edl anal\u00edtico"
    },
    "4166220680": {
        "title": "Risk & Data Engineer \u2013 Banking ",
        "company": "Lognext",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nEn Lognext llevamos m\u00e1s de 18 a\u00f1os identificando e implementando soluciones tecnol\u00f3gicas pr\u00e1cticas que nos permitan seguir avanzando y optimicen nuestras operaciones, acompa\u00f1ando a los equipos con talento experto de alto rendimiento y haciendo de la tecnolog\u00eda una fuerza transformadora en nuestro d\u00eda a d\u00eda.\n\nBuscamos un Data Engineer & Risk Modeling para unirse a nuestro equipo en una multinacional l\u00edder en el sector financiero.\n\n\ud83d\udd0d \u00bfQu\u00e9 buscamos?\n\u2022 Conocimientos en regulaci\u00f3n en riesgo de contrapartida y en capital (regulatorio y econ\u00f3mico).\n\u2022 Experiencia en desarrollo de software en entornos tecnol\u00f3gicos gobernados.\n\u2022 Se valorar\u00e1 experiencia en modelos de riesgos financieros.\n\u2022 Programaci\u00f3n test-driven development (TDD) y metodolog\u00eda Agile.\n\u2022 Experiencia en SCALA, Python, Spark y despliegues continuos (Jenkins o similares).\n\u2022 Experiencia en definici\u00f3n y desarrollo de modelos de contrapartida y/o capital en el marco de regulaci\u00f3n financiera.\n\u2022 Experiencia en proyectos de integraci\u00f3n continua y control de versiones.\n\u2022 Formaci\u00f3n en Matem\u00e1ticas, F\u00edsica, Estad\u00edstica, Ingenier\u00eda o similar.\n\u2022 Nivel de ingl\u00e9s alto (C1 m\u00ednimo).\n\n\ud83d\udee0\ufe0f \u00bfCu\u00e1les ser\u00e1n tus principales funciones y responsabilidades?\n\u2022 Apoyar al equipo de metodolog\u00eda en el desarrollo de modelos de contrapartida y capital.\n\u2022 Implementar soluciones tecnol\u00f3gicas dentro de entornos IT gobernados.\n\u2022 Programar y optimizar c\u00f3digo en Scala, Python, Spark.\n\u2022 Aplicar metodolog\u00edas Agile y TDD en el desarrollo de software.\n\u2022 Participar en proyectos de integraci\u00f3n continua y despliegue con Jenkins o herramientas similares.\n\n\ud83c\udfc6 \u00bfQu\u00e9 ofrecemos?\n\u2022 Salario Competitivo.\n\u2022 Retribuci\u00f3n flexible.\n\u2022 Programa de formaci\u00f3n t\u00e9cnica, certificaciones e idiomas.\n\u2022 Plan de Desarrollo Profesional.\n\u2022 Flexibilidad horaria.\n\u2022 Eventos de teambuilding y voluntariado.\n\u2022 Referral Program.\n\n\u00danete a un equipo de NEXTERS que van m\u00e1s all\u00e1, donde la creatividad, el ingenio y la audacia marcan la diferencia en un mundo lleno de oportunidades.\n\nEn Lognext apostamos por la igualdad entre mujeres y hombres y prueba de ello tenemos un Plan de Igualdad registrado y publicado. Asimismo, creemos que en la multiculturalidad existe una fuente de valores, experiencias y conocimiento que aporta un valor a\u00f1adido a nuestros proyectos. Creemos en la diversidad y estamos comprometidos con ella, creando entornos en los que se trate con respeto y dignidad sin importar de d\u00f3nde vengas. No discriminamos por motivos de raza, religi\u00f3n o creencias, etnia, discapacidad, edad, nacionalidad, estado civil, orientaci\u00f3n sexual o g\u00e9nero.\n\n#Empleo #Tecnolog\u00eda #Innovaci\u00f3n #DesarrolloProfesional #Igualdad #Diversidad\nEn Lognext llevamos m\u00e1s de 18 a\u00f1os identificando e implementando soluciones tecnol\u00f3gicas pr\u00e1cticas que nos permitan seguir avanzando y optimicen nuestras operaciones, acompa\u00f1ando a los equipos con talento experto de alto rendimiento y haciendo de la tecnolog\u00eda una fuerza transformadora en nuestro d\u00eda a d\u00eda.\nLognext\nBuscamos un Data Engineer & Risk Modeling para unirse a nuestro equipo en una multinacional l\u00edder en el sector financiero.\nData Engineer\n&\nRisk Modeling\n\ud83d\udd0d \u00bfQu\u00e9 buscamos?\n\u00bfQu\u00e9 buscamos?\n\u2022 Conocimientos en regulaci\u00f3n en riesgo de contrapartida y en capital (regulatorio y econ\u00f3mico).\n\u2022 Experiencia en desarrollo de software en entornos tecnol\u00f3gicos gobernados.\n\u2022 Se valorar\u00e1 experiencia en modelos de riesgos financieros.\n\u2022 Programaci\u00f3n test-driven development (TDD) y metodolog\u00eda Agile.\n\u2022 Experiencia en SCALA, Python, Spark y despliegues continuos (Jenkins o similares).\n\u2022 Experiencia en definici\u00f3n y desarrollo de modelos de contrapartida y/o capital en el marco de regulaci\u00f3n financiera.\n\u2022 Experiencia en proyectos de integraci\u00f3n continua y control de versiones.\n\u2022 Formaci\u00f3n en Matem\u00e1ticas, F\u00edsica, Estad\u00edstica, Ingenier\u00eda o similar.\n\u2022 Nivel de ingl\u00e9s alto (C1 m\u00ednimo).\n\ud83d\udee0\ufe0f \u00bfCu\u00e1les ser\u00e1n tus principales funciones y responsabilidades?\n\u00bfCu\u00e1les ser\u00e1n tus principales funciones y responsabilidades?\n\u2022 Apoyar al equipo de metodolog\u00eda en el desarrollo de modelos de contrapartida y capital.\n\u2022 Implementar soluciones tecnol\u00f3gicas dentro de entornos IT gobernados.\n\u2022 Programar y optimizar c\u00f3digo en Scala, Python, Spark.\n\u2022 Aplicar metodolog\u00edas Agile y TDD en el desarrollo de software.\n\u2022 Participar en proyectos de integraci\u00f3n continua y despliegue con Jenkins o herramientas similares.\n\ud83c\udfc6 \u00bfQu\u00e9 ofrecemos?\n\u00bfQu\u00e9 ofrecemos?\n\u2022 Salario Competitivo.\n\u2022 Retribuci\u00f3n flexible.\n\u2022 Programa de formaci\u00f3n t\u00e9cnica, certificaciones e idiomas.\n\u2022 Plan de Desarrollo Profesional.\n\u2022 Flexibilidad horaria.\n\u2022 Eventos de teambuilding y voluntariado.\n\u2022 Referral Program.\n\u00danete a un equipo de NEXTERS que van m\u00e1s all\u00e1, donde la creatividad, el ingenio y la audacia marcan la diferencia en un mundo lleno de oportunidades.\nNEXTERS\nEn Lognext apostamos por la igualdad entre mujeres y hombres y prueba de ello tenemos un Plan de Igualdad registrado y publicado. Asimismo, creemos que en la multiculturalidad existe una fuente de valores, experiencias y conocimiento que aporta un valor a\u00f1adido a nuestros proyectos. Creemos en la diversidad y estamos comprometidos con ella, creando entornos en los que se trate con respeto y dignidad sin importar de d\u00f3nde vengas. No discriminamos por motivos de raza, religi\u00f3n o creencias, etnia, discapacidad, edad, nacionalidad, estado civil, orientaci\u00f3n sexual o g\u00e9nero.\n#Empleo #Tecnolog\u00eda #Innovaci\u00f3n #DesarrolloProfesional #Igualdad #Diversidad"
    },
    "4158251846": {
        "title": "Data Engineer",
        "company": "Astrafy",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nYour mission\n\nYou will work on different projects to help Astrafy customers get the most out of their data. You will work on the business and technical sides of the customer use case.\n\nDesign and maintain scalable data pipelines leveraging technologies such as Airflow, dbt, BigQuery, and Snowflake, ensuring efficient and reliable data ingestion, transformation, and delivery.\nDevelop and optimize data infrastructure in the Google Cloud environment, implementing best practices for performance, cost, and security.\nUtilize Terraform and Kubernetes to automate infrastructure provisioning and manage containerized workloads, promoting agility and repeatability across environments.\nImplement robust data governance and quality measures, ensuring accuracy, consistency, and compliance throughout the data lifecycle.\nCollaborate with cross-functional teams to design and deploy Looker dashboards and other analytics solutions that empower stakeholders with actionable insights.\nContinuously refine data architecture to accommodate changing business needs, scaling solutions to handle increased data volume and complexity.\nChampion a culture of innovation by researching, evaluating, and recommending emerging data technologies and industry best practices.\n\nIf you don't check every box, don't worry. No one knows everything, but we expect a great motivation to learn, take ownership, and become part of a supportive team. At Astrafy, everyone is learning and teaching others every day in their role.\n\nIn addition to these core responsibilities, we strongly value knowledge-sharing and community engagement. At Astrafy, we are active contributors on platforms like Medium, where we regularly publish articles to share our expertise, insights, and innovations, helping to grow both our community and industry presence.\n\nAs part of the role, the candidate will also be encouraged to write articles, offering valuable insights and thought leadership to enhance Astrafy's impact further and foster greater collaboration within our field.\n\nYour profile\n\nEducational Background: Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field (or equivalent practical experience).\nProfessional Experience: 1+ years of hands-on experience in data engineering or software engineering roles, building and optimizing data pipelines.\nTechnical Expertise: Proficiency in SQL and at least one programming language (e.g., Python), with a proven track record of working with modern data stack components like Airflow, dbt, and BigQuery or Snowflake.\nCloud Knowledge: Familiarity with Google Cloud Platform (GCP) services for data storage, orchestration, and analytics; experience with other cloud providers is a plus.\nInfrastructure as Code (IaC): Understanding of Terraform for automating infrastructure setup and management, plus exposure to container orchestration with Kubernetes.\nAnalytics & Visualization: Experience with BI tools such as Looker, including designing and developing dashboards for data-driven insights.\nData Governance & Security: Knowledge of best practices for data quality, lineage, and security, ensuring compliance with relevant regulations and standards.\nTeam Player: Strong communication and collaboration skills, with the ability to work effectively within cross-functional teams to deliver impactful data solutions.\nContinuous Learning: Curiosity to stay updated on emerging data technologies, practices, and frameworks, and to share knowledge across the organization.\nStrong data visualization skills to convey information and results clearly\nYou speak English fluently, and a word of French and/or Spanish is a plus\n\nWas this list a bit much? Don't worry - we don't expect you to tick every box from the beginning.\n\nWhy us?\n\nWhat We Offer\n\nAttractive Salary Package: No blurry or hidden clauses. Everything is transparently outlined in our Gitbook (https://astrafy.gitbook.io/handbook/its-all-about-people/compensation)\nGenuine Innovation: An exciting role where technology innovation is more than a buzzword\u2014it's how we operate daily.\nStrong Values & Culture: Become part of a dynamic team that lives by solid values. Learn more in our \u201cCulture and Values\u201d chart (https://docs.astrafy.io/handbook/the-company/culture-and-values)\nContinuous Learning: We offer ongoing training and development for both soft and hard skills. Check out our training policy (https://docs.astrafy.io/handbook/its-all-about-people/training)\nFlexible Work Environment: Enjoy flexible hours and remote work options. Discover more here. (https://docs.astrafy.io/handbook/the-company/remote-work)\nTeam-Building & Retreats: Thrive in a supportive environment, supported by regular team activities. Learn more here (https://docs.astrafy.io/handbook/its-all-about-people/team-building-and-retreat).\nAnd Much More: Our handbook (https://docs.astrafy.io/handbook/) covers all you need to know about who we are and how we work.\n\nAbout Us\n\nOur mission is to help companies and individuals solve data analytics challenges across the full data journey\u2014from ingestion to transformation to distribution\u2014leveraging a Modern Data Stack. We find that many organizations lack the internal expertise to harness emerging data tools, and we aim to educate them while implementing powerful solutions.\n\nWe achieve this by creating a new kind of consulting company that:\n\nFosters Creativity & Strategic Thinking: We embrace out-of-the-box solutions tailored to each client\u2019s needs.\nPrioritizes Education: We ensure every project delivers both technical solutions and the knowledge to sustain them.\nInvests in People: We nurture our employees\u2019 growth and well-being, recognizing they are our most important asset.\n\nAs a Google Cloud partner, we host most solutions on Google Cloud. We\u2019re experts in dbt, Airflow, and Airbyte, continually exploring emerging technologies to stay ahead of market trends.\nYour mission\nDesign and maintain scalable data pipelines leveraging technologies such as Airflow, dbt, BigQuery, and Snowflake, ensuring efficient and reliable data ingestion, transformation, and delivery.\nDevelop and optimize data infrastructure in the Google Cloud environment, implementing best practices for performance, cost, and security.\nUtilize Terraform and Kubernetes to automate infrastructure provisioning and manage containerized workloads, promoting agility and repeatability across environments.\nImplement robust data governance and quality measures, ensuring accuracy, consistency, and compliance throughout the data lifecycle.\nCollaborate with cross-functional teams to design and deploy Looker dashboards and other analytics solutions that empower stakeholders with actionable insights.\nContinuously refine data architecture to accommodate changing business needs, scaling solutions to handle increased data volume and complexity.\nChampion a culture of innovation by researching, evaluating, and recommending emerging data technologies and industry best practices.\nDesign and maintain scalable data pipelines leveraging technologies such as Airflow, dbt, BigQuery, and Snowflake, ensuring efficient and reliable data ingestion, transformation, and delivery.\nDevelop and optimize data infrastructure in the Google Cloud environment, implementing best practices for performance, cost, and security.\nUtilize Terraform and Kubernetes to automate infrastructure provisioning and manage containerized workloads, promoting agility and repeatability across environments.\nImplement robust data governance and quality measures, ensuring accuracy, consistency, and compliance throughout the data lifecycle.\nCollaborate with cross-functional teams to design and deploy Looker dashboards and other analytics solutions that empower stakeholders with actionable insights.\nContinuously refine data architecture to accommodate changing business needs, scaling solutions to handle increased data volume and complexity.\nChampion a culture of innovation by researching, evaluating, and recommending emerging data technologies and industry best practices.\nYour profile\nEducational Background: Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field (or equivalent practical experience).\nProfessional Experience: 1+ years of hands-on experience in data engineering or software engineering roles, building and optimizing data pipelines.\nTechnical Expertise: Proficiency in SQL and at least one programming language (e.g., Python), with a proven track record of working with modern data stack components like Airflow, dbt, and BigQuery or Snowflake.\nCloud Knowledge: Familiarity with Google Cloud Platform (GCP) services for data storage, orchestration, and analytics; experience with other cloud providers is a plus.\nInfrastructure as Code (IaC): Understanding of Terraform for automating infrastructure setup and management, plus exposure to container orchestration with Kubernetes.\nAnalytics & Visualization: Experience with BI tools such as Looker, including designing and developing dashboards for data-driven insights.\nData Governance & Security: Knowledge of best practices for data quality, lineage, and security, ensuring compliance with relevant regulations and standards.\nTeam Player: Strong communication and collaboration skills, with the ability to work effectively within cross-functional teams to deliver impactful data solutions.\nContinuous Learning: Curiosity to stay updated on emerging data technologies, practices, and frameworks, and to share knowledge across the organization.\nStrong data visualization skills to convey information and results clearly\nYou speak English fluently, and a word of French and/or Spanish is a plus\nEducational Background: Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field (or equivalent practical experience).\nProfessional Experience: 1+ years of hands-on experience in data engineering or software engineering roles, building and optimizing data pipelines.\nTechnical Expertise: Proficiency in SQL and at least one programming language (e.g., Python), with a proven track record of working with modern data stack components like Airflow, dbt, and BigQuery or Snowflake.\nCloud Knowledge: Familiarity with Google Cloud Platform (GCP) services for data storage, orchestration, and analytics; experience with other cloud providers is a plus.\nInfrastructure as Code (IaC): Understanding of Terraform for automating infrastructure setup and management, plus exposure to container orchestration with Kubernetes.\nAnalytics & Visualization: Experience with BI tools such as Looker, including designing and developing dashboards for data-driven insights.\nData Governance & Security: Knowledge of best practices for data quality, lineage, and security, ensuring compliance with relevant regulations and standards.\nTeam Player: Strong communication and collaboration skills, with the ability to work effectively within cross-functional teams to deliver impactful data solutions.\nContinuous Learning: Curiosity to stay updated on emerging data technologies, practices, and frameworks, and to share knowledge across the organization.\nStrong data visualization skills to convey information and results clearly\nYou speak English fluently, and a word of French and/or Spanish is a plus\nWhy us?\nWhat We Offer\nAttractive Salary Package: No blurry or hidden clauses. Everything is transparently outlined in our Gitbook (https://astrafy.gitbook.io/handbook/its-all-about-people/compensation)\nGenuine Innovation: An exciting role where technology innovation is more than a buzzword\u2014it's how we operate daily.\nStrong Values & Culture: Become part of a dynamic team that lives by solid values. Learn more in our \u201cCulture and Values\u201d chart (https://docs.astrafy.io/handbook/the-company/culture-and-values)\nContinuous Learning: We offer ongoing training and development for both soft and hard skills. Check out our training policy (https://docs.astrafy.io/handbook/its-all-about-people/training)\nFlexible Work Environment: Enjoy flexible hours and remote work options. Discover more here. (https://docs.astrafy.io/handbook/the-company/remote-work)\nTeam-Building & Retreats: Thrive in a supportive environment, supported by regular team activities. Learn more here (https://docs.astrafy.io/handbook/its-all-about-people/team-building-and-retreat).\nAnd Much More: Our handbook (https://docs.astrafy.io/handbook/) covers all you need to know about who we are and how we work.\nAttractive Salary Package: No blurry or hidden clauses. Everything is transparently outlined in our Gitbook (https://astrafy.gitbook.io/handbook/its-all-about-people/compensation)\nGenuine Innovation: An exciting role where technology innovation is more than a buzzword\u2014it's how we operate daily.\nStrong Values & Culture: Become part of a dynamic team that lives by solid values. Learn more in our \u201cCulture and Values\u201d chart (https://docs.astrafy.io/handbook/the-company/culture-and-values)\nContinuous Learning: We offer ongoing training and development for both soft and hard skills. Check out our training policy (https://docs.astrafy.io/handbook/its-all-about-people/training)\nFlexible Work Environment: Enjoy flexible hours and remote work options. Discover more here. (https://docs.astrafy.io/handbook/the-company/remote-work)\nTeam-Building & Retreats: Thrive in a supportive environment, supported by regular team activities. Learn more here (https://docs.astrafy.io/handbook/its-all-about-people/team-building-and-retreat).\nAnd Much More: Our handbook (https://docs.astrafy.io/handbook/) covers all you need to know about who we are and how we work.\nAbout Us\nModern Data Stack\neducate\nimplementing powerful solutions\nFosters Creativity & Strategic Thinking: We embrace out-of-the-box solutions tailored to each client\u2019s needs.\nPrioritizes Education: We ensure every project delivers both technical solutions and the knowledge to sustain them.\nInvests in People: We nurture our employees\u2019 growth and well-being, recognizing they are our most important asset.\nFosters Creativity & Strategic Thinking: We embrace out-of-the-box solutions tailored to each client\u2019s needs.\nPrioritizes Education: We ensure every project delivers both technical solutions and the knowledge to sustain them.\nInvests in People: We nurture our employees\u2019 growth and well-being, recognizing they are our most important asset.\nGoogle Cloud partner\ndbt\nAirflow\nAirbyte"
    },
    "4166469537": {
        "title": "Sr. Data Engineer (Remote, International) ",
        "company": "PulsePoint",
        "location": "European Economic Area",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nA bit about us:\nPulsePoint is a leading technology company that uses real-world data in real-time to optimize campaign performance and revolutionize health decision-making. Leveraging proprietary datasets and methodology, PulsePoint targets healthcare professionals and patients with an unprecedented level of accuracy\u2014delivering unparalleled results to the clients we serve. The company is now a part of Internet Brands, a KKR portfolio company and owner of WebMD Health Corp.\n\nSr. Data Engineer\nPulsePoint Data Engineering team plays a key role in our technology company that\u2019s experiencing exponential growth. Our data pipeline processes over 80 billion impressions a day (> 20TB of data, 220 TB uncompressed). This data is used to generate reports, update budgets, and drive our optimization engines. We do all this while running against extremely tight SLAs and provide stats and reports as close to real-time as possible.\n\nThe most exciting part about working at PulsePoint is the enormous potential for personal and professional growth. We are always seeking new and better tools to help us meet challenges such as adopting proven open-source technologies to make our data infrastructure more nimble, scalable and robust. Some of the cutting-edge technologies we have recently implemented are Kafka, Spark Streaming, Presto, Airflow, and Kubernetes.\n\nWhat you'll be doing:\nDesign, build, and maintain reliable and scalable enterprise-level distributed transactional data processing systems for scaling the existing business and supporting new business initiatives\nOptimize jobs to utilize Kafka, Hadoop, Presto, Spark, and Kubernetes resources in the most efficient way\nMonitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)\nIncrease accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases)\nCollaborate within a small team with diverse technology backgrounds\nProvide mentorship and guidance to junior team members\n\nTeam Responsibilities:\nIngest, validate and process internal & third party data\nCreate, maintain and monitor data flows in Spark, Hive, SQL and Presto for consistency, accuracy and lag time\nMaintain and enhance framework for jobs(primarily aggregate jobs in Spark and Hive)\nCreate different consumers for data in Kafka using Spark Streaming for near time aggregation\nTool evaluation/selection/implementation\nBackups/Retention/High Availability/Capacity Planning\nReview/Approval - DDL for database, Hive Framework jobs and Spark Streaming to make sure they meet our standards\n\nTechnologies We Use:\nAirflow - for job scheduling\nDocker - Packaged container image with all dependencies\nGraphite/Beacon - for monitoring data flows\nHive - SQL data warehouse layer for data in HDFS\nKafka- distributed commit log storage\nKubernetes - Distributed cluster resource manager\nPresto - fast parallel data warehouse and data federation layer\nSpark Streaming - Near time aggregation\nSQL Server - Reliable OLTP RDBMS\nGCP BQ\nApache Iceberg\n\nRequirements:\n6+ years of data engineering experience\nFluency in Python and SQL\nStrong recent Spark experience\nExperience working in on-prem environments\nHadoop and Hive experience\nExperience in Scala/Java is a plus (Polyglot programmer preferred!)\nProficiency in Linux\nStrong understanding of RDBMS and query optimization\nPassion for engineering and computer science around data\nWilling and able to work East Coast U.S. hours (9am-6pm EST); you can work fully remotely\nKnowledge and exposure to distributed production systems i.e Hadoop\nKnowledge and exposure to Cloud migration (AWS/GCP/Azure) is a plus\n\nLocation:\nWe can hire as FTE in the UK and Netherlands\nWe can hire as long-term contractor (independent or B2B) in most other countries\n\nSelection Process:\n1) Initial Screen (30 mins)\n2) Hiring Manager Interview (45 mins)\n3) CodeSignal Online Assessment (90 mins)\n4) Interview with Sr. Data Engineer (60 mins)\n5) Team Interviews (90 mins + 3 x 45 mins) + SVP of Engineering (30 mins)\n6) WebMD Sr. Director, DBA (30 mins)\n\nNote that leetcode-style live coding challenges will be involved in the process.\n\nWebMD and its affiliates is an Equal Opportunity/Affirmative Action employer and does not discriminate on the basis of race, ancestry, color, religion, sex, gender, age, marital status, sexual orientation, gender identity, national origin, medical condition, disability, veterans status, or any other basis protected by law.\nA bit about us:\nA bit about us\nPulsePoint is a leading technology company that uses real-world data in real-time to optimize campaign performance and revolutionize health decision-making. Leveraging proprietary datasets and methodology, PulsePoint targets healthcare professionals and patients with an unprecedented level of accuracy\u2014delivering unparalleled results to the clients we serve. The company is now a part of Internet Brands, a KKR portfolio company and owner of WebMD Health Corp.\nPulsePoint\nSr. Data Engineer\nPulsePoint Data Engineering team plays a key role in our technology company that\u2019s experiencing exponential growth. Our data pipeline processes over 80 billion impressions a day (> 20TB of data, 220 TB uncompressed). This data is used to generate reports, update budgets, and drive our optimization engines. We do all this while running against extremely tight SLAs and provide stats and reports as close to real-time as possible.\nData Engineering\nThe most exciting part about working at PulsePoint is the enormous potential for personal and professional growth. We are always seeking new and better tools to help us meet challenges such as adopting proven open-source technologies to make our data infrastructure more nimble, scalable and robust. Some of the cutting-edge technologies we have recently implemented are Kafka, Spark Streaming, Presto, Airflow, and Kubernetes.\nWhat you'll be doing:\nWhat you'll be doing\nDesign, build, and maintain reliable and scalable enterprise-level distributed transactional data processing systems for scaling the existing business and supporting new business initiatives\nOptimize jobs to utilize Kafka, Hadoop, Presto, Spark, and Kubernetes resources in the most efficient way\nMonitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)\nIncrease accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases)\nCollaborate within a small team with diverse technology backgrounds\nProvide mentorship and guidance to junior team members\nDesign, build, and maintain reliable and scalable enterprise-level distributed transactional data processing systems for scaling the existing business and supporting new business initiatives\nOptimize jobs to utilize Kafka, Hadoop, Presto, Spark, and Kubernetes resources in the most efficient way\nMonitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)\nIncrease accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases)\nCollaborate within a small team with diverse technology backgrounds\nProvide mentorship and guidance to junior team members\nTeam Responsibilities:\nTeam Responsibilities\nIngest, validate and process internal & third party data\nCreate, maintain and monitor data flows in Spark, Hive, SQL and Presto for consistency, accuracy and lag time\nMaintain and enhance framework for jobs(primarily aggregate jobs in Spark and Hive)\nCreate different consumers for data in Kafka using Spark Streaming for near time aggregation\nTool evaluation/selection/implementation\nBackups/Retention/High Availability/Capacity Planning\nReview/Approval - DDL for database, Hive Framework jobs and Spark Streaming to make sure they meet our standards\nIngest, validate and process internal & third party data\nCreate, maintain and monitor data flows in Spark, Hive, SQL and Presto for consistency, accuracy and lag time\nMaintain and enhance framework for jobs(primarily aggregate jobs in Spark and Hive)\nCreate different consumers for data in Kafka using Spark Streaming for near time aggregation\nTool evaluation/selection/implementation\nBackups/Retention/High Availability/Capacity Planning\nReview/Approval - DDL for database, Hive Framework jobs and Spark Streaming to make sure they meet our standards\nTechnologies We Use:\nTechnologies We Use\nAirflow - for job scheduling\nDocker - Packaged container image with all dependencies\nGraphite/Beacon - for monitoring data flows\nHive - SQL data warehouse layer for data in HDFS\nKafka- distributed commit log storage\nKubernetes - Distributed cluster resource manager\nPresto - fast parallel data warehouse and data federation layer\nSpark Streaming - Near time aggregation\nSQL Server - Reliable OLTP RDBMS\nGCP BQ\nApache Iceberg\nAirflow - for job scheduling\nDocker - Packaged container image with all dependencies\nGraphite/Beacon - for monitoring data flows\nHive - SQL data warehouse layer for data in HDFS\nKafka- distributed commit log storage\nKubernetes - Distributed cluster resource manager\nPresto - fast parallel data warehouse and data federation layer\nSpark Streaming - Near time aggregation\nSQL Server - Reliable OLTP RDBMS\nGCP BQ\nApache Iceberg\nRequirements:\n6+ years of data engineering experience\nFluency in Python and SQL\nStrong recent Spark experience\nExperience working in on-prem environments\nHadoop and Hive experience\nExperience in Scala/Java is a plus (Polyglot programmer preferred!)\nProficiency in Linux\nStrong understanding of RDBMS and query optimization\nPassion for engineering and computer science around data\nWilling and able to work East Coast U.S. hours (9am-6pm EST); you can work fully remotely\nKnowledge and exposure to distributed production systems i.e Hadoop\nKnowledge and exposure to Cloud migration (AWS/GCP/Azure) is a plus\n6+ years of data engineering experience\nFluency in Python and SQL\nStrong recent Spark experience\nExperience working in on-prem environments\nHadoop and Hive experience\nExperience in Scala/Java is a plus (Polyglot programmer preferred!)\nProficiency in Linux\nStrong understanding of RDBMS and query optimization\nPassion for engineering and computer science around data\nWilling and able to work East Coast U.S. hours (9am-6pm EST); you can work fully remotely\nKnowledge and exposure to distributed production systems i.e Hadoop\nKnowledge and exposure to Cloud migration (AWS/GCP/Azure) is a plus\nLocation:\nWe can hire as FTE in the UK and Netherlands\nWe can hire as long-term contractor (independent or B2B) in most other countries\nWe can hire as FTE in the UK and Netherlands\nWe can hire as long-term contractor (independent or B2B) in most other countries\nSelection Process:\n1) Initial Screen (30 mins)\n2) Hiring Manager Interview (45 mins)\n3) CodeSignal Online Assessment (90 mins)\n4) Interview with Sr. Data Engineer (60 mins)\n5) Team Interviews (90 mins + 3 x 45 mins) + SVP of Engineering (30 mins)\n6) WebMD Sr. Director, DBA (30 mins)\nNote that leetcode-style live coding challenges will be involved in the process.\nWebMD and its affiliates is an Equal Opportunity/Affirmative Action employer and does not discriminate on the basis of race, ancestry, color, religion, sex, gender, age, marital status, sexual orientation, gender identity, national origin, medical condition, disability, veterans status, or any other basis protected by law."
    },
    "4141612874": {
        "title": "Data Engineer",
        "company": "Otto Group one.O",
        "location": "M\u00e1laga, Andalusia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nWhat will you do?\n\nBe part of the team responsible for the migration of Data and Processes for BAUR and UNITO from the legacy store data platform for BI and ERP to the Google Cloud. \nWe want to significantly increase the usability and impact of our data platform for real-time and batch processing within our customers - with your expertise!\nTogether with your colleagues, you would be responsible for the design and implementation of our new, high-performance and robust BI platform in the Google Cloud for big, fast & smart data. 100% Cloud only. The methodology for migrating all remaining processes to the Cloud is based on Lift & Shift basis.\nYou would also be responsible for working in the migration of data for the Legacy ERP into the new one. \nProviding stability into the provided solution is also part and implies participation in the maintenance of the data.\nWith a strong customer focus, we migrate the data for our customer BI analysis applications, through understanding of data and processes, with transparency along the data processing and meeting all legal and regulatory requirements. \n\nWhat's your story?\n\nYour passion: Large amounts of data, their high-performance processing and technical innovations.\nYou have excellent conceptual skills and a quick mind.\nYou are eager for knowledge in BI processes and data analysis, transforming data into knowledge to inspire customers and have a basic technical understanding.\nYou excel at designing models that reduce data redundancy and improve enterprise information management.\nVery good developer skills in SQL.\nConfident handling of relational databases (e.g. BigQuery) and concepts of data warehousing and data modelling (e.g. DataVault beneficial).\nExperience in DBT. It could optionally be beneficial experience in Python.\nBasic experience in the development of scalable cloud solutions based on Google or other cloud providers (e.g. Terraform, Google Composer, etc.).\nExperience in the development of methodical approach: a clean object-oriented or functional design, tests and monitorability characterize your software. Clean build and deploy processes are important to you.\nYou are characterized by team spirit, initiative and self-organization and strong communication skills.\nGood English writen and spoken.\n\nBenefits\n\nPermanent contract \nFlexible working hours (you decide how to organize your day to day!) \nHybrid work mode (work from home up to 80% of the time, having the rest to meet you colleagues at the office!) \nYou will be part of a fast growing company, being part of a great team \nCompetitive salary\nFlexible retribution\nMedical insurance \nAgile working methods\nTraining and Development: we have a clear focus on technical innovation, but we don't forget the personal growth!\n\n\u00bfQui\u00e9nes somos?\n\nMore than just IT\n\nOSP (Otto Group Solution Provider) is an IT service provider for retail and logistics with headquarters in Dresden. We live our passion for IT in an appreciative, trusting work environment. What drives us is the idea of working together as a team to achieve great things for our customers.\n\nWe rely on the personal responsibility and willingness to learn of our employees, on modern technologies and high quality in software development. Agile working, transparent decisions, a lively feedback culture and collegial cooperation make us successful.\n\nWe are active for customers inside and outside the Otto Group. With over 450 employees at several German and international locations, we have been developing flexible software and BI solutions since 1991 and are thus shaping the shopping worlds of tomorrow. We are part of the Otto Group and share a value system that focuses on responsible and sustainable action.\nWhat will you do?\nBe part of the team responsible for the migration of Data and Processes for BAUR and UNITO from the legacy store data platform for BI and ERP to the Google Cloud. \nWe want to significantly increase the usability and impact of our data platform for real-time and batch processing within our customers - with your expertise!\nTogether with your colleagues, you would be responsible for the design and implementation of our new, high-performance and robust BI platform in the Google Cloud for big, fast & smart data. 100% Cloud only. The methodology for migrating all remaining processes to the Cloud is based on Lift & Shift basis.\nYou would also be responsible for working in the migration of data for the Legacy ERP into the new one. \nProviding stability into the provided solution is also part and implies participation in the maintenance of the data.\nWith a strong customer focus, we migrate the data for our customer BI analysis applications, through understanding of data and processes, with transparency along the data processing and meeting all legal and regulatory requirements.\nBe part of the team responsible for the migration of Data and Processes for BAUR and UNITO from the legacy store data platform for BI and ERP to the Google Cloud.\nWe want to significantly increase the usability and impact of our data platform for real-time and batch processing within our customers - with your expertise!\nTogether with your colleagues, you would be responsible for the design and implementation of our new, high-performance and robust BI platform in the Google Cloud for big, fast & smart data. 100% Cloud only. The methodology for migrating all remaining processes to the Cloud is based on Lift & Shift basis.\nYou would also be responsible for working in the migration of data for the Legacy ERP into the new one.\nProviding stability into the provided solution is also part and implies participation in the maintenance of the data.\nWith a strong customer focus, we migrate the data for our customer BI analysis applications, through understanding of data and processes, with transparency along the data processing and meeting all legal and regulatory requirements.\nWhat's your story?\nYour passion: Large amounts of data, their high-performance processing and technical innovations.\nYou have excellent conceptual skills and a quick mind.\nYou are eager for knowledge in BI processes and data analysis, transforming data into knowledge to inspire customers and have a basic technical understanding.\nYou excel at designing models that reduce data redundancy and improve enterprise information management.\nVery good developer skills in SQL.\nConfident handling of relational databases (e.g. BigQuery) and concepts of data warehousing and data modelling (e.g. DataVault beneficial).\nExperience in DBT. It could optionally be beneficial experience in Python.\nBasic experience in the development of scalable cloud solutions based on Google or other cloud providers (e.g. Terraform, Google Composer, etc.).\nExperience in the development of methodical approach: a clean object-oriented or functional design, tests and monitorability characterize your software. Clean build and deploy processes are important to you.\nYou are characterized by team spirit, initiative and self-organization and strong communication skills.\nGood English writen and spoken.\nYour passion: Large amounts of data, their high-performance processing and technical innovations.\nYou have excellent conceptual skills and a quick mind.\nYou are eager for knowledge in BI processes and data analysis, transforming data into knowledge to inspire customers and have a basic technical understanding.\nYou excel at designing models that reduce data redundancy and improve enterprise information management.\nVery good developer skills in SQL.\nConfident handling of relational databases (e.g. BigQuery) and concepts of data warehousing and data modelling (e.g. DataVault beneficial).\nExperience in DBT. It could optionally be beneficial experience in Python.\nBasic experience in the development of scalable cloud solutions based on Google or other cloud providers (e.g. Terraform, Google Composer, etc.).\nExperience in the development of methodical approach: a clean object-oriented or functional design, tests and monitorability characterize your software. Clean build and deploy processes are important to you.\nYou are characterized by team spirit, initiative and self-organization and strong communication skills.\nGood English writen and spoken.\nBenefits\nPermanent contract \nFlexible working hours (you decide how to organize your day to day!) \nHybrid work mode (work from home up to 80% of the time, having the rest to meet you colleagues at the office!) \nYou will be part of a fast growing company, being part of a great team \nCompetitive salary\nFlexible retribution\nMedical insurance \nAgile working methods\nTraining and Development: we have a clear focus on technical innovation, but we don't forget the personal growth!\nPermanent contract\nFlexible working hours (you decide how to organize your day to day!)\nHybrid work mode (work from home up to 80% of the time, having the rest to meet you colleagues at the office!)\nYou will be part of a fast growing company, being part of a great team\nCompetitive salary\nFlexible retribution\nMedical insurance\nAgile working methods\nTraining and Development: we have a clear focus on technical innovation, but we don't forget the personal growth!\n\u00bfQui\u00e9nes somos?\nMore than just IT"
    },
    "4171638230": {
        "title": "Data Engineer - Databricks - Tech Lead ",
        "company": "Lumenalta",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nExperience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\n\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\n\nRequirements\n10+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\nYou are comfortable manipulating large data sets and handling raw SQL\nExperience using technologies such as Pyspark/AWS/Databricks is essential\nExperience creating ETL Pipeline from scratch\nExperience leading teams and managing projects\nE-commerce and Financial Services industry experience preferred\nEnglish fluency, verbal and written\nExperience dealing with C-Suite level stakeholders\nPersonality traits: Professional, problem solver, proactive, passionate, team orientated.\n\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\nBe 100% dedicated to one project at a time so that you can innovate and grow.\nBe a part of a team of talented and friendly senior-level developers.\nWork on projects that allow you to use leading tech.\n\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\n\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\n\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\nExperience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\nRequirements\n10+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\nYou are comfortable manipulating large data sets and handling raw SQL\nExperience using technologies such as Pyspark/AWS/Databricks is essential\nExperience creating ETL Pipeline from scratch\nExperience leading teams and managing projects\nE-commerce and Financial Services industry experience preferred\nEnglish fluency, verbal and written\nExperience dealing with C-Suite level stakeholders\nPersonality traits: Professional, problem solver, proactive, passionate, team orientated.\n10+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\nYou are comfortable manipulating large data sets and handling raw SQL\nExperience using technologies such as Pyspark/AWS/Databricks is essential\nExperience creating ETL Pipeline from scratch\nExperience leading teams and managing projects\nE-commerce and Financial Services industry experience preferred\nEnglish fluency, verbal and written\nExperience dealing with C-Suite level stakeholders\nPersonality traits: Professional, problem solver, proactive, passionate, team orientated.\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\nBe 100% dedicated to one project at a time so that you can innovate and grow.\nBe a part of a team of talented and friendly senior-level developers.\nWork on projects that allow you to use leading tech.\nBe 100% dedicated to one project at a time so that you can innovate and grow.\nBe a part of a team of talented and friendly senior-level developers.\nWork on projects that allow you to use leading tech.\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply"
    },
    "4121155984": {
        "title": "Lead Data Software Engineer (Databricks) ",
        "company": "EPAM Systems",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nWe are looking for a Lead Data Software Engineer with a strong background in Databricks and an open-minded personality. This role requires someone ready to join a friendly environment and become a core contributor to our team of experts.\n\nDo you have background and wide experience in Data engineering and strong knowledge in Databricks? Are you an open-minded professional with good English skills? If it sounds like you, this could be the perfect opportunity to join EPAM as a Lead Data Software Engineer.\n\nResponsibilities\n\n\nLead a small team of engineers in designing, developing, and maintaining scalable data pipelines and architectures\nDevelop and optimize data models and ETL processes using Databricks and other technologies\nImplement data quality checks and monitoring to ensure high data integrity\nStay updated with emerging trends in data engineering and recommend new tools when beneficial\nTroubleshoot and resolve data-related issues promptly\nParticipate in code reviews to maintain high standards of code quality\n\n\nRequirements\n\n\nHands-on experience with Databricks, including Delta Lake, workflows, Delta Live Tables, deployment, and versioning\nAbility to lead a development team of 2-5 engineers\nSolid understanding of data architectures and data modelling; experienced in designing and building ETL pipelines with Databricks\nExpertise in Spark using (either Scala or PySpark)\nKnowledge of SDLC and Agile frameworks\nProficiency in cloud-native technologies and software engineering best practices, including containers, unit tests, linting, and code style checks\nEngineering experience with AWS, Azure, or GCP\nExperience with big data and performance optimization of data-intensive applications\nProactivity and client-facing experience\nAbility to deal with ambiguity and work independently\nDesire to work in a transparent and fast-moving startup environment\nFluent in English communication skills at a B2+ level\n\n\nWe offer\n\n\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\n\n\nEPAM is a leading digital transformation services and product engineering company with over 52,650 EPAMers in more than 55 countries and regions. Since 1993, our multidisciplinary teams have been helping make the future real for our clients and communities around the world. In 2018, we opened an office in Spain that quickly grew to over 1,450 EPAMers distributed between the offices in M\u00e1laga and Madrid as well as remotely across the country. Here you will collaborate with multinational teams, contribute to numerous innovative projects, and have an opportunity to learn and grow continuously.\n\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nLead Data Software Engineer\nResponsibilities\nLead a small team of engineers in designing, developing, and maintaining scalable data pipelines and architectures\nDevelop and optimize data models and ETL processes using Databricks and other technologies\nImplement data quality checks and monitoring to ensure high data integrity\nStay updated with emerging trends in data engineering and recommend new tools when beneficial\nTroubleshoot and resolve data-related issues promptly\nParticipate in code reviews to maintain high standards of code quality\nLead a small team of engineers in designing, developing, and maintaining scalable data pipelines and architectures\nDevelop and optimize data models and ETL processes using Databricks and other technologies\nImplement data quality checks and monitoring to ensure high data integrity\nStay updated with emerging trends in data engineering and recommend new tools when beneficial\nTroubleshoot and resolve data-related issues promptly\nParticipate in code reviews to maintain high standards of code quality\nRequirements\nHands-on experience with Databricks, including Delta Lake, workflows, Delta Live Tables, deployment, and versioning\nAbility to lead a development team of 2-5 engineers\nSolid understanding of data architectures and data modelling; experienced in designing and building ETL pipelines with Databricks\nExpertise in Spark using (either Scala or PySpark)\nKnowledge of SDLC and Agile frameworks\nProficiency in cloud-native technologies and software engineering best practices, including containers, unit tests, linting, and code style checks\nEngineering experience with AWS, Azure, or GCP\nExperience with big data and performance optimization of data-intensive applications\nProactivity and client-facing experience\nAbility to deal with ambiguity and work independently\nDesire to work in a transparent and fast-moving startup environment\nFluent in English communication skills at a B2+ level\nHands-on experience with Databricks, including Delta Lake, workflows, Delta Live Tables, deployment, and versioning\nAbility to lead a development team of 2-5 engineers\nSolid understanding of data architectures and data modelling; experienced in designing and building ETL pipelines with Databricks\nExpertise in Spark using (either Scala or PySpark)\nKnowledge of SDLC and Agile frameworks\nProficiency in cloud-native technologies and software engineering best practices, including containers, unit tests, linting, and code style checks\nEngineering experience with AWS, Azure, or GCP\nExperience with big data and performance optimization of data-intensive applications\nProactivity and client-facing experience\nAbility to deal with ambiguity and work independently\nDesire to work in a transparent and fast-moving startup environment\nFluent in English communication skills at a B2+ level\nWe offer\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends."
    },
    "4149723775": {
        "title": "Data Engineer - Java/ Confluent (Kafka) ",
        "company": "METRICA",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nSomos M\u00e9trica Consulting, una consultora de negocio, integraci\u00f3n de sistemas, servicios y soluciones IT, comprometida con la Sociedad de la informaci\u00f3n y el desarrollo tecnol\u00f3gico, cuyos objetivos son un continuo crecimiento y convertirnos, para nuestros clientes, en un partner de negocio y tecnolog\u00eda cada d\u00eda m\u00e1s s\u00f3lido.\n\nActualmente estamos en b\u00fasqueda de un/a Data Engineer con, al menos, 2 a\u00f1os de experiencia en esta \u00e1rea con Java y Confluent (Kafka). \n\n\u00bfQu\u00e9 buscamos?\n\n-Experiencia de, al menos, 2 a\u00f1os como Data Engineer con Java y Confluent (Kafka). \n-Arquitecturas de microservicios y patrones de integraci\u00f3n con Kafka.\n-Desarrollo de Pipelines de datos. \n-Herramientas de integraci\u00f3n continua (CI/CD). \n-Control de versiones Git. \n-Metodolog\u00eda Agile. \n\n\u00bfQu\u00e9 podemos ofrecerte?\n- Contrato indefinido.\n- Modalidad remota.\n- Formaci\u00f3n online en aquellas \u00e1reas que resulten de tu inter\u00e9s.\n- Jornada completa de Lunes a Viernes.\n- Salario valorable.\n\n\u00bfCrees que puedes encajar, y busca un proyecto nuevo?\u00a1inscr\u00edbete y deja que te conozcamos!\nSomos M\u00e9trica Consulting, una consultora de negocio, integraci\u00f3n de sistemas, servicios y soluciones IT, comprometida con la Sociedad de la informaci\u00f3n y el desarrollo tecnol\u00f3gico, cuyos objetivos son un continuo crecimiento y convertirnos, para nuestros clientes, en un partner de negocio y tecnolog\u00eda cada d\u00eda m\u00e1s s\u00f3lido.\nActualmente estamos en b\u00fasqueda de un/a Data Engineer con, al menos, 2 a\u00f1os de experiencia en esta \u00e1rea con Java y Confluent (Kafka).\n\u00bfQu\u00e9 buscamos?\n-Experiencia de, al menos, 2 a\u00f1os como Data Engineer con Java y Confluent (Kafka).\n-Arquitecturas de microservicios y patrones de integraci\u00f3n con Kafka.\n-Desarrollo de Pipelines de datos.\n-Herramientas de integraci\u00f3n continua (CI/CD).\n-Control de versiones Git.\n-Metodolog\u00eda Agile.\n\u00bfQu\u00e9 podemos ofrecerte?\n- Contrato indefinido.\n- Modalidad remota.\n- Formaci\u00f3n online en aquellas \u00e1reas que resulten de tu inter\u00e9s.\n- Jornada completa de Lunes a Viernes.\n- Salario valorable.\n\u00bfCrees que puedes encajar, y busca un proyecto nuevo?\u00a1inscr\u00edbete y deja que te conozcamos!"
    },
    "4178189621": {
        "title": "Data Engineer & Analyst \u2013 Digital Advertising",
        "company": "Recodme",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nEn } ReCodme ( aceleramos el desarrollo tecnol\u00f3gico de nuestros clientes y colaboradores a trav\u00e9s de soluciones especializadas, aportando valor en su continuidad y crecimiento.\n\nCreemos en la cercan\u00eda con nuestras personas y en el valor a\u00f1adido que aportan a nuestro equipo y proyectos. \u00a1Un ReCoder es un aficionado de la tecnolog\u00eda, es [la clave de nuestro \u00e9xito], la clave de nuestra Fuerza!\n\nBuscamos un/a Data Engineer & Analyst con experiencia en publicidad digital, modelado de datos e integraci\u00f3n de plataformas de gesti\u00f3n de anuncios. El rol implica trabajar con DSPs, ETL en la nube y herramientas de visualizaci\u00f3n para optimizar estrategias de marketing basadas en datos.\n\n\ud83d\udce2 \u00bfQu\u00e9 necesitas para ser parte de este proyecto?\n\nRequisitos:\nDise\u00f1ar y optimizar modelos de datos, dashboards e informes para seguimiento de KPIs publicitarios.\nRealizar integraciones de datos con plataformas como Google Ads, Meta, TikTok, LinkedIn y otras herramientas de publicidad digital.\nGestionar la interacci\u00f3n de datos con DSPs (Demand Side Platforms) y plataformas de Big Data.\nDesarrollar y mantener pipelines de datos utilizando Python, SQL y herramientas Cloud ETL como Ribery.\nAsegurar la correcta integraci\u00f3n de datos con Google BigQuery y otras herramientas de anal\u00edtica.\nSupervisar la migraci\u00f3n, seguimiento del ROI y normalizaci\u00f3n de datos en grandes vol\u00famenes de informaci\u00f3n.\nTrabajar con herramientas de visualizaci\u00f3n como Power BI para mejorar la toma de decisiones basada en datos.\n\nRequisitos:\nExperiencia en modelado de datos, reporting y desarrollo de dashboards.\nConocimiento en Power BI para la visualizaci\u00f3n de datos.\nHabilidad en programaci\u00f3n con Python y SQL para el desarrollo de integraciones.\nExperiencia en integraciones con DSPs y plataformas publicitarias (Meta, Google Ads, TikTok, LinkedIn, etc.).\nExperiencia en ETL en la nube con herramientas como Ribery y BigQuery.\nCapacidad para analizar grandes vol\u00famenes de datos y generar insights accionables.\n\nModalidad de Trabajo:\nH\u00edbrido en Madrid.\n\n\u00bfListo para esta fuerza? \u00a1Env\u00edanos tu candidatura!\n\nEn Recodme, defendemos la igualdad y valoramos la diversidad. Creemos en un entorno seguro e inclusivo, donde todas las oportunidades son equitativas para nuestros colaboradores.\nNo discriminamos por edad, etnia, orientaci\u00f3n sexual, g\u00e9nero, discapacidad ni ning\u00fan otro factor no relacionado con el m\u00e9rito.\n\n\u00a1Todas las candidaturas que cumplan con los requisitos son bienvenidas!\nEn } ReCodme ( aceleramos el desarrollo tecnol\u00f3gico de nuestros clientes y colaboradores a trav\u00e9s de soluciones especializadas, aportando valor en su continuidad y crecimiento.\nCreemos en la cercan\u00eda con nuestras personas y en el valor a\u00f1adido que aportan a nuestro equipo y proyectos. \u00a1Un ReCoder es un aficionado de la tecnolog\u00eda, es [la clave de nuestro \u00e9xito], la clave de nuestra Fuerza!\nBuscamos un/a Data Engineer & Analyst con experiencia en publicidad digital, modelado de datos e integraci\u00f3n de plataformas de gesti\u00f3n de anuncios. El rol implica trabajar con DSPs, ETL en la nube y herramientas de visualizaci\u00f3n para optimizar estrategias de marketing basadas en datos.\nData Engineer & Analyst\npublicidad digital, modelado de datos e integraci\u00f3n de plataformas de gesti\u00f3n de anuncios\nDSPs, ETL en la nube y herramientas de visualizaci\u00f3n\n\ud83d\udce2 \u00bfQu\u00e9 necesitas para ser parte de este proyecto?\n\u00bfQu\u00e9 necesitas para ser parte de este proyecto?\nRequisitos:\nDise\u00f1ar y optimizar modelos de datos, dashboards e informes para seguimiento de KPIs publicitarios.\nRealizar integraciones de datos con plataformas como Google Ads, Meta, TikTok, LinkedIn y otras herramientas de publicidad digital.\nGestionar la interacci\u00f3n de datos con DSPs (Demand Side Platforms) y plataformas de Big Data.\nDesarrollar y mantener pipelines de datos utilizando Python, SQL y herramientas Cloud ETL como Ribery.\nAsegurar la correcta integraci\u00f3n de datos con Google BigQuery y otras herramientas de anal\u00edtica.\nSupervisar la migraci\u00f3n, seguimiento del ROI y normalizaci\u00f3n de datos en grandes vol\u00famenes de informaci\u00f3n.\nTrabajar con herramientas de visualizaci\u00f3n como Power BI para mejorar la toma de decisiones basada en datos.\nDise\u00f1ar y optimizar modelos de datos, dashboards e informes para seguimiento de KPIs publicitarios.\nmodelos de datos, dashboards e informes\nRealizar integraciones de datos con plataformas como Google Ads, Meta, TikTok, LinkedIn y otras herramientas de publicidad digital.\nintegraciones de datos\nGoogle Ads, Meta, TikTok, LinkedIn y otras herramientas de publicidad digital\nGestionar la interacci\u00f3n de datos con DSPs (Demand Side Platforms) y plataformas de Big Data.\nDSPs (Demand Side Platforms)\nBig Data\nDesarrollar y mantener pipelines de datos utilizando Python, SQL y herramientas Cloud ETL como Ribery.\npipelines de datos\nPython, SQL y herramientas Cloud ETL como Ribery\nAsegurar la correcta integraci\u00f3n de datos con Google BigQuery y otras herramientas de anal\u00edtica.\nGoogle BigQuery y otras herramientas de anal\u00edtica\nSupervisar la migraci\u00f3n, seguimiento del ROI y normalizaci\u00f3n de datos en grandes vol\u00famenes de informaci\u00f3n.\nmigraci\u00f3n, seguimiento del ROI y normalizaci\u00f3n de datos\nTrabajar con herramientas de visualizaci\u00f3n como Power BI para mejorar la toma de decisiones basada en datos.\nPower BI\nExperiencia en modelado de datos, reporting y desarrollo de dashboards.\nConocimiento en Power BI para la visualizaci\u00f3n de datos.\nHabilidad en programaci\u00f3n con Python y SQL para el desarrollo de integraciones.\nExperiencia en integraciones con DSPs y plataformas publicitarias (Meta, Google Ads, TikTok, LinkedIn, etc.).\nExperiencia en ETL en la nube con herramientas como Ribery y BigQuery.\nCapacidad para analizar grandes vol\u00famenes de datos y generar insights accionables.\nExperiencia en modelado de datos, reporting y desarrollo de dashboards.\nExperiencia en modelado de datos, reporting y desarrollo de dashboards\nConocimiento en Power BI para la visualizaci\u00f3n de datos.\nConocimiento en Power BI\nHabilidad en programaci\u00f3n con Python y SQL para el desarrollo de integraciones.\nHabilidad en programaci\u00f3n con Python y SQL\nExperiencia en integraciones con DSPs y plataformas publicitarias (Meta, Google Ads, TikTok, LinkedIn, etc.).\nExperiencia en integraciones con DSPs y plataformas publicitarias\nExperiencia en ETL en la nube con herramientas como Ribery y BigQuery.\nExperiencia en ETL en la nube\nRibery\nBigQuery\nCapacidad para analizar grandes vol\u00famenes de datos y generar insights accionables.\nCapacidad para analizar grandes vol\u00famenes de datos y generar insights accionables\nModalidad de Trabajo:\nH\u00edbrido en Madrid.\n\u00bfListo para esta fuerza? \u00a1Env\u00edanos tu candidatura!\nEn Recodme, defendemos la igualdad y valoramos la diversidad. Creemos en un entorno seguro e inclusivo, donde todas las oportunidades son equitativas para nuestros colaboradores.\nNo discriminamos por edad, etnia, orientaci\u00f3n sexual, g\u00e9nero, discapacidad ni ning\u00fan otro factor no relacionado con el m\u00e9rito.\n\u00a1Todas las candidaturas que cumplan con los requisitos son bienvenidas!"
    },
    "4170822267": {
        "title": "Data engineer senior - Databricks ",
        "company": "hiberus",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1Hola! \ud83d\ude4b\n\n\u00bfTodav\u00eda no conoces HIBERUS TECNOLOG\u00cdA? Somos una empresa de #tecnolog\u00eda construida con un ingrediente diferencial, la HIPERESPECIALIZACI\u00d3N.\n\nFormar parte de Hiberus significa crecimiento, pasi\u00f3n por la tecnolog\u00eda, inter\u00e9s por la innovaci\u00f3n, ambiente laboral flexible y colaborativo, compa\u00f1erismo, aprendizaje, formaci\u00f3n continua, motivaci\u00f3n y superaci\u00f3n ante nuevos retos...y esto es solo el principio.\nSomos m\u00e1s de 3.500 profesionales y no paramos de crecer!\ud83d\ude4c\nActualmente contamos con m\u00e1s de 36 hubs de desarrollo en Espa\u00f1a, EEUU, Reino Unido, Alemania, Ruman\u00eda, Italia, Andorra, Marruecos, Argentina, M\u00e9xico y Colombia \u00a1Estamos redefiniendo el mapa tecnol\u00f3gico mundial! \ud83c\udf10\ud83d\udcbb\n\nTrabajamos muy duro, para transformar nuestra vida a trav\u00e9s de la tecnolog\u00eda y queremos contar contigo para seguir creciendo. Si lo tuyo es el mundo IT, te gusta estar al d\u00eda en las \u00faltimas tecnolog\u00edas y quieres asumir un nuevo reto profesional \u00a1Sigue leyendo, te estamos buscando!!\n\n\ud83d\udd0e\u00bfQu\u00e9 estamos buscando?\nBuscamos un/a Data Engineer Senior con experiencia en Databricks. Este rol ser\u00e1 clave en la construcci\u00f3n de pipelines de datos a gran escala, asegurando la calidad, eficiencia y rendimiento del procesamiento de datos en entornos big data.\nBuscamos personas que nos ayuden a seguir creciendo, con ganas de aportar y seguir evolucionando de la mano de los mejores profesionales, pero sobre todo... \u00a1en Hiberus buscamos buena gente!\ud83d\ude09\n\n\ud83d\udcbbAlgunas de tus funciones:\nAplicar mejores pr\u00e1cticas de desarrollo con Scala en Databricks, optimizando consultas y asegurando un rendimiento \u00f3ptimo de Apache Spark.\nConstruir, optimizar y mantener pipelines ETL/ELT de datos a gran escala, garantizando eficiencia y calidad en el procesamiento\nImplementar estrategias de CI/CD en Azure DevOps para la automatizaci\u00f3n de despliegues en entornos de datos.\nTrabajar en entornos basados en la nube (Azure, AWS o GCP) para la gesti\u00f3n de data lakes y lakehouse architectures\n\n\ud83d\udccb\ud83d\udd8b\ufe0f Habilidades y requisitos\nExperiencia avanzada en desarrollo con Scala en Databricks.\nS\u00f3lido conocimiento en Apache Spark, optimizaci\u00f3n de queries y particionamiento de datos para el procesamiento a gran escala.\nExperiencia en construcci\u00f3n y orquestaci\u00f3n de pipelines ETL/ELT en entornos big data\nConocimiento en arquitecturas Lakehouse, Data Lakes y Data Warehouses en la nube.\nManejo de CI/CD con Azure DevOps para la automatizaci\u00f3n de despliegues en entornos Databricks.\nExperiencia en gesti\u00f3n de cl\u00fasteres y optimizaci\u00f3n de costos en Databricks.\nExperiencia con modelado de datos, particionamiento y almacenamiento en formatos optimizados (Parquet, Delta Lake, Avro, etc.).\nIngl\u00e9s avanzado\n\n\ud83d\ude4c\u00bfQu\u00e9 te ofrecemos?\n\ud83d\udcbc Contrato indefinido en una compa\u00f1\u00eda puramente tecnol\u00f3gica, que forma parte de un gran grupo, solvente y en crecimiento.\n\ud83d\udcb0 Salario fijo competitivo + bonificaci\u00f3n\n\ud83c\udfd6\ufe0f Ambiente familiar, cercano, \u00a1como una gran familia!\n\u23f0 Conciliaci\u00f3n con nuestra vida personal y laboral mediante horario flexible, desconexi\u00f3n digital, jornada intensiva viernes y verano.\n\ud83e\udd19 Cultura \u201ctechie\u201d, nos gusta estar en contacto con la tecnolog\u00eda, herramientas, y \u00faltimas novedades!\n\ud83d\udcda \u00a1Formaci\u00f3n! Siempre que quieras, disfrutar\u00e1s de un amplio cat\u00e1logo de cursos formativos, adaptados a tu perfil profesional, inquietudes y novedades del sector. Para ello, pondremos a tu disposici\u00f3n todo el potencial de nuestra Hiberus University y los acuerdos con los principales fabricantes.\n\ud83d\udcb3 \u00a1Benef\u00edciate! Programa de retribuci\u00f3n flexible a medida: seguro m\u00e9dico, tarjeta de transporte p\u00fablico, cheques guarder\u00eda, tarjeta restaurante, etc.\n\n\nEn Hiberus estamos viviendo un crecimiento explosivo \ud83d\udca5 y queremos que formes parte de nuestro equipo.\nSuena bien, \u00bfverdad? Si quieres saber m\u00e1s, \u00a1inscr\u00edbete y te contamos!\nSi quieres saber m\u00e1s busca nuestros hashtag #somoshiberus #lascosasocurrenaqu\u00ed y conoce todo lo que hacemos.\n\n\n\n\nHablamos?\n\n\nwww.hiberus.com\n\n\n#somosHiberus y t\u00fa? Te animas?\ud83d\ude80\n\u00a1Hola! \ud83d\ude4b\n\u00bfTodav\u00eda no conoces HIBERUS TECNOLOG\u00cdA? Somos una empresa de #tecnolog\u00eda construida con un ingrediente diferencial, la HIPERESPECIALIZACI\u00d3N.\nHIBERUS TECNOLOG\u00cdA\nHIPERESPECIALIZACI\u00d3N\nFormar parte de Hiberus significa crecimiento, pasi\u00f3n por la tecnolog\u00eda, inter\u00e9s por la innovaci\u00f3n, ambiente laboral flexible y colaborativo, compa\u00f1erismo, aprendizaje, formaci\u00f3n continua, motivaci\u00f3n y superaci\u00f3n ante nuevos retos...y esto es solo el principio.\nSomos m\u00e1s de 3.500 profesionales y no paramos de crecer!\ud83d\ude4c\nActualmente contamos con m\u00e1s de 36 hubs de desarrollo en Espa\u00f1a, EEUU, Reino Unido, Alemania, Ruman\u00eda, Italia, Andorra, Marruecos, Argentina, M\u00e9xico y Colombia \u00a1Estamos redefiniendo el mapa tecnol\u00f3gico mundial! \ud83c\udf10\ud83d\udcbb\nTrabajamos muy duro, para transformar nuestra vida a trav\u00e9s de la tecnolog\u00eda y queremos contar contigo para seguir creciendo. Si lo tuyo es el mundo IT, te gusta estar al d\u00eda en las \u00faltimas tecnolog\u00edas y quieres asumir un nuevo reto profesional \u00a1Sigue leyendo, te estamos buscando!!\n\ud83d\udd0e\u00bfQu\u00e9 estamos buscando?\nBuscamos un/a Data Engineer Senior con experiencia en Databricks. Este rol ser\u00e1 clave en la construcci\u00f3n de pipelines de datos a gran escala, asegurando la calidad, eficiencia y rendimiento del procesamiento de datos en entornos big data.\nData Engineer Senior\nDatabricks\nbig data\nBuscamos personas que nos ayuden a seguir creciendo, con ganas de aportar y seguir evolucionando de la mano de los mejores profesionales, pero sobre todo... \u00a1en Hiberus buscamos buena gente!\ud83d\ude09\n\ud83d\udcbbAlgunas de tus funciones:\nAlgunas de tus funciones:\nAplicar mejores pr\u00e1cticas de desarrollo con Scala en Databricks, optimizando consultas y asegurando un rendimiento \u00f3ptimo de Apache Spark.\nConstruir, optimizar y mantener pipelines ETL/ELT de datos a gran escala, garantizando eficiencia y calidad en el procesamiento\nImplementar estrategias de CI/CD en Azure DevOps para la automatizaci\u00f3n de despliegues en entornos de datos.\nTrabajar en entornos basados en la nube (Azure, AWS o GCP) para la gesti\u00f3n de data lakes y lakehouse architectures\nAplicar mejores pr\u00e1cticas de desarrollo con Scala en Databricks, optimizando consultas y asegurando un rendimiento \u00f3ptimo de Apache Spark.\nmejores pr\u00e1cticas de desarrollo con Scala en Databricks\nApache Spark\nConstruir, optimizar y mantener pipelines ETL/ELT de datos a gran escala, garantizando eficiencia y calidad en el procesamiento\nConstruir, optimizar y mantener pipelines ETL/ELT\nImplementar estrategias de CI/CD en Azure DevOps para la automatizaci\u00f3n de despliegues en entornos de datos.\nCI/CD en Azure DevOps\nTrabajar en entornos basados en la nube (Azure, AWS o GCP) para la gesti\u00f3n de data lakes y lakehouse architectures\nAzure, AWS o GCP\ndata lakes y lakehouse architectures\n\ud83d\udccb\ud83d\udd8b\ufe0f Habilidades y requisitos\nExperiencia avanzada en desarrollo con Scala en Databricks.\nS\u00f3lido conocimiento en Apache Spark, optimizaci\u00f3n de queries y particionamiento de datos para el procesamiento a gran escala.\nExperiencia en construcci\u00f3n y orquestaci\u00f3n de pipelines ETL/ELT en entornos big data\nConocimiento en arquitecturas Lakehouse, Data Lakes y Data Warehouses en la nube.\nManejo de CI/CD con Azure DevOps para la automatizaci\u00f3n de despliegues en entornos Databricks.\nExperiencia en gesti\u00f3n de cl\u00fasteres y optimizaci\u00f3n de costos en Databricks.\nExperiencia con modelado de datos, particionamiento y almacenamiento en formatos optimizados (Parquet, Delta Lake, Avro, etc.).\nIngl\u00e9s avanzado\nExperiencia avanzada en desarrollo con Scala en Databricks.\nExperiencia\nScala en Databricks.\nS\u00f3lido conocimiento en Apache Spark, optimizaci\u00f3n de queries y particionamiento de datos para el procesamiento a gran escala.\nS\u00f3lido conocimiento en Apache Spark\nExperiencia en construcci\u00f3n y orquestaci\u00f3n de pipelines ETL/ELT en entornos big data\nconstrucci\u00f3n y orquestaci\u00f3n de pipelines ETL/ELT\nConocimiento en arquitecturas Lakehouse, Data Lakes y Data Warehouses en la nube.\narquitecturas Lakehouse, Data Lakes y Data Warehouses\nManejo de CI/CD con Azure DevOps para la automatizaci\u00f3n de despliegues en entornos Databricks.\nManejo de CI/CD con Azure DevOps\nExperiencia en gesti\u00f3n de cl\u00fasteres y optimizaci\u00f3n de costos en Databricks.\ngesti\u00f3n de cl\u00fasteres y optimizaci\u00f3n de costos en Databricks\nExperiencia con modelado de datos, particionamiento y almacenamiento en formatos optimizados (Parquet, Delta Lake, Avro, etc.).\nmodelado de datos, particionamiento y almacenamiento en formatos optimizados (Parquet, Delta Lake, Avro, etc.).\nIngl\u00e9s avanzado\nIngl\u00e9s\n\ud83d\ude4c\u00bfQu\u00e9 te ofrecemos?\n\ud83d\udcbc Contrato indefinido en una compa\u00f1\u00eda puramente tecnol\u00f3gica, que forma parte de un gran grupo, solvente y en crecimiento.\n\ud83d\udcb0 Salario fijo competitivo + bonificaci\u00f3n\n\ud83c\udfd6\ufe0f Ambiente familiar, cercano, \u00a1como una gran familia!\n\u23f0 Conciliaci\u00f3n con nuestra vida personal y laboral mediante horario flexible, desconexi\u00f3n digital, jornada intensiva viernes y verano.\n\ud83e\udd19 Cultura \u201ctechie\u201d, nos gusta estar en contacto con la tecnolog\u00eda, herramientas, y \u00faltimas novedades!\n\ud83d\udcda \u00a1Formaci\u00f3n! Siempre que quieras, disfrutar\u00e1s de un amplio cat\u00e1logo de cursos formativos, adaptados a tu perfil profesional, inquietudes y novedades del sector. Para ello, pondremos a tu disposici\u00f3n todo el potencial de nuestra Hiberus University y los acuerdos con los principales fabricantes.\n\ud83d\udcb3 \u00a1Benef\u00edciate! Programa de retribuci\u00f3n flexible a medida: seguro m\u00e9dico, tarjeta de transporte p\u00fablico, cheques guarder\u00eda, tarjeta restaurante, etc.\nEn Hiberus estamos viviendo un crecimiento explosivo \ud83d\udca5 y queremos que formes parte de nuestro equipo.\nSuena bien, \u00bfverdad? Si quieres saber m\u00e1s, \u00a1inscr\u00edbete y te contamos!\nSi quieres saber m\u00e1s busca nuestros hashtag #somoshiberus #lascosasocurrenaqu\u00ed y conoce todo lo que hacemos.\nHablamos?\nwww.hiberus.com\n#somosHiberus y t\u00fa? Te animas?\ud83d\ude80"
    },
    "4146942950": {
        "title": "Data Scientist/ Data Engineer (W2/EOR- US/CAN, Europe and LATAM)",
        "company": "Braintrust",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nJob Description\n\nThis is a 6-month contract with possibility of extension, 40 hours/ week. This is classified as an EOR engagement. No C2C Engagements. No visa sponsorship/ visa transfer available.\n\nOpen to candidates based in LATAM, Europe, US and/or Canada who are able to overlap PST-EST hours.\n\nPlease note: The hourly rate may vary depending on the candidate\u2019s country of residence due to Employer of Record (EOR) fees. These fees, which cover essential services such as payroll processing, benefits administration, and compliance with local employment laws, are country-specific and can impact the final rate offered.\n\nReddit is a community of communities where people can dive into anything through experiences built around their interests, hobbies, and passions. Our mission is to bring community, belonging, and empowerment to everyone in the world. Reddit users submit, vote, and comment on content, stories, and discussions about the topics they care about the most. From pets to parenting, with over 100,000 active communities and over 70 million daily active users, it is home to the most open and authentic conversations on the internet.\n\nWe are looking for a skilled Data Scientist / Data Engineer to join our team and help drive data-driven decision-making. You will be responsible for designing and maintaining data pipelines, building analytical models, and ensuring data availability for business insights. This role requires a mix of engineering expertise, machine learning knowledge, and the ability to work with large datasets.\n\nKey Responsibilities\n\nDesign, develop, and maintain scalable data pipelines and ETL processes.\nBuild, deploy, and optimize machine learning models for predictive analytics.\nEnsure data integrity, reliability, and performance across systems.\nDevelop and maintain data warehouse architectures and data lakes.\nCollaborate with cross-functional teams, including data analysts, product managers, and software engineers.\nImplement best practices for data governance, security, and compliance.\nWork with cloud platforms (AWS, GCP, Azure) to manage data infrastructure.\nDevelop real-time data processing solutions using Kafka, Spark, or similar technologies.\nCreate data visualization dashboards and reports for business stakeholders.\n\nRequired Qualifications\n\nProgramming Languages: Proficiency in Python, SQL, and either Java or Scala.\nData Engineering: Experience with ETL pipelines, Apache Airflow, dbt, or similar tools.\nBig Data & Databases: Hands-on experience with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB, Snowflake, BigQuery).\nMachine Learning (if applicable): Familiarity with scikit-learn, TensorFlow, or PyTorch.\nCloud Platforms: Experience with AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery).\nData Streaming: Knowledge of Kafka, Kinesis, or similar technologies.\nData Warehousing: Understanding of star schema, OLAP, and data modeling principles.\nStrong analytical and problem-solving skills with attention to detail.\nJob Description\nThis is a 6-month contract with possibility of extension, 40 hours/ week. This is classified as an EOR engagement. No C2C Engagements. No visa sponsorship/ visa transfer available.\nOpen to candidates based in LATAM, Europe, US and/or Canada who are able to overlap PST-EST hours.\nPlease note: The hourly rate may vary depending on the candidate\u2019s country of residence due to Employer of Record (EOR) fees. These fees, which cover essential services such as payroll processing, benefits administration, and compliance with local employment laws, are country-specific and can impact the final rate offered.\nData Scientist / Data Engineer\nKey Responsibilities\nDesign, develop, and maintain scalable data pipelines and ETL processes.\nBuild, deploy, and optimize machine learning models for predictive analytics.\nEnsure data integrity, reliability, and performance across systems.\nDevelop and maintain data warehouse architectures and data lakes.\nCollaborate with cross-functional teams, including data analysts, product managers, and software engineers.\nImplement best practices for data governance, security, and compliance.\nWork with cloud platforms (AWS, GCP, Azure) to manage data infrastructure.\nDevelop real-time data processing solutions using Kafka, Spark, or similar technologies.\nCreate data visualization dashboards and reports for business stakeholders.\nDesign, develop, and maintain scalable data pipelines and ETL processes.\nBuild, deploy, and optimize machine learning models for predictive analytics.\nEnsure data integrity, reliability, and performance across systems.\nDevelop and maintain data warehouse architectures and data lakes.\nCollaborate with cross-functional teams, including data analysts, product managers, and software engineers.\nImplement best practices for data governance, security, and compliance.\nWork with cloud platforms (AWS, GCP, Azure) to manage data infrastructure.\nDevelop real-time data processing solutions using Kafka, Spark, or similar technologies.\nCreate data visualization dashboards and reports for business stakeholders.\nRequired Qualifications\nProgramming Languages: Proficiency in Python, SQL, and either Java or Scala.\nData Engineering: Experience with ETL pipelines, Apache Airflow, dbt, or similar tools.\nBig Data & Databases: Hands-on experience with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB, Snowflake, BigQuery).\nMachine Learning (if applicable): Familiarity with scikit-learn, TensorFlow, or PyTorch.\nCloud Platforms: Experience with AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery).\nData Streaming: Knowledge of Kafka, Kinesis, or similar technologies.\nData Warehousing: Understanding of star schema, OLAP, and data modeling principles.\nStrong analytical and problem-solving skills with attention to detail.\nProgramming Languages: Proficiency in Python, SQL, and either Java or Scala.\nData Engineering: Experience with ETL pipelines, Apache Airflow, dbt, or similar tools.\nBig Data & Databases: Hands-on experience with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB, Snowflake, BigQuery).\nMachine Learning (if applicable): Familiarity with scikit-learn, TensorFlow, or PyTorch.\nCloud Platforms: Experience with AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery).\nData Streaming: Knowledge of Kafka, Kinesis, or similar technologies.\nData Warehousing: Understanding of star schema, OLAP, and data modeling principles.\nStrong analytical and problem-solving skills with attention to detail."
    },
    "4139370241": {
        "title": "DATA ENGINEER",
        "company": "NUDE PROJECT",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nNUDE means Freedom\n\nNo tener que seguir el camino tradicional y poder crear por ti mismo la vida que deseas. Siempre siguiendo tu pasi\u00f3n.\n\nPROJECT means Process\n\nAlgo que est\u00e1 en proceso, que nunca se va a acabar y sigue cre\u00e1ndose cada d\u00eda. Un lienzo en blanco infinito donde siempre hay lugar para crear m\u00e1s.\n\nNUDE PROJECT va mucho m\u00e1s all\u00e1.\n\nNuestra misi\u00f3n es inspirar a las personas a perseguir su pasi\u00f3n.\n\nPara lograr nuestra misi\u00f3n, estamos buscando a un/una DATA ENGINEER para que se una a nuestro equipo. \ud83d\ude80\n\nComo Data Engineer en Nude Project, ser\u00e1s clave en la construcci\u00f3n y optimizaci\u00f3n de nuestra infraestructura de datos. Nos ayudar\u00e1s a transformar datos en informaci\u00f3n valiosa para potenciar nuestras decisiones estrat\u00e9gicas.\n\n\u00bfCu\u00e1les ser\u00e1n tus funciones?\n\n Colaboraci\u00f3n interdepartamental: Trabajar con otros departamentos para identificar, integrar y gestionar fuentes de datos en funci\u00f3n de sus necesidades espec\u00edficas, asegurando la alineaci\u00f3n con los objetivos de negocio. \n Optimizaci\u00f3n de procesos ETL: Dise\u00f1ar y mantener procesos de extracci\u00f3n, transformaci\u00f3n y carga (ETL) altamente eficientes mediante el uso de herramientas como DBT, garantizando la calidad, precisi\u00f3n y escalabilidad en los pipelines de datos. \n Modelado sem\u00e1ntico avanzado: Crear y gestionar modelos sem\u00e1nticos bien estructurados que faciliten la comprensi\u00f3n, el acceso y el uso efectivo de los datos por parte de los usuarios finales y sistemas de an\u00e1lisis. \n Desarrollo de dashboards personalizados: Dise\u00f1ar dashboards interactivos y personalizados utilizando Superset, adaptados a las necesidades de cada departamento para habilitar una toma de decisiones informada y oportuna.\n Automatizaci\u00f3n de an\u00e1lisis y reportes: Implementar procesos automatizados con Python y SQL para optimizar tareas repetitivas, reducir tareas manuales y entregar insights precisos de manera r\u00e1pida y escalable.\n Liderazgo t\u00e9cnico integral: Asumir la responsabilidad del liderazgo t\u00e9cnico del equipo de datos y tecnolog\u00eda, incluyendo la definici\u00f3n de prioridades estrat\u00e9gicas, la gesti\u00f3n de proyectos complejos y la supervisi\u00f3n de la calidad y consistencia de los entregables. \n\n\u00bfQu\u00e9 te har\u00e1 triunfar con nosotros?\n\nSer real y aut\u00e9ntico/aut\u00e9ntica.\nIr siempre m\u00e1s all\u00e1 buscando la magia. Think outside the box \ud83d\udca1\nAfrontar nuevos retos con ganas y de manera optimista. \u00a1Sin miedo al \u00e9xito! \nExperiencia previa de 2 a 4 a\u00f1os en ingenier\u00eda de datos o roles similares.\nConocimiento en herramientas de procesamiento de datos como SQL, Python, DBT.\nConocimiento en herramientas de visualizaci\u00f3n de datos c\u00f3mo Powerbi/ Superset/ Tableau.\nExperiencia en dise\u00f1o y optimizaci\u00f3n de bases de datos relacionales y no relacionales.\nHabilidades anal\u00edticas y capacidad de traducir datos en insights accionables.\nMentalidad innovadora, proactividad y pasi\u00f3n por la cultura digital.\n\nY\u2026 \u00bfQu\u00e9 te ofrecemos?\n\nFormar parte de un equipo joven y din\u00e1mico, con ganas de inspirar a las nuevas generaciones.\nFlexibilidad horaria.\nDescuentos exclusivos para nuestro equipo. \nOficinas nuevas en el 22@, en Barcelona. \nPosibilidad de optar por m\u00e9todos de remuneraci\u00f3n flexible.\nGympass\n\u00a1Somos pet friendly! \nClases de ingl\u00e9s seg\u00fan el nivel\nAfterworks & actividades de Teambuilding para fomentar el trabajo en equipo. \n\nSi te consideras una persona proactiva, sin zonas de confort y te apasiona el mundo del streetwear... \u00a1Te estamos esperando!\ud83e\udd0e\n\n\u00bfNo nos conoc\u00edas antes? Te animamos a echarle un vistazo a nuestro canal de youtube.\n\nNUDE PROJECT TEAM\nNUDE means Freedom\nPROJECT means Process\nNUDE PROJECT\nNuestra misi\u00f3n es inspirar a las personas a perseguir su pasi\u00f3n.\nDATA ENGINEER\nColaboraci\u00f3n interdepartamental: Trabajar con otros departamentos para identificar, integrar y gestionar fuentes de datos en funci\u00f3n de sus necesidades espec\u00edficas, asegurando la alineaci\u00f3n con los objetivos de negocio. \n Optimizaci\u00f3n de procesos ETL: Dise\u00f1ar y mantener procesos de extracci\u00f3n, transformaci\u00f3n y carga (ETL) altamente eficientes mediante el uso de herramientas como DBT, garantizando la calidad, precisi\u00f3n y escalabilidad en los pipelines de datos. \n Modelado sem\u00e1ntico avanzado: Crear y gestionar modelos sem\u00e1nticos bien estructurados que faciliten la comprensi\u00f3n, el acceso y el uso efectivo de los datos por parte de los usuarios finales y sistemas de an\u00e1lisis. \n Desarrollo de dashboards personalizados: Dise\u00f1ar dashboards interactivos y personalizados utilizando Superset, adaptados a las necesidades de cada departamento para habilitar una toma de decisiones informada y oportuna.\n Automatizaci\u00f3n de an\u00e1lisis y reportes: Implementar procesos automatizados con Python y SQL para optimizar tareas repetitivas, reducir tareas manuales y entregar insights precisos de manera r\u00e1pida y escalable.\n Liderazgo t\u00e9cnico integral: Asumir la responsabilidad del liderazgo t\u00e9cnico del equipo de datos y tecnolog\u00eda, incluyendo la definici\u00f3n de prioridades estrat\u00e9gicas, la gesti\u00f3n de proyectos complejos y la supervisi\u00f3n de la calidad y consistencia de los entregables.\nColaboraci\u00f3n interdepartamental: Trabajar con otros departamentos para identificar, integrar y gestionar fuentes de datos en funci\u00f3n de sus necesidades espec\u00edficas, asegurando la alineaci\u00f3n con los objetivos de negocio.\nOptimizaci\u00f3n de procesos ETL: Dise\u00f1ar y mantener procesos de extracci\u00f3n, transformaci\u00f3n y carga (ETL) altamente eficientes mediante el uso de herramientas como DBT, garantizando la calidad, precisi\u00f3n y escalabilidad en los pipelines de datos.\nModelado sem\u00e1ntico avanzado: Crear y gestionar modelos sem\u00e1nticos bien estructurados que faciliten la comprensi\u00f3n, el acceso y el uso efectivo de los datos por parte de los usuarios finales y sistemas de an\u00e1lisis.\nDesarrollo de dashboards personalizados: Dise\u00f1ar dashboards interactivos y personalizados utilizando Superset, adaptados a las necesidades de cada departamento para habilitar una toma de decisiones informada y oportuna.\nAutomatizaci\u00f3n de an\u00e1lisis y reportes: Implementar procesos automatizados con Python y SQL para optimizar tareas repetitivas, reducir tareas manuales y entregar insights precisos de manera r\u00e1pida y escalable.\nLiderazgo t\u00e9cnico integral: Asumir la responsabilidad del liderazgo t\u00e9cnico del equipo de datos y tecnolog\u00eda, incluyendo la definici\u00f3n de prioridades estrat\u00e9gicas, la gesti\u00f3n de proyectos complejos y la supervisi\u00f3n de la calidad y consistencia de los entregables.\nSer real y aut\u00e9ntico/aut\u00e9ntica.\nIr siempre m\u00e1s all\u00e1 buscando la magia. Think outside the box \ud83d\udca1\nAfrontar nuevos retos con ganas y de manera optimista. \u00a1Sin miedo al \u00e9xito! \nExperiencia previa de 2 a 4 a\u00f1os en ingenier\u00eda de datos o roles similares.\nConocimiento en herramientas de procesamiento de datos como SQL, Python, DBT.\nConocimiento en herramientas de visualizaci\u00f3n de datos c\u00f3mo Powerbi/ Superset/ Tableau.\nExperiencia en dise\u00f1o y optimizaci\u00f3n de bases de datos relacionales y no relacionales.\nHabilidades anal\u00edticas y capacidad de traducir datos en insights accionables.\nMentalidad innovadora, proactividad y pasi\u00f3n por la cultura digital.\nSer real y aut\u00e9ntico/aut\u00e9ntica.\nIr siempre m\u00e1s all\u00e1 buscando la magia. Think outside the box \ud83d\udca1\nAfrontar nuevos retos con ganas y de manera optimista. \u00a1Sin miedo al \u00e9xito!\nExperiencia previa de 2 a 4 a\u00f1os en ingenier\u00eda de datos o roles similares.\nConocimiento en herramientas de procesamiento de datos como SQL, Python, DBT.\nConocimiento en herramientas de visualizaci\u00f3n de datos c\u00f3mo Powerbi/ Superset/ Tableau.\nExperiencia en dise\u00f1o y optimizaci\u00f3n de bases de datos relacionales y no relacionales.\nHabilidades anal\u00edticas y capacidad de traducir datos en insights accionables.\nMentalidad innovadora, proactividad y pasi\u00f3n por la cultura digital.\nFormar parte de un equipo joven y din\u00e1mico, con ganas de inspirar a las nuevas generaciones.\nFlexibilidad horaria.\nDescuentos exclusivos para nuestro equipo. \nOficinas nuevas en el 22@, en Barcelona. \nPosibilidad de optar por m\u00e9todos de remuneraci\u00f3n flexible.\nGympass\n\u00a1Somos pet friendly! \nClases de ingl\u00e9s seg\u00fan el nivel\nAfterworks & actividades de Teambuilding para fomentar el trabajo en equipo.\nFormar parte de un equipo joven y din\u00e1mico, con ganas de inspirar a las nuevas generaciones.\nFlexibilidad horaria.\nDescuentos exclusivos para nuestro equipo.\nOficinas nuevas en el 22@, en Barcelona.\nPosibilidad de optar por m\u00e9todos de remuneraci\u00f3n flexible.\nGympass\n\u00a1Somos pet friendly!\nClases de ingl\u00e9s seg\u00fan el nivel\nAfterworks & actividades de Teambuilding para fomentar el trabajo en equipo."
    },
    "4135293079": {
        "title": "Data Engineer",
        "company": "ORBIDI",
        "location": "Sant Cugat del Vall\u00e8s, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nSobre nosotros: \nEn ORBIDI, somos pioneros en el uso de tecnolog\u00eda avanzada, inteligencia artificial y modelos predictivos para propulsar el crecimiento de las peque\u00f1as y medianas empresas (PYMEs). Nuestra pasi\u00f3n por la innovaci\u00f3n y el marketing nos motiva a buscar soluciones creativas que permitan a las empresas no solo competir, sino destacar en el mercado actual. Creemos firmemente en el poder de la digitalizaci\u00f3n y c\u00f3mo esta puede transformar los negocios para mejor. Buscamos talentos creativos y estrat\u00e9gicos que compartan nuestra visi\u00f3n y deseo de hacer una diferencia tangible en el mundo empresarial.\nEl rol:\nEstamos buscando un  Senior Data Engineer altamente capacitado para unirse a nuestro equipo. Como Ingeniero de Datos Senior, ser\u00e1s responsable de dise\u00f1ar, desarrollar y optimizar nuestra infraestructura de datos para respaldar diversas iniciativas basadas en datos en toda la organizaci\u00f3n. Desempe\u00f1ar\u00e1s un papel clave en la construcci\u00f3n de canalizaciones de datos escalables, garantizando la disponibilidad y confiabilidad de los datos, y colaborando con cient\u00edficos y analistas de datos para generar informaci\u00f3n estrat\u00e9gica para el negocio.\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n?\nDesarrollar, probar y mantener procesos ETL/ELT robustos para ingerir y transformar grandes vol\u00famenes de datos estructurados y no estructurados provenientes de diversas fuentes.\nDise\u00f1ar e implementar modelos de datos y soluciones de almacenamiento escalables para respaldar iniciativas de an\u00e1lisis, aprendizaje autom\u00e1tico e inteligencia empresarial.\nIntegrar diversas fuentes de datos de terceros e internas en plataformas de datos centralizadas y automatizar tareas repetitivas para mejorar la eficiencia.\nImplementar y mantener las mejores pr\u00e1cticas de gobernanza de datos, asegurando alta calidad de datos, seguridad y cumplimiento con est\u00e1ndares de la industria (GDPR, HIPAA, etc.).\nColaborar estrechamente con cient\u00edficos de datos, analistas y partes interesadas del negocio para comprender sus necesidades de datos y garantizar la entrega oportuna de datos precisos y relevantes.\nMonitorear continuamente el rendimiento de la infraestructura de datos, identificando cuellos de botella y optimizando los sistemas para mejorar la velocidad y la eficiencia en concurrencia, rendimiento de consultas y eficiencia de almacenamiento.\nTrabajar de cerca con el equipo de ciencia de datos para implementar canalizaciones de an\u00e1lisis de datos.\nLo que te har\u00e1 triunfar:\nM\u00e1s de 5 a\u00f1os de experiencia dise\u00f1ando y construyendo sistemas de datos a gran escala, con experiencia en ETL/ELT, modelado de datos y almacenamiento de datos.\nS\u00f3lidas habilidades de programaci\u00f3n en Python y conocimiento avanzado de SQL para manipulaci\u00f3n y an\u00e1lisis de datos.\nExperiencia pr\u00e1ctica con plataformas en la nube como AWS, Google Cloud o Azure, incluyendo servicios como AWS Redshift, BigQuery o Azure Data Lake.\nExperiencia con tecnolog\u00edas de big data como Hadoop, Spark, Kafka u otros marcos de computaci\u00f3n distribuida.\nConocimiento de bases de datos SQL y NoSQL (por ejemplo, PostgreSQL, MySQL, MongoDB, Cassandra) y experiencia en modelado de datos para un rendimiento \u00f3ptimo.\nCompetencia en herramientas de canalizaci\u00f3n y orquestaci\u00f3n de datos como Airflow, Prefect o Luigi.\nS\u00f3lida comprensi\u00f3n de soluciones de almacenamiento de datos, incluyendo lagos de datos (data lakes) y almacenes de datos (data warehouses).\nBeneficios:\n\ud83d\udd51Flexibilidad horaria\n\ud83d\udc8eOficinas modernas en Sant Cugat, frente a la estaci\u00f3n de FGC de Mirasol.\n\ud83d\udcb0Salario competitivo y contrato indefinido\n\ud83e\udd11Bonus basado en el cumplimiento de objetivos de tu equipo\n\ud83c\udf3423 d\u00edas laborales de vacaciones + \ud83c\udf82d\u00eda de cumplea\u00f1os\n\ud83c\udfe0Formato H\u00edbrido con teletrabajo 2 d\u00edas a la semana.\n\ud83d\ude0dJornada intensiva todos los viernes del a\u00f1o.\n\ud83d\udcc5Agosto flexible\n\ud83e\udd73Actividades de teambuilding y celebraciones y off-sites\n\ud83e\udd29Referral Program\n\ud83e\udd38\ud83c\udffb\u200d\u2640\ufe0fDescuento en gimnasio\n\ud83c\udf4eFruta en oficina\nSi tienes una pasi\u00f3n por el marketing digital, la creaci\u00f3n de contenido y el dise\u00f1o, y buscas un rol donde puedas dejar tu huella y contribuir al \u00e9xito de las PYMEs, \u00a1ORBIDI es el lugar para ti! Te invitamos a aplicar y ser parte de nuestro equipo creativo y estrat\u00e9gico.\nSobre nosotros:\nEn ORBIDI, somos pioneros en el uso de tecnolog\u00eda avanzada, inteligencia artificial y modelos predictivos para propulsar el crecimiento de las peque\u00f1as y medianas empresas (PYMEs). Nuestra pasi\u00f3n por la innovaci\u00f3n y el marketing nos motiva a buscar soluciones creativas que permitan a las empresas no solo competir, sino destacar en el mercado actual. Creemos firmemente en el poder de la digitalizaci\u00f3n y c\u00f3mo esta puede transformar los negocios para mejor. Buscamos talentos creativos y estrat\u00e9gicos que compartan nuestra visi\u00f3n y deseo de hacer una diferencia tangible en el mundo empresarial.\nEl rol:\nEstamos buscando un  Senior Data Engineer altamente capacitado para unirse a nuestro equipo. Como Ingeniero de Datos Senior, ser\u00e1s responsable de dise\u00f1ar, desarrollar y optimizar nuestra infraestructura de datos para respaldar diversas iniciativas basadas en datos en toda la organizaci\u00f3n. Desempe\u00f1ar\u00e1s un papel clave en la construcci\u00f3n de canalizaciones de datos escalables, garantizando la disponibilidad y confiabilidad de los datos, y colaborando con cient\u00edficos y analistas de datos para generar informaci\u00f3n estrat\u00e9gica para el negocio.\nSenior\nData Engineer\n\u00bfCu\u00e1l ser\u00e1 tu misi\u00f3n?\nDesarrollar, probar y mantener procesos ETL/ELT robustos para ingerir y transformar grandes vol\u00famenes de datos estructurados y no estructurados provenientes de diversas fuentes.\nDise\u00f1ar e implementar modelos de datos y soluciones de almacenamiento escalables para respaldar iniciativas de an\u00e1lisis, aprendizaje autom\u00e1tico e inteligencia empresarial.\nIntegrar diversas fuentes de datos de terceros e internas en plataformas de datos centralizadas y automatizar tareas repetitivas para mejorar la eficiencia.\nImplementar y mantener las mejores pr\u00e1cticas de gobernanza de datos, asegurando alta calidad de datos, seguridad y cumplimiento con est\u00e1ndares de la industria (GDPR, HIPAA, etc.).\nColaborar estrechamente con cient\u00edficos de datos, analistas y partes interesadas del negocio para comprender sus necesidades de datos y garantizar la entrega oportuna de datos precisos y relevantes.\nMonitorear continuamente el rendimiento de la infraestructura de datos, identificando cuellos de botella y optimizando los sistemas para mejorar la velocidad y la eficiencia en concurrencia, rendimiento de consultas y eficiencia de almacenamiento.\nTrabajar de cerca con el equipo de ciencia de datos para implementar canalizaciones de an\u00e1lisis de datos.\nLo que te har\u00e1 triunfar:\nM\u00e1s de 5 a\u00f1os de experiencia dise\u00f1ando y construyendo sistemas de datos a gran escala, con experiencia en ETL/ELT, modelado de datos y almacenamiento de datos.\nS\u00f3lidas habilidades de programaci\u00f3n en Python y conocimiento avanzado de SQL para manipulaci\u00f3n y an\u00e1lisis de datos.\nExperiencia pr\u00e1ctica con plataformas en la nube como AWS, Google Cloud o Azure, incluyendo servicios como AWS Redshift, BigQuery o Azure Data Lake.\nExperiencia con tecnolog\u00edas de big data como Hadoop, Spark, Kafka u otros marcos de computaci\u00f3n distribuida.\nConocimiento de bases de datos SQL y NoSQL (por ejemplo, PostgreSQL, MySQL, MongoDB, Cassandra) y experiencia en modelado de datos para un rendimiento \u00f3ptimo.\nCompetencia en herramientas de canalizaci\u00f3n y orquestaci\u00f3n de datos como Airflow, Prefect o Luigi.\nS\u00f3lida comprensi\u00f3n de soluciones de almacenamiento de datos, incluyendo lagos de datos (data lakes) y almacenes de datos (data warehouses).\nBeneficios:\n\ud83d\udd51Flexibilidad horaria\n\ud83d\udc8eOficinas modernas en Sant Cugat, frente a la estaci\u00f3n de FGC de Mirasol.\n\ud83d\udcb0Salario competitivo y contrato indefinido\n\ud83e\udd11Bonus basado en el cumplimiento de objetivos de tu equipo\n\ud83c\udf3423 d\u00edas laborales de vacaciones + \ud83c\udf82d\u00eda de cumplea\u00f1os\n\ud83c\udfe0Formato H\u00edbrido con teletrabajo 2 d\u00edas a la semana.\n\ud83d\ude0dJornada intensiva todos los viernes del a\u00f1o.\n\ud83d\udcc5Agosto flexible\n\ud83e\udd73Actividades de teambuilding y celebraciones y off-sites\n\ud83e\udd29Referral Program\n\ud83e\udd38\ud83c\udffb\u200d\u2640\ufe0fDescuento en gimnasio\n\ud83c\udf4eFruta en oficina\n\ud83d\udd51Flexibilidad horaria\n\ud83d\udc8eOficinas modernas en Sant Cugat, frente a la estaci\u00f3n de FGC de Mirasol.\n\ud83d\udcb0Salario competitivo y contrato indefinido\n\ud83e\udd11Bonus basado en el cumplimiento de objetivos de tu equipo\n\ud83c\udf3423 d\u00edas laborales de vacaciones + \ud83c\udf82d\u00eda de cumplea\u00f1os\n\ud83c\udfe0Formato H\u00edbrido con teletrabajo 2 d\u00edas a la semana.\n\ud83d\ude0dJornada intensiva todos los viernes del a\u00f1o.\n\ud83d\udcc5Agosto flexible\n\ud83e\udd73Actividades de teambuilding y celebraciones y off-sites\n\ud83e\udd29Referral Program\n\ud83e\udd38\ud83c\udffb\u200d\u2640\ufe0fDescuento en gimnasio\n\ud83c\udf4eFruta en oficina\nSi tienes una pasi\u00f3n por el marketing digital, la creaci\u00f3n de contenido y el dise\u00f1o, y buscas un rol donde puedas dejar tu huella y contribuir al \u00e9xito de las PYMEs, \u00a1ORBIDI es el lugar para ti! Te invitamos a aplicar y ser parte de nuestro equipo creativo y estrat\u00e9gico."
    },
    "4159603850": {
        "title": "Data Engineer (m/f/d) ",
        "company": "BASF",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nAbout Us\n\nAt BASF Digital Hub Madrid we develop innovative digital solutions for BASF, create new exciting customer experiences and business growth, and drive efficiencies in processes, helping to strengthen BASF\u00b4s position as the digital leader in the chemical industry. We believe the right path is through creativity, trial and error and great people working and learning together. Become part of our team and develop the future with us - in a global team that embraces diversity and equal opportunities.\n\n\nResponsibilities\n\nDesign, develop, test, and maintain features for our Databricks Data Warehouse solution. \nAssess and implement the technical aspects of access concept, including RBAC/ABAC Governance, to ensure data security and compliance. \nCollaborate with cross-functional teams to define, design, and ship new features\nWrite clean, maintainable, and efficient code using (mostly) Python\nSupport/Manage Unity Catalog capabilities, Governance & Domain driven UC Catalog management\nProcess automation\nTroubleshoot, debug, and optimize existing solutions. \nStay up to date with the latest industry standards and technologies and strive for continuous improvement. \n\n\n\nRequirements\n\nBachelor's or master\u2019s degree in Computer Science, Engineering, or a related field. \nExperience in cloud-based Data warehousing and/or Data Engineering (Azure). \nStrong knowledge of Python and SQL. (Power Apps). \nKnowledge of Databricks and Databricks Unity Catalog. \nExperience in implementing access concepts, RBAC/ABAC Governance. \nCustomer-focused with excellent communication and teamwork skills. \n\n\n\nBenefits\n\nResponsibility from day one in a challenging work environment and \"on-the-job\" training as part of a committed team. \nAdequate compensation according to your qualifications and experience\n A secure work environment because your health, safety and wellbeing is always our top priority. \nFlexible work schedule and Home-office options, so that you can balance your working life and private life. \nLearning and development opportunities\n23 holidays per year\nAnother 5 days (readjustments days) and 2 days (cultural days)\nA collaborative, trustful and innovative work environment\nBeing part of an international team and work in global projects\nRelocation assistance to Madrid provided\n\nAt BASF, the chemistry is right.\n\nBecause we are counting on innovative solutions, on sustainable actions, and on connected thinking. And on you. Become a part of our formula for success and develop the future with us - in a global team that embraces diversity and equal opportunities irrespective of gender, age, origin, sexual orientation, disability or belief.\n\nContact\n\nDo you have any questions about the application process or the position? Please reach out to Elena Perez Tornos (elena.a.perez@basf.com)\nAbout Us\nResponsibilities\nDesign, develop, test, and maintain features for our Databricks Data Warehouse solution. \nAssess and implement the technical aspects of access concept, including RBAC/ABAC Governance, to ensure data security and compliance. \nCollaborate with cross-functional teams to define, design, and ship new features\nWrite clean, maintainable, and efficient code using (mostly) Python\nSupport/Manage Unity Catalog capabilities, Governance & Domain driven UC Catalog management\nProcess automation\nTroubleshoot, debug, and optimize existing solutions. \nStay up to date with the latest industry standards and technologies and strive for continuous improvement.\nDesign, develop, test, and maintain features for our Databricks Data Warehouse solution.\nAssess and implement the technical aspects of access concept, including RBAC/ABAC Governance, to ensure data security and compliance.\nCollaborate with cross-functional teams to define, design, and ship new features\nWrite clean, maintainable, and efficient code using (mostly) Python\nSupport/Manage Unity Catalog capabilities, Governance & Domain driven UC Catalog management\nProcess automation\nTroubleshoot, debug, and optimize existing solutions.\nStay up to date with the latest industry standards and technologies and strive for continuous improvement.\nRequirements\nBachelor's or master\u2019s degree in Computer Science, Engineering, or a related field. \nExperience in cloud-based Data warehousing and/or Data Engineering (Azure). \nStrong knowledge of Python and SQL. (Power Apps). \nKnowledge of Databricks and Databricks Unity Catalog. \nExperience in implementing access concepts, RBAC/ABAC Governance. \nCustomer-focused with excellent communication and teamwork skills.\nBachelor's or master\u2019s degree in Computer Science, Engineering, or a related field.\nExperience in cloud-based Data warehousing and/or Data Engineering (Azure).\nStrong knowledge of Python and SQL. (Power Apps).\nKnowledge of Databricks and Databricks Unity Catalog.\nExperience in implementing access concepts, RBAC/ABAC Governance.\nCustomer-focused with excellent communication and teamwork skills.\nBenefits\nResponsibility from day one in a challenging work environment and \"on-the-job\" training as part of a committed team. \nAdequate compensation according to your qualifications and experience\n A secure work environment because your health, safety and wellbeing is always our top priority. \nFlexible work schedule and Home-office options, so that you can balance your working life and private life. \nLearning and development opportunities\n23 holidays per year\nAnother 5 days (readjustments days) and 2 days (cultural days)\nA collaborative, trustful and innovative work environment\nBeing part of an international team and work in global projects\nRelocation assistance to Madrid provided\nResponsibility from day one in a challenging work environment and \"on-the-job\" training as part of a committed team.\nAdequate compensation according to your qualifications and experience\nA secure work environment because your health, safety and wellbeing is always our top priority.\nFlexible work schedule and Home-office options, so that you can balance your working life and private life.\nLearning and development opportunities\n23 holidays per year\nAnother 5 days (readjustments days) and 2 days (cultural days)\nA collaborative, trustful and innovative work environment\nBeing part of an international team and work in global projects\nRelocation assistance to Madrid provided\nAt BASF, the chemistry is right."
    },
    "4168458785": {
        "title": "Data Engineer Internship (m/f/x) ",
        "company": "Procter & Gamble",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nJob Location\n\nMADRID GENERAL OFFICE\n\nJob Description\n\nWe are looking for a Data Engineer intern to work in our Multi billion dollars business to help shape the future of the Data Strategy. In this role, you will be responsible for designing and building solutions using various modern Azure components & tools. You will lead delivery of E2E scalable data & analytics applications using outstanding industry standards, actively coding and adapting them to ensure their resiliency and fit to requirements\n\nResponsibilities\n\n Craft and build scalable and resilient Data & Analytics solutions in Microsoft Azure \u2013 architect technical solutions to obtain, process, store and provide insights based on processed data.\n Develop, within existing designs of various solutions in Microsoft Azure, environments to help generate valuable insights.\n Work on automation and optimization of internal processes in Azure.\n Influence the future of these new technologies and the ways in which P&G uses them.\n Have a possibility to work with multifunctional and multinational teams within and outside of P&G.\n Focus on key business cases development within Data & Analytics + Azure.\n Manage agile projects using cloud and hybrid solutions.\n\nDesired\n\nJob Qualifications\n\nBusiness / Management Information Systems or Software Development, Information Technology.\nPython, Knime and SQL programming skills.\nCloud Understanding.\nUnderstanding of data modelling.\nUnderstanding of CI/CD.\nEnglish proficiency and at least a Bachelor's degree.\nGood understanding of ETL process and some experience in managing different databases.\nMachine Learning\n NICE TO HAVE\n Understanding of Spark framework and distributed architecture features and challenges.\n Experience in implementing projects & solutions using Microsoft Azure stack (eg. Databricks, Azure Data Factory, Azure Analysis Service, SQL Server, Azure Synapse).\n Knowledge of AGILE and DevOps methodologies.\n Hands on experience in writing clean, effective code.\n\nJob Schedule\n\nFull time\n\nJob Number\n\nR000126031\n\nJob Segmentation\n\nInternships (Job Segmentation)Starting Pay / Salary Range\nJob Location\nJob Description\nResponsibilities\nCraft and build scalable and resilient Data & Analytics solutions in Microsoft Azure \u2013 architect technical solutions to obtain, process, store and provide insights based on processed data.\n Develop, within existing designs of various solutions in Microsoft Azure, environments to help generate valuable insights.\n Work on automation and optimization of internal processes in Azure.\n Influence the future of these new technologies and the ways in which P&G uses them.\n Have a possibility to work with multifunctional and multinational teams within and outside of P&G.\n Focus on key business cases development within Data & Analytics + Azure.\n Manage agile projects using cloud and hybrid solutions.\nCraft and build scalable and resilient Data & Analytics solutions in Microsoft Azure \u2013 architect technical solutions to obtain, process, store and provide insights based on processed data.\nDevelop, within existing designs of various solutions in Microsoft Azure, environments to help generate valuable insights.\nWork on automation and optimization of internal processes in Azure.\nInfluence the future of these new technologies and the ways in which P&G uses them.\nHave a possibility to work with multifunctional and multinational teams within and outside of P&G.\nFocus on key business cases development within Data & Analytics + Azure.\nManage agile projects using cloud and hybrid solutions.\nDesired\nJob Qualifications\nBusiness / Management Information Systems or Software Development, Information Technology.\nPython, Knime and SQL programming skills.\nCloud Understanding.\nUnderstanding of data modelling.\nUnderstanding of CI/CD.\nEnglish proficiency and at least a Bachelor's degree.\nGood understanding of ETL process and some experience in managing different databases.\nMachine Learning\n NICE TO HAVE\n Understanding of Spark framework and distributed architecture features and challenges.\n Experience in implementing projects & solutions using Microsoft Azure stack (eg. Databricks, Azure Data Factory, Azure Analysis Service, SQL Server, Azure Synapse).\n Knowledge of AGILE and DevOps methodologies.\n Hands on experience in writing clean, effective code.\nBusiness / Management Information Systems or Software Development, Information Technology.\nPython, Knime and SQL programming skills.\nCloud Understanding.\nUnderstanding of data modelling.\nUnderstanding of CI/CD.\nEnglish proficiency and at least a Bachelor's degree.\nGood understanding of ETL process and some experience in managing different databases.\nMachine Learning\nNICE TO HAVE\nUnderstanding of Spark framework and distributed architecture features and challenges.\nExperience in implementing projects & solutions using Microsoft Azure stack (eg. Databricks, Azure Data Factory, Azure Analysis Service, SQL Server, Azure Synapse).\nKnowledge of AGILE and DevOps methodologies.\nHands on experience in writing clean, effective code.\nJob Schedule\nJob Number\nJob Segmentation"
    },
    "4173777592": {
        "title": "Senior Data Engineer - Azure (Global report to Zurich) ",
        "company": "Robert Walters",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Contract",
        "description": "About the job\nFor one of the biggest global insurance company based in Madrid, I am currently and urgently looking for a Senior Data Engineer to join their new European Digital Hub and lead their global reports based in Switzerland.\nTasks:\n\nAs an Azure Databricks Developer within Asset Management, you will play a key role in designing, developing, and optimizing data pipelines and workflows. Your primary responsibility will be implementing scalable and high-performance data solutions using Azure Databricks, Airflow, Unity Catalog, Data Factory, and Git. You will collaborate closely with data engineers, architects, and business stakeholders to deliver high-quality data solutions that support analytics, reporting, and operational processes.\nOffer:\n-> Perm contract with final client, full time, hybrid mode \n-> Attractive salary (in line with your experience) + bonus + package\n-> A dynamic, international and challenging work environment\nRequirements:\n* Solid experience with Apache Spark, PySpark, or Scala, with hands-on experience in large-scale data processing.\n* Strong knowledge of Azure Data Services\n* Experience with Apache Airflow \n* Solid understanding of data modeling, ETL processes\nIf you are interested, please do not hesitate to send me your CV updated and spread the word!\nFor one of the biggest global insurance company based in Madrid, I am currently and urgently looking for a Senior Data Engineer to join their new European Digital Hub and lead their global reports based in Switzerland.\nTasks:\nAs an Azure Databricks Developer within Asset Management, you will play a key role in designing, developing, and optimizing data pipelines and workflows. Your primary responsibility will be implementing scalable and high-performance data solutions using Azure Databricks, Airflow, Unity Catalog, Data Factory, and Git. You will collaborate closely with data engineers, architects, and business stakeholders to deliver high-quality data solutions that support analytics, reporting, and operational processes.\nOffer:\n-> Perm contract with final client, full time, hybrid mode \n-> Attractive salary (in line with your experience) + bonus + package\n-> A dynamic, international and challenging work environment\nRequirements:\n* Solid experience with Apache Spark, PySpark, or Scala, with hands-on experience in large-scale data processing.\n* Strong knowledge of Azure Data Services\n* Experience with Apache Airflow \n* Solid understanding of data modeling, ETL processes\nIf you are interested, please do not hesitate to send me your CV updated and spread the word!"
    },
    "4085674908": {
        "title": "Lead Data Engineer ",
        "company": "Dow Jones",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nJOB TITLE: Lead Data Engineer\n\nDow Jones Location: Barcelona Job ID: TBD\n\nAbout Us\n\nOPIS, a Dow Jones company, is one of the world\u2019s most comprehensive sources for petroleum pricing and news information. OPIS provides real-time and historical spot, wholesale/rack, and retail fuel prices for the refined products, renewable fuels, and natural gas and gas liquids (LPG) industries. We deliver award-winning news, market intelligence, and transparency to the entire refined energy marketplace and companies looking to go carbon neutral. At its core, OPIS uses a set of complex IT systems and tools, reliably handling huge amounts of data, and providing customers with business applications to use this data as efficiently as possible.\n\nYour Role\n\nThe OPIS Data Science & Analytics team is a collaborative group of Data Scientists, Data Analysts, and Data Engineers dedicated to tackling intricate data challenges. Our team leverages extensive industry knowledge, robust analytical skills, and a practical approach to address our organization's most complex business dilemmas. The team focuses on developing and deploying AI, ML, and data science solutions. This is a unique opportunity to join a fast-growing team that will become an important part of the global OPIS team. You will work closely with the OPIS IT HQ based in Gaithersburg, US, Romania, Poland, and with your remote colleagues.\n\nYour Responsibilities\n\nAs an experienced and solution-oriented Lead Data Engineer, you will design and develop database and ML architecture to process large data volumes and operationalize complex ML models. You'll optimize and automate data pipelines using SQL and Python to integrate data across disparate sources. You will also develop back-end integrations between BI tools and data sources, deploy ML code using AWS and CI/CD pipelines, and support AI, ML, and analytics solutions. Working within an Agile team, you will be a leader coaching, mentoring, and training team members, driving both employee and team development and growth.\n\nYour Expertise\n\nDegree in computer science, related technical field, or equivalent experience\nMinimum of 4+ years of experience in data management technologies (Microsoft SQL Server, Postgres, Snowflake)\nExperience with advanced SQL coding and data modeling to integrate systems\nExperience writing Python or PySpark for ML processing, data analysis, and automation\nExperience in BI visualization development such as Power BI, Tableau, or Looker\nAbility to write SQL queries and statements to explore source data and data issues\nWorking knowledge of cloud data technologies and architecture\nWorking knowledge of Big Data technologies such as Spark, AWS EMR\nWorking knowledge of building CI/CD pipelines for code deployment\nMust speak and write in English fluently; Effective communicator\n\nDoes this sound like you?\n\nA strong analytical thinker with a love for planning and organization who can manage competing demands with minimal supervision\nA self-starter with a never-ending curiosity to learn and solve problems\nA critical thinker, with mad problem-solving skills\nA team player with great interpersonal and communication skills\nA structured developer, with good time-management skills\n\nIf these things strike a chord with you, we should talk.\n\nWe\u2019d also love to hear about your experiences with any of the following:\n\nDegrees, Certifications, and/or awards/recognition that you have achieved\nKnowledge of SDLC, Agile methodology and/or SCRUM\nExperience working with scripts under source control (Git/TFVC/Mercurial/Svn/etc)\nExperience being part of a continuous integration and deployment process\nExperience using AI, ML, and Data Science tools\n\nOur Benefits\n\nPension Scheme: Company contributes up to 8% of your salary with matching contributions.\nHealthcare Coverage: Comprehensive health plan with unlimited coverage, including outpatient care, dental, and maternity.\nLife & Disability Insurance: Coverage from day one, including life insurance at 2X salary and accident coverage up to 4X salary.\nWellness Programs: Access to wellness platforms, mindfulness apps, and free fitness classes.\nMeal & Teleworking Allowance: Monthly meal card and teleworking allowance.\nFlexible Working: Ability to work from home 2 days per week with the option to work remotely for 1 week per quarter.\nFamily Care Support: Up to 4 weeks of paid compassionate leave and a generous reimbursement for emergency care.\nReferral Bonus: Earn a bonus for recommending great talent to join us.\n\nCurrent Colleagues\n\nIf you are currently employed by Dow Jones, please apply internally via the Workday internal careers site.\n\nReasonable accommodation: Dow Jones, Making Careers Newsworthy - We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law. EEO/AA/M/F/Disabled/Vets. We strongly encourage applications from all qualified individuals, including women, people with disabilities, and those from underrepresented groups. Dow Jones is committed to providing reasonable accommodation for qualified individuals with disabilities, in our job application and/or interview process. If you need assistance or accommodation in completing your application, due to a disability, email us at talentresourceteam@dowjones.com. Please put \"Reasonable Accommodation\" in the subject line and provide a brief description of the type of assistance you need. This inbox will not be monitored for application status updates.\n\nBusiness Area: Dow Jones - OPIS\n\nJob Category: Data Analytics/Warehousing & Business Intelligence\n\nUnion Status\n\nNon-Union role\n\nSince 1882, Dow Jones has been finding new ways to bring information to the world\u2019s top business entities. Beginning as a niche news agency in an obscure Wall Street basement, Dow Jones has grown to be a worldwide news and information powerhouse, with prestigious brands including The Wall Street Journal, Dow Jones Newswires, Factiva, Barron\u2019s, MarketWatch and Financial News.\n\nThis longevity and success is due to a relentless pursuit of accuracy, depth and innovation, enhanced by the wisdom of past experience and a solid grasp on the future ahead. More than its individual brands, Dow Jones is a modern gateway to intelligence, with innovative technology, advanced data feeds, integrated solutions, expert research, award-winning journalism and customizable apps and delivery systems to bring the information that matters most to customers, when and where they need it, every day.\n\nReq ID: 43572\nAbout Us\nDegree in computer science, related technical field, or equivalent experience\nMinimum of 4+ years of experience in data management technologies (Microsoft SQL Server, Postgres, Snowflake)\nExperience with advanced SQL coding and data modeling to integrate systems\nExperience writing Python or PySpark for ML processing, data analysis, and automation\nExperience in BI visualization development such as Power BI, Tableau, or Looker\nAbility to write SQL queries and statements to explore source data and data issues\nWorking knowledge of cloud data technologies and architecture\nWorking knowledge of Big Data technologies such as Spark, AWS EMR\nWorking knowledge of building CI/CD pipelines for code deployment\nMust speak and write in English fluently; Effective communicator\nDegree in computer science, related technical field, or equivalent experience\nMinimum of 4+ years of experience in data management technologies (Microsoft SQL Server, Postgres, Snowflake)\nExperience with advanced SQL coding and data modeling to integrate systems\nExperience writing Python or PySpark for ML processing, data analysis, and automation\nExperience in BI visualization development such as Power BI, Tableau, or Looker\nAbility to write SQL queries and statements to explore source data and data issues\nWorking knowledge of cloud data technologies and architecture\nWorking knowledge of Big Data technologies such as Spark, AWS EMR\nWorking knowledge of building CI/CD pipelines for code deployment\nMust speak and write in English fluently; Effective communicator\nA strong analytical thinker with a love for planning and organization who can manage competing demands with minimal supervision\nA self-starter with a never-ending curiosity to learn and solve problems\nA critical thinker, with mad problem-solving skills\nA team player with great interpersonal and communication skills\nA structured developer, with good time-management skills\nA strong analytical thinker with a love for planning and organization who can manage competing demands with minimal supervision\nA self-starter with a never-ending curiosity to learn and solve problems\nA critical thinker, with mad problem-solving skills\nA team player with great interpersonal and communication skills\nA structured developer, with good time-management skills\nDegrees, Certifications, and/or awards/recognition that you have achieved\nKnowledge of SDLC, Agile methodology and/or SCRUM\nExperience working with scripts under source control (Git/TFVC/Mercurial/Svn/etc)\nExperience being part of a continuous integration and deployment process\nExperience using AI, ML, and Data Science tools\nDegrees, Certifications, and/or awards/recognition that you have achieved\nKnowledge of SDLC, Agile methodology and/or SCRUM\nExperience working with scripts under source control (Git/TFVC/Mercurial/Svn/etc)\nExperience being part of a continuous integration and deployment process\nExperience using AI, ML, and Data Science tools\nOur Benefits\nPension Scheme: Company contributes up to 8% of your salary with matching contributions.\nHealthcare Coverage: Comprehensive health plan with unlimited coverage, including outpatient care, dental, and maternity.\nLife & Disability Insurance: Coverage from day one, including life insurance at 2X salary and accident coverage up to 4X salary.\nWellness Programs: Access to wellness platforms, mindfulness apps, and free fitness classes.\nMeal & Teleworking Allowance: Monthly meal card and teleworking allowance.\nFlexible Working: Ability to work from home 2 days per week with the option to work remotely for 1 week per quarter.\nFamily Care Support: Up to 4 weeks of paid compassionate leave and a generous reimbursement for emergency care.\nReferral Bonus: Earn a bonus for recommending great talent to join us.\nPension Scheme: Company contributes up to 8% of your salary with matching contributions.\nHealthcare Coverage: Comprehensive health plan with unlimited coverage, including outpatient care, dental, and maternity.\nLife & Disability Insurance: Coverage from day one, including life insurance at 2X salary and accident coverage up to 4X salary.\nWellness Programs: Access to wellness platforms, mindfulness apps, and free fitness classes.\nMeal & Teleworking Allowance: Monthly meal card and teleworking allowance.\nFlexible Working: Ability to work from home 2 days per week with the option to work remotely for 1 week per quarter.\nFamily Care Support: Up to 4 weeks of paid compassionate leave and a generous reimbursement for emergency care.\nReferral Bonus: Earn a bonus for recommending great talent to join us.\nUnion Status"
    },
    "4124709950": {
        "title": "Data Scientist/ML Engineer",
        "company": "Appodeal, Inc.",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nAppodeal is a dynamic US-based product company with a truly global presence.\n\nWe have offices in Warsaw, Barcelona and Virginia along with remote team members located around the world.\n\nOur company thrives on diversity, collaboration, and innovation, making us a leader in the mobile app monetization space.\n\nWhy Appodeal?\n\nAt Appodeal, we're more than just a company\u2014we're a team united by a common mission: to help every person discover and grow their talents!\n\nWe take pride in our cutting-edge product and our internationally dispersed team of talented professionals.\n\nHere's what we value, and what we hope you do too:\n\nContinuous Learning and Growth: We are passionate about learning, growing personally, and building rewarding careers.\nMaking an Impact: We are committed to building a history-defining company that leaves a lasting impact on the mobile app industry.\nSolving Exciting Challenges: We tackle complex problems every day, supported by a team of world-class professionals and mentors.\nEnjoying the Journey: We believe in having fun while working toward our goals.\n\nWe are looking for an experienced Data Scientist /Machine Learning Engineer to add to our Data team at Barcelona.\n\nResponsibilities:\n\nAnalyze data to identify trends, patterns, and actionable insights, independently solving complex data-related problems.\nDesign and develop end-to-end data science solutions, from initial analysis and model development to deployment, optimizing for performance and scalability. This includes creating models tailored to optimizing ad targeting, bid strategies, and campaign performance analysis.\nCollaborate with both technical and business teams to understand requirements and translate them into data-driven solutions and insights, particularly focusing on enhancing advertising effectiveness and ROI.\nMaintain and optimize existing data processing pipelines and machine learning models, with a strong emphasis on applications within the ad-tech ecosystem, ensuring high efficiency and alignment with dynamic market trends.\n\nTechnology Stack:\n\nProficiency in Python and SQL.\nExperience with manipulation frameworks such as PySpark and Pandas.\nSkilled at using cloud services such as AWS etc.\nHands-on experience in DNN models training libraries and frameworks (pytorch, hydra, comet etc.)\n\nQualifications:\n\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, or a related field.\n3+ years of experience in data science, preferably in a product-focused environment.\n2+ years experience in AdTech.\nStrong analytical skills with the ability to derive insights from complex datasets.\nProficiency in statistical modeling and machine learning techniques.\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\n\nThis position supposes relocation to Barcelona and work from our office in the city center.\n\nWhy Join Us:\n\nOpportunity to work on cutting-edge projects with a global impact in the mobile app industry.\nA collaborative and inclusive work culture that values innovation and continuous learning.\nCompetitive salary, flexible work arrangements, and comprehensive benefits package.\nProfessional development opportunities and career growth prospects within a fast-growing company.\n\nWith an outstanding product and a mission that excites and inspires, Appodeal offers a unique opportunity to make an impact while being part of an amazing team.\n\nJoin us and help shape the future of mobile app success!\nAppodeal\nWarsaw, Barcelona and Virginia\nWhy Appodeal?\nto help every person discover and grow their talents!\nContinuous Learning and Growth: We are passionate about learning, growing personally, and building rewarding careers.\nMaking an Impact: We are committed to building a history-defining company that leaves a lasting impact on the mobile app industry.\nSolving Exciting Challenges: We tackle complex problems every day, supported by a team of world-class professionals and mentors.\nEnjoying the Journey: We believe in having fun while working toward our goals.\nContinuous Learning and Growth: We are passionate about learning, growing personally, and building rewarding careers.\nContinuous Learning and Growth:\nMaking an Impact: We are committed to building a history-defining company that leaves a lasting impact on the mobile app industry.\nMaking an Impact:\nSolving Exciting Challenges: We tackle complex problems every day, supported by a team of world-class professionals and mentors.\nSolving Exciting Challenges:\nEnjoying the Journey: We believe in having fun while working toward our goals.\nEnjoying the Journey:\nData Scientist /Machine Learning Engineer\nData team at Barcelona.\nResponsibilities:\nAnalyze data to identify trends, patterns, and actionable insights, independently solving complex data-related problems.\nDesign and develop end-to-end data science solutions, from initial analysis and model development to deployment, optimizing for performance and scalability. This includes creating models tailored to optimizing ad targeting, bid strategies, and campaign performance analysis.\nCollaborate with both technical and business teams to understand requirements and translate them into data-driven solutions and insights, particularly focusing on enhancing advertising effectiveness and ROI.\nMaintain and optimize existing data processing pipelines and machine learning models, with a strong emphasis on applications within the ad-tech ecosystem, ensuring high efficiency and alignment with dynamic market trends.\nAnalyze data to identify trends, patterns, and actionable insights, independently solving complex data-related problems.\nDesign and develop end-to-end data science solutions, from initial analysis and model development to deployment, optimizing for performance and scalability. This includes creating models tailored to optimizing ad targeting, bid strategies, and campaign performance analysis.\nCollaborate with both technical and business teams to understand requirements and translate them into data-driven solutions and insights, particularly focusing on enhancing advertising effectiveness and ROI.\nMaintain and optimize existing data processing pipelines and machine learning models, with a strong emphasis on applications within the ad-tech ecosystem, ensuring high efficiency and alignment with dynamic market trends.\nTechnology Stack:\nProficiency in Python and SQL.\nExperience with manipulation frameworks such as PySpark and Pandas.\nSkilled at using cloud services such as AWS etc.\nHands-on experience in DNN models training libraries and frameworks (pytorch, hydra, comet etc.)\nProficiency in Python and SQL.\nExperience with manipulation frameworks such as PySpark and Pandas.\nSkilled at using cloud services such as AWS etc.\nHands-on experience in DNN models training libraries and frameworks (pytorch, hydra, comet etc.)\nQualifications:\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, or a related field.\n3+ years of experience in data science, preferably in a product-focused environment.\n2+ years experience in AdTech.\nStrong analytical skills with the ability to derive insights from complex datasets.\nProficiency in statistical modeling and machine learning techniques.\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, or a related field.\n3+ years of experience in data science, preferably in a product-focused environment.\n2+ years experience in AdTech.\nStrong analytical skills with the ability to derive insights from complex datasets.\nProficiency in statistical modeling and machine learning techniques.\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nBarcelona\nWhy Join Us:\nOpportunity to work on cutting-edge projects with a global impact in the mobile app industry.\nA collaborative and inclusive work culture that values innovation and continuous learning.\nCompetitive salary, flexible work arrangements, and comprehensive benefits package.\nProfessional development opportunities and career growth prospects within a fast-growing company.\nOpportunity to work on cutting-edge projects with a global impact in the mobile app industry.\nA collaborative and inclusive work culture that values innovation and continuous learning.\nCompetitive salary, flexible work arrangements, and comprehensive benefits package.\nProfessional development opportunities and career growth prospects within a fast-growing company.\nJoin us"
    },
    "4127609285": {
        "title": "Data Engineer (Big Data) 2025",
        "company": "KAIROS DIGITAL SOLUTIONS",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00a1\u00daNETE AL EQUIPO COMO DATA ENGINEER!\n\n\u00bfQuieres formar parte de una organizaci\u00f3n que apuesta por el cuidado de las personas y su crecimiento personal y profesional? \u00bfTe apasiona el mundo del desarrollo y te gustar\u00eda afrontar nuevos desaf\u00edos? \u00a1\u00danete a Kair\u00f3s!\n\n\u00bfQui\u00e9nes somos?\n\nSomos Empresa Great Place To Work desde 2019 por quinto a\u00f1o consecutivo, ocupando el 5 lugar en el a\u00f1o 2024 gracias a nuestros valores, nuestra cultura y a los beneficios e iniciativas internas que lanzamos centradas en las personas\n\nAcompa\u00f1amos a nuestros clientes en proyectos de transformaci\u00f3n tecnol\u00f3gica y \u00e1gil. Trabajamos en equipos organizados por disciplinas y en squads multidisciplinares en proyectos muy diversos (retail, investigaci\u00f3n sanitaria, banca, seguridad, movilidad\u2026).\n\nAunque hemos crecido mucho en el \u00faltimo a\u00f1o, no queremos olvidarnos de nuestra esencia, centrados siempre en las personas: hacemos seguimiento personal constante con el equipo t\u00e9cnico y de Talento. Adem\u00e1s, apostamos siempre por la transparencia y la claridad.\n\nEstamos en Espa\u00f1a, M\u00e9xico y Per\u00fa. En Kair\u00f3s encontrar\u00e1s proyectos en los que podr\u00e1s trabajar desde casa, \u00a1sin perder la cercan\u00eda con el resto del equipo!\n\n\u00bfQu\u00e9 estamos buscando?\n\nEn Kair\u00f3s buscamos un DATA ENGINEER para uniser a nuestra disciplina de Data&IA.\n\nNuestra capacidad de Data & IA es la de m\u00e1s reciente creaci\u00f3n. Con credenciales dentro del \u00e1mbito de la salud, la ciencia e investigaci\u00f3n, medio ambiente y proyectos en banca y seguridad, est\u00e1 liderada por In\u00e9s Huertas y crece incorporando perfiles diversos, con mucho que aportar.\n\nEn nuestra disciplina abordamos todo el ciclo de vida, desde la parte de dise\u00f1o y convergencia de datos a expertos en explotaci\u00f3n de datos con BI o Ingenieros de IT. Trabajamos en proyectos de IA generativa, algoritmos de predicci\u00f3n, creaci\u00f3n de plataformas de Big Data y generaci\u00f3n de cuadros de mando, rode\u00e1ndote de Kairoseros y Kairoseras con roles c\u00f3mo Machine Learning Engineer, Data Engineer, Data Scientist, Data Analytics y Data Visualization.\n\nRequisitos:\n\nIngeniero de datos orientado a Big Data para desarrollo de procesos en Java, Python o Scala con Spark. Experiencia en an\u00e1lisis de datos y transformaciones complejas.\nMucha solvencia en SQL.\nSe requiere experiencia previa trabajando en proyectos donde se manejasen grandes vol\u00famenes de datos, donde el rendimiento jugase un papel importante\nSe valoran positivamente conocimientos previos en Snowflake, Azure Databricks\nExperiencia m\u00ednima de 3 a\u00f1os\n\nEn Kair\u00f3s encontrar\u00e1s:\n\nBolsa individual de formaci\u00f3n de 1.750 euros, formaci\u00f3n continua corporativa y certificaciones t\u00e9cnicas.\nClases de ingl\u00e9s gratuitas, en grupos reducidos.\n23 d\u00edas de vacaciones, 24 y 31 de diciembre libres, 1 d\u00eda para eventos y \u00a13 d\u00edas de libre conciliaci\u00f3n!\nDisfrutar del verano con nuestra jornada reducida en agosto y horario intensivo todos los viernes del a\u00f1o.\nRetribuci\u00f3n flexible a trav\u00e9s de Cobee, para hacer uso en restauraci\u00f3n, transporte p\u00fablico, formaci\u00f3n y ticket guarder\u00eda.\nSeguro m\u00e9dico con Cigna o Sanitas: nos preocupamos de tu salud f\u00edsica y mental.\nAcompa\u00f1amiento personal continuo a trav\u00e9s de nuestro programa Wellness 3.0 y mentoring t\u00e9cnico constante.\nFortalecimiento de tu marca personal en eventos internos y externos.\nFormar parte de una gran comunidad y contar con el apoyo de personas incre\u00edbles y referentes dentro del sector.\nPero sobre todo... ser parte de una empresa donde se te escucha, se te cuida y donde intentamos hacer todo lo posible por cubrir tus necesidades y expectativas.\n\nVas a encajar genial si te gusta aprender, compartir tu conocimiento y tratas a la gente con respeto y cari\u00f1o. Si eres una persona interesada en participar en una cultura abierta, transparente y de confianza y si adem\u00e1s, tienes capacidad de organizaci\u00f3n, responsabilidad y compromiso.\n\n#TRY #THINK #FEEL #ENJOY\n\n \n\nEn Kair\u00f3s creemos firmemente en la importancia de la igualdad y la diversidad en el lugar de trabajo. Estamos comprometidos a crear un entorno inclusivo y respetuoso donde todos los empleados se sientan valorados y tengan la oportunidad de alcanzar su m\u00e1ximo potencial, independientemente de su raza, color, religi\u00f3n, g\u00e9nero, identidad o expresi\u00f3n de g\u00e9nero, orientaci\u00f3n sexual, origen nacional, gen\u00e9tica, discapacidad, edad o cualquier otra caracter\u00edstica protegida por la ley.\n\u00a1\u00daNETE AL EQUIPO COMO DATA ENGINEER!\n\u00bfQui\u00e9nes somos?\nGreat Place To Work\nocupando el 5 lugar en el a\u00f1o 2024\nTrabajamos en equipos organizados por disciplinas y en squads multidisciplinares en proyectos muy diversos\ncentrados siempre en las personas\nEspa\u00f1a, M\u00e9xico y Per\u00fa.\n\u00bfQu\u00e9 estamos buscando?\nEn Kair\u00f3s\nDATA ENGINEER\nIn\u00e9s Huertas\nRequisitos:\nIngeniero de datos orientado a Big Data para desarrollo de procesos en Java, Python o Scala con Spark. Experiencia en an\u00e1lisis de datos y transformaciones complejas.\nMucha solvencia en SQL.\nSe requiere experiencia previa trabajando en proyectos donde se manejasen grandes vol\u00famenes de datos, donde el rendimiento jugase un papel importante\nSe valoran positivamente conocimientos previos en Snowflake, Azure Databricks\nExperiencia m\u00ednima de 3 a\u00f1os\nIngeniero de datos orientado a Big Data para desarrollo de procesos en Java, Python o Scala con Spark. Experiencia en an\u00e1lisis de datos y transformaciones complejas.\nMucha solvencia en SQL.\nSe requiere experiencia previa trabajando en proyectos donde se manejasen grandes vol\u00famenes de datos, donde el rendimiento jugase un papel importante\nSe valoran positivamente conocimientos previos en Snowflake, Azure Databricks\nExperiencia m\u00ednima de 3 a\u00f1os\nBolsa individual de formaci\u00f3n de 1.750 euros, formaci\u00f3n continua corporativa y certificaciones t\u00e9cnicas.\nClases de ingl\u00e9s gratuitas, en grupos reducidos.\n23 d\u00edas de vacaciones, 24 y 31 de diciembre libres, 1 d\u00eda para eventos y \u00a13 d\u00edas de libre conciliaci\u00f3n!\nDisfrutar del verano con nuestra jornada reducida en agosto y horario intensivo todos los viernes del a\u00f1o.\nRetribuci\u00f3n flexible a trav\u00e9s de Cobee, para hacer uso en restauraci\u00f3n, transporte p\u00fablico, formaci\u00f3n y ticket guarder\u00eda.\nSeguro m\u00e9dico con Cigna o Sanitas: nos preocupamos de tu salud f\u00edsica y mental.\nAcompa\u00f1amiento personal continuo a trav\u00e9s de nuestro programa Wellness 3.0 y mentoring t\u00e9cnico constante.\nFortalecimiento de tu marca personal en eventos internos y externos.\nFormar parte de una gran comunidad y contar con el apoyo de personas incre\u00edbles y referentes dentro del sector.\nPero sobre todo... ser parte de una empresa donde se te escucha, se te cuida y donde intentamos hacer todo lo posible por cubrir tus necesidades y expectativas.\nBolsa individual de formaci\u00f3n de 1.750 euros, formaci\u00f3n continua corporativa y certificaciones t\u00e9cnicas.\nClases de ingl\u00e9s gratuitas, en grupos reducidos.\n23 d\u00edas de vacaciones, 24 y 31 de diciembre libres, 1 d\u00eda para eventos y \u00a13 d\u00edas de libre conciliaci\u00f3n!\nDisfrutar del verano con nuestra jornada reducida en agosto y horario intensivo todos los viernes del a\u00f1o.\nRetribuci\u00f3n flexible a trav\u00e9s de Cobee, para hacer uso en restauraci\u00f3n, transporte p\u00fablico, formaci\u00f3n y ticket guarder\u00eda.\nSeguro m\u00e9dico con Cigna o Sanitas: nos preocupamos de tu salud f\u00edsica y mental.\nAcompa\u00f1amiento personal continuo a trav\u00e9s de nuestro programa Wellness 3.0 y mentoring t\u00e9cnico constante.\nFortalecimiento de tu marca personal en eventos internos y externos.\nFormar parte de una gran comunidad y contar con el apoyo de personas incre\u00edbles y referentes dentro del sector.\nPero sobre todo... ser parte de una empresa donde se te escucha, se te cuida y donde intentamos hacer todo lo posible por cubrir tus necesidades y expectativas.\nVas a encajar genial si\naprender, compartir tu conocimiento y tratas a la gente con respeto y cari\u00f1o\ncultura abierta, transparente y de confianza\ncapacidad de organizaci\u00f3n, responsabilidad y compromiso\n#TRY #THINK #FEEL #ENJOY"
    },
    "4173596482": {
        "title": "Senior Data Engineer",
        "company": "Remobi",
        "location": "European Economic Area",
        "work_mode": "Remote",
        "job_type": "Contract",
        "description": "About the job\nWe are building the world's greatest community of remote technologists!\n\nToday, organizations that understand the value of remote working will reap the rewards. Because it doesn\u2019t just provide team members with a healthier work-life balance, it gives you the opportunity to access the brightest minds in the world.\n\nOur clients access our community to build or extend their existing teams. All are made up of remote, distributed software engineering experts\u2014best-in-class professionals. Rapidly deployed without compromising on quality.\n\nJoin our Remobi community to have access to meaningful, innovative freelance projects and play a key role in shaping how companies operate.\n\nDuration: 6 months (high chance of extension)\nLocation: Remote\nContract Type: Freelance / B2B / Contract (Candidates must be based in the EU)\n\nSummary\n\nWe are seeking a Senior Data Engineer with expertise in Snowflake and ETL pipelines to support the development of a Finance solution. You will play a key role in designing and implementing data integrations, maintaining data cataloging in Collibra, and ensuring best practices in data architecture and engineering. If you thrive in a cloud-driven, agile environment, this role is for you!\n\nYou will be responsible for:\n\nClarifying technical requirements for a Finance solution with the Product Manager and Solution Architect.\nDesigning, building, and optimizing data pipelines and ETL processes in Snowflake.\nImplementing data integration solutions following enterprise architecture best practices.\nMaintaining and managing data cataloging in Collibra.\nLeading the iteration and refinement of the solution\u2019s data architecture.\nEnsuring high-quality documentation in Jira and internal wikis to support agile development and knowledge retention.\nCollaborating with the product team to develop scalable, efficient data solutions.\nFollowing best practices for code quality, testing, and deployment.\n\nWhat You Bring (Skills, Capabilities)\n\n5+ years of experience in Data Engineering, with a strong focus on cloud-based solutions.\nExpertise in Snowflake, including ETL development, query optimization, and performance tuning.\nStrong knowledge of SQL and relational databases, with experience writing and optimizing complex queries.\nHands-on experience with data pipeline development and integration.\nExperience with Collibra for data governance and cataloging.\nFamiliarity with enterprise architecture patterns for data solutions.\nStrong analytical and problem-solving skills, with the ability to troubleshoot data pipeline issues. Excellent communication and collaboration skills to work effectively in a distributed team.\nWe are building the world's greatest community of remote technologists!\nToday, organizations that understand the value of remote working will reap the rewards. Because it doesn\u2019t just provide team members with a healthier work-life balance, it gives you the opportunity to access the brightest minds in the world.\nOur clients access our community to build or extend their existing teams. All are made up of remote, distributed software engineering experts\u2014best-in-class professionals. Rapidly deployed without compromising on quality.\nJoin our Remobi community to have access to meaningful, innovative freelance projects and play a key role in shaping how companies operate.\nRemobi\nDuration: 6 months (high chance of extension)\nDuration:\nLocation: Remote\nLocation:\nContract Type: Freelance / B2B / Contract (Candidates must be based in the EU)\nContract Type:\nSummary\nWe are seeking a Senior Data Engineer with expertise in Snowflake and ETL pipelines to support the development of a Finance solution. You will play a key role in designing and implementing data integrations, maintaining data cataloging in Collibra, and ensuring best practices in data architecture and engineering. If you thrive in a cloud-driven, agile environment, this role is for you!\nSenior Data Engineer\nSnowflake and ETL pipelines\nFinance solution\nCollibra\ndata architecture and engineering\nYou will be responsible for:\nClarifying technical requirements for a Finance solution with the Product Manager and Solution Architect.\nDesigning, building, and optimizing data pipelines and ETL processes in Snowflake.\nImplementing data integration solutions following enterprise architecture best practices.\nMaintaining and managing data cataloging in Collibra.\nLeading the iteration and refinement of the solution\u2019s data architecture.\nEnsuring high-quality documentation in Jira and internal wikis to support agile development and knowledge retention.\nCollaborating with the product team to develop scalable, efficient data solutions.\nFollowing best practices for code quality, testing, and deployment.\nClarifying technical requirements for a Finance solution with the Product Manager and Solution Architect.\nDesigning, building, and optimizing data pipelines and ETL processes in Snowflake.\ndata pipelines and ETL processes in Snowflake\nImplementing data integration solutions following enterprise architecture best practices.\nMaintaining and managing data cataloging in Collibra.\ndata cataloging\nLeading the iteration and refinement of the solution\u2019s data architecture.\nsolution\u2019s data architecture\nEnsuring high-quality documentation in Jira and internal wikis to support agile development and knowledge retention.\nJira and internal wikis\nCollaborating with the product team to develop scalable, efficient data solutions.\nscalable, efficient data solutions\nFollowing best practices for code quality, testing, and deployment.\ncode quality, testing, and deployment\nWhat You Bring (Skills, Capabilities)\n5+ years of experience in Data Engineering, with a strong focus on cloud-based solutions.\nExpertise in Snowflake, including ETL development, query optimization, and performance tuning.\nStrong knowledge of SQL and relational databases, with experience writing and optimizing complex queries.\nHands-on experience with data pipeline development and integration.\nExperience with Collibra for data governance and cataloging.\nFamiliarity with enterprise architecture patterns for data solutions.\nStrong analytical and problem-solving skills, with the ability to troubleshoot data pipeline issues. Excellent communication and collaboration skills to work effectively in a distributed team.\n5+ years of experience in Data Engineering, with a strong focus on cloud-based solutions.\n5+ years of experience\nExpertise in Snowflake, including ETL development, query optimization, and performance tuning.\nExpertise in Snowflake\nStrong knowledge of SQL and relational databases, with experience writing and optimizing complex queries.\nSQL\nHands-on experience with data pipeline development and integration.\ndata pipeline development and integration\nExperience with Collibra for data governance and cataloging.\nFamiliarity with enterprise architecture patterns for data solutions.\nenterprise architecture patterns\nStrong analytical and problem-solving skills, with the ability to troubleshoot data pipeline issues. Excellent communication and collaboration skills to work effectively in a distributed team.\ndata pipeline issues\ncommunication and collaboration"
    },
    "4149664024": {
        "title": "Junior Cloud & Data Engineer ",
        "company": "BIP",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfCu\u00e1l ser\u00e1 tu reto?\n\n\u00danete a BIP - xTech, el Centro de Excelencia de BIP especializado en consultor\u00eda y transformaci\u00f3n digital en arquitecturas y gobierno de datos, modelos IA, adopci\u00f3n y arquitecturas Cloud, Seguridad, virtualizaci\u00f3n, modern workplace, automatizaci\u00f3n de entornos, SecDevOps, modernizaci\u00f3n de aplicaciones, soluciones IoT e Industria 4.0.\n\nEn BIP \u2013 xTech somos un equipo multidisciplinar que se integra para aportar valor en los proyectos de implantaci\u00f3n tecnol\u00f3gica dentro de los procesos de transformaci\u00f3n digital, conectados a los de estrategia y consultor\u00eda, aprovechando el conocimiento sectorial para la definici\u00f3n de soluciones ad-hoc que ofrecer a nuestros clientes.\n\nComo parte de un proceso de evoluci\u00f3n continua y adopci\u00f3n de estas tecnolog\u00edas en nuestras empresas cliente, buscamos profesionales con experiencia en Cloud para participar en proyectos que impliquen la implementaci\u00f3n de arquitecturas de referencia, la definici\u00f3n de los elementos clave necesarios para dar soluci\u00f3n a las necesidades tecnol\u00f3gicas de los clientes y la administraci\u00f3n de las infraestructuras cloud asociadas.\n\n\u00bfQuieres trabajar con nosotros?\n\nRequisitos indispensables:\n\n  Al menos 1 a\u00f1o de experiencia profesional en el sector de consultor\u00eda tecnol\u00f3gica, en alguno de los siguientes bloques:\nDesarrollo de procesos de transformaci\u00f3n de datos a trav\u00e9s de c\u00f3digo con SQL, Python.\nConocimiento en bases de datos SQL (SQL Server, Synapse, PostgreSQL, \u2026), NoSQL (MongoDb, Couchbase, Cosmos DB, Cassandra, Redis, \u2026) y otros recursos de almacenamiento (cloud storage)\nManejo de herramientas de gesti\u00f3n de c\u00f3digo fuente y despliegues automatizados (Azure DevOps, Gitlab, Jenkins, \u2026)\nConocimiento en tecnolog\u00edas Big Data, principalmente spark / Databricks y su \u00e1lter ego Snowflake\n \n\nValorado de forma especial si:\n\n  Tienes experiencia pr\u00e1ctica o conoces lo que es dbt\nTe llama la atenci\u00f3n lo que se mueve alrededor de conceptos como headless BI, metrics layer, data contracts, data quality...\nHas explorado las posibilidades de tecnolog\u00edas como Clickhouse o DuckDB\nHas hecho tus pinitos con Airflow o con alguno de sus coet\u00e1neos como Dagster o Prefect (o te llama la atenci\u00f3n lo que est\u00e1 haciendo Mage)\nHas trasteado con servicios como Presto o Dremio\n \n\nDeseable:\n\n  Titulaci\u00f3n Superior en Ingenier\u00eda Inform\u00e1tica (preferiblemente, Telecomunicaciones o materias STEM, o formaci\u00f3n equivalente\nBuen nivel de Ingl\u00e9s (escrito y hablado)\nCertificaciones oficiales relacionadas\nExperiencia pr\u00e1ctica en AWS y GCP, en el uso de los servicios de c\u00f3mputo, redes, almacenamiento, monitorizaci\u00f3n, bases de datos y kubernetes\nMetodolog\u00edas Agile DevOps (Git, Pipelines, CI/CD)\nDespliegue de infraestructura como c\u00f3digo (IaC)\n \n\nSoft Skills:\n\n  Car\u00e1cter consultivo\nCapacidad para trabajar en equipo y sumar\nResponsabilidad sobre el trabajo realizado y sentimiento de pertenencia tanto del proyecto como del equipo\nCapacidad de comunicaci\u00f3n y relaciones interpersonales\nOrientaci\u00f3n al cliente y a los objetivos\nAdaptabilidad y flexibilidad\n \n\nSe dar\u00e1 prioridad a los candidatos que, durante la entrevista, demuestren aptitudes para el desarrollo y la b\u00fasqueda de soluciones innovadoras.\n\nOfrecemos:\n\n  Beneficios Sociales, paquete retributivo acorde a expectativas, 26 d\u00edas laborales de vacaciones, horario flexible, modelo de trabajo h\u00edbrido.\nPlan personalizado de carrera y formaci\u00f3n.\nPertenecer a una compa\u00f1\u00eda internacional en plena expansi\u00f3n, que forma parte de un gran grupo empresarial, con una cultura de cercan\u00eda y puertas abiertas y que valora a cada persona de forma individualizada.\n \n\nBip Iberia, en compromiso y coherencia con la Ley Integral para la Igualdad de Trato y la No discriminaci\u00f3n tiene abiertas todas las posiciones de b\u00fasqueda de perfiles a todas aquellas personas que est\u00e9n debidamente cualificadas para cubrir la vacante, independientemente de su edad, discapacidad, g\u00e9nero, orientaci\u00f3n sexual o pol\u00edtica, raza o creencia.\n\n BUSINESS INTEGRATION PARTNERS CONSULTING IBERIA, S.L., con NIF B84701903 y contacto Liliana.mendoza@bip-group.com tratar\u00e1 los datos recibidos al aplicar su candidatura en la oferta como Responsable del tratamiento, con la finalidad de gestionar el proceso de selecci\u00f3n actualmente activo en base a la aplicaci\u00f3n de medidas precontractuales del art\u00edculo 6.1. b) del GDPR, as\u00ed como para futuros procesos de selecci\u00f3n si su perfil fuera de inter\u00e9s, en base al consentimiento del art\u00edculo 6.1 a).\n\n BUSINESS INTEGRATION PARTNERS CONSULTING IBERIA, S.L. no comunicar\u00e1 sus datos a terceros salvo obligaci\u00f3n legal. As\u00ed mismo, el periodo de conservaci\u00f3n de su CV como m\u00e1ximo 1 a\u00f1o, en el que ser\u00e1 destruido.\n\n Igualmente puede ejercer sus derechos de acceso, rectificaci\u00f3n, supresi\u00f3n, oposici\u00f3n, limitaci\u00f3n al tratamiento, portabilidad, y a no ser objeto de decisiones individualizadas automatizadas o consultar informaci\u00f3n adicional sobre el tratamiento en el correo : dpo-bipspain@bip-group.com.\n\u00bfCu\u00e1l ser\u00e1 tu reto?\n\u00bfQuieres trabajar con nosotros?\nRequisitos indispensables:\nAl menos 1 a\u00f1o de experiencia profesional en el sector de consultor\u00eda tecnol\u00f3gica, en alguno de los siguientes bloques:\nDesarrollo de procesos de transformaci\u00f3n de datos a trav\u00e9s de c\u00f3digo con SQL, Python.\nConocimiento en bases de datos SQL (SQL Server, Synapse, PostgreSQL, \u2026), NoSQL (MongoDb, Couchbase, Cosmos DB, Cassandra, Redis, \u2026) y otros recursos de almacenamiento (cloud storage)\nManejo de herramientas de gesti\u00f3n de c\u00f3digo fuente y despliegues automatizados (Azure DevOps, Gitlab, Jenkins, \u2026)\nConocimiento en tecnolog\u00edas Big Data, principalmente spark / Databricks y su \u00e1lter ego Snowflake\nAl menos 1 a\u00f1o de experiencia profesional en el sector de consultor\u00eda tecnol\u00f3gica, en alguno de los siguientes bloques:\nDesarrollo de procesos de transformaci\u00f3n de datos a trav\u00e9s de c\u00f3digo con SQL, Python.\nConocimiento en bases de datos SQL (SQL Server, Synapse, PostgreSQL, \u2026), NoSQL (MongoDb, Couchbase, Cosmos DB, Cassandra, Redis, \u2026) y otros recursos de almacenamiento (cloud storage)\nManejo de herramientas de gesti\u00f3n de c\u00f3digo fuente y despliegues automatizados (Azure DevOps, Gitlab, Jenkins, \u2026)\nConocimiento en tecnolog\u00edas Big Data, principalmente spark / Databricks y su \u00e1lter ego Snowflake\nValorado de forma especial si\nTienes experiencia pr\u00e1ctica o conoces lo que es dbt\nTe llama la atenci\u00f3n lo que se mueve alrededor de conceptos como headless BI, metrics layer, data contracts, data quality...\nHas explorado las posibilidades de tecnolog\u00edas como Clickhouse o DuckDB\nHas hecho tus pinitos con Airflow o con alguno de sus coet\u00e1neos como Dagster o Prefect (o te llama la atenci\u00f3n lo que est\u00e1 haciendo Mage)\nHas trasteado con servicios como Presto o Dremio\nTienes experiencia pr\u00e1ctica o conoces lo que es dbt\nTe llama la atenci\u00f3n lo que se mueve alrededor de conceptos como headless BI, metrics layer, data contracts, data quality...\nHas explorado las posibilidades de tecnolog\u00edas como Clickhouse o DuckDB\nHas hecho tus pinitos con Airflow o con alguno de sus coet\u00e1neos como Dagster o Prefect (o te llama la atenci\u00f3n lo que est\u00e1 haciendo Mage)\nHas trasteado con servicios como Presto o Dremio\nDeseable\nTitulaci\u00f3n Superior en Ingenier\u00eda Inform\u00e1tica (preferiblemente, Telecomunicaciones o materias STEM, o formaci\u00f3n equivalente\nBuen nivel de Ingl\u00e9s (escrito y hablado)\nCertificaciones oficiales relacionadas\nExperiencia pr\u00e1ctica en AWS y GCP, en el uso de los servicios de c\u00f3mputo, redes, almacenamiento, monitorizaci\u00f3n, bases de datos y kubernetes\nMetodolog\u00edas Agile DevOps (Git, Pipelines, CI/CD)\nDespliegue de infraestructura como c\u00f3digo (IaC)\nTitulaci\u00f3n Superior en Ingenier\u00eda Inform\u00e1tica (preferiblemente, Telecomunicaciones o materias STEM, o formaci\u00f3n equivalente\nBuen nivel de Ingl\u00e9s (escrito y hablado)\nCertificaciones oficiales relacionadas\nExperiencia pr\u00e1ctica en AWS y GCP, en el uso de los servicios de c\u00f3mputo, redes, almacenamiento, monitorizaci\u00f3n, bases de datos y kubernetes\nMetodolog\u00edas Agile DevOps (Git, Pipelines, CI/CD)\nDespliegue de infraestructura como c\u00f3digo (IaC)\nSoft Skills\nCar\u00e1cter consultivo\nCapacidad para trabajar en equipo y sumar\nResponsabilidad sobre el trabajo realizado y sentimiento de pertenencia tanto del proyecto como del equipo\nCapacidad de comunicaci\u00f3n y relaciones interpersonales\nOrientaci\u00f3n al cliente y a los objetivos\nAdaptabilidad y flexibilidad\nCar\u00e1cter consultivo\nCapacidad para trabajar en equipo y sumar\nResponsabilidad sobre el trabajo realizado y sentimiento de pertenencia tanto del proyecto como del equipo\nCapacidad de comunicaci\u00f3n y relaciones interpersonales\nOrientaci\u00f3n al cliente y a los objetivos\nAdaptabilidad y flexibilidad\nOfrecemos:\nBeneficios Sociales, paquete retributivo acorde a expectativas, 26 d\u00edas laborales de vacaciones, horario flexible, modelo de trabajo h\u00edbrido.\nPlan personalizado de carrera y formaci\u00f3n.\nPertenecer a una compa\u00f1\u00eda internacional en plena expansi\u00f3n, que forma parte de un gran grupo empresarial, con una cultura de cercan\u00eda y puertas abiertas y que valora a cada persona de forma individualizada.\nBeneficios Sociales, paquete retributivo acorde a expectativas, 26 d\u00edas laborales de vacaciones, horario flexible, modelo de trabajo h\u00edbrido.\nPlan personalizado de carrera y formaci\u00f3n.\nPertenecer a una compa\u00f1\u00eda internacional en plena expansi\u00f3n, que forma parte de un gran grupo empresarial, con una cultura de cercan\u00eda y puertas abiertas y que valora a cada persona de forma individualizada.\nBip Iberia, en compromiso y coherencia con la Ley Integral para la Igualdad de Trato y la No discriminaci\u00f3n tiene abiertas todas las posiciones de b\u00fasqueda de perfiles a todas aquellas personas que est\u00e9n debidamente cualificadas para cubrir la vacante, independientemente de su edad, discapacidad, g\u00e9nero, orientaci\u00f3n sexual o pol\u00edtica, raza o creencia.\nBUSINESS INTEGRATION PARTNERS CONSULTING IBERIA, S.L., con NIF B84701903 y contacto Liliana.mendoza@bip-group.com tratar\u00e1 los datos recibidos al aplicar su candidatura en la oferta como Responsable del tratamiento, con la finalidad de gestionar el proceso de selecci\u00f3n actualmente activo en base a la aplicaci\u00f3n de medidas precontractuales del art\u00edculo 6.1. b) del GDPR, as\u00ed como para futuros procesos de selecci\u00f3n si su perfil fuera de inter\u00e9s, en base al consentimiento del art\u00edculo 6.1 a).\nBUSINESS INTEGRATION PARTNERS CONSULTING IBERIA, S.L. no comunicar\u00e1 sus datos a terceros salvo obligaci\u00f3n legal. As\u00ed mismo, el periodo de conservaci\u00f3n de su CV como m\u00e1ximo 1 a\u00f1o, en el que ser\u00e1 destruido.\nIgualmente puede ejercer sus derechos de acceso, rectificaci\u00f3n, supresi\u00f3n, oposici\u00f3n, limitaci\u00f3n al tratamiento, portabilidad, y a no ser objeto de decisiones individualizadas automatizadas o consultar informaci\u00f3n adicional sobre el tratamiento en el correo : dpo-bipspain@bip-group.com."
    },
    "4025464316": {
        "title": "Data Engineer (all genders) ",
        "company": "Budenheim",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nBarcelona\n Permanent\n Full-time\n\nYour next career step\n\nTake the next step in your career with us as a Data Engineer (all genders). The Data Engineer is responsible for developing and optimizing Budenheim's Data and Integration applications to\n\nalign with business needs.\n\nBudenheim comprises various plants in Spain and Mexico, among others. With around 1,200 employees, we have been producing specialty products and phosphates in the materials and life sciences sector for more than 100 years.\n\nYour Tasks\n\nYou design, configure, and implement Azure Synapse and MuleSoft solutions following best practices\nYou have the chance to Maintain comprehensive documentation of Azure and MuleSoft configurations, customizations, andproject-related activities\nYou propose innovative solutions to enhance efficiency and effectiveness within Synapse and MuleSoft\nYou can participate in projects focused on building and integrating a Data Lakehouse and Integrations\n\nYour Profile\n\nYou have successfully completed a degree in e.g. Business Informatics, Information Technology, Computer Science, Data Science, Mathematics or have a comparable qualification\nYou have the ability to write SQL and PySpark code and develop Mulesoft applications\nYou were already a part of a Project Team\n\nWe Offer\n\nBenefits\n\nIn our agile team, you have the chance to bring in your creativity. We offer you the chance to expand your specialist knowledge and contribute your own ideas. Enjoy our Benefits:\n\nFlexible working hours\ncompetitive remuneration and attractive salary\nlong term and stable projects\ninternational working environment \n\nFind out more about this position. Apply now!\n\nAbout Us\n\nBudenheim is a globally operating specialty chemical company. With its innovative portfolio, the chemical specialist provides sustainable products, services, and application concepts for various markets. In close cooperation with its customers, Budenheim is dedicated to advancing nutrition, health, safety, and the preservation of resources. With sites in Germany, Spain, the Netherlands, the USA, Mexico, China, Singapore, and India, Budenheim generates annual revenue of several hundred million euros and employs more than 1,350 people.\n\nBudenheim is part of the international group Geschwister Oetker Beteiligungen KG. As a partner and member of the group, Budenheim benefits from this affiliation. We at Budenheim are committed to responsible investments. This ensures the best conditions for growth and innovation.\n\nBudenheim stands for diversity, equal opportunities, and prospects. Therefore, we welcome every application, regardless of gender, sexual orientation, age, origin, religion, disability, or other individual characteristics.\n\nInterested?\n\nTake your chance. We are looking forward to your application.\n\nYour Contact\n\nRasul Karakaya\n\nPeople & Governance\n\nBack Apply\nBarcelona\n Permanent\n Full-time\nBarcelona\nPermanent\nFull-time\nYour next career step\nYour Tasks\nYou design, configure, and implement Azure Synapse and MuleSoft solutions following best practices\nYou have the chance to Maintain comprehensive documentation of Azure and MuleSoft configurations, customizations, andproject-related activities\nYou propose innovative solutions to enhance efficiency and effectiveness within Synapse and MuleSoft\nYou can participate in projects focused on building and integrating a Data Lakehouse and Integrations\nYou design, configure, and implement Azure Synapse and MuleSoft solutions following best practices\nYou have the chance to Maintain comprehensive documentation of Azure and MuleSoft configurations, customizations, andproject-related activities\nYou propose innovative solutions to enhance efficiency and effectiveness within Synapse and MuleSoft\nYou can participate in projects focused on building and integrating a Data Lakehouse and Integrations\nYour Profile\nYou have successfully completed a degree in e.g. Business Informatics, Information Technology, Computer Science, Data Science, Mathematics or have a comparable qualification\nYou have the ability to write SQL and PySpark code and develop Mulesoft applications\nYou were already a part of a Project Team\nYou have successfully completed a degree in e.g. Business Informatics, Information Technology, Computer Science, Data Science, Mathematics or have a comparable qualification\nYou have the ability to write SQL and PySpark code and develop Mulesoft applications\nYou were already a part of a Project Team\nWe Offer\nBenefits\nFlexible working hours\ncompetitive remuneration and attractive salary\nlong term and stable projects\ninternational working environment\nFlexible working hours\ncompetitive remuneration and attractive salary\nlong term and stable projects\ninternational working environment\nAbout Us\nInterested?\nYour Contact"
    },
    "4173641449": {
        "title": "Senior Data Engineer",
        "company": "Marbill Technologies",
        "location": "M\u00e1laga, Andalusia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nAt Marbill we re passionate about empowering businesses to succeed in the competitive world of e-commerce. Our innovative software solutions help merchants manage customers mitigate risk streamline payments and enhance service offerings ultimately optimizing their financial operations.\n\nJoin Marbill Technologies and thrive in a dynamic global e-commerce environment!\n\nWhat we offer\n\n Flexible working hours Work when you re most productive!\n Hybrid positions Enjoy the best of both worlds remote office work.\n Top-notch equipment Everything you need to excel in your role.\n Private health insurance Your well-being matters to us.\n Referral bonuses Get rewarded for bringing great talent on board.\n Comprehensive training We invest in your growth from day one.\n Competitive salary Based on your experience with regular reviews.\n 25 days of annual leave Recharge and enjoy your time off.\n Lunch snacks Keep energized throughout the day.\n Parking available Convenient and stress-free.\n Sunshine almost every day Work in a beautiful sunny location.\n\nWe are an ambitious fast-moving team that thrives in a dynamic and agile environment. Our professionals hailing from over 30 countries are resilient accountable and solution-oriented. We make things happen quickly embracing challenges with fresh ideas and a focus on impactful results. We equip our team with the tools and support they need to succeed fostering both professional and personal growth.\n\nIf you re looking to boost your career now is the perfect opportunity! We are currently seeking a Senior Data Engineer to join our dynamic team\n\nWhat you ll do\n\n Design develop and optimize scalable data pipelines to ensure smooth data operations.\n Debug and troubleshoot Airflow tasks for efficient workflow execution.\n Write and maintain high-performance Python code for data engineering tasks.\n Develop and optimize SQL queries for relational databases.\n Manage and maintain cloud-based infrastructure particularly AWS services.\n Utilize Git for version control and effective collaboration with the team.\n Oversee deployments and work with Jupyter Notebooks for data analysis and documentation.\n Provide technical mentorship to junior team members and contribute to best practices.\n\nWhat you ll bring\n\n Minimum of 5 years of experience as a Data Engineer.\n Strong proficiency in SQL and relational database management.\n Hands-on experience with AWS services including S3 Athena and SageMaker.\n Advanced knowledge of Python for data engineering and automation tasks.\n Familiarity with Airflow for workflow orchestration.\n Comfortable working in a Linux environment and using Linux commands.\n Experience with Agile methodologies and tools like Jira.\n Strong problem-solving skills and ability to work effectively in a team environment.\n Experience optimizing Data Science scripts and deploying them into production environments\n\nBonus Skills\n\n Experience working with data lakes and data warehouses.\n Knowledge of containerization tools like Docker.\n Exposure to infrastructure-as-code tools such as Terraform.\n Experience with distributed computing frameworks (e.g. Spark).\n Familiarity with real-time data processing tools such as Kafka.\n\nApply now and let s make an impact together!\n\npython, sql, aws, linux\nFlexible working hours Work when you re most productive!\n Hybrid positions Enjoy the best of both worlds remote office work.\n Top-notch equipment Everything you need to excel in your role.\n Private health insurance Your well-being matters to us.\n Referral bonuses Get rewarded for bringing great talent on board.\n Comprehensive training We invest in your growth from day one.\n Competitive salary Based on your experience with regular reviews.\n 25 days of annual leave Recharge and enjoy your time off.\n Lunch snacks Keep energized throughout the day.\n Parking available Convenient and stress-free.\n Sunshine almost every day Work in a beautiful sunny location.\nFlexible working hours Work when you re most productive!\nHybrid positions Enjoy the best of both worlds remote office work.\nTop-notch equipment Everything you need to excel in your role.\nPrivate health insurance Your well-being matters to us.\nReferral bonuses Get rewarded for bringing great talent on board.\nComprehensive training We invest in your growth from day one.\nCompetitive salary Based on your experience with regular reviews.\n25 days of annual leave Recharge and enjoy your time off.\nLunch snacks Keep energized throughout the day.\nParking available Convenient and stress-free.\nSunshine almost every day Work in a beautiful sunny location.\nDesign develop and optimize scalable data pipelines to ensure smooth data operations.\n Debug and troubleshoot Airflow tasks for efficient workflow execution.\n Write and maintain high-performance Python code for data engineering tasks.\n Develop and optimize SQL queries for relational databases.\n Manage and maintain cloud-based infrastructure particularly AWS services.\n Utilize Git for version control and effective collaboration with the team.\n Oversee deployments and work with Jupyter Notebooks for data analysis and documentation.\n Provide technical mentorship to junior team members and contribute to best practices.\nDesign develop and optimize scalable data pipelines to ensure smooth data operations.\nDebug and troubleshoot Airflow tasks for efficient workflow execution.\nWrite and maintain high-performance Python code for data engineering tasks.\nDevelop and optimize SQL queries for relational databases.\nManage and maintain cloud-based infrastructure particularly AWS services.\nUtilize Git for version control and effective collaboration with the team.\nOversee deployments and work with Jupyter Notebooks for data analysis and documentation.\nProvide technical mentorship to junior team members and contribute to best practices.\nMinimum of 5 years of experience as a Data Engineer.\n Strong proficiency in SQL and relational database management.\n Hands-on experience with AWS services including S3 Athena and SageMaker.\n Advanced knowledge of Python for data engineering and automation tasks.\n Familiarity with Airflow for workflow orchestration.\n Comfortable working in a Linux environment and using Linux commands.\n Experience with Agile methodologies and tools like Jira.\n Strong problem-solving skills and ability to work effectively in a team environment.\n Experience optimizing Data Science scripts and deploying them into production environments\nMinimum of 5 years of experience as a Data Engineer.\nStrong proficiency in SQL and relational database management.\nHands-on experience with AWS services including S3 Athena and SageMaker.\nAdvanced knowledge of Python for data engineering and automation tasks.\nFamiliarity with Airflow for workflow orchestration.\nComfortable working in a Linux environment and using Linux commands.\nExperience with Agile methodologies and tools like Jira.\nStrong problem-solving skills and ability to work effectively in a team environment.\nExperience optimizing Data Science scripts and deploying them into production environments\nExperience working with data lakes and data warehouses.\n Knowledge of containerization tools like Docker.\n Exposure to infrastructure-as-code tools such as Terraform.\n Experience with distributed computing frameworks (e.g. Spark).\n Familiarity with real-time data processing tools such as Kafka.\nExperience working with data lakes and data warehouses.\nKnowledge of containerization tools like Docker.\nExposure to infrastructure-as-code tools such as Terraform.\nExperience with distributed computing frameworks (e.g. Spark).\nFamiliarity with real-time data processing tools such as Kafka.\nDesired Skills and Experience\npython, sql, aws, linux"
    },
    "4173877344": {
        "title": "Data Engineer",
        "company": "Prenomics",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfEres una persona con experiencia en el dise\u00f1o e industrializaci\u00f3n de soluciones end to end basadas en datos? \u00bfTe apasiona la innovaci\u00f3n, la inteligencia artificial y las tecnolog\u00edas de datos? Si la respuesta es s\u00ed, \u00a1te estamos buscando!\n\nComo Data Engineer, ser\u00e1s clave en proyectos de transformaci\u00f3n de datos para uno de los grandes clientes del IBEX35. Trabajar\u00e1s en un entorno innovador y din\u00e1mico, contribuyendo a la puesta en producci\u00f3n de soluciones avanzadas basadas en datos.\n\nTareas\n\nDise\u00f1ar, desarrollar e industrializar soluciones \u201cend to end\u201d basadas en la transformaci\u00f3n de datos. \n\nCoordinar y priorizar proyectos de datos, asegurando su entrega en tiempo y forma. \n\nImplementar pipelines de datos y procesos automatizados para diversas aplicaciones empresariales. \n\nIntegrar y mantener soluciones en plataformas avanzadas como Apache Airflow y Jupyter Notebooks. \n\nGestionar el control de versiones y procesos de CI/CD a trav\u00e9s de herramientas de control de versionado GitLab/GitHub.\n\n\nRequisitos\n\nFormaci\u00f3n: STEM. Grado en Matem\u00e1ticas, F\u00edsica, Ingenier\u00eda, Inform\u00e1tica o experiencia equivalente.\n\nExperiencia laboral dise\u00f1ando e industrializando soluciones \u201cend to end\u201d basadas en transformaci\u00f3n de datos.\n\nLenguajes de programaci\u00f3n: Python y SQL (otros lenguajes como R ser\u00e1n valorados).\n\nPlataformas: Apache Airflow y Jupyter Notebooks.\n\nControl de versiones: Experiencia con GitLab/GitHub y manejo de procesos CI/CD.\n\n\nDeseables:\n\n\nExperiencia con la suite de Google Cloud Platform (BigQuery, Vertex AI, etc.).\n\nHerramienta de transformaci\u00f3n de datos: DBT.\n\nConocimiento en herramientas de BI (QlikSense, Looker, Power BI, Tableau).\n\nFamiliaridad con el ciclo de vida de procesos de Machine Learning y LLMs.\n\nConocimientos b\u00e1sicos en IA Generativa.\n\nInquietud por la innovaci\u00f3n y las \u00faltimas novedades en inteligencia artificial.\n\n\nBeneficios\n\nParticipar en proyectos de alto impacto con un cliente del IBEX35.\n\nOportunidad de desarrollo profesional en un entorno innovador y tecnol\u00f3gico.\n\nAcceso a las \u00faltimas tecnolog\u00edas y metodolog\u00edas de datos e inteligencia artificial.\n\n\nSi te apasiona transformar datos en soluciones pr\u00e1cticas y te interesa la innovaci\u00f3n en el \u00e1mbito de la IA, \u00a1queremos conocerte!\n\u00bfEres una persona con experiencia en el dise\u00f1o e industrializaci\u00f3n de soluciones end to end basadas en datos? \u00bfTe apasiona la innovaci\u00f3n, la inteligencia artificial y las tecnolog\u00edas de datos? Si la respuesta es s\u00ed, \u00a1te estamos buscando!\nComo Data Engineer, ser\u00e1s clave en proyectos de transformaci\u00f3n de datos para uno de los grandes clientes del IBEX35. Trabajar\u00e1s en un entorno innovador y din\u00e1mico, contribuyendo a la puesta en producci\u00f3n de soluciones avanzadas basadas en datos.\nDise\u00f1ar, desarrollar e industrializar soluciones \u201cend to end\u201d basadas en la transformaci\u00f3n de datos. \n\nCoordinar y priorizar proyectos de datos, asegurando su entrega en tiempo y forma. \n\nImplementar pipelines de datos y procesos automatizados para diversas aplicaciones empresariales. \n\nIntegrar y mantener soluciones en plataformas avanzadas como Apache Airflow y Jupyter Notebooks. \n\nGestionar el control de versiones y procesos de CI/CD a trav\u00e9s de herramientas de control de versionado GitLab/GitHub.\nDise\u00f1ar, desarrollar e industrializar soluciones \u201cend to end\u201d basadas en la transformaci\u00f3n de datos.\nCoordinar y priorizar proyectos de datos, asegurando su entrega en tiempo y forma.\nImplementar pipelines de datos y procesos automatizados para diversas aplicaciones empresariales.\nIntegrar y mantener soluciones en plataformas avanzadas como Apache Airflow y Jupyter Notebooks.\nGestionar el control de versiones y procesos de CI/CD a trav\u00e9s de herramientas de control de versionado GitLab/GitHub.\nFormaci\u00f3n: STEM. Grado en Matem\u00e1ticas, F\u00edsica, Ingenier\u00eda, Inform\u00e1tica o experiencia equivalente.\n\nExperiencia laboral dise\u00f1ando e industrializando soluciones \u201cend to end\u201d basadas en transformaci\u00f3n de datos.\n\nLenguajes de programaci\u00f3n: Python y SQL (otros lenguajes como R ser\u00e1n valorados).\n\nPlataformas: Apache Airflow y Jupyter Notebooks.\n\nControl de versiones: Experiencia con GitLab/GitHub y manejo de procesos CI/CD.\nFormaci\u00f3n: STEM. Grado en Matem\u00e1ticas, F\u00edsica, Ingenier\u00eda, Inform\u00e1tica o experiencia equivalente.\nExperiencia laboral dise\u00f1ando e industrializando soluciones \u201cend to end\u201d basadas en transformaci\u00f3n de datos.\nLenguajes de programaci\u00f3n: Python y SQL (otros lenguajes como R ser\u00e1n valorados).\nPlataformas: Apache Airflow y Jupyter Notebooks.\nControl de versiones: Experiencia con GitLab/GitHub y manejo de procesos CI/CD.\nDeseables:\nExperiencia con la suite de Google Cloud Platform (BigQuery, Vertex AI, etc.).\n\nHerramienta de transformaci\u00f3n de datos: DBT.\n\nConocimiento en herramientas de BI (QlikSense, Looker, Power BI, Tableau).\n\nFamiliaridad con el ciclo de vida de procesos de Machine Learning y LLMs.\n\nConocimientos b\u00e1sicos en IA Generativa.\n\nInquietud por la innovaci\u00f3n y las \u00faltimas novedades en inteligencia artificial.\nExperiencia con la suite de Google Cloud Platform (BigQuery, Vertex AI, etc.).\nHerramienta de transformaci\u00f3n de datos: DBT.\nConocimiento en herramientas de BI (QlikSense, Looker, Power BI, Tableau).\nFamiliaridad con el ciclo de vida de procesos de Machine Learning y LLMs.\nConocimientos b\u00e1sicos en IA Generativa.\nInquietud por la innovaci\u00f3n y las \u00faltimas novedades en inteligencia artificial.\nParticipar en proyectos de alto impacto con un cliente del IBEX35.\n\nOportunidad de desarrollo profesional en un entorno innovador y tecnol\u00f3gico.\n\nAcceso a las \u00faltimas tecnolog\u00edas y metodolog\u00edas de datos e inteligencia artificial.\nParticipar en proyectos de alto impacto con un cliente del IBEX35.\nOportunidad de desarrollo profesional en un entorno innovador y tecnol\u00f3gico.\nAcceso a las \u00faltimas tecnolog\u00edas y metodolog\u00edas de datos e inteligencia artificial.\nSi te apasiona transformar datos en soluciones pr\u00e1cticas y te interesa la innovaci\u00f3n en el \u00e1mbito de la IA, \u00a1queremos conocerte!\nDesired Skills and Experience\nan\u00e1lisis datos\n\nairflow\n\nHerramientas ETL\n\nData Engineer\n\nPython\nan\u00e1lisis datos\nairflow\nHerramientas ETL\nData Engineer\nPython"
    },
    "4121164179": {
        "title": "Senior Data Software Engineer (Databricks) ",
        "company": "EPAM Systems",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nWe are seeking a Senior Data Software Engineer skilled in Databricks to join our team.\n\nYou will be integral to our efforts, bringing a deep understanding of data engineering, expertise in Databricks, and an open-minded approach to our collaborative and friendly work environment. This role is ideal for a professional eager to develop scalable data solutions and drive innovation in our data practices.\n\nResponsibilities\n\n\nDesign, develop, and maintain scalable data pipelines and architectures\nOptimize data models and enhance ETL processes utilizing Databricks and related technologies\nImplement rigorous data quality checks and monitoring to guarantee high integrity\nKeep abreast of the latest trends and technologies in data engineering and advocate for the adoption of beneficial new tools\nTroubleshoot and resolve data-related issues promptly\nParticipate in code reviews ensuring adherence to high standards of code quality\n\n\nRequirements\n\n\nHands-on experience with Databricks, including Delta Lake, workflows, Delta Live Tables, along with deployment and versioning strategies\nSolid understanding of data architectures and data modeling skills\nExpertise in Spark, using either Scala or PySpark\nProficiency in cloud-native technologies and software engineering best practices such as containers, unit tests, linting, and code style checks\nEngineering background in AWS, Azure, or GCP\nExperience with big data and performance optimization of data-intensive applications\nProactive approach and prior client-facing experience\nAbility to navigate ambiguity and work independently\nDesire to engage in a transparent, fast-moving startup environment\nFluent English communication skills at a B2+ level or higher\n\n\nWe offer\n\n\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\n\n\nEPAM is a leading digital transformation services and product engineering company with over 52,650 EPAMers in more than 55 countries and regions. Since 1993, our multidisciplinary teams have been helping make the future real for our clients and communities around the world. In 2018, we opened an office in Spain that quickly grew to over 1,450 EPAMers distributed between the offices in M\u00e1laga and Madrid as well as remotely across the country. Here you will collaborate with multinational teams, contribute to numerous innovative projects, and have an opportunity to learn and grow continuously.\n\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nSenior Data Software Engineer\nResponsibilities\nDesign, develop, and maintain scalable data pipelines and architectures\nOptimize data models and enhance ETL processes utilizing Databricks and related technologies\nImplement rigorous data quality checks and monitoring to guarantee high integrity\nKeep abreast of the latest trends and technologies in data engineering and advocate for the adoption of beneficial new tools\nTroubleshoot and resolve data-related issues promptly\nParticipate in code reviews ensuring adherence to high standards of code quality\nDesign, develop, and maintain scalable data pipelines and architectures\nOptimize data models and enhance ETL processes utilizing Databricks and related technologies\nImplement rigorous data quality checks and monitoring to guarantee high integrity\nKeep abreast of the latest trends and technologies in data engineering and advocate for the adoption of beneficial new tools\nTroubleshoot and resolve data-related issues promptly\nParticipate in code reviews ensuring adherence to high standards of code quality\nRequirements\nHands-on experience with Databricks, including Delta Lake, workflows, Delta Live Tables, along with deployment and versioning strategies\nSolid understanding of data architectures and data modeling skills\nExpertise in Spark, using either Scala or PySpark\nProficiency in cloud-native technologies and software engineering best practices such as containers, unit tests, linting, and code style checks\nEngineering background in AWS, Azure, or GCP\nExperience with big data and performance optimization of data-intensive applications\nProactive approach and prior client-facing experience\nAbility to navigate ambiguity and work independently\nDesire to engage in a transparent, fast-moving startup environment\nFluent English communication skills at a B2+ level or higher\nHands-on experience with Databricks, including Delta Lake, workflows, Delta Live Tables, along with deployment and versioning strategies\nSolid understanding of data architectures and data modeling skills\nExpertise in Spark, using either Scala or PySpark\nProficiency in cloud-native technologies and software engineering best practices such as containers, unit tests, linting, and code style checks\nEngineering background in AWS, Azure, or GCP\nExperience with big data and performance optimization of data-intensive applications\nProactive approach and prior client-facing experience\nAbility to navigate ambiguity and work independently\nDesire to engage in a transparent, fast-moving startup environment\nFluent English communication skills at a B2+ level or higher\nWe offer\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends."
    },
    "4157318031": {
        "title": "Cloud Data Engineer - Microsoft Azure (m/f/d)",
        "company": "TD SYNNEX Spain",
        "location": "Alcobendas, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nWhy Choose TD SYNNEX:\n\nTD SYNNEX is a Fortune 100 company with over $58 billion in revenue (NYSE: SNX), recognized as one of the world\u2019s leading technology distributors and solutions aggregators. With a diverse team of 24,000 employees operating across more than 100 countries, we support over 150,000 customers in maximizing the value of their technology investments, driving business results, and unlocking growth opportunities. TD SYNNEX is a certified Great Place to Work, celebrated for our dynamic culture and comprehensive benefits. Our diverse workforce is our greatest strength, fostering success and inclusivity.\n\nAbout the role:\n\nTD SYNNEX has built a strong data analytics capability, focused on creating predictive insights, automation of the business processes and personalization of the partner interactions.\n\nWe are looking for a skilled Data Engineer to join our team and play a key role in designing, building, and optimizing data infrastructure. You will work with cutting-edge technologies like Azure, Snowflake, and Python to develop scalable ETL pipelines, automate workflows, and deliver impactful data solutions. This position requires a collaborative mindset, technical expertise, and the ability to transform complex data into actionable insights that drive business success.\n\nWhat you will do:\n\nProcess and manage complex datasets using Python, SQL, and Snowflake, ensuring accurate and efficient ETL workflows on Azure infrastructure.\nAutomate and optimize data workflows, developing tools to empower analytics and engineering teams.\nApply advanced data techniques such as machine learning and statistical analysis to drive business insights.\nBuild interactive dashboards and visualizations in Power BI to support decision-making processes.\nCollaborate with stakeholders and IT teams to address data-related challenges and ensure scalable solutions.\n\nWhat We're Looking For\n\nMinimum 3 years of experience in a similar role (Data Engineer, Data Scientist or BI).\nStrong knowledge of Python for data manipulation and scripting.\nHands-on experience with data warehousing, ideally using Snowflake.\nProficiency in Azure services, including data integration and cloud-based workflows.\nExpertise in ETL processes, including API integration, data extraction, and testing.\nFamiliarity with Power BI for data visualization and reporting.\nProfessional English level\n\nNice to Have:\n\nCertification PL-200: Microsoft Power Platform Functional Consultant or hands-on experience with tools like Power Apps and Power Automate.\nFamiliarity or certification in DP-600 (Fabric) and experience with technologies such as Logic Apps, Azure Synapse, and Data Factory.\n\nWhat We Offer you:\n\n\ud83c\udfe5 Comprehensive private health and life insurance to keep you covered.\n\ud83c\udf0d Hybrid work model with the opportunity to work remotely three weeks annually.\n\ud83c\udf9f\ufe0f Tailored salary perks covering transportation, meals, learning and childcare needs.\n\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Special rates on gym memberships through Wellhub.\n\ud83d\udc76 On-site nursery and physiotherapist at the office.\n\ud83d\udc86\u200d\u2640\ufe0f Mental health support, including online therapy with LEAD and wellness programs through Wellhub (iFeel, Calm...)\n\ud83d\udcd6 Learning Opportunities: Access to a comprehensive learning platform to support your professional growth.\n\ud83c\udf0d A Global Atmosphere: Join a multicultural and diverse environment where opportunities for growth and collaboration abound.\n\ud83c\udfc5 Certified Workplace Excellence: Work in a certified Great Place to Work where we take work-life balance seriously.\n\nPlease note that benefits may vary depending on the position and the country.\n\nKey Skills\n\nData ETL, Data Warehousing (DW), Machine Learning, Microsoft Azure, Python (Programming Language)\n\nWhat\u2019s In It For You?\n\nElective Benefits: Our programs are tailored to your country to best accommodate your lifestyle.\nGrow Your Career: Accelerate your path to success (and keep up with the future) with formal programs on leadership and professional development, and many more on-demand courses.\nElevate Your Personal Well-Being: Boost your financial, physical, and mental well-being through seminars, events, and our global Life Empowerment Assistance Program.\nDiversity, Equity & Inclusion: It\u2019s not just a phrase to us; valuing every voice is how we succeed. Join us in celebrating our global diversity through inclusive education, meaningful peer-to-peer conversations, and equitable growth and development opportunities.\nMake the Most of our Global Organization: Network with other new co-workers within your first 30 days through our onboarding program.\nConnect with Your Community: Participate in internal, peer-led inclusive communities and activities, including business resource groups, local volunteering events, and more environmental and social initiatives.\n\nDon\u2019t meet every single requirement? Apply anyway. \n\nAt TD SYNNEX, we\u2019re proud to be recognized as a great place to work and a leader in the promotion and practice of diversity, equity and inclusion. If you\u2019re excited about working for our company and believe you\u2019re a good fit for this role, we encourage you to apply. You may be exactly the person we\u2019re looking for!\nWhy Choose TD SYNNEX:\nFortune 100 company\ntechnology distributors and solutions aggregators\ndiverse\n24,000 employees\n100 countries\nGreat Place to Work\ninclusivity\nData Engineer\nAzure\nSnowflake\nPython\nWhat you will do:\nProcess and manage complex datasets using Python, SQL, and Snowflake, ensuring accurate and efficient ETL workflows on Azure infrastructure.\nAutomate and optimize data workflows, developing tools to empower analytics and engineering teams.\nApply advanced data techniques such as machine learning and statistical analysis to drive business insights.\nBuild interactive dashboards and visualizations in Power BI to support decision-making processes.\nCollaborate with stakeholders and IT teams to address data-related challenges and ensure scalable solutions.\nProcess and manage complex datasets using Python, SQL, and Snowflake, ensuring accurate and efficient ETL workflows on Azure infrastructure.\nAutomate and optimize data workflows, developing tools to empower analytics and engineering teams.\nApply advanced data techniques such as machine learning and statistical analysis to drive business insights.\nBuild interactive dashboards and visualizations in Power BI to support decision-making processes.\nCollaborate with stakeholders and IT teams to address data-related challenges and ensure scalable solutions.\nWhat We're Looking For\nMinimum 3 years of experience in a similar role (Data Engineer, Data Scientist or BI).\nStrong knowledge of Python for data manipulation and scripting.\nHands-on experience with data warehousing, ideally using Snowflake.\nProficiency in Azure services, including data integration and cloud-based workflows.\nExpertise in ETL processes, including API integration, data extraction, and testing.\nFamiliarity with Power BI for data visualization and reporting.\nProfessional English level\nMinimum 3 years of experience in a similar role (Data Engineer, Data Scientist or BI).\nStrong knowledge of Python for data manipulation and scripting.\nHands-on experience with data warehousing, ideally using Snowflake.\nProficiency in Azure services, including data integration and cloud-based workflows.\nExpertise in ETL processes, including API integration, data extraction, and testing.\nFamiliarity with Power BI for data visualization and reporting.\nProfessional English level\nNice to Have:\nCertification PL-200: Microsoft Power Platform Functional Consultant or hands-on experience with tools like Power Apps and Power Automate.\nFamiliarity or certification in DP-600 (Fabric) and experience with technologies such as Logic Apps, Azure Synapse, and Data Factory.\nCertification PL-200: Microsoft Power Platform Functional Consultant or hands-on experience with tools like Power Apps and Power Automate.\nFamiliarity or certification in DP-600 (Fabric) and experience with technologies such as Logic Apps, Azure Synapse, and Data Factory.\nWhat We Offer you:\n\ud83c\udfe5 Comprehensive private health and life insurance to keep you covered.\n\ud83c\udf0d Hybrid work model with the opportunity to work remotely three weeks annually.\n\ud83c\udf9f\ufe0f Tailored salary perks covering transportation, meals, learning and childcare needs.\n\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Special rates on gym memberships through Wellhub.\n\ud83d\udc76 On-site nursery and physiotherapist at the office.\n\ud83d\udc86\u200d\u2640\ufe0f Mental health support, including online therapy with LEAD and wellness programs through Wellhub (iFeel, Calm...)\n\ud83d\udcd6 Learning Opportunities: Access to a comprehensive learning platform to support your professional growth.\n\ud83c\udf0d A Global Atmosphere: Join a multicultural and diverse environment where opportunities for growth and collaboration abound.\n\ud83c\udfc5 Certified Workplace Excellence: Work in a certified Great Place to Work where we take work-life balance seriously.\n\ud83c\udfe5 Comprehensive private health and life insurance to keep you covered.\n\ud83c\udf0d Hybrid work model with the opportunity to work remotely three weeks annually.\n\ud83c\udf9f\ufe0f Tailored salary perks covering transportation, meals, learning and childcare needs.\n\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Special rates on gym memberships through Wellhub.\n\ud83d\udc76 On-site nursery and physiotherapist at the office.\n\ud83d\udc86\u200d\u2640\ufe0f Mental health support, including online therapy with LEAD and wellness programs through Wellhub (iFeel, Calm...)\n\ud83d\udcd6 Learning Opportunities: Access to a comprehensive learning platform to support your professional growth.\n\ud83c\udf0d A Global Atmosphere: Join a multicultural and diverse environment where opportunities for growth and collaboration abound.\n\ud83c\udfc5 Certified Workplace Excellence: Work in a certified Great Place to Work where we take work-life balance seriously.\nPlease note that benefits may vary depending on the position and the country.\nKey Skills\nWhat\u2019s In It For You?\nElective Benefits: Our programs are tailored to your country to best accommodate your lifestyle.\nGrow Your Career: Accelerate your path to success (and keep up with the future) with formal programs on leadership and professional development, and many more on-demand courses.\nElevate Your Personal Well-Being: Boost your financial, physical, and mental well-being through seminars, events, and our global Life Empowerment Assistance Program.\nDiversity, Equity & Inclusion: It\u2019s not just a phrase to us; valuing every voice is how we succeed. Join us in celebrating our global diversity through inclusive education, meaningful peer-to-peer conversations, and equitable growth and development opportunities.\nMake the Most of our Global Organization: Network with other new co-workers within your first 30 days through our onboarding program.\nConnect with Your Community: Participate in internal, peer-led inclusive communities and activities, including business resource groups, local volunteering events, and more environmental and social initiatives.\nElective Benefits: Our programs are tailored to your country to best accommodate your lifestyle.\nGrow Your Career: Accelerate your path to success (and keep up with the future) with formal programs on leadership and professional development, and many more on-demand courses.\nElevate Your Personal Well-Being: Boost your financial, physical, and mental well-being through seminars, events, and our global Life Empowerment Assistance Program.\nDiversity, Equity & Inclusion: It\u2019s not just a phrase to us; valuing every voice is how we succeed. Join us in celebrating our global diversity through inclusive education, meaningful peer-to-peer conversations, and equitable growth and development opportunities.\nMake the Most of our Global Organization: Network with other new co-workers within your first 30 days through our onboarding program.\nConnect with Your Community: Participate in internal, peer-led inclusive communities and activities, including business resource groups, local volunteering events, and more environmental and social initiatives.\nDon\u2019t meet every single requirement? Apply anyway."
    },
    "4071747801": {
        "title": "Data Engineer especialista en Snowflake - Delivery Center ",
        "company": "Deloitte",
        "location": "Seville, Andalusia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\u00bfTe imaginas participando en la transformaci\u00f3n de las principales organizaciones nacionales e internacionales?\n\nEn Deloitte estamos comprometidos con generar un impacto en la sociedad, en nuestros clientes y en ti.\n\nDesde nuestro equipo de Deloitte Engineering Center , estamos buscando personas que se incorporen a nuestra \u00e1rea de Artificial Intelligence & Data, en nuestros oficinas de Coru\u00f1a, Vigo, Huesca, Zaragoza o Sevilla. \n\n \u00bfCu\u00e1l es el reto? \n\nBuscamos un Data Engineer con experiencia en Snowflake, para participar en proyectos nacionales e internacionales.\n\n \u00bfC\u00f3mo te imaginamos? \n\n Eres Graduado o posees M\u00e1ster en Ingenier\u00eda Inform\u00e1tica, Data Science, o similares. \n Posees al menos 2 a\u00f1os de experiencia como ingeniero de datos trabajando en procesamiento de grandes vol\u00famenes de datos, tanto con servicios Cloud como con herramientas de integraci\u00f3n. \n Tienes experiencia en desarrollo y optimizaci\u00f3n de queries complejas SQL/NoSQL. \n Tienes experiencia con PySpark, SparkSQL y Phyton para el desarrollo de procesos ETL y pipelines de datos. \n Tienes experiencia trabajando con Snowflake y tienes un buen conocimiento y entendimiento sobre su arquitectura y sus diferentes funcionalidades. \n Valorar\u00edamos especialmente que est\u00e9s certificado en Snowflake (Snowflake SnowPro Core). \n Posees experiencia con sistemas de control de versiones, preferiblemente Git. \n Tienes experiencia en buenas pr\u00e1cticas de desarrollo de software, por ejemplo: pruebas unitarias, arquitectura de aplicaciones de datos, optimizaci\u00f3n\u2026 \n Posees conocimientos de los sistemas de bases de datos y modelado de datos. \n Tienes capacidad de trabajo en equipo, resoluci\u00f3n de problemas y has trabajado en entornos colaborativos. \n Buenas habilidades de comunicaci\u00f3n y capacidad para aprender de manera autodidacta. \n Nivel avanzado de ingl\u00e9s (B2 o superior), con capacidad para comprender y comunicar ideas t\u00e9cnicas complejas tanto de forma oral como escrita. \n\n \u00bfC\u00f3mo es trabajar en Deloitte? \n\n \ud83e\udd29  Proyectos de alto impacto  donde tendr\u00e1s un largo recorrido y aprendizaje\n\n \u262f\ufe0f  Un d\u00eda a d\u00eda h\u00edbrido-flexible:  tendr\u00e1s horario flexible y un buen equilibrio entre el teletrabajo y el trabajo en equipo en nuestras oficinas o las de nuestros clientes\n\n \u26bd  Buen ambiente dentro y fuera de la oficina:  disfrutar\u00e1s de varios teambuildings al a\u00f1o, actividades culturales y deportivas\u2026 \u00a1y mucho m\u00e1s!\n\n \ud83e\uddd8  \u200d\u2642\ufe0f Bienestar integral  : cu\u00eddate con nuestro programa de salud f\u00edsica, mental y financiera\u2026 \u00a1y con equipo m\u00e9dico en las oficinas!\n\n \ud83d\ude4c  Impacto social:  Podr\u00e1s apuntarte a una gran cantidad de voluntariados de alcance nacional e internacional y a proyectos pro-bono con los que poner tu tiempo y talento al servicio de quienes m\u00e1s lo necesitan\n\n \ud83d\udde3  \ufe0f Cultura del feedback y aprendizaje continuo  : crecer\u00e1s en un entorno inclusivo donde la igualdad de oportunidades y tu plan personalizado de formaci\u00f3n impulsar\u00e1n tu desarrollo. \u00bfYa te visualizas en la Deloitte University de Par\u00eds?\n\n \ud83e\udd1d  Beneficios exclusivos por ser parte de Deloitte:  podr\u00e1s disfrutar de un gran cat\u00e1logo de beneficios y de un completo plan de retribuci\u00f3n flexible\n\u00bfTe imaginas participando en la transformaci\u00f3n de las principales organizaciones nacionales e internacionales?\nDeloitte Engineering Center\nCoru\u00f1a, Vigo, Huesca, Zaragoza o Sevilla.\n\u00bfCu\u00e1l es el reto?\nData Engineer con experiencia en Snowflake\n\u00bfC\u00f3mo te imaginamos?\nEres Graduado o posees M\u00e1ster en Ingenier\u00eda Inform\u00e1tica, Data Science, o similares. \n Posees al menos 2 a\u00f1os de experiencia como ingeniero de datos trabajando en procesamiento de grandes vol\u00famenes de datos, tanto con servicios Cloud como con herramientas de integraci\u00f3n. \n Tienes experiencia en desarrollo y optimizaci\u00f3n de queries complejas SQL/NoSQL. \n Tienes experiencia con PySpark, SparkSQL y Phyton para el desarrollo de procesos ETL y pipelines de datos. \n Tienes experiencia trabajando con Snowflake y tienes un buen conocimiento y entendimiento sobre su arquitectura y sus diferentes funcionalidades. \n Valorar\u00edamos especialmente que est\u00e9s certificado en Snowflake (Snowflake SnowPro Core). \n Posees experiencia con sistemas de control de versiones, preferiblemente Git. \n Tienes experiencia en buenas pr\u00e1cticas de desarrollo de software, por ejemplo: pruebas unitarias, arquitectura de aplicaciones de datos, optimizaci\u00f3n\u2026 \n Posees conocimientos de los sistemas de bases de datos y modelado de datos. \n Tienes capacidad de trabajo en equipo, resoluci\u00f3n de problemas y has trabajado en entornos colaborativos. \n Buenas habilidades de comunicaci\u00f3n y capacidad para aprender de manera autodidacta. \n Nivel avanzado de ingl\u00e9s (B2 o superior), con capacidad para comprender y comunicar ideas t\u00e9cnicas complejas tanto de forma oral como escrita.\nEres Graduado o posees M\u00e1ster en Ingenier\u00eda Inform\u00e1tica, Data Science, o similares.\nPosees al menos 2 a\u00f1os de experiencia como ingeniero de datos trabajando en procesamiento de grandes vol\u00famenes de datos, tanto con servicios Cloud como con herramientas de integraci\u00f3n.\nTienes experiencia en desarrollo y optimizaci\u00f3n de queries complejas SQL/NoSQL.\nTienes experiencia con PySpark, SparkSQL y Phyton para el desarrollo de procesos ETL y pipelines de datos.\nTienes experiencia trabajando con Snowflake y tienes un buen conocimiento y entendimiento sobre su arquitectura y sus diferentes funcionalidades.\nValorar\u00edamos especialmente que est\u00e9s certificado en Snowflake (Snowflake SnowPro Core).\nPosees experiencia con sistemas de control de versiones, preferiblemente Git.\nTienes experiencia en buenas pr\u00e1cticas de desarrollo de software, por ejemplo: pruebas unitarias, arquitectura de aplicaciones de datos, optimizaci\u00f3n\u2026\nPosees conocimientos de los sistemas de bases de datos y modelado de datos.\nTienes capacidad de trabajo en equipo, resoluci\u00f3n de problemas y has trabajado en entornos colaborativos.\nBuenas habilidades de comunicaci\u00f3n y capacidad para aprender de manera autodidacta.\nNivel avanzado de ingl\u00e9s (B2 o superior), con capacidad para comprender y comunicar ideas t\u00e9cnicas complejas tanto de forma oral como escrita.\n\u00bfC\u00f3mo es trabajar en Deloitte?\n\ud83e\udd29\nProyectos de alto impacto\n\u262f\ufe0f\nUn d\u00eda a d\u00eda h\u00edbrido-flexible:\n\u26bd\nBuen ambiente dentro y fuera de la oficina:\n\ud83e\uddd8\n\u200d\u2642\ufe0f Bienestar integral\n\ud83d\ude4c\nImpacto social:\n\ud83d\udde3\n\ufe0f Cultura del feedback y aprendizaje continuo\n\ud83e\udd1d\nBeneficios exclusivos por ser parte de Deloitte:"
    },
    "4158763688": {
        "title": "Data Security Engineer ",
        "company": "Rover.com",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nThis role is hybrid -in our Barcelona office one day per week in office (Thursday). Applicants must reside in or relocate to Barcelona and be authorized to work in Spain.\n\nWho we're looking for:\nWe're looking for a Data Security Engineer to join our Cyber Security team. You'll primarily be responsible for implementing and managing the data security program, monitoring data workflows, and implementing solutions to meet compliance regulations. Ideally, you'll also have experience with privacy management platforms, privacy regulations, and data protection processes.\nYou'll succeed in this role if you have a keen eye for detail, strong organizational skills, and a passion for data security and privacy, enabling you to navigate complex data workflows and support cross-functional teams in maintaining robust data privacy practices.\n\nYour Responsibilities:\nServe as the subject matter expert for the Data Security Engineering program at Rover and Rover companies\nProvide expertise and guidance to internal stakeholders on privacy best practices\nPrimary administrator of the Ketch platform and ensure configuration to best practices\nMonitor and fix vulnerabilities in our underlying Data infrastructure\nWork with our data platform team to engineer best practices when it comes to encryption, sanitization, and monitoring\nCollaborate with external vendors and consultants as needed to support privacy due diligence and assessments of third parties\nSupport efforts to maintain documentation related to data protection, including records of data processing activities, data breach notifications, and training records\nPartner with Legal and compliance to ensure we are meeting business goals\nSupport incident response activities, including documentation, investigation, and remediation\nImplement encryption strategies for data at rest and in transit using AWS services such as KMS, S3, RDS, and others\nDevelop and deploy secure AWS architectures that ensure data privacy and compliance with relevant regulations (e.g., GDPR, CCPA)\nAutomate data subject requests, including access, rectification, erasure, restriction, portability, and objections to processing\nDevelop data loss protection technical controls across our application, corporate environment and code base\n\nYour Qualifications:\nBachelor\u2019s degree in Information Technology, Computer Science, Cybersecurity, Information Security, or a related field.\nExperience with data mapping, automating data deletion workflows, and managing privacy management software (Ketch or equivalent)\n3+ years of experience in data privacy, security engineering, or a related role with a focus on AWS environments.\nProven experience with AWS security services, tools, and best practices.\nStrong understanding of data privacy regulations and compliance requirements.\n\nYour Bonus skills:\nPrivacy certifications such as CIPP (preferably CIPP-US and/or CIPP-E), CIPM, or CIPT.\nSecurity Certifications such as CISSP, CISM, OSCP\nExperience using Ketch\nFamiliarity with privacy incident response processes.\nKnowledge of data governance and information security.\n\nBenefits:\nCompetitive compensation\nLong-term incentive plan with a company performance-based cash payout\nPermanent contract\nPension Plan\nMeal tickets through Cobee\nGenerous PTO Allowance\nHybrid schedule\nPrivate health insurance\nDiscounted Gym Membership\nBring your dog to work (and unlimited puppy time)\nMonetary help for adopting a dog plus yearly credit to use on our platform\nFlexible work hours, sometimes you\u2019ll need to be in at certain times, but on the whole, we\u2019re pretty flexible when it comes to managing workload and time\nGrab snacks, fresh fruit, in our kitchen to keep yourself going\nRegular team activities, including happy hours, game nights, and more\n\n\n\nWho we are: \nWant to make an impact? Join our pack and come work (and play!) with us.\n\nWe believe everyone deserves the unconditional love of a pet\u2014and at Rover, our mission is to make it easier to experience that love. Founded in 2011, the Rover app and website connect dog and cat parents with loving pet sitters and dog walkers in neighborhoods across the US, Canada, and Europe. We empower our community of trusted pet sitters and dog walkers to run their own pet care businesses on Rover with the tools and security of a global company to back them.\n\nHeadquartered in Seattle, Washington, we work closely with our teams in Barcelona, San Antonio, Spokane, and remote locations. We\u2019ve got a reputation for being a great place to work, having been named among the 100 Best Companies to Work For in Seattle Business Magazine and Washington\u2019s Best Workplaces in the Puget Sound Business Journal. We're an agile, fast-growing company, and our leadership comes from some of the world's most respected tech companies.\n\nAt Rover, our furry coworkers are just as important as our human ones\u2014and we wouldn\u2019t have it any other way. Along with making the joys of pet parenthood more accessible, we\u2019re committed to fostering a diverse, inclusive, and welcoming community of pet people\u2014and that starts with our employees.\nThis role is hybrid -in our Barcelona office one day per week in office (Thursday). Applicants must reside in or relocate to Barcelona and be authorized to work in Spain.\nWho we're looking for:\nWe're looking for a Data Security Engineer to join our Cyber Security team. You'll primarily be responsible for implementing and managing the data security program, monitoring data workflows, and implementing solutions to meet compliance regulations. Ideally, you'll also have experience with privacy management platforms, privacy regulations, and data protection processes.\nYou'll succeed in this role if you have a keen eye for detail, strong organizational skills, and a passion for data security and privacy, enabling you to navigate complex data workflows and support cross-functional teams in maintaining robust data privacy practices.\nYour Responsibilities:\nServe as the subject matter expert for the Data Security Engineering program at Rover and Rover companies\nProvide expertise and guidance to internal stakeholders on privacy best practices\nPrimary administrator of the Ketch platform and ensure configuration to best practices\nMonitor and fix vulnerabilities in our underlying Data infrastructure\nWork with our data platform team to engineer best practices when it comes to encryption, sanitization, and monitoring\nCollaborate with external vendors and consultants as needed to support privacy due diligence and assessments of third parties\nSupport efforts to maintain documentation related to data protection, including records of data processing activities, data breach notifications, and training records\nPartner with Legal and compliance to ensure we are meeting business goals\nSupport incident response activities, including documentation, investigation, and remediation\nImplement encryption strategies for data at rest and in transit using AWS services such as KMS, S3, RDS, and others\nDevelop and deploy secure AWS architectures that ensure data privacy and compliance with relevant regulations (e.g., GDPR, CCPA)\nAutomate data subject requests, including access, rectification, erasure, restriction, portability, and objections to processing\nDevelop data loss protection technical controls across our application, corporate environment and code base\nServe as the subject matter expert for the Data Security Engineering program at Rover and Rover companies\nProvide expertise and guidance to internal stakeholders on privacy best practices\nPrimary administrator of the Ketch platform and ensure configuration to best practices\nMonitor and fix vulnerabilities in our underlying Data infrastructure\nWork with our data platform team to engineer best practices when it comes to encryption, sanitization, and monitoring\nCollaborate with external vendors and consultants as needed to support privacy due diligence and assessments of third parties\nSupport efforts to maintain documentation related to data protection, including records of data processing activities, data breach notifications, and training records\nPartner with Legal and compliance to ensure we are meeting business goals\nSupport incident response activities, including documentation, investigation, and remediation\nImplement encryption strategies for data at rest and in transit using AWS services such as KMS, S3, RDS, and others\nDevelop and deploy secure AWS architectures that ensure data privacy and compliance with relevant regulations (e.g., GDPR, CCPA)\nAutomate data subject requests, including access, rectification, erasure, restriction, portability, and objections to processing\nDevelop data loss protection technical controls across our application, corporate environment and code base\nYour Qualifications:\nBachelor\u2019s degree in Information Technology, Computer Science, Cybersecurity, Information Security, or a related field.\nExperience with data mapping, automating data deletion workflows, and managing privacy management software (Ketch or equivalent)\n3+ years of experience in data privacy, security engineering, or a related role with a focus on AWS environments.\nProven experience with AWS security services, tools, and best practices.\nStrong understanding of data privacy regulations and compliance requirements.\nBachelor\u2019s degree in Information Technology, Computer Science, Cybersecurity, Information Security, or a related field.\nExperience with data mapping, automating data deletion workflows, and managing privacy management software (Ketch or equivalent)\n3+ years of experience in data privacy, security engineering, or a related role with a focus on AWS environments.\nProven experience with AWS security services, tools, and best practices.\nStrong understanding of data privacy regulations and compliance requirements.\nYour Bonus skills:\nPrivacy certifications such as CIPP (preferably CIPP-US and/or CIPP-E), CIPM, or CIPT.\nSecurity Certifications such as CISSP, CISM, OSCP\nExperience using Ketch\nFamiliarity with privacy incident response processes.\nKnowledge of data governance and information security.\nPrivacy certifications such as CIPP (preferably CIPP-US and/or CIPP-E), CIPM, or CIPT.\nSecurity Certifications such as CISSP, CISM, OSCP\nExperience using Ketch\nFamiliarity with privacy incident response processes.\nKnowledge of data governance and information security.\nBenefits:\nCompetitive compensation\nLong-term incentive plan with a company performance-based cash payout\nPermanent contract\nPension Plan\nMeal tickets through Cobee\nGenerous PTO Allowance\nHybrid schedule\nPrivate health insurance\nDiscounted Gym Membership\nBring your dog to work (and unlimited puppy time)\nMonetary help for adopting a dog plus yearly credit to use on our platform\nFlexible work hours, sometimes you\u2019ll need to be in at certain times, but on the whole, we\u2019re pretty flexible when it comes to managing workload and time\nGrab snacks, fresh fruit, in our kitchen to keep yourself going\nRegular team activities, including happy hours, game nights, and more\nCompetitive compensation\nLong-term incentive plan with a company performance-based cash payout\nPermanent contract\nPension Plan\nMeal tickets through Cobee\nGenerous PTO Allowance\nHybrid schedule\nPrivate health insurance\nDiscounted Gym Membership\nBring your dog to work (and unlimited puppy time)\nMonetary help for adopting a dog plus yearly credit to use on our platform\nFlexible work hours, sometimes you\u2019ll need to be in at certain times, but on the whole, we\u2019re pretty flexible when it comes to managing workload and time\nGrab snacks, fresh fruit, in our kitchen to keep yourself going\nRegular team activities, including happy hours, game nights, and more\nWho we are:\nWant to make an impact? Join our pack and come work (and play!) with us.\nWe believe everyone deserves the unconditional love of a pet\u2014and at Rover, our mission is to make it easier to experience that love. Founded in 2011, the Rover app and website connect dog and cat parents with loving pet sitters and dog walkers in neighborhoods across the US, Canada, and Europe. We empower our community of trusted pet sitters and dog walkers to run their own pet care businesses on Rover with the tools and security of a global company to back them.\nHeadquartered in Seattle, Washington, we work closely with our teams in Barcelona, San Antonio, Spokane, and remote locations. We\u2019ve got a reputation for being a great place to work, having been named among the 100 Best Companies to Work For in Seattle Business Magazine and Washington\u2019s Best Workplaces in the Puget Sound Business Journal. We're an agile, fast-growing company, and our leadership comes from some of the world's most respected tech companies.\nAt Rover, our furry coworkers are just as important as our human ones\u2014and we wouldn\u2019t have it any other way. Along with making the joys of pet parenthood more accessible, we\u2019re committed to fostering a diverse, inclusive, and welcoming community of pet people\u2014and that starts with our employees."
    },
    "4150631094": {
        "title": "Data Technology Sales Engineer ",
        "company": "IBM",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nIntroduction\n\nA Technology Sales Engineer role (what we internally call a,\n\n'Brand Technical\n\nSpecialist') within IBM's Data & AI brand means accelerating enterprises' success\n\nby improving their ability to understand their data. It means providing solutions\n\nthat enable people across organizations, in multiple roles, the ability to turn data\n\ninto actionable insights without having to wait for IT. And it means selling multi-\n\naward winning software, and world-class design practices that enables business Excellent onboarding and an industry leading learning culture will set you up for\n\npositive impact and success, whilst ongoing development will advance your career\n\nthrough an upward trajectory. Our sales environment is collaborative and\n\nexperiential. Part of a team, you'll be surrounded by bright minds and keen co-\n\ncreators \u2013 always willing to help and be helped \u2013 as you apply passion to work that\n\nwill compel our clients to invest in IBM's products and services.\n\nYour Role And Responsibilities\n\nAs a Brand Technical Specialist, you'll work closely with clients to develop\n\nrelationships, understand their needs, earn their trust and show them how IBM's\n\nindustry leading solutions will solve their problems whilst delivering value to their\n\nbusiness.\n\nYour Primary Responsibilities Will Include\n\nClient Strategy Design: Creating client strategies for Data & AI infrastructure.\n\nSolution Definition: Defining IBM Data & AI solutions that enhance technology\n\nstacks.\n\nEducational Support: Providing proof of concepts and simplifying complex\n\ntopics to educate clients.\n\nCredibility Building: Establishing credibility and trust to facilitate the closure\n\nof intricate Data & AI tech deals.\n\nPreferred Education\n\nMaster's Degree\n\nRequired Technical And Professional Expertise\n\nRequired Professional and Technical Expertise\n\nExposure to Tech Sales: Actively acquiring new clients.\n\nBusiness Goal Translation: Translating business goals into technical solutions\n\nthrough collaborative teamwork.\n\nStakeholder Influence: Utilizing strong interpersonal and communication skills\n\nto influence diverse senior stakeholders within enterprises.\n\nAbility to Convey Tech Solutions: Effectively articulating the business and\n\nfinancial impact of tech solutions.\n\nExcellent Communication and Presentation Skills: Demonstrating engaging\n\nand influential communication and presentation abilities.\n\nEnglish and Spanish fluent\n\nPreferred Technical And Professional Experience\n\nPreferred Professional and Technical Expertise\n\nData & AI Market Knowledge: Expertise in the Data & AI market to quickly\n\nbecome a trusted client advisor (training on IBM's Data & AI offerings will be\n\nprovided).\n\nTechnology Solution Expertise: Proven exposure working with a diverse range\n\nof technology solutions, including Cloud, Data & AI, and more (training on IBM's\n\nData & AI offerings will be provided).\nIntroduction\nYour Role And Responsibilities\nYour Primary Responsibilities Will Include\nClient Strategy Design:\nSolution Definition:\nEducational Support:\nCredibility Building:\nPreferred Education\nRequired Technical And Professional Expertise\nRequired Professional and Technical Expertise\nExposure to Tech Sales:\nBusiness Goal Translation:\nStakeholder Influence:\nAbility to Convey Tech Solutions:\nExcellent Communication and Presentation Skills:\nPreferred Technical And Professional Experience\nPreferred Professional and Technical Expertise\nData & AI Market Knowledge:\nTechnology Solution Expertise"
    },
    "4154543911": {
        "title": "Cloud Data Engineer ",
        "company": "TIS (Treasury Intelligence Solutions)",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nWe\u2019re on the mission to set a new industry standard for corporate payment, cashflow and liquidity management. Every day we process hundreds of thousands of payment transactions and accumulate a massive amount of data.\n\nAt TIS we want to build best in class data driven solutions that are effective and scale well, offering deep insights to our customers and internal stakeholders.\n\nJob Description\nAs a Senior Data Engineer you will design, implement and maintain our cloud-native, business-critical big data platform solution with our internal and external customers as the backbone of our analytical data platform at TIS.\n\nYou will work aligned with the Data Platform Engineering team that is responsible for the core data and AI architecture of TIS.\n\nJob Responsibilities\nCreate and maintain robust and resilient ETL pipelines to process massive daily volumes of data;\nScale our data lake leveraging AWS cloud technology;\nPromote and protect the integrity of data according to law and company rules;\nSupport the adoption of the Data Lake within TIS squads;\nImplementation and scaling of our GenAI platform embedded into the data platform.\n\nJob Requirements\nStrong background with at least one Hyperscaler (AWS, GCP, Azure). Preferred AWS, especially S3, Lambda, Glue, Athena, StepFunctions, IAM, Bedrock, Lakeformation, DataZones or QuickSights;\nFamiliar with RDBMS and/or data warehouse technologies;\nExperiences with data processing pipelines and data governance;\nProficient with Python 3 or JVM based language like Scala;\nExperience in data protection and security topics;\nExperience with automation of IaC using tools like Terraform or CDK;\nExperience with Continuous Integration and Delivery;\nExperience with GenAI platforms and frameworks such as OpenAI, Amazon Bedrock, Langchain, Llama Index is a plus;\nStrong knowledge of professional software engineering skills (SOLID, Clean Code, Separation of Concerns);\nStrong communication skills in English.\n\nBenefits\nAt TIS, we prioritize the well-being and professional growth of our employees by offering various benefits, company perks, and work-life balance policies. We foster a supportive and dynamic work environment with team events and flexible work options, including fully remote and hybrid office-based setups.\nWe\u2019re on the mission to set a new industry standard for corporate payment, cashflow and liquidity management. Every day we process hundreds of thousands of payment transactions and accumulate a massive amount of data.\nAt TIS we want to build best in class data driven solutions that are effective and scale well, offering deep insights to our customers and internal stakeholders.\nJob Description\nAs a Senior Data Engineer you will design, implement and maintain our cloud-native, business-critical big data platform solution with our internal and external customers as the backbone of our analytical data platform at TIS.\nYou will work aligned with the Data Platform Engineering team that is responsible for the core data and AI architecture of TIS.\nJob Responsibilities\nCreate and maintain robust and resilient ETL pipelines to process massive daily volumes of data;\nScale our data lake leveraging AWS cloud technology;\nPromote and protect the integrity of data according to law and company rules;\nSupport the adoption of the Data Lake within TIS squads;\nImplementation and scaling of our GenAI platform embedded into the data platform.\nCreate and maintain robust and resilient ETL pipelines to process massive daily volumes of data;\nScale our data lake leveraging AWS cloud technology;\nPromote and protect the integrity of data according to law and company rules;\nSupport the adoption of the Data Lake within TIS squads;\nImplementation and scaling of our GenAI platform embedded into the data platform.\nJob Requirements\nStrong background with at least one Hyperscaler (AWS, GCP, Azure). Preferred AWS, especially S3, Lambda, Glue, Athena, StepFunctions, IAM, Bedrock, Lakeformation, DataZones or QuickSights;\nFamiliar with RDBMS and/or data warehouse technologies;\nExperiences with data processing pipelines and data governance;\nProficient with Python 3 or JVM based language like Scala;\nExperience in data protection and security topics;\nExperience with automation of IaC using tools like Terraform or CDK;\nExperience with Continuous Integration and Delivery;\nExperience with GenAI platforms and frameworks such as OpenAI, Amazon Bedrock, Langchain, Llama Index is a plus;\nStrong knowledge of professional software engineering skills (SOLID, Clean Code, Separation of Concerns);\nStrong communication skills in English.\nStrong background with at least one Hyperscaler (AWS, GCP, Azure). Preferred AWS, especially S3, Lambda, Glue, Athena, StepFunctions, IAM, Bedrock, Lakeformation, DataZones or QuickSights;\nFamiliar with RDBMS and/or data warehouse technologies;\nExperiences with data processing pipelines and data governance;\nProficient with Python 3 or JVM based language like Scala;\nExperience in data protection and security topics;\nExperience with automation of IaC using tools like Terraform or CDK;\nExperience with Continuous Integration and Delivery;\nExperience with GenAI platforms and frameworks such as OpenAI, Amazon Bedrock, Langchain, Llama Index is a plus;\nStrong knowledge of professional software engineering skills (SOLID, Clean Code, Separation of Concerns);\nStrong communication skills in English.\nBenefits\nAt TIS, we prioritize the well-being and professional growth of our employees by offering various benefits, company perks, and work-life balance policies. We foster a supportive and dynamic work environment with team events and flexible work options, including fully remote and hybrid office-based setups."
    },
    "4176296760": {
        "title": "Senior Data Engineer",
        "company": "VISEO IBERIA",
        "location": "Greater Barcelona Metropolitan Area",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nVISEO\n\nSi te gustan los nuevos retos y quieres progresar en tu carrera profesional en entornos Microsoft, VISEO te ofrece proyectos estables, cerrados e internacionales en grandes clientes tecnol\u00f3gicos.\n\nActualmente buscamos un Senior Data Engineer para formar parte de nuestro equipo en la zona de Barcelona. Es un rol enfocado a la parte t\u00e9cnica, para desarrollo de ETLs en entornos AWS con SAP Data. \n\nAlcance\n\nDise\u00f1o y arquitectura de soluciones de datos escalables.\nDesarrollo de ETLs en entornos SAP Data.\nDirigir el desarrollo y la optimizaci\u00f3n de canalizaciones de datos complejas utilizando Airflow.\nSupervisar el dise\u00f1o, implementaci\u00f3n y optimizaci\u00f3n de procesos ETL utilizando Python y SQL.\nUtilizar Github para el control de versiones y el desarrollo colaborativo.\nImplantar sistemas de monitorizaci\u00f3n y alertas para las canalizaciones de datos.\n\nTu Perfil:\n\nExperiencia de 4 a\u00f1os o superior como Data Engineer. \nEstudios de ingenier\u00eda inform\u00e1tica, matem\u00e1ticas o campos relacionados.\nDominio de programaci\u00f3n SQL y Python. \nExperiencia con servicios de AWS, SAP Data Intelligence Cloud y soluciones de almacenamiento de datos en la nube. \nExperiencia en desarrollo de ETLs \nExperiencia con Github.\nNivel de ingl\u00e9s alto hablado y escrito.\n\n\u00bfQu\u00e9 podemos aportarte?\n\nOportunidad de incorporarte en una empresa pionera a nivel tecnol\u00f3gico.\nFormaci\u00f3n continua t\u00e9cnica y competencial.\nContrato Indefinido o freelance.\nBeneficios Sociales (seguro m\u00e9dico, ayuda al remoto, ticket restaurant, transporte, 26 d\u00edas de vacaciones al a\u00f1o etc.).\nConciliaci\u00f3n familiar y laboral gracias a la flexibilidad y la posibilidad de teletrabajo.\nSalario competitivo seg\u00fan la experiencia aportada.\n\nTendr\u00e1s las siguientes responsabilidades en seguridad dentro de la compa\u00f1\u00eda:\n\nAcceso a la informaci\u00f3n confidencial relativa al cliente del que es responsable.\nSolicitud de medidas / usuarios.\nObservancia del cumplimiento de las pol\u00edticas de seguridad y las pol\u00edticas internas.\nVISEO\nSi te gustan los nuevos retos y quieres progresar en tu carrera profesional en entornos Microsoft, VISEO te ofrece proyectos estables, cerrados e internacionales en grandes clientes tecnol\u00f3gicos.\nActualmente buscamos un Senior Data Engineer para formar parte de nuestro equipo en la zona de Barcelona. Es un rol enfocado a la parte t\u00e9cnica, para desarrollo de ETLs en entornos AWS con SAP Data.\nSenior Data Engineer\nAlcance\nDise\u00f1o y arquitectura de soluciones de datos escalables.\nDesarrollo de ETLs en entornos SAP Data.\nDirigir el desarrollo y la optimizaci\u00f3n de canalizaciones de datos complejas utilizando Airflow.\nSupervisar el dise\u00f1o, implementaci\u00f3n y optimizaci\u00f3n de procesos ETL utilizando Python y SQL.\nUtilizar Github para el control de versiones y el desarrollo colaborativo.\nImplantar sistemas de monitorizaci\u00f3n y alertas para las canalizaciones de datos.\nDise\u00f1o y arquitectura de soluciones de datos escalables.\nDesarrollo de ETLs en entornos SAP Data.\nDirigir el desarrollo y la optimizaci\u00f3n de canalizaciones de datos complejas utilizando Airflow.\nAirflow\nSupervisar el dise\u00f1o, implementaci\u00f3n y optimizaci\u00f3n de procesos ETL utilizando Python y SQL.\nUtilizar Github para el control de versiones y el desarrollo colaborativo.\nGithub\nImplantar sistemas de monitorizaci\u00f3n y alertas para las canalizaciones de datos.\nTu Perfil:\nExperiencia de 4 a\u00f1os o superior como Data Engineer. \nEstudios de ingenier\u00eda inform\u00e1tica, matem\u00e1ticas o campos relacionados.\nDominio de programaci\u00f3n SQL y Python. \nExperiencia con servicios de AWS, SAP Data Intelligence Cloud y soluciones de almacenamiento de datos en la nube. \nExperiencia en desarrollo de ETLs \nExperiencia con Github.\nNivel de ingl\u00e9s alto hablado y escrito.\nExperiencia de 4 a\u00f1os o superior como Data Engineer.\nEstudios de ingenier\u00eda inform\u00e1tica, matem\u00e1ticas o campos relacionados.\nDominio de programaci\u00f3n SQL y Python.\nExperiencia con servicios de AWS, SAP Data Intelligence Cloud y soluciones de almacenamiento de datos en la nube.\nExperiencia en desarrollo de ETLs\nExperiencia con Github.\nGithub.\nNivel de ingl\u00e9s alto hablado y escrito.\n\u00bfQu\u00e9 podemos aportarte?\nOportunidad de incorporarte en una empresa pionera a nivel tecnol\u00f3gico.\nFormaci\u00f3n continua t\u00e9cnica y competencial.\nContrato Indefinido o freelance.\nBeneficios Sociales (seguro m\u00e9dico, ayuda al remoto, ticket restaurant, transporte, 26 d\u00edas de vacaciones al a\u00f1o etc.).\nConciliaci\u00f3n familiar y laboral gracias a la flexibilidad y la posibilidad de teletrabajo.\nSalario competitivo seg\u00fan la experiencia aportada.\nOportunidad de incorporarte en una empresa pionera a nivel tecnol\u00f3gico.\nFormaci\u00f3n continua t\u00e9cnica y competencial.\nContrato Indefinido o freelance.\nBeneficios Sociales (seguro m\u00e9dico, ayuda al remoto, ticket restaurant, transporte, 26 d\u00edas de vacaciones al a\u00f1o etc.).\nConciliaci\u00f3n familiar y laboral gracias a la flexibilidad y la posibilidad de teletrabajo.\nSalario competitivo seg\u00fan la experiencia aportada.\nTendr\u00e1s las siguientes responsabilidades en seguridad dentro de la compa\u00f1\u00eda:\nAcceso a la informaci\u00f3n confidencial relativa al cliente del que es responsable.\nSolicitud de medidas / usuarios.\nObservancia del cumplimiento de las pol\u00edticas de seguridad y las pol\u00edticas internas.\nAcceso a la informaci\u00f3n confidencial relativa al cliente del que es responsable.\nSolicitud de medidas / usuarios.\nObservancia del cumplimiento de las pol\u00edticas de seguridad y las pol\u00edticas internas."
    },
    "4090186639": {
        "title": "Senior Data Engineer ",
        "company": "CoachHub - The digital coaching platform",
        "location": "EMEA",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nCoachHub is on a mission to democratise coaching for all career levels worldwide. We need an amazing team to achieve this, so we\u2019re bringing together kind, smart and highly-skilled people from all corners of the globe.\n\nIf you'd like to shape the success story of a fast-growing, award-winning company and the leading global digital coaching platform - get in touch! \n\nAbout The Data & Insights Team\n\nWe a remote first team (Spain, France, Germany, Austria, Italy). The team is composed of a group of talented Data Engineers, Machine Learning Engineers and Full Stack Engineers. This allows us to bring different perspectives, experiences, and ideas to the table, making our work richer and more impactful.\n\nWe work in \"tribes\" which are composed by multidisciplinary colleagues (product managers, analysts, and engineers). Depending on the tribe we use Scrum or Kanban. Your work will have a direct impact on our products, our customers, and our success as a company. We're looking for someone who is passionate about making a difference\n\nWhat We Value\n\n Autonomy\nProactiveness\nLearning from Mistakes\nMaking an Impact\nCollaboration \n\nOur Golden Rules\n\nMove Fast (Iterate)\nWith High Quality\nAnd true ownership \n\nPosition\n\nWe are seeking an experienced and talented Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a key role in designing, implementing, and maintaining our data architecture and infrastructure.\n\nLocation: remote within the EU\n\nResponsibilities\n\nDesign, implement, and maintain scalable and efficient data architectures to support the company's data needs.\nDevelop and implement data models, ensuring data accuracy, consistency, and reliability.\nCreate and optimize ETL processes to extract, transform, and load data from various sources into our data warehouse.\nImplement and enforce data quality standards, ensuring the accuracy and completeness of the data.\nMonitor and optimize the performance of data pipelines and queries.\nCreate and maintain comprehensive documentation for data engineering processes, data models, and systems.\nWork closely with data analysts, data product managers and other stakeholders to understand and address their data requirements.\n\nRequirements\n\nProven professional data engineering experience.\nProficient in SQL and experience with relational and column databases (e.g., PostgreSQL, Redshift respectively).\nStrong programming skills in Python.\nExperience with cloud platforms (AWS).\nExperience with data modeling, ETL processes, and data warehousing concepts.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nAbility to work independently and as part of a team in a fast-paced environment.\n\nPreferred Qualifications\n\nExperience in the use of AWS Lambda, EKS, API Gateway, S3, CDK, Airflow, dbt, Git\nFamiliarity with data governance and compliance standards.\nKnowledge of machine learning concepts and frameworks.\nGenerative AI enthusiast\n\n\nAbout CoachHub\n\nCoachHub is the leading global talent development platform that enables organizations to create a personalized, measurable and scalable coaching program for the entire workforce, regardless of department and seniority level. By doing so, organizations are able to reap a multitude of benefits, including increased employee engagement, higher levels of productivity, improved job performance and increased retention.\n\nCoachHub\u2019s global pool of coaches is comprised of over 3,000 certified business coaches in 70 countries across six continents with coaching sessions available in over 60 languages, to serve more than 500 clients.\n\nOur programs are based on advanced R&D from our Coaching Lab. CoachHub is backed by leading tech investors, including Draper Esprit, Holtzbrinck Ventures, Partech, RTP Global, Signals Venture Capital, Sofina, Softbank and Speedinvest.\n\nAs of 2022, CoachHub is a certified Carbon Neutral Company. Find out more about CoachHub\u2019s dedication to positive impact at www.coachhub.com/esg.\nIf you'd like to shape the success story of a fast-growing, award-winning company and the leading global digital coaching platform - get in touch!\nAbout The Data & Insights Team\nWhat We Value\nAutonomy\nProactiveness\nLearning from Mistakes\nMaking an Impact\nCollaboration\nAutonomy\nProactiveness\nLearning from Mistakes\nMaking an Impact\nCollaboration\nOur Golden Rules\nMove Fast (Iterate)\nWith High Quality\nAnd true ownership\nMove Fast (Iterate)\nWith High Quality\nAnd true ownership\nPosition\nResponsibilities\nDesign, implement, and maintain scalable and efficient data architectures to support the company's data needs.\nDevelop and implement data models, ensuring data accuracy, consistency, and reliability.\nCreate and optimize ETL processes to extract, transform, and load data from various sources into our data warehouse.\nImplement and enforce data quality standards, ensuring the accuracy and completeness of the data.\nMonitor and optimize the performance of data pipelines and queries.\nCreate and maintain comprehensive documentation for data engineering processes, data models, and systems.\nWork closely with data analysts, data product managers and other stakeholders to understand and address their data requirements.\nDesign, implement, and maintain scalable and efficient data architectures to support the company's data needs.\nDevelop and implement data models, ensuring data accuracy, consistency, and reliability.\nCreate and optimize ETL processes to extract, transform, and load data from various sources into our data warehouse.\nImplement and enforce data quality standards, ensuring the accuracy and completeness of the data.\nMonitor and optimize the performance of data pipelines and queries.\nCreate and maintain comprehensive documentation for data engineering processes, data models, and systems.\nWork closely with data analysts, data product managers and other stakeholders to understand and address their data requirements.\nRequirements\nProven professional data engineering experience.\nProficient in SQL and experience with relational and column databases (e.g., PostgreSQL, Redshift respectively).\nStrong programming skills in Python.\nExperience with cloud platforms (AWS).\nExperience with data modeling, ETL processes, and data warehousing concepts.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nAbility to work independently and as part of a team in a fast-paced environment.\nProven professional data engineering experience.\nProficient in SQL and experience with relational and column databases (e.g., PostgreSQL, Redshift respectively).\nStrong programming skills in Python.\nExperience with cloud platforms (AWS).\nExperience with data modeling, ETL processes, and data warehousing concepts.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nAbility to work independently and as part of a team in a fast-paced environment.\nPreferred Qualifications\nExperience in the use of AWS Lambda, EKS, API Gateway, S3, CDK, Airflow, dbt, Git\nFamiliarity with data governance and compliance standards.\nKnowledge of machine learning concepts and frameworks.\nGenerative AI enthusiast\nExperience in the use of AWS Lambda, EKS, API Gateway, S3, CDK, Airflow, dbt, Git\nFamiliarity with data governance and compliance standards.\nKnowledge of machine learning concepts and frameworks.\nGenerative AI enthusiast\nAbout CoachHub"
    },
    "4110181525": {
        "title": "Data Engineer",
        "company": "Alter Solutions Group",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nCompany Description\n\n\u00bfQuienes somos?\n\nAlter Solutions es una compa\u00f1\u00eda de consultor\u00eda de tecnolog\u00edas de la informaci\u00f3n y un catalizador de la transformaci\u00f3n digital. Somos un equipo, una comunidad, un grupo de personas con un objetivo com\u00fan: crecer y hacer crecer a la compa\u00f1\u00eda.\n\nJob Description\n\n\u00bfQu\u00e9 buscamos?\n\nUn perfil de Data Engineer con experiencia de al menos 4 a\u00f1os utilizando alguna de las siguientes tecnolog\u00edas:\n\nDominio de Python\nSpark.\nAWS Cloud.\nAzure Data Lake, Data Factory y Databricks.\nCI&CD\nGit.\nNivel de ingles: C1\n\nAdditional Information\n\n\u00bfQu\u00e9 podemos ofrecerte?\n\n Modalidad de trabajo: Totalmente remota.\nTrabajar en un cliente internacional.\n Salarios competitivos con el mercado.\n Crecimiento profesional.\n Participar en proyectos internacionales.\n Re-ubicaci\u00f3n en otros pa\u00edses.\nCompany Description\n\u00bfQuienes somos?\nJob Description\n\u00bfQu\u00e9 buscamos?\nData Engineer\nDominio de Python\nSpark.\nAWS Cloud.\nAzure Data Lake, Data Factory y Databricks.\nCI&CD\nGit.\nNivel de ingles: C1\nDominio de Python\nSpark.\nAWS Cloud.\nAzure Data Lake, Data Factory y Databricks.\nCI&CD\nGit.\nNivel de ingles: C1\nAdditional Information\n\u00bfQu\u00e9 podemos ofrecerte?\nModalidad de trabajo: Totalmente remota.\nTrabajar en un cliente internacional.\n Salarios competitivos con el mercado.\n Crecimiento profesional.\n Participar en proyectos internacionales.\n Re-ubicaci\u00f3n en otros pa\u00edses.\nModalidad de trabajo: Totalmente remota.\nTrabajar en un cliente internacional.\nSalarios competitivos con el mercado.\nCrecimiento profesional.\nParticipar en proyectos internacionales.\nRe-ubicaci\u00f3n en otros pa\u00edses."
    },
    "4070523630": {
        "title": "Data Engineer",
        "company": "Tiger Analytics",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nTiger Analytics is the largest AI and advanced analytics consulting firm. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our consultants bring depth in the industry and deep expertise in Data Science, Data Engineering, Machine Learning, and AI. Various market research firms, including Forrester and Gartner, have recognized our business value and leadership. We are headquartered in Silicon Valley and have our global delivery center in Chennai, India. We also have a presence in Europe, Singapore and LATAM markets.\n\nRequirements\n\n8+ years of overall industry experience specifically in data engineering\nStrong knowledge of data engineering principles, data integration, and data warehousing concepts\nShould have MLops and Data Science knowledge\nCPG/FMCG domain experience preferred\nSolid understanding of ETL processes and tools and Azure cloud platform\nStrong programming skills in Python, SQL, or Scala\nGood understanding of data modeling, data architecture, and database design\nShould have exposure to technologies such as dbt, Apache airflow and Snowflake\nExperience extracting/querying/joining large data sets at scale\nA desire to work in a collaborative, intellectually curious environment\nStrong communication and organizational skills\n\n\nBenefits\n\nThis position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.\nRequirements\n8+ years of overall industry experience specifically in data engineering\nStrong knowledge of data engineering principles, data integration, and data warehousing concepts\nShould have MLops and Data Science knowledge\nCPG/FMCG domain experience preferred\nSolid understanding of ETL processes and tools and Azure cloud platform\nStrong programming skills in Python, SQL, or Scala\nGood understanding of data modeling, data architecture, and database design\nShould have exposure to technologies such as dbt, Apache airflow and Snowflake\nExperience extracting/querying/joining large data sets at scale\nA desire to work in a collaborative, intellectually curious environment\nStrong communication and organizational skills\n8+ years of overall industry experience specifically in data engineering\nStrong knowledge of data engineering principles, data integration, and data warehousing concepts\nShould have MLops and Data Science knowledge\nCPG/FMCG domain experience preferred\nSolid understanding of ETL processes and tools and Azure cloud platform\nStrong programming skills in Python, SQL, or Scala\nGood understanding of data modeling, data architecture, and database design\nShould have exposure to technologies such as dbt, Apache airflow and Snowflake\nExperience extracting/querying/joining large data sets at scale\nA desire to work in a collaborative, intellectually curious environment\nStrong communication and organizational skills\nBenefits"
    },
    "4162648297": {
        "title": "Senior Data Engineer",
        "company": "Investing.com",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nInvesting.com, the leading global financial news & data platform, is on the lookout for a Senior Data Engineer to join our growing data team!\nIn this role, you will design and develop scalable data solutions, optimize data workflows, and support critical business processes. You will work with a variety of databases and big data tools in a cloud environment, focusing on data modeling, governance, and analytics.\nWhat you\u2019ll be doing:\nDesign, develop, and maintain end-to-end ETL pipelines, from gathering business requirements to implementation.\nWork with multiple database technologies, especially BigQuery.\nOptimize data models (DWH, fact & dimension tables, RI, SCDs) for performance and scalability.\nImplement data governance best practices and maintain comprehensive documentation.\nUtilize Big Data tools in cloud environments (GCP preferred).\nDevelop and support complex business workflows and data processes.\nDesign and implement monitoring systems to ensure data quality throughout the pipeline.\nWorkflow orchestration using Apache Airflow.\nCollaborate with analysts and stakeholders to ensure high-quality data for business insights.\nSupport and optimize Tableau infrastructure for data visualization.\nWhat you\u2019ll bring:\n4+ years of experience in Data Engineering.\nStrong SQL skills and expertise in BigQuery or similar databases.\n4+ years of Python experience for data processing and automation.\nProven experience in designing complex business workflows and data processes.\nDeep understanding of data modeling principles and best practices.\nHands-on experience with cloud-based big data tools (GCP preferred).\nMust have experience with Apache Airflow for orchestrating data workflows.\nStrong analytical skills with the ability to translate business needs into technical solutions.\nExperience with Tableau infrastructure management is an advantage.\nExcellent communication skills and ability to work cross-functionally.\nNice to have:\nFamiliarity with streaming data frameworks (Kafka, Pub/Sub).\nIf you are passionate about data, scalability, and building efficient solutions, we\u2019d love to hear from you! Apply now and be part of our data-driven journey.\nWho we are:\nFounded in 2007, Investing.com now has over 300 employees worldwide.\nWith 60 million monthly unique visitors, 4 billion page views, over 500 million sessions, and over 300,000 financial instruments covered, we are one of the top two global financial websites.\nInvesting.com offers unlimited access to cutting-edge financial markets tools such as real-time quotes and charts, customized portfolios, personal live alerts, calendars, calculators, and financial insights\u2014completely free of charge. In addition to global stock markets, we also cover world indices, ETFs, commodities, bonds & interest rates, cryptocurrencies, futures, and options.\nWhat we offer:\nA chance to join one of the leading financial markets platforms worldwide\nChallenging assignments in a local and international environment\nEnjoy excellent benefits and a high-energy working environment\nAs with all Investing.com positions, we're looking for someone self-motivated, meticulous, and resourceful, with a bright, proactive mindset\u2014able to work both independently and as part of a team.\nCheck out our openings on our career website\nOur Privacy Policy: Your resume and information will be kept confidential.\nInvesting.com, the leading global financial news & data platform, is on the lookout for a Senior Data Engineer to join our growing data team!\nInvesting.com\nSenior Data Engineer\nIn this role, you will design and develop scalable data solutions, optimize data workflows, and support critical business processes. You will work with a variety of databases and big data tools in a cloud environment, focusing on data modeling, governance, and analytics.\ndata modeling, governance, and analytics\nWhat you\u2019ll be doing:\nDesign, develop, and maintain end-to-end ETL pipelines, from gathering business requirements to implementation.\nWork with multiple database technologies, especially BigQuery.\nOptimize data models (DWH, fact & dimension tables, RI, SCDs) for performance and scalability.\nImplement data governance best practices and maintain comprehensive documentation.\nUtilize Big Data tools in cloud environments (GCP preferred).\nDevelop and support complex business workflows and data processes.\nDesign and implement monitoring systems to ensure data quality throughout the pipeline.\nWorkflow orchestration using Apache Airflow.\nCollaborate with analysts and stakeholders to ensure high-quality data for business insights.\nSupport and optimize Tableau infrastructure for data visualization.\nDesign, develop, and maintain end-to-end ETL pipelines, from gathering business requirements to implementation.\nETL pipelines\nWork with multiple database technologies, especially BigQuery.\nmultiple database technologies\nBigQuery\nOptimize data models (DWH, fact & dimension tables, RI, SCDs) for performance and scalability.\ndata models\nDWH, fact & dimension tables, RI, SCDs\nImplement data governance best practices and maintain comprehensive documentation.\ndata governance\nUtilize Big Data tools in cloud environments (GCP preferred).\nBig Data tools\ncloud environments\nDevelop and support complex business workflows and data processes.\ncomplex business workflows\nDesign and implement monitoring systems to ensure data quality throughout the pipeline.\nmonitoring systems\ndata quality\nWorkflow orchestration using Apache Airflow.\nCollaborate with analysts and stakeholders to ensure high-quality data for business insights.\nhigh-quality data\nSupport and optimize Tableau infrastructure for data visualization.\nTableau infrastructure\nWhat you\u2019ll bring:\n4+ years of experience in Data Engineering.\nStrong SQL skills and expertise in BigQuery or similar databases.\n4+ years of Python experience for data processing and automation.\nProven experience in designing complex business workflows and data processes.\nDeep understanding of data modeling principles and best practices.\nHands-on experience with cloud-based big data tools (GCP preferred).\nMust have experience with Apache Airflow for orchestrating data workflows.\nStrong analytical skills with the ability to translate business needs into technical solutions.\nExperience with Tableau infrastructure management is an advantage.\nExcellent communication skills and ability to work cross-functionally.\n4+ years of experience in Data Engineering.\n4+ years\nData Engineering\nStrong SQL skills and expertise in BigQuery or similar databases.\nSQL skills\n4+ years of Python experience for data processing and automation.\nPython\ndata processing\nautomation\nProven experience in designing complex business workflows and data processes.\ndata processes\nDeep understanding of data modeling principles and best practices.\ndata modeling principles\nHands-on experience with cloud-based big data tools (GCP preferred).\ncloud-based big data tools\nGCP preferred\nMust have experience with Apache Airflow for orchestrating data workflows.\nMust have experience with Apache Airflow\nStrong analytical skills with the ability to translate business needs into technical solutions.\ntranslate business needs into technical solutions\nExperience with Tableau infrastructure management is an advantage.\nTableau infrastructure management\nExcellent communication skills and ability to work cross-functionally.\ncommunication skills\nNice to have:\nFamiliarity with streaming data frameworks (Kafka, Pub/Sub).\nstreaming data frameworks\nIf you are passionate about data, scalability, and building efficient solutions, we\u2019d love to hear from you! Apply now and be part of our data-driven journey.\ndata, scalability, and building efficient solutions\ndata-driven journey\nWho we are:\nFounded in 2007, Investing.com now has over 300 employees worldwide.\nover 300 employees worldwide\nWith 60 million monthly unique visitors, 4 billion page views, over 500 million sessions, and over 300,000 financial instruments covered, we are one of the top two global financial websites.\n60 million monthly unique visitors\n4 billion page views\nover 500 million sessions\nover 300,000 financial instruments covered\nglobal financial websites\nInvesting.com offers unlimited access to cutting-edge financial markets tools such as real-time quotes and charts, customized portfolios, personal live alerts, calendars, calculators, and financial insights\u2014completely free of charge. In addition to global stock markets, we also cover world indices, ETFs, commodities, bonds & interest rates, cryptocurrencies, futures, and options.\ncutting-edge financial markets tools\nworld indices, ETFs, commodities, bonds & interest rates, cryptocurrencies, futures, and options\nWhat we offer:\nA chance to join one of the leading financial markets platforms worldwide\nChallenging assignments in a local and international environment\nEnjoy excellent benefits and a high-energy working environment\nAs with all Investing.com positions, we're looking for someone self-motivated, meticulous, and resourceful, with a bright, proactive mindset\u2014able to work both independently and as part of a team.\nA chance to join one of the leading financial markets platforms worldwide\njoin one of the leading financial markets platforms worldwide\nChallenging assignments in a local and international environment\nChallenging assignments\nEnjoy excellent benefits and a high-energy working environment\nexcellent benefits\nAs with all Investing.com positions, we're looking for someone self-motivated, meticulous, and resourceful, with a bright, proactive mindset\u2014able to work both independently and as part of a team.\nself-motivated, meticulous, and resourceful\nbright, proactive mindset\nboth independently and as part of a team\nCheck out our openings on our career website\ncareer website\nOur Privacy Policy: Your resume and information will be kept confidential.\nOur Privacy Policy\nconfidential"
    },
    "4158750551": {
        "title": "Data Engineer",
        "company": "Interstock Development",
        "location": "Valencia, Valencian Community, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nInterstock es una empresa radicada en Donosti dedicada al dise\u00f1o y desarrollo de soluciones de inteligencia artificial para mercados financieros.\n\nSe realizan servicios personalizados aplicando machine learning al desarrollo de estrategias de trading, a la gesti\u00f3n activa de carteras de inversi\u00f3n y a la gesti\u00f3n del riesgo.Buscamos unData Engineeraltamente cualificado y motivado, que se encargue de asegurar la disponibilidad, rendimiento, actualizaci\u00f3n continua, consistencia e integridad de los datos en toda nuestra organizaci\u00f3n.\n\nEntre sus tareas:Actualizaci\u00f3n de la infraestructura de datos existente.Integrar APIs para la recogida y procesamiento interno de datos.Gestionar bases de datos y asegurar su disponibilidad y rendimiento.Supervisar la integridad y calidad de los datos.Garantizar la alta disponibilidad y recuperaci\u00f3n de datos cr\u00edticos.Desarrollar interfaces gr\u00e1ficas para el control y la gesti\u00f3n de datos.Realizar migraciones a la nube.Estudios m\u00ednimosGrado en ingenier\u00eda inform\u00e1tica o similaresExperiencia m\u00ednimaAl menos 3 a\u00f1osRequisitos m\u00ednimosExperiencia en arquitectura y gesti\u00f3n de infraestructuras de datos.Conocimientos en tecnolog\u00edas de almacenamiento y gesti\u00f3n de bases de datos.Experiencia con APIs, integridad de datos y soluciones en la nube.Lenguajes de programaci\u00f3n:Java, PythonRequisitos deseadosInter\u00e9s en el mundo de los mercados financieros.Conocimientos de arquitecturas basadas en eventos (Kafka,...)Capacidad de autoaprendizaje, iniciativa y proactividad.Se ofrece:Horario flexible y modalidad de trabajo h\u00edbrida (remoto/presencial)Contrato indefinidoOportunidad de crecimiento y desarrollo profesional.Interesados enviar CV a:******"
    },
    "4161977119": {
        "title": "Data Engineer",
        "company": "pasiona",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nDescripci\u00f3n del puesto / Funciones\n\nEvaluaci\u00f3n de necesidades para establecer una lista de requisitos funcionales y t\u00e9cnicos\nApoyo en la coordinaci\u00f3n con equipos externos (equipo de integraci\u00f3n de datos y equipo de Datalake)\nImplementaci\u00f3n de la ingesta y transformaci\u00f3n de datos\nRealizaci\u00f3n de c\u00e1lculos espec\u00edficos y filtrado seg\u00fan los requisitos\nImplementaci\u00f3n de medidas de seguridad y control de acceso\n\nRequisitos m\u00ednimos\n\nDise\u00f1o de la soluci\u00f3n t\u00e9cnica que cubra las necesidades (modelo de datos, flujo de integraci\u00f3n, mapeo de datos).\nConstrucci\u00f3n de la capa de integraci\u00f3n basada en Azure Data Lake: DAX, PySpark, Databricks, Azure Analysis Services, Azure DevOps.\nDashboards en Power BI conectados a Cubos de Azure.\nImplementaci\u00f3n de c\u00e1lculos espec\u00edficos.\n\nIdiomas\n\nIngl\u00e9s\n\nUbicaci\u00f3n\n\nRemoto\n\n\u00bfPor qu\u00e9 nosotros?\n\nUn d\u00eda libre en tu cumplea\u00f1os para celebrar a lo grande.\n\nMejoras en los permisos retribuidos para que puedas disfrutar de tu tiempo libre al m\u00e1ximo.\n\nAcceso a mutua m\u00e9dica para cuidar de tu bienestar y tranquilidad.\n\nOpciones de retribuci\u00f3n flexible para adaptarse a tus necesidades financieras.\n\nD\u00edas de vacaciones extras por a\u00f1o de antig\u00fcedad, porque creemos que tu tiempo es valioso y mereces descansar y recargar energ\u00edas.\n\n\u00bfQui\u00e9nes somos?\n\nPasiona Consulting es una consultora tecnol\u00f3gica, partner de Microsoft, dedicada al dise\u00f1o, desarrollo e implementaci\u00f3n de aplicaciones de software a medida para clientes de m\u00faltiples sectores.\nDescripci\u00f3n del puesto / Funciones\nEvaluaci\u00f3n de necesidades para establecer una lista de requisitos funcionales y t\u00e9cnicos\nApoyo en la coordinaci\u00f3n con equipos externos (equipo de integraci\u00f3n de datos y equipo de Datalake)\nImplementaci\u00f3n de la ingesta y transformaci\u00f3n de datos\nRealizaci\u00f3n de c\u00e1lculos espec\u00edficos y filtrado seg\u00fan los requisitos\nImplementaci\u00f3n de medidas de seguridad y control de acceso\nEvaluaci\u00f3n de necesidades para establecer una lista de requisitos funcionales y t\u00e9cnicos\nApoyo en la coordinaci\u00f3n con equipos externos (equipo de integraci\u00f3n de datos y equipo de Datalake)\nImplementaci\u00f3n de la ingesta y transformaci\u00f3n de datos\nRealizaci\u00f3n de c\u00e1lculos espec\u00edficos y filtrado seg\u00fan los requisitos\nImplementaci\u00f3n de medidas de seguridad y control de acceso\nRequisitos m\u00ednimos\nDise\u00f1o de la soluci\u00f3n t\u00e9cnica que cubra las necesidades (modelo de datos, flujo de integraci\u00f3n, mapeo de datos).\nConstrucci\u00f3n de la capa de integraci\u00f3n basada en Azure Data Lake: DAX, PySpark, Databricks, Azure Analysis Services, Azure DevOps.\nDashboards en Power BI conectados a Cubos de Azure.\nImplementaci\u00f3n de c\u00e1lculos espec\u00edficos.\nDise\u00f1o de la soluci\u00f3n t\u00e9cnica que cubra las necesidades (modelo de datos, flujo de integraci\u00f3n, mapeo de datos).\nConstrucci\u00f3n de la capa de integraci\u00f3n basada en Azure Data Lake: DAX, PySpark, Databricks, Azure Analysis Services, Azure DevOps.\nDashboards en Power BI conectados a Cubos de Azure.\nImplementaci\u00f3n de c\u00e1lculos espec\u00edficos.\nIdiomas\nUbicaci\u00f3n\n\u00bfPor qu\u00e9 nosotros?\n\u00bfQui\u00e9nes somos?"
    },
    "3598180449": {
        "title": "Data Engineer",
        "company": "WayOps",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Contract",
        "description": "About the job\nEn WayOps buscamos un perfil Data Engineer que quiera desarrollar su carrera profesional formando parte de un equipo Data & AI de primer nivel y trabajando en proyectos cloud con las \u00faltimas tecnolog\u00edas.\n\nCONTEXTO & RESPONSABILIDADES\n\nLa persona seleccionada se incorporar\u00e1 dentro de nuestro equipo Data & AI core, participando en m\u00faltiples proyectos de Anal\u00edtica Avanzada para clientes nacionales e internacionales. Junto al Technical Lead y otros perfiles como Data Architects, Data Scientists o Data Visualizers, el Data Engineer deber\u00e1 desarrollar c\u00f3digo PySpark que permita la transformaci\u00f3n de los datos siguiendo el paradigma medallion. Adem\u00e1s, ser\u00e1 responsable de la ingesta de datos, orquestaci\u00f3n de procesos y garantizar\u00e1 la calidad del dato.\n\nEXPERIENCIA & CONOCIMIENTOS\n\nEl perfil a incorporar deber\u00e1 contar con 3-4 a\u00f1os de experiencia como Ingeniero de Datos desarrollando pipelines de transformaci\u00f3n de datos con tecnolog\u00eda PySpark, 1-2 a\u00f1os de experiencia en entornos Cloud preferentemente Azure y 1-2 a\u00f1os de experiencia en modelado de datos con herramientas como Lucidchart, orquestaci\u00f3n de datos con herramientas como Data Factory y colaboraci\u00f3n en el desarrollo de cuadros de mandos con herramientas como Power BI.\n\nSe valorar\u00e1 experiencia previa en proyectos orientados a calidad del dato, as\u00ed como experiencia en resoluci\u00f3n y gesti\u00f3n de incidencias en entornos productivos, conocimientos y experiencia con CI/CD en Azure DevOps y conocimientos en Machine Learning,\n\nSer\u00e1 necesario tener experiencia previa con tecnolog\u00edas:\n\nDatabricks (PySpark, Dataframes, Delta Tables, Unity Catalog, Databricks Connect)\nAzure (Data Factory, Databricks, Storage, SQL Databases)\nETL (ER, Relational-model, Star-model, MDM)\nAzure DevOps (Boards)\nAnalytics (PowerBI)\nEcosistema Hadoop (HDFS, Map/Reduce, Spark, Hive)\nHerramientas (Visual Studio Code, Git)\n\n\nAdem\u00e1s se valorar\u00e1 positivamente contar con experiencia o conocimientos en:\n\nAzure (Event Hubs, IoTHub, CosmosDB, Application Insights)\nAzure DevOps (Pipelines, Repos, Test Plans, Artifacts)\nMachine Learning (sklearn, mllib, h2o, tensorflow, keras)\nSpark (GraphX & GraphFrames, Structured Streaming, Scala, Optimizaci\u00f3n)\nProperty Graphs (Neo4j+Cypher, Tinkerpop+Gremlin)\n\n\nCONTRATACI\u00d3N & UBICACI\u00d3N\n\nLa contrataci\u00f3n ser\u00e1 mediante contrato anual prorrogable como aut\u00f3nomo en jornada completa. El trabajo se desarrollar\u00e1 en remoto preferentemente dentro del horario de oficina del cliente para facilitar la coordinaci\u00f3n con el resto del equipo. Banda salarial negociable en funci\u00f3n de la experiencia aportada. Incorporaci\u00f3n inmediata.\nData Engineer\nCONTEXTO & RESPONSABILIDADES\ncore\nmedallion\nEXPERIENCIA & CONOCIMIENTOS\npipelines\nDatabricks (PySpark, Dataframes, Delta Tables, Unity Catalog, Databricks Connect)\nAzure (Data Factory, Databricks, Storage, SQL Databases)\nETL (ER, Relational-model, Star-model, MDM)\nAzure DevOps (Boards)\nAnalytics (PowerBI)\nEcosistema Hadoop (HDFS, Map/Reduce, Spark, Hive)\nHerramientas (Visual Studio Code, Git)\nDatabricks (PySpark, Dataframes, Delta Tables, Unity Catalog, Databricks Connect)\nAzure (Data Factory, Databricks, Storage, SQL Databases)\nETL (ER, Relational-model, Star-model, MDM)\nAzure DevOps (Boards)\nAnalytics (PowerBI)\nEcosistema Hadoop (HDFS, Map/Reduce, Spark, Hive)\nHerramientas (Visual Studio Code, Git)\nAzure (Event Hubs, IoTHub, CosmosDB, Application Insights)\nAzure DevOps (Pipelines, Repos, Test Plans, Artifacts)\nMachine Learning (sklearn, mllib, h2o, tensorflow, keras)\nSpark (GraphX & GraphFrames, Structured Streaming, Scala, Optimizaci\u00f3n)\nProperty Graphs (Neo4j+Cypher, Tinkerpop+Gremlin)\nAzure (Event Hubs, IoTHub, CosmosDB, Application Insights)\nAzure DevOps (Pipelines, Repos, Test Plans, Artifacts)\nMachine Learning (sklearn, mllib, h2o, tensorflow, keras)\nSpark (GraphX & GraphFrames, Structured Streaming, Scala, Optimizaci\u00f3n)\nProperty Graphs (Neo4j+Cypher, Tinkerpop+Gremlin)\nCONTRATACI\u00d3N\n&\nUBICACI\u00d3N"
    },
    "4084889135": {
        "title": "Senior Data Software Engineer (AWS/Databricks/PySpark) ",
        "company": "EPAM Systems",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nAre you an open-minded professional with hands-on experience in Databricks or similar ETL Tools? If it sounds like you, this could be the perfect opportunity to join EPAM as a Senior Data Engineer. We are looking for a team player with excellent communication and organizational skills, mastery of engineering and a B2+/C1 level of English to communicate fluently with English-speaking stakeholders, share ideas and provide reasoning. As part of this project, our Data teams are working on migrating Data Products pipelines from Oracle workloads to Databricks. At EPAM, you will enter a dynamic, agile work environment, serving Fortune 1000 clients while adhering to best engineering practices.\n\nIf you're a dynamic professional with a passion for data transformation and innovation, join us in Madrid, M\u00e1laga, or remotely across Spain to be at the forefront of innovation with EPAM!\n\nResponsibilities\n\n\nDesign, develop, monitor, and operate data pipelines \nIntegrate high-quality datasets for analytical use-cases \nEnable other data teams to follow the client standards \nTesting (preparation and execution) \nMaintenance of existing data pipelines incl. alerting, bug fixing, etc.\n\n\nRequirements\n\n\n3+ years hands-on experience in Databricks or similar ETL Tools\nStrong understanding of Apache Spark, PySpark, and Python\nProven expertise in AWS Glue and Amazon S3 for efficient data storage and processing\nExperience in utilizing GitHub and CI/CD pipelines for version control and continuous integration\nProficiency in MicroStrategy or similar analytical tools for data visualization and reporting\n\n\nNice to have\n\n\nAdvanced Databricks skills like delta, cluster configuration or mlflow \nExperience with integrating sources like Websites, Exasol, REST APIs, SAP, BigQuery, GSheets, Salesforce, Jedox and more \nData Quality \nApache NiFi \nJava\n\n\nWe offer\n\n\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\n\n\nEPAM is a leading digital transformation services and product engineering company with over 52,650 EPAMers in more than 55 countries and regions. Since 1993, our multidisciplinary teams have been helping make the future real for our clients and communities around the world. In 2018, we opened an office in Spain that quickly grew to over 1,450 EPAMers distributed between the offices in M\u00e1laga and Madrid as well as remotely across the country. Here you will collaborate with multinational teams, contribute to numerous innovative projects, and have an opportunity to learn and grow continuously.\n\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nSenior Data Engineer\nResponsibilities\nDesign, develop, monitor, and operate data pipelines \nIntegrate high-quality datasets for analytical use-cases \nEnable other data teams to follow the client standards \nTesting (preparation and execution) \nMaintenance of existing data pipelines incl. alerting, bug fixing, etc.\nDesign, develop, monitor, and operate data pipelines\nIntegrate high-quality datasets for analytical use-cases\nEnable other data teams to follow the client standards\nTesting (preparation and execution)\nMaintenance of existing data pipelines incl. alerting, bug fixing, etc.\nRequirements\n3+ years hands-on experience in Databricks or similar ETL Tools\nStrong understanding of Apache Spark, PySpark, and Python\nProven expertise in AWS Glue and Amazon S3 for efficient data storage and processing\nExperience in utilizing GitHub and CI/CD pipelines for version control and continuous integration\nProficiency in MicroStrategy or similar analytical tools for data visualization and reporting\n3+ years hands-on experience in Databricks or similar ETL Tools\nStrong understanding of Apache Spark, PySpark, and Python\nProven expertise in AWS Glue and Amazon S3 for efficient data storage and processing\nExperience in utilizing GitHub and CI/CD pipelines for version control and continuous integration\nProficiency in MicroStrategy or similar analytical tools for data visualization and reporting\nNice to have\nAdvanced Databricks skills like delta, cluster configuration or mlflow \nExperience with integrating sources like Websites, Exasol, REST APIs, SAP, BigQuery, GSheets, Salesforce, Jedox and more \nData Quality \nApache NiFi \nJava\nAdvanced Databricks skills like delta, cluster configuration or mlflow\nExperience with integrating sources like Websites, Exasol, REST APIs, SAP, BigQuery, GSheets, Salesforce, Jedox and more\nData Quality\nApache NiFi\nJava\nWe offer\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends."
    },
    "4168755711": {
        "title": "Data Scientist (NLP Deep Learning Engineer) ",
        "company": "Revolut",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nAbout Revolut\n\nPeople deserve more from their money. More visibility, more control, and more freedom. Since 2015, Revolut has been on a mission to deliver just that. Our powerhouse of products \u2014 including spending, saving, investing, exchanging, travelling, and more \u2014 help our 50+ million customers get more from their money every day.\n\nAs we continue our lightning-fast growth,\u200c 2 things are essential to our success: our people and our culture. In recognition of our outstanding employee experience, we've been certified as a Great Place to Work\u2122. So far, we have 10,000+ people working around the world, from our offices and remotely, to help us achieve our mission. And we're looking for more brilliant people. People who love building great products, redefining success, and turning the complexity of a chaotic world into the simplicity of a beautiful solution.\n\nAbout The Role\n\nDeep Learning Engineers at Revolut are at the forefront of GenAI and LLM integration, building transformative products that range from user-facing tools to advanced process optimisation and data understanding solutions. You\u2019ll work with the most advanced LLMs available out there, developing solutions that have a tangible impact on millions of Revolut customers worldwide.\n\nAs a Deep Learning Engineer, you\u2019ll be embedded in dynamic, cross-functional teams with Product Owners, Software Engineers, Data Analysts, and Operations Managers. Together, you\u2019ll solve complex problems and deliver automated, scalable solutions that continually improve. This role offers the opportunity to collaborate with top-tier professionals in Product, Design, Engineering, and AI to shape the future of Revolut\u2019s AI-driven capabilities.\n\nOur projects span the creation of personal assistants, intelligent chatbots, copilots, and more, all designed to elevate user experience and revolutionise customer interaction. We experiment, iterate, and build intelligent systems that improve over time through advanced algorithms and deep learning techniques.\n\nWhat You\u2019ll Be Doing\n\nBuilding AI-driven features from scratch like personal assistants, chatbots, copilots and more.\nDeveloping user-focused and backend features using deep learning\nDelivering impactful, scalable, data-driven AI solutions\nCollaborating with Product, Engineering, and Data teams to solve deep learning challenges\nIntegrating cutting-edge AI technologies to drive innovation at Revolut\n\nWhat You'll Need\n\nExperience in Deep Learning within Natural Language Processing area and Large Language Models\nA Bachelor's degree in a STEM major (mathematics, computer science, engineering)\nExcellent knowledge of data science (python, SQL) and production tools\nA deep understanding of probability and statistics fundamentals\nBig picture thinking to correctly diagnose problems and productionising research\nExcellent communication and collaboration skills to partner with Product Owners and business heads\n\nNice to have\n\nA master's or PhD in a quantitative discipline\nStrong experience with additional programming languages, such as Java, Scala, C++\nExperience at a large tech company worth >$15B\nSchool/University Olympic medal competitions in physics, maths, economics or programming\n\nBuilding a global financial super app isn\u2019t enough. Our Revoluters are a priority, and that\u2019s why in 2021 we launched our inaugural D&I Framework, designed to help us thrive and grow everyday. We're not just doing this because it's the right thing to do. We\u2019re doing it because we know that seeking out diverse talent and creating an inclusive workplace is the way to create exceptional, innovative products and services for our customers. That\u2019s why we encourage applications from people with diverse backgrounds and experiences to join this multicultural, hard-working team.\n\nImportant notice for candidates:\n\nJob scams are on the rise. Please keep these guidelines in mind when applying for any open roles.\n\n Only apply through official Revolut channels. We don\u2019t use any third-party services or platforms for our recruitment.\n Always double-check the emails you receive. Make sure all communications are being done through official Revolut emails, with an @revolut.com domain.\n\nWe won't ask for payment or personal financial information during the hiring process. If anyone does ask you for this, it\u2019s a scam. Report it immediately.\n\nBy submitting this application, I confirm that all the information given by me in this application for employment and any additional documents attached hereto are true to the best of my knowledge and that I have not wilfully suppressed any material fact. I confirm I have disclosed if applicable any previous employment with Revolut. I accept that if any of the information given by me in this application is in any way false or incorrect, my application may be rejected, any offer of employment may be withdrawn or my employment with Revolut may be terminated summarily or I may be dismissed. By submitting this application, I agree that my personal data will be processed in accordance with Revolut's <a class=\"postings-link\" href=\"https://www.revolut.com/legal/data-privacy-for-candidates\">Candidate Privacy Notice\nAbout Revolut\nAbout The Role\nWhat You\u2019ll Be Doing\nBuilding AI-driven features from scratch like personal assistants, chatbots, copilots and more.\nDeveloping user-focused and backend features using deep learning\nDelivering impactful, scalable, data-driven AI solutions\nCollaborating with Product, Engineering, and Data teams to solve deep learning challenges\nIntegrating cutting-edge AI technologies to drive innovation at Revolut\nBuilding AI-driven features from scratch like personal assistants, chatbots, copilots and more.\nDeveloping user-focused and backend features using deep learning\nDelivering impactful, scalable, data-driven AI solutions\nCollaborating with Product, Engineering, and Data teams to solve deep learning challenges\nIntegrating cutting-edge AI technologies to drive innovation at Revolut\nWhat You'll Need\nExperience in Deep Learning within Natural Language Processing area and Large Language Models\nA Bachelor's degree in a STEM major (mathematics, computer science, engineering)\nExcellent knowledge of data science (python, SQL) and production tools\nA deep understanding of probability and statistics fundamentals\nBig picture thinking to correctly diagnose problems and productionising research\nExcellent communication and collaboration skills to partner with Product Owners and business heads\nExperience in Deep Learning within Natural Language Processing area and Large Language Models\nA Bachelor's degree in a STEM major (mathematics, computer science, engineering)\nExcellent knowledge of data science (python, SQL) and production tools\nA deep understanding of probability and statistics fundamentals\nBig picture thinking to correctly diagnose problems and productionising research\nExcellent communication and collaboration skills to partner with Product Owners and business heads\nNice to have\nA master's or PhD in a quantitative discipline\nStrong experience with additional programming languages, such as Java, Scala, C++\nExperience at a large tech company worth >$15B\nSchool/University Olympic medal competitions in physics, maths, economics or programming\nA master's or PhD in a quantitative discipline\nStrong experience with additional programming languages, such as Java, Scala, C++\nExperience at a large tech company worth >$15B\nSchool/University Olympic medal competitions in physics, maths, economics or programming\nImportant notice for candidates:\nOnly apply through official Revolut channels. We don\u2019t use any third-party services or platforms for our recruitment.\n Always double-check the emails you receive. Make sure all communications are being done through official Revolut emails, with an @revolut.com domain.\nOnly apply through official Revolut channels. We don\u2019t use any third-party services or platforms for our recruitment.\nAlways double-check the emails you receive. Make sure all communications are being done through official Revolut emails, with an @revolut.com domain.\nWe won't ask for payment or personal financial information during the hiring process."
    },
    "4157314409": {
        "title": "Cloud Data Engineer - Microsoft Azure (m/f/d)",
        "company": "TD SYNNEX Spain",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nWhy Choose TD SYNNEX:\n\nTD SYNNEX is a Fortune 100 company with over $58 billion in revenue (NYSE: SNX), recognized as one of the world\u2019s leading technology distributors and solutions aggregators. With a diverse team of 24,000 employees operating across more than 100 countries, we support over 150,000 customers in maximizing the value of their technology investments, driving business results, and unlocking growth opportunities. TD SYNNEX is a certified Great Place to Work, celebrated for our dynamic culture and comprehensive benefits. Our diverse workforce is our greatest strength, fostering success and inclusivity.\n\nAbout the role:\n\nTD SYNNEX has built a strong data analytics capability, focused on creating predictive insights, automation of the business processes and personalization of the partner interactions.\n\nWe are looking for a skilled Data Engineer to join our team and play a key role in designing, building, and optimizing data infrastructure. You will work with cutting-edge technologies like Azure, Snowflake, and Python to develop scalable ETL pipelines, automate workflows, and deliver impactful data solutions. This position requires a collaborative mindset, technical expertise, and the ability to transform complex data into actionable insights that drive business success.\n\nWhat you will do:\n\nProcess and manage complex datasets using Python, SQL, and Snowflake, ensuring accurate and efficient ETL workflows on Azure infrastructure.\nAutomate and optimize data workflows, developing tools to empower analytics and engineering teams.\nApply advanced data techniques such as machine learning and statistical analysis to drive business insights.\nBuild interactive dashboards and visualizations in Power BI to support decision-making processes.\nCollaborate with stakeholders and IT teams to address data-related challenges and ensure scalable solutions.\n\nWhat We're Looking For\n\nMinimum 3 years of experience in a similar role (Data Engineer, Data Scientist or BI).\nStrong knowledge of Python for data manipulation and scripting.\nHands-on experience with data warehousing, ideally using Snowflake.\nProficiency in Azure services, including data integration and cloud-based workflows.\nExpertise in ETL processes, including API integration, data extraction, and testing.\nFamiliarity with Power BI for data visualization and reporting.\nProfessional English level\n\nNice to Have:\n\nCertification PL-200: Microsoft Power Platform Functional Consultant or hands-on experience with tools like Power Apps and Power Automate.\nFamiliarity or certification in DP-600 (Fabric) and experience with technologies such as Logic Apps, Azure Synapse, and Data Factory.\n\nWhat We Offer you:\n\n\ud83c\udfe5 Comprehensive private health and life insurance to keep you covered.\n\ud83c\udf0d Hybrid work model with the opportunity to work remotely three weeks annually.\n\ud83c\udf9f\ufe0f Tailored salary perks covering transportation, meals, learning and childcare needs.\n\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Special rates on gym memberships through Wellhub.\n\ud83d\udc76 On-site nursery and physiotherapist at the office.\n\ud83d\udc86\u200d\u2640\ufe0f Mental health support, including online therapy with LEAD and wellness programs through Wellhub (iFeel, Calm...)\n\ud83d\udcd6 Learning Opportunities: Access to a comprehensive learning platform to support your professional growth.\n\ud83c\udf0d A Global Atmosphere: Join a multicultural and diverse environment where opportunities for growth and collaboration abound.\n\ud83c\udfc5 Certified Workplace Excellence: Work in a certified Great Place to Work where we take work-life balance seriously.\n\nPlease note that benefits may vary depending on the position and the country.\n\nKey Skills\n\nData ETL, Data Warehousing (DW), Machine Learning, Microsoft Azure, Python (Programming Language)\n\nWhat\u2019s In It For You?\n\nElective Benefits: Our programs are tailored to your country to best accommodate your lifestyle.\nGrow Your Career: Accelerate your path to success (and keep up with the future) with formal programs on leadership and professional development, and many more on-demand courses.\nElevate Your Personal Well-Being: Boost your financial, physical, and mental well-being through seminars, events, and our global Life Empowerment Assistance Program.\nDiversity, Equity & Inclusion: It\u2019s not just a phrase to us; valuing every voice is how we succeed. Join us in celebrating our global diversity through inclusive education, meaningful peer-to-peer conversations, and equitable growth and development opportunities.\nMake the Most of our Global Organization: Network with other new co-workers within your first 30 days through our onboarding program.\nConnect with Your Community: Participate in internal, peer-led inclusive communities and activities, including business resource groups, local volunteering events, and more environmental and social initiatives.\n\nDon\u2019t meet every single requirement? Apply anyway. \n\nAt TD SYNNEX, we\u2019re proud to be recognized as a great place to work and a leader in the promotion and practice of diversity, equity and inclusion. If you\u2019re excited about working for our company and believe you\u2019re a good fit for this role, we encourage you to apply. You may be exactly the person we\u2019re looking for!\nWhy Choose TD SYNNEX:\nFortune 100 company\ntechnology distributors and solutions aggregators\ndiverse\n24,000 employees\n100 countries\nGreat Place to Work\ninclusivity\nData Engineer\nAzure\nSnowflake\nPython\nWhat you will do:\nProcess and manage complex datasets using Python, SQL, and Snowflake, ensuring accurate and efficient ETL workflows on Azure infrastructure.\nAutomate and optimize data workflows, developing tools to empower analytics and engineering teams.\nApply advanced data techniques such as machine learning and statistical analysis to drive business insights.\nBuild interactive dashboards and visualizations in Power BI to support decision-making processes.\nCollaborate with stakeholders and IT teams to address data-related challenges and ensure scalable solutions.\nProcess and manage complex datasets using Python, SQL, and Snowflake, ensuring accurate and efficient ETL workflows on Azure infrastructure.\nAutomate and optimize data workflows, developing tools to empower analytics and engineering teams.\nApply advanced data techniques such as machine learning and statistical analysis to drive business insights.\nBuild interactive dashboards and visualizations in Power BI to support decision-making processes.\nCollaborate with stakeholders and IT teams to address data-related challenges and ensure scalable solutions.\nWhat We're Looking For\nMinimum 3 years of experience in a similar role (Data Engineer, Data Scientist or BI).\nStrong knowledge of Python for data manipulation and scripting.\nHands-on experience with data warehousing, ideally using Snowflake.\nProficiency in Azure services, including data integration and cloud-based workflows.\nExpertise in ETL processes, including API integration, data extraction, and testing.\nFamiliarity with Power BI for data visualization and reporting.\nProfessional English level\nMinimum 3 years of experience in a similar role (Data Engineer, Data Scientist or BI).\nStrong knowledge of Python for data manipulation and scripting.\nHands-on experience with data warehousing, ideally using Snowflake.\nProficiency in Azure services, including data integration and cloud-based workflows.\nExpertise in ETL processes, including API integration, data extraction, and testing.\nFamiliarity with Power BI for data visualization and reporting.\nProfessional English level\nNice to Have:\nCertification PL-200: Microsoft Power Platform Functional Consultant or hands-on experience with tools like Power Apps and Power Automate.\nFamiliarity or certification in DP-600 (Fabric) and experience with technologies such as Logic Apps, Azure Synapse, and Data Factory.\nCertification PL-200: Microsoft Power Platform Functional Consultant or hands-on experience with tools like Power Apps and Power Automate.\nFamiliarity or certification in DP-600 (Fabric) and experience with technologies such as Logic Apps, Azure Synapse, and Data Factory.\nWhat We Offer you:\n\ud83c\udfe5 Comprehensive private health and life insurance to keep you covered.\n\ud83c\udf0d Hybrid work model with the opportunity to work remotely three weeks annually.\n\ud83c\udf9f\ufe0f Tailored salary perks covering transportation, meals, learning and childcare needs.\n\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Special rates on gym memberships through Wellhub.\n\ud83d\udc76 On-site nursery and physiotherapist at the office.\n\ud83d\udc86\u200d\u2640\ufe0f Mental health support, including online therapy with LEAD and wellness programs through Wellhub (iFeel, Calm...)\n\ud83d\udcd6 Learning Opportunities: Access to a comprehensive learning platform to support your professional growth.\n\ud83c\udf0d A Global Atmosphere: Join a multicultural and diverse environment where opportunities for growth and collaboration abound.\n\ud83c\udfc5 Certified Workplace Excellence: Work in a certified Great Place to Work where we take work-life balance seriously.\n\ud83c\udfe5 Comprehensive private health and life insurance to keep you covered.\n\ud83c\udf0d Hybrid work model with the opportunity to work remotely three weeks annually.\n\ud83c\udf9f\ufe0f Tailored salary perks covering transportation, meals, learning and childcare needs.\n\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Special rates on gym memberships through Wellhub.\n\ud83d\udc76 On-site nursery and physiotherapist at the office.\n\ud83d\udc86\u200d\u2640\ufe0f Mental health support, including online therapy with LEAD and wellness programs through Wellhub (iFeel, Calm...)\n\ud83d\udcd6 Learning Opportunities: Access to a comprehensive learning platform to support your professional growth.\n\ud83c\udf0d A Global Atmosphere: Join a multicultural and diverse environment where opportunities for growth and collaboration abound.\n\ud83c\udfc5 Certified Workplace Excellence: Work in a certified Great Place to Work where we take work-life balance seriously.\nPlease note that benefits may vary depending on the position and the country.\nKey Skills\nWhat\u2019s In It For You?\nElective Benefits: Our programs are tailored to your country to best accommodate your lifestyle.\nGrow Your Career: Accelerate your path to success (and keep up with the future) with formal programs on leadership and professional development, and many more on-demand courses.\nElevate Your Personal Well-Being: Boost your financial, physical, and mental well-being through seminars, events, and our global Life Empowerment Assistance Program.\nDiversity, Equity & Inclusion: It\u2019s not just a phrase to us; valuing every voice is how we succeed. Join us in celebrating our global diversity through inclusive education, meaningful peer-to-peer conversations, and equitable growth and development opportunities.\nMake the Most of our Global Organization: Network with other new co-workers within your first 30 days through our onboarding program.\nConnect with Your Community: Participate in internal, peer-led inclusive communities and activities, including business resource groups, local volunteering events, and more environmental and social initiatives.\nElective Benefits: Our programs are tailored to your country to best accommodate your lifestyle.\nGrow Your Career: Accelerate your path to success (and keep up with the future) with formal programs on leadership and professional development, and many more on-demand courses.\nElevate Your Personal Well-Being: Boost your financial, physical, and mental well-being through seminars, events, and our global Life Empowerment Assistance Program.\nDiversity, Equity & Inclusion: It\u2019s not just a phrase to us; valuing every voice is how we succeed. Join us in celebrating our global diversity through inclusive education, meaningful peer-to-peer conversations, and equitable growth and development opportunities.\nMake the Most of our Global Organization: Network with other new co-workers within your first 30 days through our onboarding program.\nConnect with Your Community: Participate in internal, peer-led inclusive communities and activities, including business resource groups, local volunteering events, and more environmental and social initiatives.\nDon\u2019t meet every single requirement? Apply anyway."
    },
    "4167034649": {
        "title": "Data Engineer Internship | Pr\u00e1cticas (Remoto o Presencial)",
        "company": "BEONx",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nBIENVENIDO A BEONx\ud83d\udc4b\n\n\u00bfAlguna vez te has preguntado c\u00f3mo se fijan los precios que aparecen en webs como Booking o Trivago y qu\u00e9 tecnolog\u00eda se esconde detr\u00e1s? Si este es tu caso y eres un apasionado de los datos y las startups, en BEONx tenemos una oportunidad para que puedas dar respuesta a tus inquietudes.\n\nEstamos buscando un nuevo Data Engineer Intern que quiera lanzar su carrera y que nos ayude en nuestro camino para ayudar a los hoteles a desbloquear la Rentabilidad Sostenible.\n\n\u00bfListo para ser parte del viaje? \ud83d\ude80\n\nEs necesaria la posibilidad de un convenio de pr\u00e1cticas y se valora la excelencia acad\u00e9mica\n\nQUI\u00c9N ES BEONx\n\nSomos una startup tecnol\u00f3gica con m\u00e1s de 10 a\u00f1os de vida que queremos transformar la industria hotelera a trav\u00e9s de las tecnolog\u00edas de Big-Data y la Inteligencia Artificial en la nube, facilitando y optimizando las decisiones de rentabilidad.\n\nTrabajamos con las principales cadenas hoteleras nacionales e internacionales que conoces y la confianza que depositan en nosotros nos inspira para seguir trabajando y evolucionando incansablemente en nuestro producto y servicio. Como podr\u00e1s ver, creemos firmemente en lo que hacemos, pero sobre todo creemos en las personas que lo hacen posible: nuestro equipo y nuestros clientes.\n\nEs nuestro equipo de actualmente m\u00e1s de 65 personas el responsable de nuestro \u00e9xito y estamos entusiasmados de seguir sumando talento a esta gran familia.\n\nPara saber m\u00e1s sobre nosotros por favor echa un vistazo a nuestra website, LinkedIn o X.\n\nQU\u00c9 RESPONSABILIDADES TENDR\u00cdAS \ud83d\udcbb\n\nComo Data Engineer Intern, dar\u00e1s soporte, junto al resto del equipo de Data en las siguientes facetas_\n\nDise\u00f1ar, desarrollar y mantener pipelines de datos para la adquisici\u00f3n, almacenamiento, procesamiento y an\u00e1lisis de datos.\nImplementar y mantener soluciones de almacenamiento y procesamiento de datos a gran escala, como bases de datos, almacenes de datos y sistemas de procesamiento distribuido.\nDise\u00f1ar e implementar soluciones de integraci\u00f3n de datos y ETL (Extract, Transform, Load) para garantizar la calidad y consistencia de los datos.\nResponsabilizarse de la puesta en producci\u00f3n de los modelos generados por los equipos de Data Scientists.\nColaborar con los equipos de Data Science y otros equipos t\u00e9cnicos para entender los requisitos de datos y garantizar la entrega de datos confiables y de alta calidad.\nOptimizar el rendimiento y la escalabilidad de los sistemas de datos para garantizar un procesamiento eficiente y r\u00e1pido de los datos.\nMonitorear y solucionar problemas de rendimiento, disponibilidad y confiabilidad de la infraestructura de datos.\n\n\nQU\u00c9 PERFIL BUSCAMOS\ud83e\uddd0\n\nNuestro inter\u00e9s es seguir evolucionando nuestros algoritmos de predicci\u00f3n y recomendaci\u00f3n, incluyendo nuevo talento en el equipo con un perfil Data Engineering Intern con ganas de lanzar su carrera en el mundo de los datos\n\nRequisitos Necesarios:\n\nEstar estudiando un Grado/M\u00e1ster/Phd STEM en alg\u00fan centro con la posibilidad de convenio de pr\u00e1cticas.\nConocimiento del lenguaje de programaci\u00f3n Python y SQL\nExperiencia en el desarrollo de ETLs y ELTs\nConocimiento de Git (Gitlab, github, gitflow)\n\n\nBonos Points:\n\nConocimiento de Cloud Computing (AWS, GCP, Azure) yComputaci\u00f3n distribuida (Spark)\nNociones de Docker y Kubernetes\nExperiencia con Data architectures (Data lake/Lake house/Data factory)\nOrquestaci\u00f3n de procesos (Airflow)\n\n\nAdem\u00e1s, valoramos muy positivamente la excelencia acad\u00e9mica.\n\nQU\u00c9 OFRECEMOS \ud83d\udcb0\u23f3\n\nBeca remunerada\nContrato a tiempo completo o parcial, seg\u00fan necesidad\nFlexibilidad horaria para compaginar con los estudios\nPosibilidad de incorporaci\u00f3n tras el periodo de pr\u00e1cticas. En BEONx, nos gusta apostar por el talento interno, y la visi\u00f3n de estas pr\u00e1cticas es un punto de entrada para encontrar perfiles que se queden con nosotros\nTrabajo 100% Remoto desde Espa\u00f1a o regi\u00f3n UE (tenemos oficinas en Madrid, Barcelona y Salamanca).\nLa oportunidad de ser parte del equipo durante nuestra expansi\u00f3n global y aprender de un equipo de primer nivel para el \u00e9xito del cliente.\nUn plan de carrera convincente que desbloquear\u00e1 infinitas posibilidades e impulsar\u00e1 tu trayectoria profesional.\nUn ambiente familiar y unido, donde la comunicaci\u00f3n abierta fluye libre y horizontalmente.\n\n\n\u00bfTe gustar\u00eda arrasar en el sector de la gesti\u00f3n de ingresos hoteleros e impulsar el mercado con BEONx? Si la respuesta es s\u00ed, te queremos en nuestro equipo.\nBIENVENIDO A BEONx\nData Engineer Intern que quiera lanzar su carrera y\nQUI\u00c9N ES BEONx\nstartup tecnol\u00f3gica con m\u00e1s de 10 a\u00f1os de vida\nm\u00e1s de 65 personas\nQU\u00c9 RESPONSABILIDADES TENDR\u00cdAS\n\ud83d\udcbb\nDise\u00f1ar, desarrollar y mantener pipelines de datos para la adquisici\u00f3n, almacenamiento, procesamiento y an\u00e1lisis de datos.\nImplementar y mantener soluciones de almacenamiento y procesamiento de datos a gran escala, como bases de datos, almacenes de datos y sistemas de procesamiento distribuido.\nDise\u00f1ar e implementar soluciones de integraci\u00f3n de datos y ETL (Extract, Transform, Load) para garantizar la calidad y consistencia de los datos.\nResponsabilizarse de la puesta en producci\u00f3n de los modelos generados por los equipos de Data Scientists.\nColaborar con los equipos de Data Science y otros equipos t\u00e9cnicos para entender los requisitos de datos y garantizar la entrega de datos confiables y de alta calidad.\nOptimizar el rendimiento y la escalabilidad de los sistemas de datos para garantizar un procesamiento eficiente y r\u00e1pido de los datos.\nMonitorear y solucionar problemas de rendimiento, disponibilidad y confiabilidad de la infraestructura de datos.\nDise\u00f1ar, desarrollar y mantener pipelines de datos para la adquisici\u00f3n, almacenamiento, procesamiento y an\u00e1lisis de datos.\nImplementar y mantener soluciones de almacenamiento y procesamiento de datos a gran escala, como bases de datos, almacenes de datos y sistemas de procesamiento distribuido.\nDise\u00f1ar e implementar soluciones de integraci\u00f3n de datos y ETL (Extract, Transform, Load) para garantizar la calidad y consistencia de los datos.\nResponsabilizarse de la puesta en producci\u00f3n de los modelos generados por los equipos de Data Scientists.\nColaborar con los equipos de Data Science y otros equipos t\u00e9cnicos para entender los requisitos de datos y garantizar la entrega de datos confiables y de alta calidad.\nOptimizar el rendimiento y la escalabilidad de los sistemas de datos para garantizar un procesamiento eficiente y r\u00e1pido de los datos.\nMonitorear y solucionar problemas de rendimiento, disponibilidad y confiabilidad de la infraestructura de datos.\nQU\u00c9 PERFIL BUSCAMOS\ud83e\uddd0\nRequisitos Necesarios:\nEstar estudiando un Grado/M\u00e1ster/Phd STEM en alg\u00fan centro con la posibilidad de convenio de pr\u00e1cticas.\nConocimiento del lenguaje de programaci\u00f3n Python y SQL\nExperiencia en el desarrollo de ETLs y ELTs\nConocimiento de Git (Gitlab, github, gitflow)\nEstar estudiando un Grado/M\u00e1ster/Phd STEM en alg\u00fan centro con la posibilidad de convenio de pr\u00e1cticas.\nConocimiento del lenguaje de programaci\u00f3n Python y SQL\nExperiencia en el desarrollo de ETLs y ELTs\nConocimiento de Git (Gitlab, github, gitflow)\nBonos Points:\nConocimiento de Cloud Computing (AWS, GCP, Azure) yComputaci\u00f3n distribuida (Spark)\nNociones de Docker y Kubernetes\nExperiencia con Data architectures (Data lake/Lake house/Data factory)\nOrquestaci\u00f3n de procesos (Airflow)\nConocimiento de Cloud Computing (AWS, GCP, Azure) yComputaci\u00f3n distribuida (Spark)\nNociones de Docker y Kubernetes\nExperiencia con Data architectures (Data lake/Lake house/Data factory)\nOrquestaci\u00f3n de procesos (Airflow)\nQU\u00c9 OFRECEMOS\n\ud83d\udcb0\u23f3\nBeca remunerada\nContrato a tiempo completo o parcial, seg\u00fan necesidad\nFlexibilidad horaria para compaginar con los estudios\nPosibilidad de incorporaci\u00f3n tras el periodo de pr\u00e1cticas. En BEONx, nos gusta apostar por el talento interno, y la visi\u00f3n de estas pr\u00e1cticas es un punto de entrada para encontrar perfiles que se queden con nosotros\nTrabajo 100% Remoto desde Espa\u00f1a o regi\u00f3n UE (tenemos oficinas en Madrid, Barcelona y Salamanca).\nLa oportunidad de ser parte del equipo durante nuestra expansi\u00f3n global y aprender de un equipo de primer nivel para el \u00e9xito del cliente.\nUn plan de carrera convincente que desbloquear\u00e1 infinitas posibilidades e impulsar\u00e1 tu trayectoria profesional.\nUn ambiente familiar y unido, donde la comunicaci\u00f3n abierta fluye libre y horizontalmente.\nBeca remunerada\nContrato a tiempo completo o parcial, seg\u00fan necesidad\nFlexibilidad horaria para compaginar con los estudios\nPosibilidad de incorporaci\u00f3n tras el periodo de pr\u00e1cticas. En BEONx, nos gusta apostar por el talento interno, y la visi\u00f3n de estas pr\u00e1cticas es un punto de entrada para encontrar perfiles que se queden con nosotros\nTrabajo 100% Remoto desde Espa\u00f1a o regi\u00f3n UE (tenemos oficinas en Madrid, Barcelona y Salamanca).\nLa oportunidad de ser parte del equipo durante nuestra expansi\u00f3n global y aprender de un equipo de primer nivel para el \u00e9xito del cliente.\nUn plan de carrera convincente que desbloquear\u00e1 infinitas posibilidades e impulsar\u00e1 tu trayectoria profesional.\nUn ambiente familiar y unido, donde la comunicaci\u00f3n abierta fluye libre y horizontalmente.\n\u00bfTe gustar\u00eda arrasar en el sector de la gesti\u00f3n de ingresos hoteleros e impulsar el mercado con BEONx? Si la respuesta es s\u00ed, te queremos en nuestro equipo."
    },
    "4128830805": {
        "title": "Data Engineer (Azure)",
        "company": "Otto Group one.O",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nWhat will you do?\n\nDesigning interfaces to the data warehouse.\nImplementation of ETL processes.\nOptimization and monitoring of existing loading processes.\nSetup, maintenance and operation of the analytical data warehouse\nIdentification of the information requirements from the user's point of view (departments), as well as creation of analyses and evaluations for the various departments.\nYou will work with a cross-functional together with our colleagues in Spain and Germany - Dresden and Hamburg! \n\nWhat's your story?\n\nDegree in information technology or similar desirable; alternatively, a professional education with IT background. \nKnowledge of Cloud Data Warehouse and Cloud Business Intelligence (Azure preferred)\nExperience in database queries (Oracle SQL); PL/SQL \nETL tool knowledge\nAnalytical, conceptual and strategic thinking skills. \nPleasure in teamwork, high commitment as well as initiative and perseverance. \nGood English skills, written and spoken.\n\nBenefits\n\nPermanent contract \nFlexible working hours (you decide how to organize your day to day!) \nHybrid work mode (work from home up to 80% of the time, having the rest to meet you colleagues at the office!) \nYou will be part of a fast growing company, being part of a great team \nCompetitive salary\nFlexible retribution\nMedical insurance \nAgile working methods\nTraining and Development: we have a clear focus on technical innovation, but we don't forget the personal growth!\n\n\u00bfQui\u00e9nes somos?\n\nMore than just IT\n\nOSP (Otto Group Solution Provider) is an IT service provider for retail and logistics with headquarters in Dresden. We live our passion for IT in an appreciative, trusting work environment. What drives us is the idea of working together as a team to achieve great things for our customers.\n\nWe rely on the personal responsibility and willingness to learn of our employees, on modern technologies and high quality in software development. Agile working, transparent decisions, a lively feedback culture and collegial cooperation make us successful.\n\nWe are active for customers inside and outside the Otto Group. With over 450 employees at several German and international locations, we have been developing flexible software and BI solutions since 1991 and are thus shaping the shopping worlds of tomorrow. We are part of the Otto Group and share a value system that focuses on responsible and sustainable action.\nWhat will you do?\nDesigning interfaces to the data warehouse.\nImplementation of ETL processes.\nOptimization and monitoring of existing loading processes.\nSetup, maintenance and operation of the analytical data warehouse\nIdentification of the information requirements from the user's point of view (departments), as well as creation of analyses and evaluations for the various departments.\nYou will work with a cross-functional together with our colleagues in Spain and Germany - Dresden and Hamburg!\nDesigning interfaces to the data warehouse.\nImplementation of ETL processes.\nOptimization and monitoring of existing loading processes.\nSetup, maintenance and operation of the analytical data warehouse\nIdentification of the information requirements from the user's point of view (departments), as well as creation of analyses and evaluations for the various departments.\nYou will work with a cross-functional together with our colleagues in Spain and Germany - Dresden and Hamburg!\nWhat's your story?\nDegree in information technology or similar desirable; alternatively, a professional education with IT background. \nKnowledge of Cloud Data Warehouse and Cloud Business Intelligence (Azure preferred)\nExperience in database queries (Oracle SQL); PL/SQL \nETL tool knowledge\nAnalytical, conceptual and strategic thinking skills. \nPleasure in teamwork, high commitment as well as initiative and perseverance. \nGood English skills, written and spoken.\nDegree in information technology or similar desirable; alternatively, a professional education with IT background.\nKnowledge of Cloud Data Warehouse and Cloud Business Intelligence (Azure preferred)\nExperience in database queries (Oracle SQL); PL/SQL\nETL tool knowledge\nAnalytical, conceptual and strategic thinking skills.\nPleasure in teamwork, high commitment as well as initiative and perseverance.\nGood English skills, written and spoken.\nBenefits\nPermanent contract \nFlexible working hours (you decide how to organize your day to day!) \nHybrid work mode (work from home up to 80% of the time, having the rest to meet you colleagues at the office!) \nYou will be part of a fast growing company, being part of a great team \nCompetitive salary\nFlexible retribution\nMedical insurance \nAgile working methods\nTraining and Development: we have a clear focus on technical innovation, but we don't forget the personal growth!\nPermanent contract\nFlexible working hours (you decide how to organize your day to day!)\nHybrid work mode (work from home up to 80% of the time, having the rest to meet you colleagues at the office!)\nYou will be part of a fast growing company, being part of a great team\nCompetitive salary\nFlexible retribution\nMedical insurance\nAgile working methods\nTraining and Development: we have a clear focus on technical innovation, but we don't forget the personal growth!\n\u00bfQui\u00e9nes somos?\nMore than just IT"
    },
    "4178498004": {
        "title": "Contractor Data Engineer",
        "company": "Codurance",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nAbout Us\n\nCodurance is a boutique IT consultancy. One of the good ones!\n\nIt's not just about making code that works, we're passionate about crafting quality software which translates to applying Extreme Programming practices and Software Craftsmanship values in continuous delivery.\n\nWe are stack agnostic although we have a slight preference for OOP languages, what are yours?\n\nWhat else?\n\nEasy to summarize: we are passionate about learning and love to share knowledge, internally and externally! Our horizontal structure allows for a truly collaborative environment, with trust and transparency which also enable autonomy at work. You can truly have an impact on how the organization works and are expected to get involved in different areas of the business to participate to a podcast, an event, interviews and you're not limited by your job title.\n\nWe aim to create a more sustainable and adaptable future via well-crafted software and positive cultural change. Not only as a partner for our clients but also as an employer/a great place to work so read on!\n\nThe Role\n\nWe are looking for a contractor Data Engineer who shares the same values of pragmatism, professionalism and transparency that we do.\n\nYou will use various methods to transform raw data into useful information systems. For example, you'll create algorithms and conduct statistical analysis, build and maintain data pipelines and APIs to expose models, architect data solutions with scalability and performance in mind, etc.\n\nTo succeed in this data engineering position, you should have strong analytical, communication and teamwork skills and the ability to make an effective use of cloud services and data science related frameworks and libraries to deliver solutions.\n\nOverall, you'll work on aligning data needs and solutions with business goals and enable the delivery of well-crafted software.\n\nSounds interesting? Read on!\n\nResponsibilities\n\nCreate and maintain optimal data pipeline architectures and data models\nAnalyze and assemble large, complex data sets that meet business requirements\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and NoSQL databases, AWS, Azure or GCP technologies\nBuild analytics tools for both business and data scientists \nProvide and generate actionable insights from data around business performance metrics and report on them\nWork with internal and external stakeholders to assist with data-related technical issues and support data infrastructure needs\nHelp in the implementation of algorithms and prototypes \nEnhance data quality and reliability\nCollaborate with data scientists, architects and the rest of the business and engineering team to successfully deliver projects\n\n\nRequirements\n\nExcellent level of English and Spanish (written and spoken)\nDatabase systems like SQL and NoSQL (MongoDB, Cassandra, CosmosDB, Redis, MSSQL, MySQL...)\nData warehousing solutions (Azure Synapse, AWS Redshift, etc.)\nETL tools (Data Factory, Databricks, Amazon EMR, Hadoop, Spark, etc.)\nMachine learning (Tensorflow, PyTorch, scikit-learn, NumPy, Pandas, etc.)\nData APIs (Knowledge of OData, GraphQL, as well as frameworks like FastAPI, Flask etc.)\nProgramming skills (preferably Python, Java and Scala)\nUnderstanding the basics of distributed systems\nKnowledge of algorithms and data structures\nIdeally a master's degree in data engineering\n\nEveryone should have the right to bring their whole self to work and be celebrated for who they are. Our people are hired purely on their commitment to these values and their ambition to deliver outstanding results for our clients. Codurance is proud to be an Equal Opportunities Employer and is committed to fostering an inclusive workplace.\n\nBenefits\n\nWhat's in it for you?\n\nAs a freelance you'd have the flexibility and autonomy that you were looking for!\nFull-remote option from whatever location you are in (on the Spanish territory)\nOur People - You'll be working alongside Craftspeople who share your interest in learning, whether that's on a client project or contributing to our internal projects\n\n\nWe are interested in all qualified candidates who are based in Spain and eligible to work here. However, we are not able to sponsor visas or provide relocation assistance for this role.\nAbout Us\nWhat else?\nThe Role\nResponsibilities\nCreate and maintain optimal data pipeline architectures and data models\nAnalyze and assemble large, complex data sets that meet business requirements\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and NoSQL databases, AWS, Azure or GCP technologies\nBuild analytics tools for both business and data scientists \nProvide and generate actionable insights from data around business performance metrics and report on them\nWork with internal and external stakeholders to assist with data-related technical issues and support data infrastructure needs\nHelp in the implementation of algorithms and prototypes \nEnhance data quality and reliability\nCollaborate with data scientists, architects and the rest of the business and engineering team to successfully deliver projects\nCreate and maintain optimal data pipeline architectures and data models\nAnalyze and assemble large, complex data sets that meet business requirements\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and NoSQL databases, AWS, Azure or GCP technologies\nBuild analytics tools for both business and data scientists\nProvide and generate actionable insights from data around business performance metrics and report on them\nWork with internal and external stakeholders to assist with data-related technical issues and support data infrastructure needs\nHelp in the implementation of algorithms and prototypes\nEnhance data quality and reliability\nCollaborate with data scientists, architects and the rest of the business and engineering team to successfully deliver projects\nRequirements\nExcellent level of English and Spanish (written and spoken)\nDatabase systems like SQL and NoSQL (MongoDB, Cassandra, CosmosDB, Redis, MSSQL, MySQL...)\nData warehousing solutions (Azure Synapse, AWS Redshift, etc.)\nETL tools (Data Factory, Databricks, Amazon EMR, Hadoop, Spark, etc.)\nMachine learning (Tensorflow, PyTorch, scikit-learn, NumPy, Pandas, etc.)\nData APIs (Knowledge of OData, GraphQL, as well as frameworks like FastAPI, Flask etc.)\nProgramming skills (preferably Python, Java and Scala)\nUnderstanding the basics of distributed systems\nKnowledge of algorithms and data structures\nIdeally a master's degree in data engineering\nExcellent level of English and Spanish (written and spoken)\nDatabase systems like SQL and NoSQL (MongoDB, Cassandra, CosmosDB, Redis, MSSQL, MySQL...)\nData warehousing solutions (Azure Synapse, AWS Redshift, etc.)\nETL tools (Data Factory, Databricks, Amazon EMR, Hadoop, Spark, etc.)\nMachine learning (Tensorflow, PyTorch, scikit-learn, NumPy, Pandas, etc.)\nData APIs (Knowledge of OData, GraphQL, as well as frameworks like FastAPI, Flask etc.)\nProgramming skills (preferably Python, Java and Scala)\nUnderstanding the basics of distributed systems\nKnowledge of algorithms and data structures\nIdeally a master's degree in data engineering\nEveryone should have the right to bring their whole self to work and be celebrated for who they are. Our people are hired purely on their commitment to these values and their ambition to deliver outstanding results for our clients. Codurance is proud to be an Equal Opportunities Employer and is committed to fostering an inclusive workplace.\nBenefits\nWhat's in it for you?\nAs a freelance you'd have the flexibility and autonomy that you were looking for!\nFull-remote option from whatever location you are in (on the Spanish territory)\nOur People - You'll be working alongside Craftspeople who share your interest in learning, whether that's on a client project or contributing to our internal projects\nAs a freelance you'd have the flexibility and autonomy that you were looking for!\nFull-remote option from whatever location you are in (on the Spanish territory)\nOur People - You'll be working alongside Craftspeople who share your interest in learning, whether that's on a client project or contributing to our internal projects\nWe are interested in all qualified candidates who are based in Spain and eligible to work here. However, we are not able to sponsor visas or provide relocation assistance for this role."
    },
    "4108531505": {
        "title": "ADP Data Engineer",
        "company": "Allianz Technology",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nAre you a skilled Data Engineer with a passion for working with cutting-edge technologies in the cloud? Join us at Allianz Technology SE in Munich and play a key role in designing and maintaining our Allianz Data Platform Technical Services (ADPTS) on Microsoft Azure. In this exciting position, you will work with Azure Synapse Analytics, Azure SQL, and Spark technologies to support data warehousing, transformation, and integration processes. Collaborating with cross-functional teams, you\u2019ll ensure the optimal performance and seamless integration of our data solutions. If you are eager to contribute to innovative data engineering at a leading global company, we invite you to apply and become part of our forward-thinking team!\n\nWhat you can find at Allianz Technology:\n\nWork Life - Balance: We offer flexible working hours so you can start your working day according to your needs. In addition, during the summer and every Friday of the year, you have the option of intensive working hours, which will allow you to enjoy more free time for yourself.\nInternational and Multicultural Environment: We value the diversity and richness that comes from working in an international and multicultural environment. With more than 52 different nationalities represented in our company, we offer our employees the opportunity to collaborate with people from all over the world, enriching the work environment and providing the opportunity to learn and grow in a truly global setting. This diversity allows us to have a broader perspective, foster creativity and innovation, and promote an inclusive work environment where every voice is heard and valued.\nProfessional Growth Opportunities: We are committed to the professional development of our employees and provide numerous opportunities for growth within the company. We value internal talent and promote internal advancement, offering training programs, mentoring, and skills development that allow our employees to advance in their careers. \nCompensation and Benefits Package: It includes a company bonus plan, pension, employee stock program, and multiple employee discounts. \n\nWhat will make you succeed in this position?\n\n Proven experience in data engineering, particularly with Microsoft Azure and Azure Synapse Analytics. \n Strong understanding of enterprise data warehousing, ETL/ELT processes, and data transformation. \n Experience with Spark technologies and data integration pipelines. \n Proficiency in SQL and other relevant programming languages. \n Familiarity with Azure services and Microsoft SaaS solutions such as Power BI. \n Excellent problem-solving skills and attention to detail. \n Strong communication and collaboration skills, with the ability to work effectively in an agile environment. \n Languages: German & English (m/s): mandatory \n\nYour mission in the role will be:\n\n Design and implement data solutions using Azure Synapse Analytics, Azure SQL, and Spark technologies. \n Develop and maintain ETL/ELT pipelines for data integration and transformation. \n Ensure deep integration with other Azure and Microsoft SaaS services such as Power BI. \n Collaborate with cross-functional teams to define and implement best practices for data architecture and engineering. \n Optimize performance and scalability of data solutions within the Allianz Azure group tenant. \n Continuously explore and implement new technologies and methodologies to improve data processing and analytics. \n\nAbout Allianz Technology\n\nAllianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 13,000 employees located in 22 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry.\n\nWe oversee the full digitalization spectrum \u2013 from one of the industry\u2019s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n\nD&I statement\n\nAllianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, ethnicity and cultural background, age, nationality, religion, disability, or philosophy of life.\n\nJoin us. Let\u00b4s care for tomorrow.\n\nYou. IT\n\n63222 | Ingenier\u00eda inform\u00e1tica y tecnol\u00f3gica | Profesional / Senior | Non-Executive | Allianz Technology | Jornada completa | Indefinido\nData Engineer\nWhat you can find at Allianz Technology:\nWork Life - Balance: We offer flexible working hours so you can start your working day according to your needs. In addition, during the summer and every Friday of the year, you have the option of intensive working hours, which will allow you to enjoy more free time for yourself.\nInternational and Multicultural Environment: We value the diversity and richness that comes from working in an international and multicultural environment. With more than 52 different nationalities represented in our company, we offer our employees the opportunity to collaborate with people from all over the world, enriching the work environment and providing the opportunity to learn and grow in a truly global setting. This diversity allows us to have a broader perspective, foster creativity and innovation, and promote an inclusive work environment where every voice is heard and valued.\nProfessional Growth Opportunities: We are committed to the professional development of our employees and provide numerous opportunities for growth within the company. We value internal talent and promote internal advancement, offering training programs, mentoring, and skills development that allow our employees to advance in their careers. \nCompensation and Benefits Package: It includes a company bonus plan, pension, employee stock program, and multiple employee discounts.\nWork Life - Balance: We offer flexible working hours so you can start your working day according to your needs. In addition, during the summer and every Friday of the year, you have the option of intensive working hours, which will allow you to enjoy more free time for yourself.\nInternational and Multicultural Environment: We value the diversity and richness that comes from working in an international and multicultural environment. With more than 52 different nationalities represented in our company, we offer our employees the opportunity to collaborate with people from all over the world, enriching the work environment and providing the opportunity to learn and grow in a truly global setting. This diversity allows us to have a broader perspective, foster creativity and innovation, and promote an inclusive work environment where every voice is heard and valued.\nProfessional Growth Opportunities: We are committed to the professional development of our employees and provide numerous opportunities for growth within the company. We value internal talent and promote internal advancement, offering training programs, mentoring, and skills development that allow our employees to advance in their careers.\nCompensation and Benefits Package: It includes a company bonus plan, pension, employee stock program, and multiple employee discounts.\nWhat will make you succeed in this position?\nProven experience in data engineering, particularly with Microsoft Azure and Azure Synapse Analytics. \n Strong understanding of enterprise data warehousing, ETL/ELT processes, and data transformation. \n Experience with Spark technologies and data integration pipelines. \n Proficiency in SQL and other relevant programming languages. \n Familiarity with Azure services and Microsoft SaaS solutions such as Power BI. \n Excellent problem-solving skills and attention to detail. \n Strong communication and collaboration skills, with the ability to work effectively in an agile environment. \n Languages: German & English (m/s): mandatory\nProven experience in data engineering, particularly with Microsoft Azure and Azure Synapse Analytics.\nStrong understanding of enterprise data warehousing, ETL/ELT processes, and data transformation.\nExperience with Spark technologies and data integration pipelines.\nProficiency in SQL and other relevant programming languages.\nFamiliarity with Azure services and Microsoft SaaS solutions such as Power BI.\nExcellent problem-solving skills and attention to detail.\nStrong communication and collaboration skills, with the ability to work effectively in an agile environment.\nLanguages: German & English (m/s): mandatory\nYour mission in the role will be:\nDesign and implement data solutions using Azure Synapse Analytics, Azure SQL, and Spark technologies. \n Develop and maintain ETL/ELT pipelines for data integration and transformation. \n Ensure deep integration with other Azure and Microsoft SaaS services such as Power BI. \n Collaborate with cross-functional teams to define and implement best practices for data architecture and engineering. \n Optimize performance and scalability of data solutions within the Allianz Azure group tenant. \n Continuously explore and implement new technologies and methodologies to improve data processing and analytics.\nDesign and implement data solutions using Azure Synapse Analytics, Azure SQL, and Spark technologies.\nDevelop and maintain ETL/ELT pipelines for data integration and transformation.\nEnsure deep integration with other Azure and Microsoft SaaS services such as Power BI.\nCollaborate with cross-functional teams to define and implement best practices for data architecture and engineering.\nOptimize performance and scalability of data solutions within the Allianz Azure group tenant.\nContinuously explore and implement new technologies and methodologies to improve data processing and analytics.\nAbout Allianz Technology\nD&I statement"
    },
    "4121162571": {
        "title": "Senior Data Software Engineer (Databricks) ",
        "company": "EPAM Systems",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nWe are looking for a Senior Data Software Engineer with a strong background in Databricks to join our team.\n\nThe ideal candidate will be an open-minded professional, well-versed in data engineering, who can effortlessly integrate into our friendly work environment and contribute significantly to our projects.\n\nResponsibilities\n\n\nDesign, develop, and maintain scalable data pipelines and architectures\nOptimize data models and ETL processes using Databricks and other relevant technologies\nImplement data quality checks and monitoring to ensure high data integrity\nStay abreast of emerging trends and technologies in data engineering, proposing new tools where beneficial\nTroubleshoot and resolve data-related issues promptly\nParticipate in code reviews to maintain high standards of code quality\nCollaborate with other team members and stakeholders to enhance data-driven decisions\nMentor junior data engineers and share knowledge within the team\nContribute to the continuous improvement of our data engineering practices and procedures\n\n\nRequirements\n\n\nHands-on experience with Databricks: Delta Lake, workflows, Delta Live Tables, deployment, and versioning\nSolid understanding of data architectures and strong data modeling skills\nProficiency in Spark (either Scala or PySpark)\nExpertise in designing and building ETL pipelines with Databricks using external orchestrators like Airflow\nExperience with cloud-native technologies, containers, and software engineering best practices including unit tests, linting, and code style checks\nEngineering background with either AWS, Azure, or GCP\nFamiliarity with big data and performance optimization of data-intensive applications\nProactivity and experience in client-facing roles\nAbility to work independently in ambiguous situations\nDesire to function effectively in a transparent, fast-moving startup environment\nFluent English communication skills at a B2+ level\n\n\nWe offer\n\n\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\n\n\nEPAM is a leading digital transformation services and product engineering company with over 52,650 EPAMers in more than 55 countries and regions. Since 1993, our multidisciplinary teams have been helping make the future real for our clients and communities around the world. In 2018, we opened an office in Spain that quickly grew to over 1,450 EPAMers distributed between the offices in M\u00e1laga and Madrid as well as remotely across the country. Here you will collaborate with multinational teams, contribute to numerous innovative projects, and have an opportunity to learn and grow continuously.\n\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nSenior Data Software Engineer\nResponsibilities\nDesign, develop, and maintain scalable data pipelines and architectures\nOptimize data models and ETL processes using Databricks and other relevant technologies\nImplement data quality checks and monitoring to ensure high data integrity\nStay abreast of emerging trends and technologies in data engineering, proposing new tools where beneficial\nTroubleshoot and resolve data-related issues promptly\nParticipate in code reviews to maintain high standards of code quality\nCollaborate with other team members and stakeholders to enhance data-driven decisions\nMentor junior data engineers and share knowledge within the team\nContribute to the continuous improvement of our data engineering practices and procedures\nDesign, develop, and maintain scalable data pipelines and architectures\nOptimize data models and ETL processes using Databricks and other relevant technologies\nImplement data quality checks and monitoring to ensure high data integrity\nStay abreast of emerging trends and technologies in data engineering, proposing new tools where beneficial\nTroubleshoot and resolve data-related issues promptly\nParticipate in code reviews to maintain high standards of code quality\nCollaborate with other team members and stakeholders to enhance data-driven decisions\nMentor junior data engineers and share knowledge within the team\nContribute to the continuous improvement of our data engineering practices and procedures\nRequirements\nHands-on experience with Databricks: Delta Lake, workflows, Delta Live Tables, deployment, and versioning\nSolid understanding of data architectures and strong data modeling skills\nProficiency in Spark (either Scala or PySpark)\nExpertise in designing and building ETL pipelines with Databricks using external orchestrators like Airflow\nExperience with cloud-native technologies, containers, and software engineering best practices including unit tests, linting, and code style checks\nEngineering background with either AWS, Azure, or GCP\nFamiliarity with big data and performance optimization of data-intensive applications\nProactivity and experience in client-facing roles\nAbility to work independently in ambiguous situations\nDesire to function effectively in a transparent, fast-moving startup environment\nFluent English communication skills at a B2+ level\nHands-on experience with Databricks: Delta Lake, workflows, Delta Live Tables, deployment, and versioning\nSolid understanding of data architectures and strong data modeling skills\nProficiency in Spark (either Scala or PySpark)\nExpertise in designing and building ETL pipelines with Databricks using external orchestrators like Airflow\nExperience with cloud-native technologies, containers, and software engineering best practices including unit tests, linting, and code style checks\nEngineering background with either AWS, Azure, or GCP\nFamiliarity with big data and performance optimization of data-intensive applications\nProactivity and experience in client-facing roles\nAbility to work independently in ambiguous situations\nDesire to function effectively in a transparent, fast-moving startup environment\nFluent English communication skills at a B2+ level\nWe offer\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends."
    },
    "4165297436": {
        "title": "Senior Data Engineer",
        "company": "companies",
        "location": "Zaragoza, Aragon, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nJob DescriptionAre you a data expert with a passion for building scalable data solutions?\n\nAs a Senior Data Engineer at Stefanini, you will design, develop, and optimize modern data architectures, enabling advanced analytics and business intelligence capabilities.\n\nYou'll work with cutting-edge technologies, including Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory, while collaborating with cross-functional teams to deliver high-performance data solutions that drive business success.Key ResponsibilitiesData Architecture & Engineering: Design, build, and maintain robust data pipelines and architectures using Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory.Data Integration & ETL: Develop scalable ETL/ELT processes to extract, transform, and load data from various sources into cloud-based data platforms.Performance Optimization: Optimize data models, queries, and storage strategies for high-performance analytics and reporting.Big Data Processing: Work with large-scale datasets and distributed computing frameworks to support advanced analytics, machine learning, and business intelligence.Cloud Data Solutions: Design and implement data solutions leveraging cloud services, ensuring security, scalability, and cost-efficiency.Collaboration & Stakeholder Engagement: Partner with business analysts, data scientists, and BI teams to understand data needs and deliver efficient, scalable solutions.Data Governance & Security: Implement best practices for data governance, quality, lineage, and compliance with industry standards.Automation & CI/CD: Utilize DevOps principles to automate data workflows, testing, and deployment using CI/CD pipelines.Documentation & Maintenance: Maintain clear documentation of data pipelines, architecture, and processes for transparency and knowledge sharing.\n\nRequired Skills & ExperienceEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, Information Technology, or a related field.Experience: 5+ years of hands-on experience in data engineering, working with large-scale data processing and analytics.Technical Proficiency:Expertise in Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory for data engineering and analytics workloads.Strong skills in SQL for data modeling, querying, and performance tuning.Experience with Python or Scala for data processing and automation.Proficiency in ETL/ELT pipeline development and data integration across cloud platforms.Strong knowledge of data modeling, warehousing, and optimization techniques.Cloud Expertise: Proven experience with Azure cloud services and modern data architectures.Data Governance & Security: Understanding of best practices in data governance, security, compliance, and lineage tracking.\n\nNice to have:Big Data & Distributed Computing: Familiarity with Spark, Delta Lake, and Parquet for large-scale data processing.DevOps & Automation: Experience in CI/CD for data engineering, using tools like Terraform, Git, and Azure DevOps.Additional Tools: Knowledge of SAP BI, SAP Business Warehouse, and integration with cloud-based data platforms is a plus.Machine Learning & AI: Experience supporting machine learning workflows and feature engineering pipelines.Streaming Data & IoT: Familiarity with Kafka, Event Hubs, or real-time data processing is advantageous.API & Integration: Experience with REST APIs, GraphQL, and data API development is a plus.\n\nWhy Join Us?Work with cutting-edge data technologies to shape modern data solutions.Collaborate with a team of data-driven professionals on impactful projects.Gain exposure to AI, machine learning, and advanced analytics use cases.Enjoy a flexible, innovative, and growth-oriented work environment.Competitive salary, benefits, and continuous learning opportunities.\n\nThe preceding job description had been designed to indicate the general nature and level of work performed by employees within this classification.\n\nIt is not designed to contain or be interpreted as a comprehensive inventory of all duties and responsibilities required of employees assigned to this job.What We OfferYou will find here not only a challenging and interesting workplace, but also a rewarding work experience, with competitive compensation and benefit packages:Career development: opportunity to grow within the team;Meal card;Flexible working hours and work from home, aligned with project needs;Friendly team who is eager to meet you.\n\nWhat's NextIt's best to apply today, because job postings can be taken down and we wouldn't want you to miss this opportunity.\n\nIn case you need further information, just send us a message at ****** and we'll be happy to assist!Diversity & inclusionHere at the Stefanini Group, we value plurality and equity, regardless of race, sexual orientation, disability, age, ancestry, religion, gender, and nationality.\n\nWe understand and encourage the importance of being you!About UsWe are a Brazilian company with over 35 years of experience in delivering IT services worldwide, ranging from IT outsourcing to application development or IT staffing.\n\nWe have a direct presence in 41 countries, through our 70 offices located throughout the world.\n\nWe have managed to become the preferred partner of many small-to-midsize local and regional companies as well.\n\nMost of our clients come from industries such as financial services, manufacturing, telecommunications, chemical, services, technology, public sector and utilities.Stefanini has career opportunities locally and around the world for professionals interested in a vibrant, passionate, team-oriented workplace.\n\nIf you are a customer-centric person with a \"get it done\" attitude, come over for coffee and a talk on your future career with us!Learn more about us on www.stefanini.com and join us on LinkedIn , Facebook and Instagram where we regularly post insights from our colleagues."
    },
    "4165295684": {
        "title": "Senior Data Engineer",
        "company": "companies",
        "location": "Santa Cruz de Tenerife, Canary Islands, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nJob DescriptionAre you a data expert with a passion for building scalable data solutions?\n\nAs a Senior Data Engineer at Stefanini, you will design, develop, and optimize modern data architectures, enabling advanced analytics and business intelligence capabilities.\n\nYou'll work with cutting-edge technologies, including Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory, while collaborating with cross-functional teams to deliver high-performance data solutions that drive business success.Key ResponsibilitiesData Architecture & Engineering: Design, build, and maintain robust data pipelines and architectures using Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory.Data Integration & ETL: Develop scalable ETL/ELT processes to extract, transform, and load data from various sources into cloud-based data platforms.Performance Optimization: Optimize data models, queries, and storage strategies for high-performance analytics and reporting.Big Data Processing: Work with large-scale datasets and distributed computing frameworks to support advanced analytics, machine learning, and business intelligence.Cloud Data Solutions: Design and implement data solutions leveraging cloud services, ensuring security, scalability, and cost-efficiency.Collaboration & Stakeholder Engagement: Partner with business analysts, data scientists, and BI teams to understand data needs and deliver efficient, scalable solutions.Data Governance & Security: Implement best practices for data governance, quality, lineage, and compliance with industry standards.Automation & CI/CD: Utilize DevOps principles to automate data workflows, testing, and deployment using CI/CD pipelines.Documentation & Maintenance: Maintain clear documentation of data pipelines, architecture, and processes for transparency and knowledge sharing.\n\nRequired Skills & ExperienceEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, Information Technology, or a related field.Experience: 5+ years of hands-on experience in data engineering, working with large-scale data processing and analytics.Technical Proficiency:Expertise in Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory for data engineering and analytics workloads.Strong skills in SQL for data modeling, querying, and performance tuning.Experience with Python or Scala for data processing and automation.Proficiency in ETL/ELT pipeline development and data integration across cloud platforms.Strong knowledge of data modeling, warehousing, and optimization techniques.Cloud Expertise: Proven experience with Azure cloud services and modern data architectures.Data Governance & Security: Understanding of best practices in data governance, security, compliance, and lineage tracking.\n\nNice to have:Big Data & Distributed Computing: Familiarity with Spark, Delta Lake, and Parquet for large-scale data processing.DevOps & Automation: Experience in CI/CD for data engineering, using tools like Terraform, Git, and Azure DevOps.Additional Tools: Knowledge of SAP BI, SAP Business Warehouse, and integration with cloud-based data platforms is a plus.Machine Learning & AI: Experience supporting machine learning workflows and feature engineering pipelines.Streaming Data & IoT: Familiarity with Kafka, Event Hubs, or real-time data processing is advantageous.API & Integration: Experience with REST APIs, GraphQL, and data API development is a plus.\n\nWhy Join Us?Work with cutting-edge data technologies to shape modern data solutions.Collaborate with a team of data-driven professionals on impactful projects.Gain exposure to AI, machine learning, and advanced analytics use cases.Enjoy a flexible, innovative, and growth-oriented work environment.Competitive salary, benefits, and continuous learning opportunities.\n\nThe preceding job description had been designed to indicate the general nature and level of work performed by employees within this classification.\n\nIt is not designed to contain or be interpreted as a comprehensive inventory of all duties and responsibilities required of employees assigned to this job.What We OfferYou will find here not only a challenging and interesting workplace, but also a rewarding work experience, with competitive compensation and benefit packages:Career development: opportunity to grow within the team;Meal card;Flexible working hours and work from home, aligned with project needs;Friendly team who is eager to meet you.\n\nWhat's NextIt's best to apply today, because job postings can be taken down and we wouldn't want you to miss this opportunity.\n\nIn case you need further information, just send us a message at ****** and we'll be happy to assist!Diversity & inclusionHere at the Stefanini Group, we value plurality and equity, regardless of race, sexual orientation, disability, age, ancestry, religion, gender, and nationality.\n\nWe understand and encourage the importance of being you!About UsWe are a Brazilian company with over 35 years of experience in delivering IT services worldwide, ranging from IT outsourcing to application development or IT staffing.\n\nWe have a direct presence in 41 countries, through our 70 offices located throughout the world.\n\nWe have managed to become the preferred partner of many small-to-midsize local and regional companies as well.\n\nMost of our clients come from industries such as financial services, manufacturing, telecommunications, chemical, services, technology, public sector and utilities.Stefanini has career opportunities locally and around the world for professionals interested in a vibrant, passionate, team-oriented workplace.\n\nIf you are a customer-centric person with a \"get it done\" attitude, come over for coffee and a talk on your future career with us!Learn more about us on www.stefanini.com and join us on LinkedIn , Facebook and Instagram where we regularly post insights from our colleagues."
    },
    "4165295554": {
        "title": "Senior Data Engineer",
        "company": "companies",
        "location": "Las Palmas de Gran Canaria, Canary Islands, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nJob DescriptionAre you a data expert with a passion for building scalable data solutions?\n\nAs a Senior Data Engineer at Stefanini, you will design, develop, and optimize modern data architectures, enabling advanced analytics and business intelligence capabilities.\n\nYou'll work with cutting-edge technologies, including Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory, while collaborating with cross-functional teams to deliver high-performance data solutions that drive business success.Key ResponsibilitiesData Architecture & Engineering: Design, build, and maintain robust data pipelines and architectures using Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory.Data Integration & ETL: Develop scalable ETL/ELT processes to extract, transform, and load data from various sources into cloud-based data platforms.Performance Optimization: Optimize data models, queries, and storage strategies for high-performance analytics and reporting.Big Data Processing: Work with large-scale datasets and distributed computing frameworks to support advanced analytics, machine learning, and business intelligence.Cloud Data Solutions: Design and implement data solutions leveraging cloud services, ensuring security, scalability, and cost-efficiency.Collaboration & Stakeholder Engagement: Partner with business analysts, data scientists, and BI teams to understand data needs and deliver efficient, scalable solutions.Data Governance & Security: Implement best practices for data governance, quality, lineage, and compliance with industry standards.Automation & CI/CD: Utilize DevOps principles to automate data workflows, testing, and deployment using CI/CD pipelines.Documentation & Maintenance: Maintain clear documentation of data pipelines, architecture, and processes for transparency and knowledge sharing.\n\nRequired Skills & ExperienceEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, Information Technology, or a related field.Experience: 5+ years of hands-on experience in data engineering, working with large-scale data processing and analytics.Technical Proficiency:Expertise in Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory for data engineering and analytics workloads.Strong skills in SQL for data modeling, querying, and performance tuning.Experience with Python or Scala for data processing and automation.Proficiency in ETL/ELT pipeline development and data integration across cloud platforms.Strong knowledge of data modeling, warehousing, and optimization techniques.Cloud Expertise: Proven experience with Azure cloud services and modern data architectures.Data Governance & Security: Understanding of best practices in data governance, security, compliance, and lineage tracking.\n\nNice to have:Big Data & Distributed Computing: Familiarity with Spark, Delta Lake, and Parquet for large-scale data processing.DevOps & Automation: Experience in CI/CD for data engineering, using tools like Terraform, Git, and Azure DevOps.Additional Tools: Knowledge of SAP BI, SAP Business Warehouse, and integration with cloud-based data platforms is a plus.Machine Learning & AI: Experience supporting machine learning workflows and feature engineering pipelines.Streaming Data & IoT: Familiarity with Kafka, Event Hubs, or real-time data processing is advantageous.API & Integration: Experience with REST APIs, GraphQL, and data API development is a plus.\n\nWhy Join Us?Work with cutting-edge data technologies to shape modern data solutions.Collaborate with a team of data-driven professionals on impactful projects.Gain exposure to AI, machine learning, and advanced analytics use cases.Enjoy a flexible, innovative, and growth-oriented work environment.Competitive salary, benefits, and continuous learning opportunities.\n\nThe preceding job description had been designed to indicate the general nature and level of work performed by employees within this classification.\n\nIt is not designed to contain or be interpreted as a comprehensive inventory of all duties and responsibilities required of employees assigned to this job.What We OfferYou will find here not only a challenging and interesting workplace, but also a rewarding work experience, with competitive compensation and benefit packages:Career development: opportunity to grow within the team;Meal card;Flexible working hours and work from home, aligned with project needs;Friendly team who is eager to meet you.\n\nWhat's NextIt's best to apply today, because job postings can be taken down and we wouldn't want you to miss this opportunity.\n\nIn case you need further information, just send us a message at ****** and we'll be happy to assist!Diversity & inclusionHere at the Stefanini Group, we value plurality and equity, regardless of race, sexual orientation, disability, age, ancestry, religion, gender, and nationality.\n\nWe understand and encourage the importance of being you!About UsWe are a Brazilian company with over 35 years of experience in delivering IT services worldwide, ranging from IT outsourcing to application development or IT staffing.\n\nWe have a direct presence in 41 countries, through our 70 offices located throughout the world.\n\nWe have managed to become the preferred partner of many small-to-midsize local and regional companies as well.\n\nMost of our clients come from industries such as financial services, manufacturing, telecommunications, chemical, services, technology, public sector and utilities.Stefanini has career opportunities locally and around the world for professionals interested in a vibrant, passionate, team-oriented workplace.\n\nIf you are a customer-centric person with a \"get it done\" attitude, come over for coffee and a talk on your future career with us!Learn more about us on www.stefanini.com and join us on LinkedIn , Facebook and Instagram where we regularly post insights from our colleagues."
    },
    "4165293714": {
        "title": "Senior Data Engineer",
        "company": "companies",
        "location": "Logro\u00f1o, Autonom\u00eda de La Rioja, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nJob DescriptionAre you a data expert with a passion for building scalable data solutions?\n\nAs a Senior Data Engineer at Stefanini, you will design, develop, and optimize modern data architectures, enabling advanced analytics and business intelligence capabilities.\n\nYou'll work with cutting-edge technologies, including Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory, while collaborating with cross-functional teams to deliver high-performance data solutions that drive business success.Key ResponsibilitiesData Architecture & Engineering: Design, build, and maintain robust data pipelines and architectures using Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory.Data Integration & ETL: Develop scalable ETL/ELT processes to extract, transform, and load data from various sources into cloud-based data platforms.Performance Optimization: Optimize data models, queries, and storage strategies for high-performance analytics and reporting.Big Data Processing: Work with large-scale datasets and distributed computing frameworks to support advanced analytics, machine learning, and business intelligence.Cloud Data Solutions: Design and implement data solutions leveraging cloud services, ensuring security, scalability, and cost-efficiency.Collaboration & Stakeholder Engagement: Partner with business analysts, data scientists, and BI teams to understand data needs and deliver efficient, scalable solutions.Data Governance & Security: Implement best practices for data governance, quality, lineage, and compliance with industry standards.Automation & CI/CD: Utilize DevOps principles to automate data workflows, testing, and deployment using CI/CD pipelines.Documentation & Maintenance: Maintain clear documentation of data pipelines, architecture, and processes for transparency and knowledge sharing.\n\nRequired Skills & ExperienceEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, Information Technology, or a related field.Experience: 5+ years of hands-on experience in data engineering, working with large-scale data processing and analytics.Technical Proficiency:Expertise in Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory for data engineering and analytics workloads.Strong skills in SQL for data modeling, querying, and performance tuning.Experience with Python or Scala for data processing and automation.Proficiency in ETL/ELT pipeline development and data integration across cloud platforms.Strong knowledge of data modeling, warehousing, and optimization techniques.Cloud Expertise: Proven experience with Azure cloud services and modern data architectures.Data Governance & Security: Understanding of best practices in data governance, security, compliance, and lineage tracking.\n\nNice to have:Big Data & Distributed Computing: Familiarity with Spark, Delta Lake, and Parquet for large-scale data processing.DevOps & Automation: Experience in CI/CD for data engineering, using tools like Terraform, Git, and Azure DevOps.Additional Tools: Knowledge of SAP BI, SAP Business Warehouse, and integration with cloud-based data platforms is a plus.Machine Learning & AI: Experience supporting machine learning workflows and feature engineering pipelines.Streaming Data & IoT: Familiarity with Kafka, Event Hubs, or real-time data processing is advantageous.API & Integration: Experience with REST APIs, GraphQL, and data API development is a plus.\n\nWhy Join Us?Work with cutting-edge data technologies to shape modern data solutions.Collaborate with a team of data-driven professionals on impactful projects.Gain exposure to AI, machine learning, and advanced analytics use cases.Enjoy a flexible, innovative, and growth-oriented work environment.Competitive salary, benefits, and continuous learning opportunities.\n\nThe preceding job description had been designed to indicate the general nature and level of work performed by employees within this classification.\n\nIt is not designed to contain or be interpreted as a comprehensive inventory of all duties and responsibilities required of employees assigned to this job.What We OfferYou will find here not only a challenging and interesting workplace, but also a rewarding work experience, with competitive compensation and benefit packages:Career development: opportunity to grow within the team;Meal card;Flexible working hours and work from home, aligned with project needs;Friendly team who is eager to meet you.\n\nWhat's NextIt's best to apply today, because job postings can be taken down and we wouldn't want you to miss this opportunity.\n\nIn case you need further information, just send us a message at ****** and we'll be happy to assist!Diversity & inclusionHere at the Stefanini Group, we value plurality and equity, regardless of race, sexual orientation, disability, age, ancestry, religion, gender, and nationality.\n\nWe understand and encourage the importance of being you!About UsWe are a Brazilian company with over 35 years of experience in delivering IT services worldwide, ranging from IT outsourcing to application development or IT staffing.\n\nWe have a direct presence in 41 countries, through our 70 offices located throughout the world.\n\nWe have managed to become the preferred partner of many small-to-midsize local and regional companies as well.\n\nMost of our clients come from industries such as financial services, manufacturing, telecommunications, chemical, services, technology, public sector and utilities.Stefanini has career opportunities locally and around the world for professionals interested in a vibrant, passionate, team-oriented workplace.\n\nIf you are a customer-centric person with a \"get it done\" attitude, come over for coffee and a talk on your future career with us!Learn more about us on www.stefanini.com and join us on LinkedIn , Facebook and Instagram where we regularly post insights from our colleagues."
    },
    "4167173325": {
        "title": "Data Engineer On Google Cloud Platform Cloud \u00b7 Barcelona \u00b7 Hybrid Remote",
        "company": "Nordic Social by s360",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nAre you excited about transforming cloud technologies into game-changing business value?\n\nDo you want to work across markets with some of the industry's top specialists?\n\nAt s360, we are expanding our footprint with a new office in Barcelona.\n\nNow is your chance to play a key role in a startup-like environment while also benefiting from the support and stability of a 300-person organization.\n\nPlay an instrumental role in constructing and managing our data infrastructure As our Data Engineer, you will develop and execute robust data pipelines, ensuring the availability of high-quality data for analytics and decision-making.\n\nYour Key Responsibilities Will Be To\n\nDesign, build, and maintain scalable and efficient data pipelines on GCP.\n\nLeverage services such as Dataflow, Dataproc, and Cloud Data Fusion to ingest, transform, and load data from diverse sources.Develop and maintain data warehouse solutions using BigQuery, ensuring data quality, consistency, and optimal performance for analytical queries.Implement data governance and security policies, adhering to industry best practices and regulatory requirements.Collaborate with data scientists, analysts, and business stakeholders across Northern Europe, the UK, and Germany to comprehend their data needs and provide tailored solutions.Automate data processing tasks, including data validation, cleansing, and aggregation, to enhance efficiency and accuracy.Monitor and troubleshoot data pipeline performance, proactively identifying and resolving issues to guarantee data availability and reliability.Remain current on the latest advancements in GCP data engineering services, continuously seeking opportunities to optimise our data infrastructure and expand its capabilities.About the team You will join the s360 cloud team, responsible for building cloud data warehouses that empower clients to drive strategic decisions through powerful data insights.\n\nThe team delivers custom projects to clients from proofs of concepts to organization-wide implementations.\n\nYou will join a dynamic and growing team with colleagues from Denmark, Finland, and Spain and report directly to the group CTO.\n\nWho we are looking for If you possess a passion for data and are enthusiastic about leveraging cloud technologies to generate business value, you are a great match.\n\nYou Can Also Recognise The Following About Yourself\n\nComprehensive understanding of GCP data engineering services, including Dataflow, Dataproc, Cloud Data Fusion, and BigQuery.Experience with infrastructure-as-code tools like Terraform for managing GCP resources.Excellent communication skills, enabling clear and concise conveyance of technical concepts to both technical and non-technical audiences.Fluency in English, both written and spoken, is essential.\n\nKnowledge of German would be beneficial considering the target markets.Ability to work independently and as part of a geographically dispersed team, displaying strong teamwork and collaboration skills.It is a plus if you have a GCP professional data engineer certification.\n\nNote: You must be eligible to work in Spain and the EU without requiring sponsorship.\n\nWhat we offer You will be a part of a strong team with some of the industry's top specialists who all strive to ensure that our next delivery is always better than before.\n\nYou will join an international team working with clients across the s360 group.\n\nMoreover, you will be a part of a company characterised by:\n\nA culture that is down-to-earth and full of constructive feedback and sparring that drives your personal and professional development.Freedom under responsibility and flexible working hours.Ambitious and professional colleagues - who also like to have fun!Continuous learning and development opportunities to enhance your skills and advance your career.Social events like our MeetUp for all s360-employees.Competitive salary and benefits package.Opportunity to work on challenging and impactful projects for leading brands across Europe.Our new office, in the popular Norrsken House Barcelona, offers the city's best location in front of the beach and W-hotel, as well as many social and networking activities.\n\nWe'd love to hear from you! Is this the right fit for you?\n\nThen don't hesitate to apply or reach out to Andrej Plancic, Managing Director, at ******.\n\nThe position is based in Barcelona, but we are also open to applicants from Denmark and Finland.\n\nWe review applications on an ongoing basis and hire as soon as the right candidate is found, so please send your CV and application as soon as possible.\n\nAll inquiries are treated confidentially.\n\nAbout s360 s360 is a leading Northern European digital marketing agency that helps brands and retailers grow by providing a strong tech foundation, best-in-class execution, and deep industry expertise.\n\nWith a team of +300 digital specialists, s360 operates ten offices across seven countries: Denmark, Finland, Norway, Sweden, the United Kingdom, the Netherlands, and Spain.\n\nIn recent years, s360 has been recognised with numerous national and European awards, making it one of Europe's most award-winning digital marketing agencies.\n\n#J-18808-Ljbffr\nYour Key Responsibilities Will Be To\nYou Can Also Recognise The Following About Yourself"
    },
    "4174437343": {
        "title": "Big Data Engineer",
        "company": "Vermont Solutions",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nBig Data Engineer \ud83d\ude80\n\n\u00a1Hola, data lovers! \ud83d\udc4b En Vermont Solutions, no solo somos una consultora, somos tu mejor aliado en el mundo de los datos. Actualmente, estamos en la b\u00fasqueda de un Big Data Engineer para un importante cliente del sector financiero. \u00bfTe imaginas trabajando 100% remoto, con ganas de transformar datos en oro? \ud83d\udcb0\u2728 Entonces sigue leyendo.\n\n\u00bfQui\u00e9n es el candidato ideal? \ud83e\udd14\n\nBuscamos a alguien que no solo tenga cinco a\u00f1os de experiencia, sino que tambi\u00e9n tenga una pasi\u00f3n incontrolable por los Big Data. \ud83e\udd13 Si tu desayuno consiste en caf\u00e9 y l\u00edneas de c\u00f3digo (y quiz\u00e1s un poco de pastel de datos), es posible que seas la persona que estamos buscando.\n\nRequisitos: \ud83d\udcdc\n\n Experiencia: M\u00ednimo 5 a\u00f1os trabajando como Data Engineer o Big Data Developer. \n Lenguajes de programaci\u00f3n: Si puedes hablar Java, Python o Scala como un nativo, \u00a1te queremos en nuestro equipo!\n Herramientas: Debes ser un guerrero del ETL y tener experiencia con Hadoop, Spark y Hive. \n SQL complejo: Si puedes lidiar con SQL como si estuvieras en un torneo de ajedrez, genial; si no, bueno, que no se te suban los puntos. \ud83e\udd74\n Plataformas en la nube: Experiencia con AWS, Azure o GCP es un must. Extra bonus si has usado AWS EMR o GCP Dataproc. \ud83d\ude80\n DevOps: Habilidades con CI/CD y GitHub Workflows;\n Calidad, seguridad e integridad de datos: Queremos a alguien que entienda la importancia de los datos. No queremos resolver crisis de datos, \u00a1queremos dormir tranquilos! \ud83d\udca4\n Arquitectura Big Data: Dise\u00f1o de sistemas que hagan que los datos fluyan como si estuvieran en un spa. \ud83d\ude0c\n Gobernanza de datos: Probemos a alguien que sepa que los datos son un recurso valioso, \u00a1no un chisme del jard\u00edn!\n Tendencias de Machine Learning: Si has trabajado con TensorFlow o PyTorch, tienes un pie adentro. \ud83d\udcaa\n Ingl\u00e9s: Un nivel C1 ser\u00eda ideal. \n\nLo que te ofrecemos: \ud83c\udf81\n\nPlan de Formaci\u00f3n y certificaciones t\u00e9cnicas.\nDesarrollo profesional y plan de carrera definido.\nFlexibilidad horaria\nPlan de retribuci\u00f3n flexible acorde a tus necesidades (seguro m\u00e9dico privado, cheques formaci\u00f3n para estudiar idiomas, cheques guarder\u00eda, tarjeta transporte...).\nIntegraci\u00f3n en un equipo de profesionales motivados, en un ambiente de trabajo innovador y din\u00e1mico.\nModelo de gesti\u00f3n sostenible y pol\u00edticas de igualdad efectiva, ambientes de trabajo abiertos e inclusivos.\n\nSi cumples con estos requisitos y te apasiona trabajar en un entorno din\u00e1mico y en crecimiento, \u00a1queremos saber de ti! \ud83c\udf1f\nVermont Solutions\nBig Data Engineer\nExperiencia: M\u00ednimo 5 a\u00f1os trabajando como Data Engineer o Big Data Developer. \n Lenguajes de programaci\u00f3n: Si puedes hablar Java, Python o Scala como un nativo, \u00a1te queremos en nuestro equipo!\n Herramientas: Debes ser un guerrero del ETL y tener experiencia con Hadoop, Spark y Hive. \n SQL complejo: Si puedes lidiar con SQL como si estuvieras en un torneo de ajedrez, genial; si no, bueno, que no se te suban los puntos. \ud83e\udd74\n Plataformas en la nube: Experiencia con AWS, Azure o GCP es un must. Extra bonus si has usado AWS EMR o GCP Dataproc. \ud83d\ude80\n DevOps: Habilidades con CI/CD y GitHub Workflows;\n Calidad, seguridad e integridad de datos: Queremos a alguien que entienda la importancia de los datos. No queremos resolver crisis de datos, \u00a1queremos dormir tranquilos! \ud83d\udca4\n Arquitectura Big Data: Dise\u00f1o de sistemas que hagan que los datos fluyan como si estuvieran en un spa. \ud83d\ude0c\n Gobernanza de datos: Probemos a alguien que sepa que los datos son un recurso valioso, \u00a1no un chisme del jard\u00edn!\n Tendencias de Machine Learning: Si has trabajado con TensorFlow o PyTorch, tienes un pie adentro. \ud83d\udcaa\n Ingl\u00e9s: Un nivel C1 ser\u00eda ideal.\nExperiencia: M\u00ednimo 5 a\u00f1os trabajando como Data Engineer o Big Data Developer.\nLenguajes de programaci\u00f3n: Si puedes hablar Java, Python o Scala como un nativo, \u00a1te queremos en nuestro equipo!\nHerramientas: Debes ser un guerrero del ETL y tener experiencia con Hadoop, Spark y Hive.\nSQL complejo: Si puedes lidiar con SQL como si estuvieras en un torneo de ajedrez, genial; si no, bueno, que no se te suban los puntos. \ud83e\udd74\nPlataformas en la nube: Experiencia con AWS, Azure o GCP es un must. Extra bonus si has usado AWS EMR o GCP Dataproc. \ud83d\ude80\nDevOps: Habilidades con CI/CD y GitHub Workflows;\nCalidad, seguridad e integridad de datos: Queremos a alguien que entienda la importancia de los datos. No queremos resolver crisis de datos, \u00a1queremos dormir tranquilos! \ud83d\udca4\nArquitectura Big Data: Dise\u00f1o de sistemas que hagan que los datos fluyan como si estuvieran en un spa. \ud83d\ude0c\nGobernanza de datos: Probemos a alguien que sepa que los datos son un recurso valioso, \u00a1no un chisme del jard\u00edn!\nTendencias de Machine Learning: Si has trabajado con TensorFlow o PyTorch, tienes un pie adentro. \ud83d\udcaa\nIngl\u00e9s: Un nivel C1 ser\u00eda ideal.\nPlan de Formaci\u00f3n y certificaciones t\u00e9cnicas.\nDesarrollo profesional y plan de carrera definido.\nFlexibilidad horaria\nPlan de retribuci\u00f3n flexible acorde a tus necesidades (seguro m\u00e9dico privado, cheques formaci\u00f3n para estudiar idiomas, cheques guarder\u00eda, tarjeta transporte...).\nIntegraci\u00f3n en un equipo de profesionales motivados, en un ambiente de trabajo innovador y din\u00e1mico.\nModelo de gesti\u00f3n sostenible y pol\u00edticas de igualdad efectiva, ambientes de trabajo abiertos e inclusivos.\nPlan de Formaci\u00f3n y certificaciones t\u00e9cnicas.\nDesarrollo profesional y plan de carrera definido.\nFlexibilidad horaria\nPlan de retribuci\u00f3n flexible acorde a tus necesidades (seguro m\u00e9dico privado, cheques formaci\u00f3n para estudiar idiomas, cheques guarder\u00eda, tarjeta transporte...).\nIntegraci\u00f3n en un equipo de profesionales motivados, en un ambiente de trabajo innovador y din\u00e1mico.\nModelo de gesti\u00f3n sostenible y pol\u00edticas de igualdad efectiva, ambientes de trabajo abiertos e inclusivos."
    },
    "3990429438": {
        "title": "Senior Data Engineer ",
        "company": "The Knot Worldwide",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nJob Description\n\nABOUT THE ROLE AND YOUR TEAM:\n\nThe Knot Worldwide\u2019s Data team is looking for a highly motivated, highly energetic team-player with a strong analytical mindset to join as a Senior Data Engineer supporting end-to-end data pipelines for our business.\n\nThis role will be partnering closely with our stakeholders and also with other Data teams to develop data models and data-driven solutions that enable deep analysis and self-service analytics for our Business Domains (Product, Marketing, Revenue and Core).\n\nWe see a Senior Data Engineer as a hybrid role that combines mostly data engineering skills but also having the ability to pivot through business intelligence or reporting models. You will be responsible for building (leading) end-to-end data pipelines using tools like dbt, airbyte, airflow and other modern data stack. This role requires proficiency in SQL, python, data modeling, data transformation, data visualization, but also collaboration skills and business acumen as they play a crucial role in enabling data-driven decision-making within our organization.\n\nThe position is based at Wedding Planner\u2019s head office in Barcelona. This position offers the flexibility of working remotely. As a Data Engineer, you will have the opportunity to work from your preferred location (Spain), collaborating with a distributed team of professionals. We embrace a remote-first culture that values work-life balance and provides the necessary tools and resources for effective remote collaboration.\n\nRESPONSIBILITIES:\n\nDesign, build, and maintenance of end-to-end scalable data pipelines to collect, process, and analyze large and complex datasets from various sources, including but not only, website visitors behavioral tracking, customer data, transactional data and other business data using tools like Airbyte, Python and dbt to write mainly SQL-based data transformations and ensure the data is clean and ready for analysis.\nApply dimensional modeling techniques to design tables and views that map business processes into an enterprise data model.\nDevelop and support complex ETL infrastructure to ensure the delivery of clean and reliable data to the organization.\nAutomate manual processes to improve efficiency, robustness, and speed.\nParticipate in overall architecture and strategy for the deployment of our end-to-end data pipelines.\nPartner with the rest of Data Platform Engineering Teams to provide business logics to design, develop and move data pipelines into production.\nPartner with Director / Lead to define best practices, templates, scalable code for our tech stack development, mainly in dbt.\nAssist with performance and tuning of our existing models, pipelines and data-applications.\nFoster data governance practices, ensuring data privacy and security, and documenting data pipelines, transformations, and models, contributing to maintaining a data-driven culture within the organization.\nUnderstand our business and processes including how our data pipelines and data apps support the business processes and apply this knowledge to best solve problems.\nWork and communicate with end users / stakeholders across the organization to understand their needs, providing insights, training them on use of the Information Mart models, data-applications or any actionable results in support of decisions.\nClearly scope, track, execute, and communicate on projects in an Agile environment.\nMentor other Data Engineering team members, provide technical leadership and guidance in data engineering projects and initiatives.\nOversee the top of funnel metrics for at least one Business Domain or Pillar and ensure consistency in their usage across models and reporting by maintaining great documentation. \n\nSUCCESSFUL SENIOR DATA ENGINEER CANDIDATES HAVE:\n\nBachelor\u2019s degree (Computer Science, Engineering, Information Systems or relational functional field).\n4+ years experience including design, development, data management, administration and support building data models, data pipelines, data warehousing (Snowflake preferred).\nAdvanced proficiency in modeling using SQL and dbt, strength in Python.\nExperience in data integration & orchestration (Airbyte & Airflow) is a plus.\nSolid understanding of modern data engineering and architecture concepts and practices. This includes knowledge of data warehousing, data pipelines, data marts, and data integration techniques and tools mostly in cloud infrastructure.\nSolid understanding of different business process models and reporting needs and ability to convert requirements into models and end-to-end data pipeline designs.\nMust be self-motivated and able to work both independently and with others.\nVersatile and quick learner with ability to pick up any new skills necessary to get the job done.\nPositive attitude and ability to receive and provide objective and constructive feedback.\nStrong organizational and troubleshooting skills with attention to detail, curiosity to detect and explain data anomalies.\nStrong interpersonal skills with the ability to work effectively in a cross-functional team.\nExcellent communication skills, with the ability to explain complex data insights to non-technical stakeholders.\nExperience in different reporting tools or data applications like Qlik, Tableau, Power BI, Looker, Streamlit \u2026is a plus.\nWork with Agile methodology and JIRA are a plus.\nOur international work environment requires fluency in spoken and written English.\n\nBENEFITS:\n\nPrivate health insurance.\nLife insurance.\nEmployee assistance program: on-demand mental health assistance program for any professional or personal matter.\nOptional salary advances.\nTax relief options: pre-tax savings for food, transport, daycare and work related training expenses.\nGym membership discount.\nUdemy courses.\nRemote work: TKWW believes in remote work with voluntary attendance to our office in the center of Barcelona.\nPersonal time off: 23 working days of holidays per year + additional days each year considering the application of the max. anual working hours of the Collective Bargaining Agreement\nOther leave entitlements: nhancements to statutory leave permissions, for instance: maternity, paternity, childcare, illness and others to support work-life balance.\nWork from home allowance (40\u20ac/month).\nFood allowance (55\u20ac/month).\nEmployee gifting.\nSignature events.\nEmployee resource groups.\nDEI & Sustainability councils.\nRobust communication and feedback channels.\nFocused Fridays.\nRecognition iniciatives.\nLocal events.\nGROW program.\nTKWW University for Employee & Leader development.\nEmployee referral program.\n\nAbout Us\n\nAt The Knot Worldwide, we believe you are more than a resume and invite you to go for it, take the leap of faith, and apply for this job if it sparks your passion to join TKWW and make a difference!\n\nWHAT WE LOVE ABOUT YOU:\n\n You Dream Big. You iterate and experiment to drive innovation. \n You Love Our Users. You keep our global community at the center of everything you do. \n You Do the Right Thing. You strengthen your team through respect, fairness, and inclusion. \n You Hustle Every Day. You favor urgency and own your outcomes. \n You Win Together. People are at the heart of our success and you play as a team. \n\nWHAT YOU LOVE ABOUT US:\n\nWe believe in a wide range of holistic offerings to support our employees so that they can live our values day in and day out. From mental wellbeing, physical health and financial planning, to engaging perks and discounts, we are in the business of celebrating and supporting the Moments that Matter both in and out of the \u201coffice\u201d. We offer flexible vacation, generous parental leave and prioritize initiatives that support the growth, development, and happiness of our people.\n\nTo facilitate in-person collaboration, we have office spaces in Barcelona, Spain; Delhi, India; Galway, Ireland; London, England; New York, NY; and Washington, D.C.\n\n--\n\nUS Notice: The Knot Worldwide provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, or disability. In addition to federal law requirements, The Knot Worldwide complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. The Knot Worldwide expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status.\n\nAbout The Team\n\nWHAT WE DO MATTERS:\n\nHere at The Knot Worldwide, we believe in doing work that matters. In more than 16 countries around the world, The Knot Worldwide\u2019s leading family of brands\u2014including The Knot, WeddingWire, Bodas, The Bash, The Bump, and more\u2014help people take celebration planning from inspiration to action. When you join our global team, you\u2019ll be a part of a diverse group of individuals passionate about serving and enabling our communities to celebrate the moments that make us.\nJob Description\nABOUT THE ROLE AND YOUR TEAM:\nRESPONSIBILITIES\nDesign, build, and maintenance of end-to-end scalable data pipelines to collect, process, and analyze large and complex datasets from various sources, including but not only, website visitors behavioral tracking, customer data, transactional data and other business data using tools like Airbyte, Python and dbt to write mainly SQL-based data transformations and ensure the data is clean and ready for analysis.\nApply dimensional modeling techniques to design tables and views that map business processes into an enterprise data model.\nDevelop and support complex ETL infrastructure to ensure the delivery of clean and reliable data to the organization.\nAutomate manual processes to improve efficiency, robustness, and speed.\nParticipate in overall architecture and strategy for the deployment of our end-to-end data pipelines.\nPartner with the rest of Data Platform Engineering Teams to provide business logics to design, develop and move data pipelines into production.\nPartner with Director / Lead to define best practices, templates, scalable code for our tech stack development, mainly in dbt.\nAssist with performance and tuning of our existing models, pipelines and data-applications.\nFoster data governance practices, ensuring data privacy and security, and documenting data pipelines, transformations, and models, contributing to maintaining a data-driven culture within the organization.\nUnderstand our business and processes including how our data pipelines and data apps support the business processes and apply this knowledge to best solve problems.\nWork and communicate with end users / stakeholders across the organization to understand their needs, providing insights, training them on use of the Information Mart models, data-applications or any actionable results in support of decisions.\nClearly scope, track, execute, and communicate on projects in an Agile environment.\nMentor other Data Engineering team members, provide technical leadership and guidance in data engineering projects and initiatives.\nOversee the top of funnel metrics for at least one Business Domain or Pillar and ensure consistency in their usage across models and reporting by maintaining great documentation.\nDesign, build, and maintenance of end-to-end scalable data pipelines to collect, process, and analyze large and complex datasets from various sources, including but not only, website visitors behavioral tracking, customer data, transactional data and other business data using tools like Airbyte, Python and dbt to write mainly SQL-based data transformations and ensure the data is clean and ready for analysis.\nApply dimensional modeling techniques to design tables and views that map business processes into an enterprise data model.\nDevelop and support complex ETL infrastructure to ensure the delivery of clean and reliable data to the organization.\nAutomate manual processes to improve efficiency, robustness, and speed.\nParticipate in overall architecture and strategy for the deployment of our end-to-end data pipelines.\nPartner with the rest of Data Platform Engineering Teams to provide business logics to design, develop and move data pipelines into production.\nPartner with Director / Lead to define best practices, templates, scalable code for our tech stack development, mainly in dbt.\nAssist with performance and tuning of our existing models, pipelines and data-applications.\nFoster data governance practices, ensuring data privacy and security, and documenting data pipelines, transformations, and models, contributing to maintaining a data-driven culture within the organization.\nUnderstand our business and processes including how our data pipelines and data apps support the business processes and apply this knowledge to best solve problems.\nWork and communicate with end users / stakeholders across the organization to understand their needs, providing insights, training them on use of the Information Mart models, data-applications or any actionable results in support of decisions.\nClearly scope, track, execute, and communicate on projects in an Agile environment.\nMentor other Data Engineering team members, provide technical leadership and guidance in data engineering projects and initiatives.\nOversee the top of funnel metrics for at least one Business Domain or Pillar and ensure consistency in their usage across models and reporting by maintaining great documentation.\nSUCCESSFUL SENIOR DATA ENGINEER CANDIDATES HAVE:\nBachelor\u2019s degree (Computer Science, Engineering, Information Systems or relational functional field).\n4+ years experience including design, development, data management, administration and support building data models, data pipelines, data warehousing (Snowflake preferred).\nAdvanced proficiency in modeling using SQL and dbt, strength in Python.\nExperience in data integration & orchestration (Airbyte & Airflow) is a plus.\nSolid understanding of modern data engineering and architecture concepts and practices. This includes knowledge of data warehousing, data pipelines, data marts, and data integration techniques and tools mostly in cloud infrastructure.\nSolid understanding of different business process models and reporting needs and ability to convert requirements into models and end-to-end data pipeline designs.\nMust be self-motivated and able to work both independently and with others.\nVersatile and quick learner with ability to pick up any new skills necessary to get the job done.\nPositive attitude and ability to receive and provide objective and constructive feedback.\nStrong organizational and troubleshooting skills with attention to detail, curiosity to detect and explain data anomalies.\nStrong interpersonal skills with the ability to work effectively in a cross-functional team.\nExcellent communication skills, with the ability to explain complex data insights to non-technical stakeholders.\nExperience in different reporting tools or data applications like Qlik, Tableau, Power BI, Looker, Streamlit \u2026is a plus.\nWork with Agile methodology and JIRA are a plus.\nOur international work environment requires fluency in spoken and written English.\nBachelor\u2019s degree (Computer Science, Engineering, Information Systems or relational functional field).\n4+ years experience including design, development, data management, administration and support building data models, data pipelines, data warehousing (Snowflake preferred).\nAdvanced proficiency in modeling using SQL and dbt, strength in Python.\nExperience in data integration & orchestration (Airbyte & Airflow) is a plus.\nSolid understanding of modern data engineering and architecture concepts and practices. This includes knowledge of data warehousing, data pipelines, data marts, and data integration techniques and tools mostly in cloud infrastructure.\nSolid understanding of different business process models and reporting needs and ability to convert requirements into models and end-to-end data pipeline designs.\nMust be self-motivated and able to work both independently and with others.\nVersatile and quick learner with ability to pick up any new skills necessary to get the job done.\nPositive attitude and ability to receive and provide objective and constructive feedback.\nStrong organizational and troubleshooting skills with attention to detail, curiosity to detect and explain data anomalies.\nStrong interpersonal skills with the ability to work effectively in a cross-functional team.\nExcellent communication skills, with the ability to explain complex data insights to non-technical stakeholders.\nExperience in different reporting tools or data applications like Qlik, Tableau, Power BI, Looker, Streamlit \u2026is a plus.\nWork with Agile methodology and JIRA are a plus.\nOur international work environment requires fluency in spoken and written English.\nBENEFITS:\nPrivate health insurance.\nLife insurance.\nEmployee assistance program: on-demand mental health assistance program for any professional or personal matter.\nOptional salary advances.\nTax relief options: pre-tax savings for food, transport, daycare and work related training expenses.\nGym membership discount.\nUdemy courses.\nRemote work: TKWW believes in remote work with voluntary attendance to our office in the center of Barcelona.\nPersonal time off: 23 working days of holidays per year + additional days each year considering the application of the max. anual working hours of the Collective Bargaining Agreement\nOther leave entitlements: nhancements to statutory leave permissions, for instance: maternity, paternity, childcare, illness and others to support work-life balance.\nWork from home allowance (40\u20ac/month).\nFood allowance (55\u20ac/month).\nEmployee gifting.\nSignature events.\nEmployee resource groups.\nDEI & Sustainability councils.\nRobust communication and feedback channels.\nFocused Fridays.\nRecognition iniciatives.\nLocal events.\nGROW program.\nTKWW University for Employee & Leader development.\nEmployee referral program.\nPrivate health insurance.\nLife insurance.\nEmployee assistance program: on-demand mental health assistance program for any professional or personal matter.\nOptional salary advances.\nTax relief options: pre-tax savings for food, transport, daycare and work related training expenses.\nGym membership discount.\nUdemy courses.\nRemote work: TKWW believes in remote work with voluntary attendance to our office in the center of Barcelona.\nPersonal time off: 23 working days of holidays per year + additional days each year considering the application of the max. anual working hours of the Collective Bargaining Agreement\nOther leave entitlements: nhancements to statutory leave permissions, for instance: maternity, paternity, childcare, illness and others to support work-life balance.\nWork from home allowance (40\u20ac/month).\nFood allowance (55\u20ac/month).\nEmployee gifting.\nSignature events.\nEmployee resource groups.\nDEI & Sustainability councils.\nRobust communication and feedback channels.\nFocused Fridays.\nRecognition iniciatives.\nLocal events.\nGROW program.\nTKWW University for Employee & Leader development.\nEmployee referral program.\nAbout Us\nYou Dream Big. You iterate and experiment to drive innovation. \n You Love Our Users. You keep our global community at the center of everything you do. \n You Do the Right Thing. You strengthen your team through respect, fairness, and inclusion. \n You Hustle Every Day. You favor urgency and own your outcomes. \n You Win Together. People are at the heart of our success and you play as a team.\nYou Dream Big. You iterate and experiment to drive innovation.\nYou Love Our Users. You keep our global community at the center of everything you do.\nYou Do the Right Thing. You strengthen your team through respect, fairness, and inclusion.\nYou Hustle Every Day. You favor urgency and own your outcomes.\nYou Win Together. People are at the heart of our success and you play as a team.\nAbout The Team"
    },
    "4159665870": {
        "title": "Data Engineer",
        "company": "DATAIS",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Contract",
        "description": "About the job\nDATAIS, a consultancy specializing in the design and execution of data and artificial intelligence projects, adopts an integrated approach to cover the entire data cycle and strategy. As a network of expert professionals, DATAIS fosters trust and collaborates to provide data and IA solutions, and tailored support services, including training and consulting. \n\nBorn as a Spin-off of VISUALITICS focused on data analysis and visualization expertise, our projects have expanded to engage with diverse aspects of the data landscape.\n\nWe want to empower our clients with a competitive advantage by unlocking true value and creating meaningful, actionable and timely business insights with our ability to use state-of-the-art technologies and algorithms. We develop the strategic, technical, and human capabilities that take companies from vision to value and create truly data-driven organizations.\n\nWe are now looking for Data Engineers (3+ years of experience) to join our team based in Madrid. Would you like to work in a young and friendly environment? If you love technology, challenges and willing to contribute to our mission\u2026then you could be the person we are looking for!\n\nTareas\nAs a Data Engineer you will:\n\n\nDesign and implementation of solutions processing large and unstructured datasets (Data Lake Architecture, Streaming Architecture)\n\nDesign, build and maintain big data architecture and data pipeline\n\nDevelop and implement CI/CD pipeline automation solutions.\n\nImplementation, optimization and testing of modern DWH/Big Data solutions based on Azure cloud, GCP or AWS platform and Continuous Delivery / Continuous Integration environment\n\nData processing efficiency improvement, migrations from on-prem to public cloud platforms.\n\nBuild relationships with client stakeholders to establish a high level of rapport and confidence\n\nWork with clients and teams to deliver modern data products\n\nYou are a very creative person, who drives to build your own prototype solutions and provides training and guidance to team members and partners.\n\n\nRequisitos\nQualifications & Compentences:\n\n3+ years of experience in Data Engineering field\n\n\nDegree in Engineering, Computer Science, or other scientific fields\n\nData Enthusiast: Driven by a passion for data and technology.\n\nETL Maestro: Skilled in orchestrating ETL processes with precision and finesse.\n\nSpark Wizard: Adept at conjuring data cleaning and transformation magic in a Spark/Scala environment using Python.\n\nSQL Sage: Master of SQL and the Apache Spark realm.\n\nOLAP Explorer: Navigates OLAP architectures with ease.\n\nCloud Commander: Technically savvy in architecting and managing cloud-based solutions.\n\nDevOps and Kubernetes explorer: Loves working collaboratively and follows best practices in Git and DevOps and best practices Steers the ship of Kubernetes even if you're just starting out. \n\nData Alchemist: Comfortable with schema-on-read databases like Redis.\n\nMeticulous Organizer: Possesses an eye for detail and organizational finesse.\n\nIndependent Trailblazer: Thrives when working proactively and autonomously.\n\nProblem-Solving Maestro: Solves challenges through information gathering, thoughtful evaluation, and ingenious solutions.\n\nGreate communicator, both verbal and written \n\nPressure Player: Thrives under pressure, even within complex organizational landscapes.\n\nEnglish Pro: Proficiency in English is non-negotiable.r data and technology\n\n\nBeneficios\nWhat do we offer:\n\n\nCompetitive salary\n\nSemi-annual bonus on target achievement\n\nFlexible Benefits\n\nRemote or Hybrid work\n\nFlexible working hours\n\nWellness days\n\nFriendly and challenging working environment\n\nLifelong learning mindset with budget for certifications, training and personal development\n\nBe protagonist of disruptive tech events in collaboration with several tech communities, scaleups, Universities and Bootcamps\n\n\nBecome a part of our community on LinkedIn to discover more about our activities and stay updated on all the latest news in machine learning, big data, and AI. Join Datais People on meetup .com/es-ES/datais/ to receive updates on our upcoming events.\nDATAIS, a consultancy specializing in the design and execution of data and artificial intelligence projects, adopts an integrated approach to cover the entire data cycle and strategy. As a network of expert professionals, DATAIS fosters trust and collaborates to provide data and IA solutions, and tailored support services, including training and consulting.\nDATAIS\nBorn as a Spin-off of VISUALITICS focused on data analysis and visualization expertise, our projects have expanded to engage with diverse aspects of the data landscape.\nWe want to empower our clients with a competitive advantage by unlocking true value and creating meaningful, actionable and timely business insights with our ability to use state-of-the-art technologies and algorithms. We develop the strategic, technical, and human capabilities that take companies from vision to value and create truly data-driven organizations.\nWe are now looking for Data Engineers (3+ years of experience) to join our team based in Madrid. Would you like to work in a young and friendly environment? If you love technology, challenges and willing to contribute to our mission\u2026then you could be the person we are looking for!\nData Engineers\nAs a Data Engineer you will:\nDesign and implementation of solutions processing large and unstructured datasets (Data Lake Architecture, Streaming Architecture)\n\nDesign, build and maintain big data architecture and data pipeline\n\nDevelop and implement CI/CD pipeline automation solutions.\n\nImplementation, optimization and testing of modern DWH/Big Data solutions based on Azure cloud, GCP or AWS platform and Continuous Delivery / Continuous Integration environment\n\nData processing efficiency improvement, migrations from on-prem to public cloud platforms.\n\nBuild relationships with client stakeholders to establish a high level of rapport and confidence\n\nWork with clients and teams to deliver modern data products\n\nYou are a very creative person, who drives to build your own prototype solutions and provides training and guidance to team members and partners.\nDesign and implementation of solutions processing large and unstructured datasets (Data Lake Architecture, Streaming Architecture)\nDesign, build and maintain big data architecture and data pipeline\nDevelop and implement CI/CD pipeline automation solutions.\nImplementation, optimization and testing of modern DWH/Big Data solutions based on Azure cloud, GCP or AWS platform and Continuous Delivery / Continuous Integration environment\nData processing efficiency improvement, migrations from on-prem to public cloud platforms.\nBuild relationships with client stakeholders to establish a high level of rapport and confidence\nWork with clients and teams to deliver modern data products\nYou are a very creative person, who drives to build your own prototype solutions and provides training and guidance to team members and partners.\nQualifications & Compentences:\n3+ years of experience in Data Engineering field\nDegree in Engineering, Computer Science, or other scientific fields\n\nData Enthusiast: Driven by a passion for data and technology.\n\nETL Maestro: Skilled in orchestrating ETL processes with precision and finesse.\n\nSpark Wizard: Adept at conjuring data cleaning and transformation magic in a Spark/Scala environment using Python.\n\nSQL Sage: Master of SQL and the Apache Spark realm.\n\nOLAP Explorer: Navigates OLAP architectures with ease.\n\nCloud Commander: Technically savvy in architecting and managing cloud-based solutions.\n\nDevOps and Kubernetes explorer: Loves working collaboratively and follows best practices in Git and DevOps and best practices Steers the ship of Kubernetes even if you're just starting out. \n\nData Alchemist: Comfortable with schema-on-read databases like Redis.\n\nMeticulous Organizer: Possesses an eye for detail and organizational finesse.\n\nIndependent Trailblazer: Thrives when working proactively and autonomously.\n\nProblem-Solving Maestro: Solves challenges through information gathering, thoughtful evaluation, and ingenious solutions.\n\nGreate communicator, both verbal and written \n\nPressure Player: Thrives under pressure, even within complex organizational landscapes.\n\nEnglish Pro: Proficiency in English is non-negotiable.r data and technology\nDegree in Engineering, Computer Science, or other scientific fields\nData Enthusiast: Driven by a passion for data and technology.\nETL Maestro: Skilled in orchestrating ETL processes with precision and finesse.\norchestrating ETL\nSpark Wizard: Adept at conjuring data cleaning and transformation magic in a Spark/Scala environment using Python.\nSpark/Scala\nPython\nSQL Sage: Master of SQL and the Apache Spark realm.\nSQL\nOLAP Explorer: Navigates OLAP architectures with ease.\nCloud Commander: Technically savvy in architecting and managing cloud-based solutions.\nDevOps and Kubernetes explorer: Loves working collaboratively and follows best practices in Git and DevOps and best practices Steers the ship of Kubernetes even if you're just starting out.\nKubernetes\nGit\nDevOps\nData Alchemist: Comfortable with schema-on-read databases like Redis.\nRedis\nMeticulous Organizer: Possesses an eye for detail and organizational finesse.\nIndependent Trailblazer: Thrives when working proactively and autonomously.\nProblem-Solving Maestro: Solves challenges through information gathering, thoughtful evaluation, and ingenious solutions.\nGreate communicator, both verbal and written\nPressure Player: Thrives under pressure, even within complex organizational landscapes.\nEnglish Pro: Proficiency in English is non-negotiable.r data and technology\nWhat do we offer:\nCompetitive salary\n\nSemi-annual bonus on target achievement\n\nFlexible Benefits\n\nRemote or Hybrid work\n\nFlexible working hours\n\nWellness days\n\nFriendly and challenging working environment\n\nLifelong learning mindset with budget for certifications, training and personal development\n\nBe protagonist of disruptive tech events in collaboration with several tech communities, scaleups, Universities and Bootcamps\nCompetitive salary\nSemi-annual bonus on target achievement\nFlexible Benefits\nRemote or Hybrid work\nFlexible working hours\nWellness days\nFriendly and challenging working environment\nLifelong learning mindset with budget for certifications, training and personal development\nBe protagonist of disruptive tech events in collaboration with several tech communities, scaleups, Universities and Bootcamps\nBecome a part of our community on LinkedIn to discover more about our activities and stay updated on all the latest news in machine learning, big data, and AI. Join Datais People on meetup .com/es-ES/datais/ to receive updates on our upcoming events.\nDesired Skills and Experience\nPython\n\nSql\n\nScala\n\nPyspark\n\nSnowflake\n\nAmazon Web Services\n\nMicrosoft Azure\n\nGoogle Cloud\n\nAzure Databricks\n\nApache Spark\n\nkubernetes\n\ndevops\n\ngit\n\ndocker\nSql\nScala\nPyspark\nSnowflake\nAmazon Web Services\nMicrosoft Azure\nGoogle Cloud\nAzure Databricks\nApache Spark\nkubernetes\ndevops\ngit\ndocker"
    },
    "4146352103": {
        "title": "Ingenieros/as de datos ",
        "company": "\u00c1lamoConsulting",
        "location": "Tres Cantos, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nDescripci\u00f3n de la empresa\n\n\u00bfQui\u00e9nes somos?\nFirma espa\u00f1ola de consultor\u00eda tecnol\u00f3gica con m\u00e1s de 300 profesionales con perfil multidisciplinar, de alt\u00edsimo rendimiento, titulados superiores y presencia en toda Espa\u00f1a (8 oficinas). Las claves del \u00e9xito de nuestro modelo de Compa\u00f1\u00eda son: profesionalidad y rigor, orientaci\u00f3n al cliente, uso de todo tipo de tecnolog\u00edas, innovaci\u00f3n para construir soluciones diferenciales en el mercado, experiencia (grupo con m\u00e1s de 20 a\u00f1os trabajando juntos, caso \u00fanico en el mercado), sentido pr\u00e1ctico y m\u00e1ximo compromiso con nuestros proyectos, clientes y equipos.\n\n\u00bfQu\u00e9 te ofrecemos?\nProyectos funcional y tecnol\u00f3gicamente atractivos (intensivos en el uso del dato extremo a extremo), carrera profesional y seguimiento personalizado de tu evoluci\u00f3n (nunca ser\u00e1s un n\u00famero), buen ambiente de trabajo y entusiasmo por crecer conjuntamente (el profesional y la Firma) en una Compa\u00f1\u00eda entre las 100 Compa\u00f1\u00edas TIC m\u00e1s grandes de Espa\u00f1a y en proceso continuo de expansi\u00f3n (diez a\u00f1os seguidos creciendo a doble d\u00edgito).\n\n\u00bfQu\u00e9 te proporcionamos desde el primer d\u00eda?\n\u2022 Heterogeneidad de proyectos y tecnolog\u00edas.\n\u2022 Integraci\u00f3n en un equipo de profesionales con amplia experiencia en el mercado.\n\u2022 Oportunidad \u00fanica de participar en una Firma en continuo crecimiento.\n\u2022 Aprendizaje acelerado y desarrollo continuo de competencias.\n\u2022 Buen ambiente de trabajo y un entorno orientado a la innovaci\u00f3n continua.\n\u2022 Buenas condiciones laborales: flexibilidad en la incorporaci\u00f3n (inmediata o a convenir), contrato indefinido, retribuci\u00f3n en funci\u00f3n de experiencia, subidas salariales semestrales\u2026\n\nDescripci\u00f3n del puesto\n\n\u00bfQu\u00e9 buscamos?\n\nReci\u00e9n titulados o con una experiencia de hasta tres a\u00f1os en Ingenier\u00eda Inform\u00e1tica, Ingenier\u00eda de Telecomunicaciones, Matem\u00e1ticas, Estad\u00edstica, Ciencias F\u00edsicas, Computaci\u00f3n, etc.\nSe valorar\u00e1n conocimientos en bases de datos.\nActitud positiva y mentalidad innovadora: ilusi\u00f3n por aprender y desarrollar una carrera profesional en consultor\u00eda; ganas de \"disfrutar\" trabajando en un entorno distinto; capacidad de compromiso con los compa\u00f1eros, con los proyectos y con los clientes.\n\n\u00c1lamoConsulting vela por la inclusi\u00f3n e igualdad de oportunidades: contamos con un Plan de Igualdad y un C\u00f3digo \u00c9tico que recoge estos principios para garantizar la no discriminaci\u00f3n de nuestros profesionales por cualquier condici\u00f3n personal, f\u00edsica o social.\nDescripci\u00f3n de la empresa\n\u00bfQui\u00e9nes somos?\nFirma espa\u00f1ola de consultor\u00eda tecnol\u00f3gica con m\u00e1s de 300 profesionales con perfil multidisciplinar, de alt\u00edsimo rendimiento, titulados superiores y presencia en toda Espa\u00f1a (8 oficinas). Las claves del \u00e9xito de nuestro modelo de Compa\u00f1\u00eda son: profesionalidad y rigor, orientaci\u00f3n al cliente, uso de todo tipo de tecnolog\u00edas, innovaci\u00f3n para construir soluciones diferenciales en el mercado, experiencia (grupo con m\u00e1s de 20 a\u00f1os trabajando juntos, caso \u00fanico en el mercado), sentido pr\u00e1ctico y m\u00e1ximo compromiso con nuestros proyectos, clientes y equipos.\n\u00bfQu\u00e9 te ofrecemos?\nProyectos funcional y tecnol\u00f3gicamente atractivos (intensivos en el uso del dato extremo a extremo), carrera profesional y seguimiento personalizado de tu evoluci\u00f3n (nunca ser\u00e1s un n\u00famero), buen ambiente de trabajo y entusiasmo por crecer conjuntamente (el profesional y la Firma) en una Compa\u00f1\u00eda entre las 100 Compa\u00f1\u00edas TIC m\u00e1s grandes de Espa\u00f1a y en proceso continuo de expansi\u00f3n (diez a\u00f1os seguidos creciendo a doble d\u00edgito).\n\u00bfQu\u00e9 te proporcionamos desde el primer d\u00eda?\n\u2022 Heterogeneidad de proyectos y tecnolog\u00edas.\n\u2022 Integraci\u00f3n en un equipo de profesionales con amplia experiencia en el mercado.\n\u2022 Oportunidad \u00fanica de participar en una Firma en continuo crecimiento.\n\u2022 Aprendizaje acelerado y desarrollo continuo de competencias.\n\u2022 Buen ambiente de trabajo y un entorno orientado a la innovaci\u00f3n continua.\n\u2022 Buenas condiciones laborales: flexibilidad en la incorporaci\u00f3n (inmediata o a convenir), contrato indefinido, retribuci\u00f3n en funci\u00f3n de experiencia, subidas salariales semestrales\u2026\nDescripci\u00f3n del puesto\n\u00bfQu\u00e9 buscamos?\nReci\u00e9n titulados o con una experiencia de hasta tres a\u00f1os en Ingenier\u00eda Inform\u00e1tica, Ingenier\u00eda de Telecomunicaciones, Matem\u00e1ticas, Estad\u00edstica, Ciencias F\u00edsicas, Computaci\u00f3n, etc.\nSe valorar\u00e1n conocimientos en bases de datos.\nActitud positiva y mentalidad innovadora: ilusi\u00f3n por aprender y desarrollar una carrera profesional en consultor\u00eda; ganas de \"disfrutar\" trabajando en un entorno distinto; capacidad de compromiso con los compa\u00f1eros, con los proyectos y con los clientes.\nReci\u00e9n titulados o con una experiencia de hasta tres a\u00f1os en Ingenier\u00eda Inform\u00e1tica, Ingenier\u00eda de Telecomunicaciones, Matem\u00e1ticas, Estad\u00edstica, Ciencias F\u00edsicas, Computaci\u00f3n, etc.\nSe valorar\u00e1n conocimientos en bases de datos.\nActitud positiva y mentalidad innovadora: ilusi\u00f3n por aprender y desarrollar una carrera profesional en consultor\u00eda; ganas de \"disfrutar\" trabajando en un entorno distinto; capacidad de compromiso con los compa\u00f1eros, con los proyectos y con los clientes.\n\u00c1lamoConsulting vela por la inclusi\u00f3n e igualdad de oportunidades: contamos con un Plan de Igualdad y un C\u00f3digo \u00c9tico que recoge estos principios para garantizar la no discriminaci\u00f3n de nuestros profesionales por cualquier condici\u00f3n personal, f\u00edsica o social."
    },
    "4175056894": {
        "title": "Data Engineer ",
        "company": "K2 Partnering Solutions",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nBuscamos un Ingeniero de Datos en AWS para una posici\u00f3n 100% Remota en Espa\u00f1a.\nEsta es una gran oportunidad para unirte a un equipo innovador y trabajar en proyectos desafiantes. Nuestro cliente es una empresa de referencia en su sector, con un equipo internacional y en crecimiento, que busca talento con experiencia en AWS para impulsar sus soluciones de datos\nRequisitos:\nExperiencia m\u00ednima: 4 a\u00f1os como Ingeniero de Datos en AWS.\nNivel de Ingl\u00e9s intermedio.\nFunciones:\nDise\u00f1o e implementaci\u00f3n de soluciones de ingenier\u00eda de datos en AWS.\nCreaci\u00f3n de pipelines, lagos y almacenes de datos escalables.\nOptimizaci\u00f3n del almacenamiento y procesamiento de datos para m\u00e1ximo rendimiento.\nColaboraci\u00f3n con equipos de Data Science, Analytics y DevOps.\nLo que ofrecemos:\n100% remoto en Espa\u00f1a.\nContrato indefinido.\nRetribuci\u00f3n flexible y beneficios adicionales.\nSi est\u00e1s buscando un nuevo reto y quieres trabajar en una empresa en crecimiento con tecnolog\u00edas punteras, aplica ahora o env\u00edame un mensaje privado.\n#DataEngineer #AWS #Python #Spark #Remoto #OfertaDeTrabajo\nBuscamos un Ingeniero de Datos en AWS para una posici\u00f3n 100% Remota en Espa\u00f1a.\nEsta es una gran oportunidad para unirte a un equipo innovador y trabajar en proyectos desafiantes. Nuestro cliente es una empresa de referencia en su sector, con un equipo internacional y en crecimiento, que busca talento con experiencia en AWS para impulsar sus soluciones de datos\nNuestro cliente es una empresa de referencia en su sector\nRequisitos:\nExperiencia m\u00ednima: 4 a\u00f1os como Ingeniero de Datos en AWS.\nNivel de Ingl\u00e9s intermedio.\nFunciones:\nDise\u00f1o e implementaci\u00f3n de soluciones de ingenier\u00eda de datos en AWS.\nCreaci\u00f3n de pipelines, lagos y almacenes de datos escalables.\nOptimizaci\u00f3n del almacenamiento y procesamiento de datos para m\u00e1ximo rendimiento.\nColaboraci\u00f3n con equipos de Data Science, Analytics y DevOps.\nLo que ofrecemos:\n100% remoto en Espa\u00f1a.\nContrato indefinido.\nRetribuci\u00f3n flexible y beneficios adicionales.\nSi est\u00e1s buscando un nuevo reto y quieres trabajar en una empresa en crecimiento con tecnolog\u00edas punteras, aplica ahora o env\u00edame un mensaje privado.\n#DataEngineer #AWS #Python #Spark #Remoto #OfertaDeTrabajo"
    },
    "4178483316": {
        "title": "Data Engineer ",
        "company": "OCU",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nWe are looking for a data engineer for our Business Analytics team to form the bridge between our raw business data, with their specific technical and business representations, and the creative data-needs of our Data Scientists and Visualization experts.\n\nThe engineer will be responsible for managing, optimizing and monitoring data retrieval, storage and distribution throughout the team, thereby supporting our business objectives and fostering data-driven decision-making across the organization.\n\nOur data engineer will collaborate with Data Scientists, Visualisation Experts, Business Analysts and IT colleagues in several domains, in order to improve:\n\nOur reaction time towards answering business requests.\nThe quality and depth of reports and models.\nData availability and quality.\nGeneral data engineering knowledge within the team.\n\nMain responsabilities:\n\nBuild and maintain data pipelines to support continuing increases in data volume and complexity.\nBuild dimensional models to help migrate reports and models from an on-premise Data Warehouse to a new Enterprise Data Hub.\nImplement and monitor data quality checks and work directly with end users to understand data quality expectations.\nImprove, optimize and monitor data processing efficiency.\nCombine raw information from different sources to create new and valuable datasets.\nTroubleshoot data-related issues and assist in their resolution.\nMonitor cloud costs to ensure monthly budget is respected.\nTrain and support the team in more technical questions\nStay on top of new technologies and best practices and implement these in the team.\nEnsure compliance with GDPR and privacy restrictions when working with sensitive data.\n\nRequired skills:\n\nThe role requires a set of technical and interpersonal skills, including:\nBachelor or Master in IT.\n3+ years of experience in a data engineering role.\nGood knowledge of SQL and database design.\nGood knowledge of Python.\nExperience with Azure (or similar cloud environment)\nExperience with Databricks (Apache Spark, ...)\nHigh level of English (minimum C1).\nStrong communication/interpersonal skills to clarify and validate requirements with stakeholders.\nGood team player.\n\nExperience in the following is considered an asset:\nSqlServer\nGoogle BigQuery\nPySpark\nAzure Data Factory\nAzure devops\nGood understanding of data modelling methods (data warehouse, data vault, dimensional modelling).\nBasic understanding of Data Science principles and theory\nBasic knowledge of Power BI (or other BI tools)\n\nWhat do we offer?\n\nThis is a great opportunity to join an organization with a social mission, in an international environment, and to work with specialized professionals involved in projects that are carried out with the utmost rigor.\n\nWe offer you a permanent full-time contract (35 hours) with flexible schedule, hybrid work model (on-site working hours in Madrid), competitive salary, according with the responsibilities of the position, and social benefits.\n\n\nAre you ready to face the challenge? Don't think twice and send us your CV in English!\n\n\nWe are firmly committed to equal opportunities, respect for diversity and the reconciliation of work, family and personal life. We work every day to promote a diverse and inclusive workplace and work environment, in which all people, regardless of gender, age, ethnic origin, social or economic status, sexual orientation, gender expression or identity, physical or health condition, religion, political affiliation, or any other difference or condition, have the real opportunity to develop and reach their potential.\nWe are looking for a data engineer for our Business Analytics team to form the bridge between our raw business data, with their specific technical and business representations, and the creative data-needs of our Data Scientists and Visualization experts.\nThe engineer will be responsible for managing, optimizing and monitoring data retrieval, storage and distribution throughout the team, thereby supporting our business objectives and fostering data-driven decision-making across the organization.\nOur data engineer will collaborate with Data Scientists, Visualisation Experts, Business Analysts and IT colleagues in several domains, in order to improve:\nOur reaction time towards answering business requests.\nThe quality and depth of reports and models.\nData availability and quality.\nGeneral data engineering knowledge within the team.\nOur reaction time towards answering business requests.\nThe quality and depth of reports and models.\nData availability and quality.\nGeneral data engineering knowledge within the team.\nMain responsabilities:\nMain responsabilities\nBuild and maintain data pipelines to support continuing increases in data volume and complexity.\nBuild dimensional models to help migrate reports and models from an on-premise Data Warehouse to a new Enterprise Data Hub.\nImplement and monitor data quality checks and work directly with end users to understand data quality expectations.\nImprove, optimize and monitor data processing efficiency.\nCombine raw information from different sources to create new and valuable datasets.\nTroubleshoot data-related issues and assist in their resolution.\nMonitor cloud costs to ensure monthly budget is respected.\nTrain and support the team in more technical questions\nStay on top of new technologies and best practices and implement these in the team.\nEnsure compliance with GDPR and privacy restrictions when working with sensitive data.\nBuild and maintain data pipelines to support continuing increases in data volume and complexity.\nBuild dimensional models to help migrate reports and models from an on-premise Data Warehouse to a new Enterprise Data Hub.\nImplement and monitor data quality checks and work directly with end users to understand data quality expectations.\nImprove, optimize and monitor data processing efficiency.\nCombine raw information from different sources to create new and valuable datasets.\nTroubleshoot data-related issues and assist in their resolution.\nMonitor cloud costs to ensure monthly budget is respected.\nTrain and support the team in more technical questions\nStay on top of new technologies and best practices and implement these in the team.\nEnsure compliance with GDPR and privacy restrictions when working with sensitive data.\nRequired skills:\nRequired skills\nThe role requires a set of technical and interpersonal skills, including:\nBachelor or Master in IT.\n3+ years of experience in a data engineering role.\nGood knowledge of SQL and database design.\nGood knowledge of Python.\nExperience with Azure (or similar cloud environment)\nExperience with Databricks (Apache Spark, ...)\nHigh level of English (minimum C1).\nStrong communication/interpersonal skills to clarify and validate requirements with stakeholders.\nGood team player.\nBachelor or Master in IT.\n3+ years of experience in a data engineering role.\nGood knowledge of SQL and database design.\nGood knowledge of Python.\nExperience with Azure (or similar cloud environment)\nExperience with Databricks (Apache Spark, ...)\nHigh level of English (minimum C1).\nStrong communication/interpersonal skills to clarify and validate requirements with stakeholders.\nGood team player.\nExperience in the following is considered an asset:\nSqlServer\nGoogle BigQuery\nPySpark\nAzure Data Factory\nAzure devops\nGood understanding of data modelling methods (data warehouse, data vault, dimensional modelling).\nBasic understanding of Data Science principles and theory\nBasic knowledge of Power BI (or other BI tools)\nSqlServer\nGoogle BigQuery\nPySpark\nAzure Data Factory\nAzure devops\nGood understanding of data modelling methods (data warehouse, data vault, dimensional modelling).\nBasic understanding of Data Science principles and theory\nBasic knowledge of Power BI (or other BI tools)\nWhat do we offer?\nThis is a great opportunity to join an organization with a social mission, in an international environment, and to work with specialized professionals involved in projects that are carried out with the utmost rigor.\norganization with a social mission\ninternational environment\nspecialized professionals\nWe offer you a permanent full-time contract (35 hours) with flexible schedule, hybrid work model (on-site working hours in Madrid), competitive salary, according with the responsibilities of the position, and social benefits.\npermanent full-time contract (35 hours)\nflexible schedule\nhybrid work\ncompetitive salary\nsocial benefits\nAre you ready to face the challenge? Don't think twice and send us your CV in English!\nsend us your CV in English\nWe are firmly committed to equal opportunities, respect for diversity and the reconciliation of work, family and personal life. We work every day to promote a diverse and inclusive workplace and work environment, in which all people, regardless of gender, age, ethnic origin, social or economic status, sexual orientation, gender expression or identity, physical or health condition, religion, political affiliation, or any other difference or condition, have the real opportunity to develop and reach their potential."
    },
    "4165297402": {
        "title": "Senior Data Engineer",
        "company": "companies",
        "location": "Vitoria-Gasteiz, Basque Country, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nJob DescriptionAre you a data expert with a passion for building scalable data solutions?\n\nAs a Senior Data Engineer at Stefanini, you will design, develop, and optimize modern data architectures, enabling advanced analytics and business intelligence capabilities.\n\nYou'll work with cutting-edge technologies, including Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory, while collaborating with cross-functional teams to deliver high-performance data solutions that drive business success.Key ResponsibilitiesData Architecture & Engineering: Design, build, and maintain robust data pipelines and architectures using Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory.Data Integration & ETL: Develop scalable ETL/ELT processes to extract, transform, and load data from various sources into cloud-based data platforms.Performance Optimization: Optimize data models, queries, and storage strategies for high-performance analytics and reporting.Big Data Processing: Work with large-scale datasets and distributed computing frameworks to support advanced analytics, machine learning, and business intelligence.Cloud Data Solutions: Design and implement data solutions leveraging cloud services, ensuring security, scalability, and cost-efficiency.Collaboration & Stakeholder Engagement: Partner with business analysts, data scientists, and BI teams to understand data needs and deliver efficient, scalable solutions.Data Governance & Security: Implement best practices for data governance, quality, lineage, and compliance with industry standards.Automation & CI/CD: Utilize DevOps principles to automate data workflows, testing, and deployment using CI/CD pipelines.Documentation & Maintenance: Maintain clear documentation of data pipelines, architecture, and processes for transparency and knowledge sharing.\n\nRequired Skills & ExperienceEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, Information Technology, or a related field.Experience: 5+ years of hands-on experience in data engineering, working with large-scale data processing and analytics.Technical Proficiency:Expertise in Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory for data engineering and analytics workloads.Strong skills in SQL for data modeling, querying, and performance tuning.Experience with Python or Scala for data processing and automation.Proficiency in ETL/ELT pipeline development and data integration across cloud platforms.Strong knowledge of data modeling, warehousing, and optimization techniques.Cloud Expertise: Proven experience with Azure cloud services and modern data architectures.Data Governance & Security: Understanding of best practices in data governance, security, compliance, and lineage tracking.\n\nNice to have:Big Data & Distributed Computing: Familiarity with Spark, Delta Lake, and Parquet for large-scale data processing.DevOps & Automation: Experience in CI/CD for data engineering, using tools like Terraform, Git, and Azure DevOps.Additional Tools: Knowledge of SAP BI, SAP Business Warehouse, and integration with cloud-based data platforms is a plus.Machine Learning & AI: Experience supporting machine learning workflows and feature engineering pipelines.Streaming Data & IoT: Familiarity with Kafka, Event Hubs, or real-time data processing is advantageous.API & Integration: Experience with REST APIs, GraphQL, and data API development is a plus.\n\nWhy Join Us?Work with cutting-edge data technologies to shape modern data solutions.Collaborate with a team of data-driven professionals on impactful projects.Gain exposure to AI, machine learning, and advanced analytics use cases.Enjoy a flexible, innovative, and growth-oriented work environment.Competitive salary, benefits, and continuous learning opportunities.\n\nThe preceding job description had been designed to indicate the general nature and level of work performed by employees within this classification.\n\nIt is not designed to contain or be interpreted as a comprehensive inventory of all duties and responsibilities required of employees assigned to this job.What We OfferYou will find here not only a challenging and interesting workplace, but also a rewarding work experience, with competitive compensation and benefit packages:Career development: opportunity to grow within the team;Meal card;Flexible working hours and work from home, aligned with project needs;Friendly team who is eager to meet you.\n\nWhat's NextIt's best to apply today, because job postings can be taken down and we wouldn't want you to miss this opportunity.\n\nIn case you need further information, just send us a message at ****** and we'll be happy to assist!Diversity & inclusionHere at the Stefanini Group, we value plurality and equity, regardless of race, sexual orientation, disability, age, ancestry, religion, gender, and nationality.\n\nWe understand and encourage the importance of being you!About UsWe are a Brazilian company with over 35 years of experience in delivering IT services worldwide, ranging from IT outsourcing to application development or IT staffing.\n\nWe have a direct presence in 41 countries, through our 70 offices located throughout the world.\n\nWe have managed to become the preferred partner of many small-to-midsize local and regional companies as well.\n\nMost of our clients come from industries such as financial services, manufacturing, telecommunications, chemical, services, technology, public sector and utilities.Stefanini has career opportunities locally and around the world for professionals interested in a vibrant, passionate, team-oriented workplace.\n\nIf you are a customer-centric person with a \"get it done\" attitude, come over for coffee and a talk on your future career with us!Learn more about us on www.stefanini.com and join us on LinkedIn , Facebook and Instagram where we regularly post insights from our colleagues."
    },
    "3927387075": {
        "title": "Data Engineer \u2013 Bilbao, Madrid",
        "company": "Teknei",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nJob Description\n\nIn the Teknei team we continue to grow and add value for our customers in the data environment.\n\nWith an ambitious growth plan we expand our team and select a Data Engineer to join the Engineering team of the projects division.\n\nRequirements\n\nKnowledge in:\n\nOpen Source technologies such as Spark, Airflow, Kafka, Hdfs and Elasticsearch.\nKnowledge of Azure, Google and Amazon public cloud.\nExperience in containerization is also highly valued.\n\nWhat do we offer you?\n\nPermanent contract.\nFlexible working hours.\nGreat possibilities for professional growth, with an adapted career plan.\n\nIf you are interested in the offer, do not hesitate to sign up!\nJob Description\nRequirements\nOpen Source technologies such as Spark, Airflow, Kafka, Hdfs and Elasticsearch.\nKnowledge of Azure, Google and Amazon public cloud.\nExperience in containerization is also highly valued.\nOpen Source technologies such as Spark, Airflow, Kafka, Hdfs and Elasticsearch.\nKnowledge of Azure, Google and Amazon public cloud.\nExperience in containerization is also highly valued.\nPermanent contract.\nFlexible working hours.\nGreat possibilities for professional growth, with an adapted career plan.\nPermanent contract.\nFlexible working hours.\nGreat possibilities for professional growth, with an adapted career plan.\ndo not hesitate to sign up!"
    },
    "4137753740": {
        "title": "Sr. Data Engineer ",
        "company": "Nestl\u00e9",
        "location": "Esplugues de Llobregat, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nWe are currently looking for a Sr. Data Engineer in order to complete our Analytics Service Line (ASL) department.\n\nPosition Snapshot\n\nLocation: Barcelona\nType of Contract: Permanent\nStream: IT Analytics, Data and Integration\nType of work: Hybrid\nWork Language: Fluent Business English\n\nThe role \n\nAs a Sr. Data Engineer, should have technical experience in building scalable distributed software on the cloud that will combine cognitive computing / advanced analytics technology with traditional data engineering/science and apply it at scale to transform enterprise business processes. A Data Engineer will also be able to modify a traditional open-source or other data stack to incorporate cognition. Potential resources should come from a strong data engineering background and need to have experience with un-structured and structured data and being able to transform and analyze the data using various tools, including components delivered through the Azure market place, especially from the Cortana Intelligence suite. This is in particular required to build new platforms, which may need to move data to new technologies requiring data parity guarantees between new and old feeds.\n\nTravel activity is low, depending on the project assigned, averaging at below 10%.\n\nWhat You\u2019ll Do\n\nPartner with Market functions, Globally and Regionally Managed Businesses, and Above Market entities to build and deliver towards a portfolio of analytics services.\nAnalyze complex data sets to answer strategic and operational business questions.\nExpertise in working with structured data; apply methods, technologies and techniques that address data architecture, integration and governance of data. \nExperience in database concepts, data modelling; data integration including design and architecture. \nMaster data management, including customer data strategy, product data strategy and organizational hierarchy, and Information (Data) Governance including strategy, implementation, business glossary, metadata analysis and proving hands on business knowledge working with database developers, DBA's architects, data quality analysts, and other teams while seamlessly managing client relationships within context of individual role.\nCollaborate with other Business Analytics teams and networks to utilize new sources of data, for the purpose of developing insights and applicability across markets.\nProvide enablement services around predictive, diagnostic, and optimization analytics.\nWork with large, complex data sets.\nSolve difficult, non-routine analysis problems, applying advanced analytical methods as needed.\n\nWe offer you \n\nWe offer more than just a job. We put people first and inspire you to become the best version of yourself:\n\nGreat benefits including competitive salary and a comprehensive social benefits package. We have one of the most competitive pension plans on the market, as well as flexible remuneration with tax advantages: health insurance, restaurant card, mobility plan, etc.\nPersonal and professional growth through ongoing training and constant career opportunities reflecting our conviction that people are our most important asset.\nHybrid working environment with flexible working scheme. Our state-of-the-art campus is dog friendly and equipped with a medical center, canteen and areas to co-create network and chill!\nRecreation activities such as yoga, Zumba, etc. and a wide range of volunteering activities.\n\nMinimum qualifications: \n\nBachelor's Degree in Computer Science, Systems Analysis or a related study, or equivalent experience.\n+7 years\u2019 experience technical and critical thinking skills and service oriented architecture (SOA) principles and Web services standards and best practices.\n+7 years\u2019 experience as a software architect designing and delivering large scale distributed software systems, preferably in large scale global business.\nCapability to architect highly scalable distributed systems using open source tools and big data technologies (such as Hadoop, HBase, Spark, Impala, Storm, etc.) in integration with other open-source or proprietary tools available through the Azure Market Place, especially the Cortana Intelligence components.\nGather and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.).\nExperience on programming in SQL SAP, Snowflake, DBT, Azure, Dev OPS, methods for efficiently retrieving data, as well as data preparation/wrangling both on demand and in an industrialized way.\n\nBonus Points if you: \n\nMasters would be good to have (in disciplines like Data Science, computer Science, or Economics).\nExperience on programming in R, PythonSoft skills.\nYou have strategic thinking and business acumen.\nYou are capable to organize several projects at the same time.\nExperience working in a global environment and with virtual team.\n\nAbout The IT Hub\n\nAt Nestl\u00e9 IT, we are a diverse, global team of IT professionals in the biggest health, nutrition and wellness company of the world. We strive to create an environment where people are valued for who they are. We innovate every day through future ready technologies to create opportunities for Nestl\u00e9 to delight consumers, customers and employees alike. We collaborate with partners around the world to deliver tangible value at global scale. We continuously work to develop our people to be future ready.\n\nAbout Nestl\u00e9 We are Nestl\u00e9, the largest food and beverage company in the world, with a presence in more than 185 countries. With net sales of CHF 94.4 billion in 2022, the company has over 291,000 employees and 418 factories in 85 countries. Our values are based on respect: respect for ourselves, respect for others, respect for diversity, and respect for our future. Nestl\u00e9 is dedicated to offering high-quality food and beverage products and services that contribute to the nutrition, health, and well-being of people, pets, and the planet. Additionally, it is committed to being a leading company in sustainability and achieving net zero greenhouse gas emissions by 2050. Want to learn more? Visit us at: www.nestle.com\n\nWe encourage the diversity of applicants across gender, age, ethnicity, nationality, sexual orientation, social background, religion or belief and disability.\n\nStep outside your comfort zone; share your ideas, way of thinking and working to make a difference to the world, every single day. You own a piece of the action \u2013 make it count.\n\nJoin Nestl\u00e9\u2019s IT Hub #beaforceforgood\nSr. Data Engineer\nAnalytics Service Line (ASL)\n.\nPosition Snapshot\nLocation: Barcelona\nType of Contract: Permanent\nStream: IT Analytics, Data and Integration\nType of work: Hybrid\nWork Language: Fluent Business English\nLocation: Barcelona\nType of Contract: Permanent\nStream: IT Analytics, Data and Integration\nType of work: Hybrid\nWork Language: Fluent Business English\nThe role\nWhat You\u2019ll Do\nPartner with Market functions, Globally and Regionally Managed Businesses, and Above Market entities to build and deliver towards a portfolio of analytics services.\nAnalyze complex data sets to answer strategic and operational business questions.\nExpertise in working with structured data; apply methods, technologies and techniques that address data architecture, integration and governance of data. \nExperience in database concepts, data modelling; data integration including design and architecture. \nMaster data management, including customer data strategy, product data strategy and organizational hierarchy, and Information (Data) Governance including strategy, implementation, business glossary, metadata analysis and proving hands on business knowledge working with database developers, DBA's architects, data quality analysts, and other teams while seamlessly managing client relationships within context of individual role.\nCollaborate with other Business Analytics teams and networks to utilize new sources of data, for the purpose of developing insights and applicability across markets.\nProvide enablement services around predictive, diagnostic, and optimization analytics.\nWork with large, complex data sets.\nSolve difficult, non-routine analysis problems, applying advanced analytical methods as needed.\nPartner with Market functions, Globally and Regionally Managed Businesses, and Above Market entities to build and deliver towards a portfolio of analytics services.\nAnalyze complex data sets to answer strategic and operational business questions.\nExpertise in working with structured data; apply methods, technologies and techniques that address data architecture, integration and governance of data.\nExperience in database concepts, data modelling; data integration including design and architecture.\nMaster data management, including customer data strategy, product data strategy and organizational hierarchy, and Information (Data) Governance including strategy, implementation, business glossary, metadata analysis and proving hands on business knowledge working with database developers, DBA's architects, data quality analysts, and other teams while seamlessly managing client relationships within context of individual role.\nCollaborate with other Business Analytics teams and networks to utilize new sources of data, for the purpose of developing insights and applicability across markets.\nProvide enablement services around predictive, diagnostic, and optimization analytics.\nWork with large, complex data sets.\nSolve difficult, non-routine analysis problems, applying advanced analytical methods as needed.\nWe offer you\nGreat benefits including competitive salary and a comprehensive social benefits package. We have one of the most competitive pension plans on the market, as well as flexible remuneration with tax advantages: health insurance, restaurant card, mobility plan, etc.\nPersonal and professional growth through ongoing training and constant career opportunities reflecting our conviction that people are our most important asset.\nHybrid working environment with flexible working scheme. Our state-of-the-art campus is dog friendly and equipped with a medical center, canteen and areas to co-create network and chill!\nRecreation activities such as yoga, Zumba, etc. and a wide range of volunteering activities.\nGreat benefits including competitive salary and a comprehensive social benefits package. We have one of the most competitive pension plans on the market, as well as flexible remuneration with tax advantages: health insurance, restaurant card, mobility plan, etc.\nPersonal and professional growth through ongoing training and constant career opportunities reflecting our conviction that people are our most important asset.\nHybrid working environment with flexible working scheme. Our state-of-the-art campus is dog friendly and equipped with a medical center, canteen and areas to co-create network and chill!\nRecreation activities such as yoga, Zumba, etc. and a wide range of volunteering activities.\nMinimum qualifications:\nBachelor's Degree in Computer Science, Systems Analysis or a related study, or equivalent experience.\n+7 years\u2019 experience technical and critical thinking skills and service oriented architecture (SOA) principles and Web services standards and best practices.\n+7 years\u2019 experience as a software architect designing and delivering large scale distributed software systems, preferably in large scale global business.\nCapability to architect highly scalable distributed systems using open source tools and big data technologies (such as Hadoop, HBase, Spark, Impala, Storm, etc.) in integration with other open-source or proprietary tools available through the Azure Market Place, especially the Cortana Intelligence components.\nGather and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.).\nExperience on programming in SQL SAP, Snowflake, DBT, Azure, Dev OPS, methods for efficiently retrieving data, as well as data preparation/wrangling both on demand and in an industrialized way.\nBachelor's Degree in Computer Science, Systems Analysis or a related study, or equivalent experience.\n+7 years\u2019 experience technical and critical thinking skills and service oriented architecture (SOA) principles and Web services standards and best practices.\n+7 years\u2019 experience as a software architect designing and delivering large scale distributed software systems, preferably in large scale global business.\nCapability to architect highly scalable distributed systems using open source tools and big data technologies (such as Hadoop, HBase, Spark, Impala, Storm, etc.) in integration with other open-source or proprietary tools available through the Azure Market Place, especially the Cortana Intelligence components.\nGather and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.).\nExperience on programming in SQL SAP, Snowflake, DBT, Azure, Dev OPS, methods for efficiently retrieving data, as well as data preparation/wrangling both on demand and in an industrialized way.\nBonus Points if you:\nMasters would be good to have (in disciplines like Data Science, computer Science, or Economics).\nExperience on programming in R, PythonSoft skills.\nYou have strategic thinking and business acumen.\nYou are capable to organize several projects at the same time.\nExperience working in a global environment and with virtual team.\nMasters would be good to have (in disciplines like Data Science, computer Science, or Economics).\nExperience on programming in R, PythonSoft skills.\nYou have strategic thinking and business acumen.\nYou are capable to organize several projects at the same time.\nExperience working in a global environment and with virtual team.\nAbout The IT Hub\nAbout Nestl\u00e9\nWe encourage the diversity of applicants across gender, age, ethnicity, nationality, sexual orientation, social background, religion or belief and disability.\nJoin Nestl\u00e9\u2019s IT Hub #beaforceforgood"
    },
    "4124697772": {
        "title": "MLops & Data Engineer ",
        "company": "dentsu",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nJob Description:\n\nEl candidato seleccionado formar\u00e1 parte del equipo de IA de Dentsu Espa\u00f1a, participando en diversos proyectos de an\u00e1lisis de datos, modelado y proporcionando recomendaciones a nuestros clientes.\n\nAdicionalmente, tendr\u00e1 responsabilidades en la implementaci\u00f3n, mantenimiento y evoluci\u00f3n de la infraestructura cloud, ejecutando las pr\u00e1cticas necesarias para garantizar el ciclo de vida completo de los datos, as\u00ed como el funcionamiento eficiente y exitoso de los modelos de machine learning y su despliegue en producci\u00f3n.\n\nAdem\u00e1s, el candidato desempe\u00f1ar\u00e1 un papel crucial en la transformaci\u00f3n de la organizaci\u00f3n a trav\u00e9s de la gobernanza de datos y la automatizaci\u00f3n de procesos, garantizando que la toma de decisiones basada en datos y la racionalizaci\u00f3n de las operaciones est\u00e9n a la vanguardia de nuestra estrategia.\n\n Automatizaci\u00f3n y orquestaci\u00f3n de procesos:\nDesarrollar y aplicar estrategias de arquitectura de datos, garantizando la alineaci\u00f3n con los objetivos empresariales y los requisitos t\u00e9cnicos. \nSupervisar el almacenamiento, la recuperaci\u00f3n y la seguridad de los datos, garantizando su integridad y el cumplimiento de la normativa. \nAutomatizar tareas repetitivas y procesos de despliegue de modelos para mejorar la eficiencia y la reproducibilidad. \n Dise\u00f1o, implementaci\u00f3n y despliegue de MLOps Pipeline:\nColaborar con los cient\u00edficos de datos y otros equipos de ingenier\u00eda para dise\u00f1ar e implementar MLOps para el despliegue automatizado de ETL (Extract, Transform, Load) y modelos. \nEstablecer pr\u00e1cticas s\u00f3lidas de control de versiones, control de calidad y validaci\u00f3n de modelos. \nDefinir y desplegar canalizaciones CI/CD para cada producto. \nConfigurar y mantener la infraestructura necesaria para ejecutar y escalar modelos de aprendizaje autom\u00e1tico en producci\u00f3n. \nImplementar estrategias de supervisi\u00f3n y registro de modelos para garantizar el rendimiento y la detecci\u00f3n temprana de anomal\u00edas. \nUtilizar herramientas de orquestaci\u00f3n para coordinar los flujos de trabajo de formaci\u00f3n, validaci\u00f3n y despliegue de modelos con el fin de garantizar la disponibilidad y el rendimiento de los modelos. \n\n\nExperiencia, habilidades y conocimientos valorados positivamente:\n\nParticipaci\u00f3n activa en proyectos de desarrollo y despliegue de modelos, incluyendo tareas de testeo. \nExperiencia en proyectos de I+D utilizando productos en la nube, diferentes t\u00e9cnicas de modelado y aprendizaje autom\u00e1tico. \nExperiencia con APIs, web scraping, y manejo de datos de CRM, marketing, anal\u00edtica web, as\u00ed como otras fuentes relevantes como datos p\u00fablicos. \nSe valorar\u00e1 positivamente la experiencia en los sectores de Marketing y Publicidad o Investigaci\u00f3n de Mercados, especialmente en la medici\u00f3n del impacto publicitario mediante modelos matem\u00e1ticos. \nCapacidad de an\u00e1lisis, resoluci\u00f3n de problemas y s\u00edntesis. \nCapacidad para trabajar en equipo. Proactividad, agilidad y compromiso. \nOrientaci\u00f3n al cliente y a la obtenci\u00f3n de resultados. \nExcelentes habilidades de comunicaci\u00f3n con equipos interfuncionales. \n\n\nRequisitos:\n\nFormaci\u00f3n acad\u00e9mica deseada: Doble titulaci\u00f3n en Matem\u00e1ticas e Ingenier\u00eda Inform\u00e1tica; Licenciatura o M\u00e1ster en Matem\u00e1ticas/Estad\u00edstica e Ingenier\u00eda Matem\u00e1tica o Big Data; Titulaci\u00f3n universitaria en Inform\u00e1tica o Ingenier\u00eda Inform\u00e1tica. \nM\u00ednimo 3-4 a\u00f1os de experiencia en consultor\u00eda, institutos de investigaci\u00f3n, agencias de medios o en un puesto relacionado con Ingenier\u00eda de Datos / Arquitecto de Datos. \nDominio de sistemas de gesti\u00f3n de bases de datos, modelado de datos, procesos ETL y plataformas en la nube. Especialmente en el caso de entornos de bases de datos relacionales: PostgreSQL y MySQL, as\u00ed como lenguaje SQL. \nDominio de al menos uno de los principales proveedores de cloud (AWS, GCP y Azure). Se valorar\u00e1 positivamente la experiencia con Google Cloud Platform y sus servicios clave como Cloud SQL, Cloud Storage, BigQuery, Cloud Run, App Engine, etc. \nExperiencia demostrada en el dise\u00f1o y desarrollo de productos/herramientas desde cero, as\u00ed como capacidad de an\u00e1lisis y recomendaciones sobre productos ya desarrollados. \nConocimiento de GIT y metodolog\u00eda CI/CD y su implementaci\u00f3n a trav\u00e9s de herramientas de orquestaci\u00f3n (Jenkins, Azure DevOps, Gitlab CI/CD...). \nConocimientos avanzados de lenguajes de programaci\u00f3n Python. Tambi\u00e9n se valorar\u00e1 positivamente R. \nNivel alto de ingl\u00e9s (relacionado con clientes internacionales). Comunicaci\u00f3n fluida en ingl\u00e9s, tanto hablado como escrito. \n\n\nLocation:\n\nMadrid\n\nBrand:\n\nCarat\n\nTime Type:\n\nFull time\n\nContract Type:\n\nPermanent\nJob Description:\nAutomatizaci\u00f3n y orquestaci\u00f3n de procesos:\nDesarrollar y aplicar estrategias de arquitectura de datos, garantizando la alineaci\u00f3n con los objetivos empresariales y los requisitos t\u00e9cnicos. \nSupervisar el almacenamiento, la recuperaci\u00f3n y la seguridad de los datos, garantizando su integridad y el cumplimiento de la normativa. \nAutomatizar tareas repetitivas y procesos de despliegue de modelos para mejorar la eficiencia y la reproducibilidad. \n Dise\u00f1o, implementaci\u00f3n y despliegue de MLOps Pipeline:\nColaborar con los cient\u00edficos de datos y otros equipos de ingenier\u00eda para dise\u00f1ar e implementar MLOps para el despliegue automatizado de ETL (Extract, Transform, Load) y modelos. \nEstablecer pr\u00e1cticas s\u00f3lidas de control de versiones, control de calidad y validaci\u00f3n de modelos. \nDefinir y desplegar canalizaciones CI/CD para cada producto. \nConfigurar y mantener la infraestructura necesaria para ejecutar y escalar modelos de aprendizaje autom\u00e1tico en producci\u00f3n. \nImplementar estrategias de supervisi\u00f3n y registro de modelos para garantizar el rendimiento y la detecci\u00f3n temprana de anomal\u00edas. \nUtilizar herramientas de orquestaci\u00f3n para coordinar los flujos de trabajo de formaci\u00f3n, validaci\u00f3n y despliegue de modelos con el fin de garantizar la disponibilidad y el rendimiento de los modelos.\nAutomatizaci\u00f3n y orquestaci\u00f3n de procesos:\nDesarrollar y aplicar estrategias de arquitectura de datos, garantizando la alineaci\u00f3n con los objetivos empresariales y los requisitos t\u00e9cnicos.\nSupervisar el almacenamiento, la recuperaci\u00f3n y la seguridad de los datos, garantizando su integridad y el cumplimiento de la normativa.\nAutomatizar tareas repetitivas y procesos de despliegue de modelos para mejorar la eficiencia y la reproducibilidad.\nDise\u00f1o, implementaci\u00f3n y despliegue de MLOps Pipeline:\nColaborar con los cient\u00edficos de datos y otros equipos de ingenier\u00eda para dise\u00f1ar e implementar MLOps para el despliegue automatizado de ETL (Extract, Transform, Load) y modelos.\nEstablecer pr\u00e1cticas s\u00f3lidas de control de versiones, control de calidad y validaci\u00f3n de modelos.\nDefinir y desplegar canalizaciones CI/CD para cada producto.\nConfigurar y mantener la infraestructura necesaria para ejecutar y escalar modelos de aprendizaje autom\u00e1tico en producci\u00f3n.\nImplementar estrategias de supervisi\u00f3n y registro de modelos para garantizar el rendimiento y la detecci\u00f3n temprana de anomal\u00edas.\nUtilizar herramientas de orquestaci\u00f3n para coordinar los flujos de trabajo de formaci\u00f3n, validaci\u00f3n y despliegue de modelos con el fin de garantizar la disponibilidad y el rendimiento de los modelos.\nParticipaci\u00f3n activa en proyectos de desarrollo y despliegue de modelos, incluyendo tareas de testeo. \nExperiencia en proyectos de I+D utilizando productos en la nube, diferentes t\u00e9cnicas de modelado y aprendizaje autom\u00e1tico. \nExperiencia con APIs, web scraping, y manejo de datos de CRM, marketing, anal\u00edtica web, as\u00ed como otras fuentes relevantes como datos p\u00fablicos. \nSe valorar\u00e1 positivamente la experiencia en los sectores de Marketing y Publicidad o Investigaci\u00f3n de Mercados, especialmente en la medici\u00f3n del impacto publicitario mediante modelos matem\u00e1ticos. \nCapacidad de an\u00e1lisis, resoluci\u00f3n de problemas y s\u00edntesis. \nCapacidad para trabajar en equipo. Proactividad, agilidad y compromiso. \nOrientaci\u00f3n al cliente y a la obtenci\u00f3n de resultados. \nExcelentes habilidades de comunicaci\u00f3n con equipos interfuncionales.\nParticipaci\u00f3n activa en proyectos de desarrollo y despliegue de modelos, incluyendo tareas de testeo.\nExperiencia en proyectos de I+D utilizando productos en la nube, diferentes t\u00e9cnicas de modelado y aprendizaje autom\u00e1tico.\nExperiencia con APIs, web scraping, y manejo de datos de CRM, marketing, anal\u00edtica web, as\u00ed como otras fuentes relevantes como datos p\u00fablicos.\nSe valorar\u00e1 positivamente la experiencia en los sectores de Marketing y Publicidad o Investigaci\u00f3n de Mercados, especialmente en la medici\u00f3n del impacto publicitario mediante modelos matem\u00e1ticos.\nCapacidad de an\u00e1lisis, resoluci\u00f3n de problemas y s\u00edntesis.\nCapacidad para trabajar en equipo. Proactividad, agilidad y compromiso.\nOrientaci\u00f3n al cliente y a la obtenci\u00f3n de resultados.\nExcelentes habilidades de comunicaci\u00f3n con equipos interfuncionales.\nRequisitos:\nFormaci\u00f3n acad\u00e9mica deseada: Doble titulaci\u00f3n en Matem\u00e1ticas e Ingenier\u00eda Inform\u00e1tica; Licenciatura o M\u00e1ster en Matem\u00e1ticas/Estad\u00edstica e Ingenier\u00eda Matem\u00e1tica o Big Data; Titulaci\u00f3n universitaria en Inform\u00e1tica o Ingenier\u00eda Inform\u00e1tica. \nM\u00ednimo 3-4 a\u00f1os de experiencia en consultor\u00eda, institutos de investigaci\u00f3n, agencias de medios o en un puesto relacionado con Ingenier\u00eda de Datos / Arquitecto de Datos. \nDominio de sistemas de gesti\u00f3n de bases de datos, modelado de datos, procesos ETL y plataformas en la nube. Especialmente en el caso de entornos de bases de datos relacionales: PostgreSQL y MySQL, as\u00ed como lenguaje SQL. \nDominio de al menos uno de los principales proveedores de cloud (AWS, GCP y Azure). Se valorar\u00e1 positivamente la experiencia con Google Cloud Platform y sus servicios clave como Cloud SQL, Cloud Storage, BigQuery, Cloud Run, App Engine, etc. \nExperiencia demostrada en el dise\u00f1o y desarrollo de productos/herramientas desde cero, as\u00ed como capacidad de an\u00e1lisis y recomendaciones sobre productos ya desarrollados. \nConocimiento de GIT y metodolog\u00eda CI/CD y su implementaci\u00f3n a trav\u00e9s de herramientas de orquestaci\u00f3n (Jenkins, Azure DevOps, Gitlab CI/CD...). \nConocimientos avanzados de lenguajes de programaci\u00f3n Python. Tambi\u00e9n se valorar\u00e1 positivamente R. \nNivel alto de ingl\u00e9s (relacionado con clientes internacionales). Comunicaci\u00f3n fluida en ingl\u00e9s, tanto hablado como escrito.\nFormaci\u00f3n acad\u00e9mica deseada: Doble titulaci\u00f3n en Matem\u00e1ticas e Ingenier\u00eda Inform\u00e1tica; Licenciatura o M\u00e1ster en Matem\u00e1ticas/Estad\u00edstica e Ingenier\u00eda Matem\u00e1tica o Big Data; Titulaci\u00f3n universitaria en Inform\u00e1tica o Ingenier\u00eda Inform\u00e1tica.\nM\u00ednimo 3-4 a\u00f1os de experiencia en consultor\u00eda, institutos de investigaci\u00f3n, agencias de medios o en un puesto relacionado con Ingenier\u00eda de Datos / Arquitecto de Datos.\nDominio de sistemas de gesti\u00f3n de bases de datos, modelado de datos, procesos ETL y plataformas en la nube. Especialmente en el caso de entornos de bases de datos relacionales: PostgreSQL y MySQL, as\u00ed como lenguaje SQL.\nDominio de al menos uno de los principales proveedores de cloud (AWS, GCP y Azure). Se valorar\u00e1 positivamente la experiencia con Google Cloud Platform y sus servicios clave como Cloud SQL, Cloud Storage, BigQuery, Cloud Run, App Engine, etc.\nExperiencia demostrada en el dise\u00f1o y desarrollo de productos/herramientas desde cero, as\u00ed como capacidad de an\u00e1lisis y recomendaciones sobre productos ya desarrollados.\nConocimiento de GIT y metodolog\u00eda CI/CD y su implementaci\u00f3n a trav\u00e9s de herramientas de orquestaci\u00f3n (Jenkins, Azure DevOps, Gitlab CI/CD...).\nConocimientos avanzados de lenguajes de programaci\u00f3n Python. Tambi\u00e9n se valorar\u00e1 positivamente R.\nNivel alto de ingl\u00e9s (relacionado con clientes internacionales). Comunicaci\u00f3n fluida en ingl\u00e9s, tanto hablado como escrito.\nLocation:\nBrand:\nTime Type:\nContract Type:"
    },
    "4175814676": {
        "title": "Data Engineer",
        "company": "Aszendit Tech",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nEn Aszendit, crecemos juntos!\nEstamos buscando un/a Data Engineer con experiencia en AWS para unirse a nuestro equipo joven y din\u00e1mico en el sector de retail.\n\nFunciones y responsabilidades:\nDise\u00f1ar, construir y optimizar pipelines de datos en entornos de Big Data.\nAutomatizar procesos ETL garantizando eficiencia y confiabilidad.\nDesarrollar y gestionar workflows utilizando Python.\nIntegraci\u00f3n de datos con APIs para la ingesta y procesamiento de informaci\u00f3n.\nEscribir y optimizar consultas avanzadas en SQL para an\u00e1lisis y transformaci\u00f3n de datos.\nImplementar procesos de validaci\u00f3n y auditor\u00eda de datos para asegurar su calidad.\nColaborar con equipos multidisciplinarios para integrar soluciones de datos en diferentes \u00e1reas del negocio.\n\nRequisitos:\n M\u00ednimo 2 a\u00f1os de experiencia en roles de Data Engineer.\nExperiencia comprobada trabajando con AWS (Redshift, S3, Lambda, Glue, etc.).\nConocimientos avanzados en Python para el desarrollo de soluciones de datos.\nExperiencia en automatizaci\u00f3n y gesti\u00f3n de ETLs.\nDominio avanzado de SQL y manejo de grandes vol\u00famenes de datos.\nFamiliaridad con herramientas de Big Data.\nExperiencia en integraci\u00f3n de datos a trav\u00e9s de APIs.\nTerraform y Git como plus.\nPower BI e ingl\u00e9s como plus.\nCapacidad de trabajar en equipo y comunicar ideas t\u00e9cnicas de manera efectiva.\n\nPor qu\u00e9 Aszendit?\nEn Aszendit, nos enfocamos en ser el partner tecnol\u00f3gico de nuestros clientes y el partner profesional de nuestro equipo. Ponemos en el centro a nuestro talento humano, porque sin vosotros nada ser\u00eda posible.\n\nTe ofrecemos:\nHibrido : 3 d\u00edas presenciales y 2 Remoto , en Pozuelo de Alarcon , Madrid.\nProyectos de gran impacto.\nContrato indefinido.\nAcompa\u00f1amiento continuo con un l\u00edder t\u00e9cnico.\nBonus de formaci\u00f3n para tu desarrollo profesional.\nPlan de carrera claro y adaptado a tus objetivos.\nActividades de Team Building.\nBeneficios como tickets restaurante, guarder\u00eda o transporte.\n\u00a1\u00a1\u00a1\u00a1Acepta el reto y \u00fanete a nuestro equipo!!!\nEn Aszendit, crecemos juntos!\nEstamos buscando un/a Data Engineer con experiencia en AWS para unirse a nuestro equipo joven y din\u00e1mico en el sector de retail.\nData Engineer\nAWS\nFunciones y responsabilidades:\nDise\u00f1ar, construir y optimizar pipelines de datos en entornos de Big Data.\nAutomatizar procesos ETL garantizando eficiencia y confiabilidad.\nDesarrollar y gestionar workflows utilizando Python.\nIntegraci\u00f3n de datos con APIs para la ingesta y procesamiento de informaci\u00f3n.\nEscribir y optimizar consultas avanzadas en SQL para an\u00e1lisis y transformaci\u00f3n de datos.\nImplementar procesos de validaci\u00f3n y auditor\u00eda de datos para asegurar su calidad.\nColaborar con equipos multidisciplinarios para integrar soluciones de datos en diferentes \u00e1reas del negocio.\nDise\u00f1ar, construir y optimizar pipelines de datos en entornos de Big Data.\npipelines de datos\nAutomatizar procesos ETL garantizando eficiencia y confiabilidad.\nETL\nDesarrollar y gestionar workflows utilizando Python.\nPython\nIntegraci\u00f3n de datos con APIs para la ingesta y procesamiento de informaci\u00f3n.\nAPIs\nEscribir y optimizar consultas avanzadas en SQL para an\u00e1lisis y transformaci\u00f3n de datos.\nSQL\nImplementar procesos de validaci\u00f3n y auditor\u00eda de datos para asegurar su calidad.\nColaborar con equipos multidisciplinarios para integrar soluciones de datos en diferentes \u00e1reas del negocio.\nRequisitos:\nM\u00ednimo 2 a\u00f1os de experiencia en roles de Data Engineer.\nExperiencia comprobada trabajando con AWS (Redshift, S3, Lambda, Glue, etc.).\nConocimientos avanzados en Python para el desarrollo de soluciones de datos.\nExperiencia en automatizaci\u00f3n y gesti\u00f3n de ETLs.\nDominio avanzado de SQL y manejo de grandes vol\u00famenes de datos.\nFamiliaridad con herramientas de Big Data.\nExperiencia en integraci\u00f3n de datos a trav\u00e9s de APIs.\nTerraform y Git como plus.\nPower BI e ingl\u00e9s como plus.\nCapacidad de trabajar en equipo y comunicar ideas t\u00e9cnicas de manera efectiva.\nM\u00ednimo 2 a\u00f1os de experiencia en roles de Data Engineer.\nM\u00ednimo 2 a\u00f1os de experiencia\nExperiencia comprobada trabajando con AWS (Redshift, S3, Lambda, Glue, etc.).\nConocimientos avanzados en Python para el desarrollo de soluciones de datos.\nExperiencia en automatizaci\u00f3n y gesti\u00f3n de ETLs.\nETLs\nDominio avanzado de SQL y manejo de grandes vol\u00famenes de datos.\nFamiliaridad con herramientas de Big Data.\nBig Data\nExperiencia en integraci\u00f3n de datos a trav\u00e9s de APIs.\nTerraform y Git como plus.\nTerraform y Git\nPower BI e ingl\u00e9s como plus.\nPower BI e ingl\u00e9s\nCapacidad de trabajar en equipo y comunicar ideas t\u00e9cnicas de manera efectiva.\nPor qu\u00e9 Aszendit?\nEn Aszendit, nos enfocamos en ser el partner tecnol\u00f3gico de nuestros clientes y el partner profesional de nuestro equipo. Ponemos en el centro a nuestro talento humano, porque sin vosotros nada ser\u00eda posible.\nTe ofrecemos:\nHibrido : 3 d\u00edas presenciales y 2 Remoto , en Pozuelo de Alarcon , Madrid.\nProyectos de gran impacto.\nContrato indefinido.\nAcompa\u00f1amiento continuo con un l\u00edder t\u00e9cnico.\nBonus de formaci\u00f3n para tu desarrollo profesional.\nPlan de carrera claro y adaptado a tus objetivos.\nActividades de Team Building.\nBeneficios como tickets restaurante, guarder\u00eda o transporte.\nHibrido : 3 d\u00edas presenciales y 2 Remoto , en Pozuelo de Alarcon , Madrid.\nProyectos de gran impacto.\nContrato indefinido.\nAcompa\u00f1amiento continuo con un l\u00edder t\u00e9cnico.\nBonus de formaci\u00f3n para tu desarrollo profesional.\nPlan de carrera claro y adaptado a tus objetivos.\nActividades de Team Building.\nTeam Building\nBeneficios como tickets restaurante, guarder\u00eda o transporte.\n\u00a1\u00a1\u00a1\u00a1Acepta el reto y \u00fanete a nuestro equipo!!!"
    },
    "4157823028": {
        "title": "Data Engineer Google Cloud Platform - Hybrid (Madrid)",
        "company": "Infoser Technological solutions, Hardware, Big Data Experts",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nEn Infoser New Technologies, estamos en la b\u00fasqueda de Data Engineer especializado en la suite de datos de Google Cloud Platform para incorporarse a uno de nuestros clientes estrat\u00e9gicos. Si te apasiona la gesti\u00f3n y an\u00e1lisis de datos, la automatizaci\u00f3n de procesos y el dise\u00f1o de modelos predictivos, \u00a1esta oportunidad es para ti!\n\nResponsabilidad:\nDise\u00f1ar y desarrollar procesos de extracci\u00f3n, transformaci\u00f3n y carga de datos (ETL/ELT) en GCP\nOptimizar y automatizar procesos de an\u00e1lisis y gesti\u00f3n de datos.\nExtraer, validar y analizar datos para la generaci\u00f3n de informes.\nParticipar en proyectos de transformaci\u00f3n y anal\u00edtica de datos.\nDesarrollar y actualizar interfaces de intercambio de datos en Python.\nGestionar la ingesta de datos y el mantenimiento del Data Warehouse de la compa\u00f1\u00eda.\nDar soporte a modelos predictivos de IA y colaborar en el desarrollo de nuevos modelos seg\u00fan necesidades.\n\nRequisitos:\nExperiencia m\u00ednima de 2 a\u00f1os en posiciones similares.\nConocimientos en herramientas nativas de Google Cloud Platform (Dataform, BigQuery, etc.).\nDominio avanzado de SQL.\nExperiencia en ETL/ELT y manejo de grandes vol\u00famenes de datos.\nProgramaci\u00f3n en Python.\nConocimiento en metodolog\u00eda DevOps.\nIngl\u00e9s intermedio/avanzado.\n\n\u00bfQue buscamos en ti?\nPersona ordenada, met\u00f3dica y resolutiva.\nProactividad y ganas de aprender y aportar nuevas ideas.\nHabilidades de trabajo en equipo y capacidad para asumir responsabilidades.\nAdaptabilidad a nuevos retos y compromiso con la calidad en la entrega.\nPasi\u00f3n por la innovaci\u00f3n y la transformaci\u00f3n digital.\n\nSi cumples con el perfil y quieres unirte a un equipo din\u00e1mico en un entorno de alto impacto, \u00a1esperamos tu candidatura! \ud83d\ude80\n\n\ud83d\udce9 Aplica ahora y s\u00e9 parte de esta gran oportunidad.\nEn Infoser New Technologies, estamos en la b\u00fasqueda de Data Engineer especializado en la suite de datos de Google Cloud Platform para incorporarse a uno de nuestros clientes estrat\u00e9gicos. Si te apasiona la gesti\u00f3n y an\u00e1lisis de datos, la automatizaci\u00f3n de procesos y el dise\u00f1o de modelos predictivos, \u00a1esta oportunidad es para ti!\nResponsabilidad:\nDise\u00f1ar y desarrollar procesos de extracci\u00f3n, transformaci\u00f3n y carga de datos (ETL/ELT) en GCP\nOptimizar y automatizar procesos de an\u00e1lisis y gesti\u00f3n de datos.\nExtraer, validar y analizar datos para la generaci\u00f3n de informes.\nParticipar en proyectos de transformaci\u00f3n y anal\u00edtica de datos.\nDesarrollar y actualizar interfaces de intercambio de datos en Python.\nGestionar la ingesta de datos y el mantenimiento del Data Warehouse de la compa\u00f1\u00eda.\nDar soporte a modelos predictivos de IA y colaborar en el desarrollo de nuevos modelos seg\u00fan necesidades.\nDise\u00f1ar y desarrollar procesos de extracci\u00f3n, transformaci\u00f3n y carga de datos (ETL/ELT) en GCP\nOptimizar y automatizar procesos de an\u00e1lisis y gesti\u00f3n de datos.\nExtraer, validar y analizar datos para la generaci\u00f3n de informes.\nParticipar en proyectos de transformaci\u00f3n y anal\u00edtica de datos.\nDesarrollar y actualizar interfaces de intercambio de datos en Python.\nGestionar la ingesta de datos y el mantenimiento del Data Warehouse de la compa\u00f1\u00eda.\nDar soporte a modelos predictivos de IA y colaborar en el desarrollo de nuevos modelos seg\u00fan necesidades.\nRequisitos:\nExperiencia m\u00ednima de 2 a\u00f1os en posiciones similares.\nConocimientos en herramientas nativas de Google Cloud Platform (Dataform, BigQuery, etc.).\nDominio avanzado de SQL.\nExperiencia en ETL/ELT y manejo de grandes vol\u00famenes de datos.\nProgramaci\u00f3n en Python.\nConocimiento en metodolog\u00eda DevOps.\nIngl\u00e9s intermedio/avanzado.\nExperiencia m\u00ednima de 2 a\u00f1os en posiciones similares.\nConocimientos en herramientas nativas de Google Cloud Platform (Dataform, BigQuery, etc.).\nDominio avanzado de SQL.\nExperiencia en ETL/ELT y manejo de grandes vol\u00famenes de datos.\nProgramaci\u00f3n en Python.\nConocimiento en metodolog\u00eda DevOps.\nIngl\u00e9s intermedio/avanzado.\n\u00bfQue buscamos en ti?\nPersona ordenada, met\u00f3dica y resolutiva.\nProactividad y ganas de aprender y aportar nuevas ideas.\nHabilidades de trabajo en equipo y capacidad para asumir responsabilidades.\nAdaptabilidad a nuevos retos y compromiso con la calidad en la entrega.\nPasi\u00f3n por la innovaci\u00f3n y la transformaci\u00f3n digital.\nPersona ordenada, met\u00f3dica y resolutiva.\nProactividad y ganas de aprender y aportar nuevas ideas.\nHabilidades de trabajo en equipo y capacidad para asumir responsabilidades.\nAdaptabilidad a nuevos retos y compromiso con la calidad en la entrega.\nPasi\u00f3n por la innovaci\u00f3n y la transformaci\u00f3n digital.\nSi cumples con el perfil y quieres unirte a un equipo din\u00e1mico en un entorno de alto impacto, \u00a1esperamos tu candidatura! \ud83d\ude80\n\ud83d\udce9 Aplica ahora y s\u00e9 parte de esta gran oportunidad."
    },
    "4173763174": {
        "title": "International Data Engineer",
        "company": "PUE DATA",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nEn PUEDATA, nos especializamos en proyectos 100% DATA. Nuestro equipo est\u00e1 certificado en tecnolog\u00edas de vanguardia, asegurando la mejor implementaci\u00f3n. Abarcamos todo el ciclo de vida de los proyectos de datos en entornos h\u00edbridos, multicloud y on-premise. Utilizamos metodolog\u00edas DevOps, Agile y Scrum para ofrecer soluciones eficientes y de alta calidad. \u00a1\u00danete a nosotros y marca la diferencia en el mundo del Dato! \ud83c\udf1f\n\nPara uno de nuestros principales proyectos Internacionales, necesitamos incorporar de manera indefinida un Data Engineer que viajar\u00e1 semanalmente al cliente (pa\u00eds europeo de habla inglesa).\n\n\u00bfQu\u00e9 funciones har\u00e1s?\ud83d\uddd2\ufe0f\ud83d\udcbc\nDise\u00f1ar, desarrollar y optimizar pipelines de datos escalables y de alto rendimiento para ingesta, transformaci\u00f3n y almacenamiento de datos.\nCrear productos de datos reutilizables siguiendo el enfoque Data as a Product, garantizando calidad, gobernanza y seguridad.\nColaborar con equipos de negocio para entender sus necesidades y traducirlas en soluciones de datos eficientes y escalables.\nImplementar arquitecturas Data Mesh para fomentar la descentralizaci\u00f3n y la interoperabilidad de datos entre equipos.\nAplicar herramientas de monitorizaci\u00f3n y observabilidad para asegurar la fiabilidad de los flujos de datos.\nAutomatizar despliegues y gesti\u00f3n de infraestructura con Terraform e integraci\u00f3n CI/CD.\n\n\u00bfQu\u00e9 buscamos?\ud83c\udfaf\ud83d\udc40\nNivel alto de ingl\u00e9s.\nGrado Universitario en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencias de Datos o Matem\u00e1ticas.\nM\u00e1s de 3 a\u00f1os de experiencia como Data Engineering en Big Data o plataformas en la nube.\nExperiencia con frameworks de procesamiento de datos (Apache Spark, Databricks, Flink, dbt).\nExperiencia con Azure Data Stack.\nDominio de SQL, Python y computaci\u00f3n distribuida.\nExperiencia en la implementaci\u00f3n de Data Mesh y desarrollo de productos de datos orientados a dominios.\nConocimientos en gobernanza de datos y seguridad (RBAC, GDPR, herramientas como Collibra o Microsoft Purview).\n\n\u00bfQu\u00e9 te ofrecemos?\nContrato indefinido a jornada completa.\nHorario intensivo de verano en julio y agosto.\nSeguro m\u00e9dico subvencionado al 50%.\nHorario de lunes a jueves de 9h a 18:30h y viernes de 8h a 15h.\nEntorno de trabajo fant\u00e1stico, desafiante y flexible, con un equipo humano excepcional.\nParticipaci\u00f3n en proyectos punteros en el \u00e1rea de DATA.\nDesarrollo y plan de carrera profesional en un sector de nicho tecnol\u00f3gico.\n\n\u00a1\u00danete a nosotros y lleva tu carrera al siguiente nivel en el apasionante mundo del Dato! \u00a1Esperamos conocerte pronto!\nEn PUEDATA, nos especializamos en proyectos 100% DATA. Nuestro equipo est\u00e1 certificado en tecnolog\u00edas de vanguardia, asegurando la mejor implementaci\u00f3n. Abarcamos todo el ciclo de vida de los proyectos de datos en entornos h\u00edbridos, multicloud y on-premise. Utilizamos metodolog\u00edas DevOps, Agile y Scrum para ofrecer soluciones eficientes y de alta calidad. \u00a1\u00danete a nosotros y marca la diferencia en el mundo del Dato! \ud83c\udf1f\nPara uno de nuestros principales proyectos Internacionales, necesitamos incorporar de manera indefinida un Data Engineer que viajar\u00e1 semanalmente al cliente (pa\u00eds europeo de habla inglesa).\n\u00bfQu\u00e9 funciones har\u00e1s?\ud83d\uddd2\ufe0f\ud83d\udcbc\nDise\u00f1ar, desarrollar y optimizar pipelines de datos escalables y de alto rendimiento para ingesta, transformaci\u00f3n y almacenamiento de datos.\nCrear productos de datos reutilizables siguiendo el enfoque Data as a Product, garantizando calidad, gobernanza y seguridad.\nColaborar con equipos de negocio para entender sus necesidades y traducirlas en soluciones de datos eficientes y escalables.\nImplementar arquitecturas Data Mesh para fomentar la descentralizaci\u00f3n y la interoperabilidad de datos entre equipos.\nAplicar herramientas de monitorizaci\u00f3n y observabilidad para asegurar la fiabilidad de los flujos de datos.\nAutomatizar despliegues y gesti\u00f3n de infraestructura con Terraform e integraci\u00f3n CI/CD.\nDise\u00f1ar, desarrollar y optimizar pipelines de datos escalables y de alto rendimiento para ingesta, transformaci\u00f3n y almacenamiento de datos.\nCrear productos de datos reutilizables siguiendo el enfoque Data as a Product, garantizando calidad, gobernanza y seguridad.\nData as a Product\nColaborar con equipos de negocio para entender sus necesidades y traducirlas en soluciones de datos eficientes y escalables.\nImplementar arquitecturas Data Mesh para fomentar la descentralizaci\u00f3n y la interoperabilidad de datos entre equipos.\nAplicar herramientas de monitorizaci\u00f3n y observabilidad para asegurar la fiabilidad de los flujos de datos.\nAutomatizar despliegues y gesti\u00f3n de infraestructura con Terraform e integraci\u00f3n CI/CD.\n\u00bfQu\u00e9 buscamos?\ud83c\udfaf\ud83d\udc40\nNivel alto de ingl\u00e9s.\nGrado Universitario en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencias de Datos o Matem\u00e1ticas.\nM\u00e1s de 3 a\u00f1os de experiencia como Data Engineering en Big Data o plataformas en la nube.\nExperiencia con frameworks de procesamiento de datos (Apache Spark, Databricks, Flink, dbt).\nExperiencia con Azure Data Stack.\nDominio de SQL, Python y computaci\u00f3n distribuida.\nExperiencia en la implementaci\u00f3n de Data Mesh y desarrollo de productos de datos orientados a dominios.\nConocimientos en gobernanza de datos y seguridad (RBAC, GDPR, herramientas como Collibra o Microsoft Purview).\nNivel alto de ingl\u00e9s.\nGrado Universitario en Ingenier\u00eda Inform\u00e1tica, Telecomunicaciones, Ciencias de Datos o Matem\u00e1ticas.\nM\u00e1s de 3 a\u00f1os de experiencia como Data Engineering en Big Data o plataformas en la nube.\nExperiencia con frameworks de procesamiento de datos (Apache Spark, Databricks, Flink, dbt).\nExperiencia con Azure Data Stack.\nDominio de SQL, Python y computaci\u00f3n distribuida.\nExperiencia en la implementaci\u00f3n de Data Mesh y desarrollo de productos de datos orientados a dominios.\nConocimientos en gobernanza de datos y seguridad (RBAC, GDPR, herramientas como Collibra o Microsoft Purview).\n\u00bfQu\u00e9 te ofrecemos?\nContrato indefinido a jornada completa.\nHorario intensivo de verano en julio y agosto.\nSeguro m\u00e9dico subvencionado al 50%.\nHorario de lunes a jueves de 9h a 18:30h y viernes de 8h a 15h.\nEntorno de trabajo fant\u00e1stico, desafiante y flexible, con un equipo humano excepcional.\nParticipaci\u00f3n en proyectos punteros en el \u00e1rea de DATA.\nDesarrollo y plan de carrera profesional en un sector de nicho tecnol\u00f3gico.\nContrato indefinido a jornada completa.\nHorario intensivo de verano en julio y agosto.\nSeguro m\u00e9dico subvencionado al 50%.\nHorario de lunes a jueves de 9h a 18:30h y viernes de 8h a 15h.\nEntorno de trabajo fant\u00e1stico, desafiante y flexible, con un equipo humano excepcional.\nParticipaci\u00f3n en proyectos punteros en el \u00e1rea de DATA.\nDesarrollo y plan de carrera profesional en un sector de nicho tecnol\u00f3gico.\n\u00a1\u00danete a nosotros y lleva tu carrera al siguiente nivel en el apasionante mundo del Dato! \u00a1Esperamos conocerte pronto!"
    },
    "4151390502": {
        "title": "Data Engineer (H/M)",
        "company": "Clikalia",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nClikalia es la proptech espa\u00f1ola que ha modernizado el mercado residencial aportando transparencia y convirtiendo procesos complejos y lentos en transacciones r\u00e1pidas y sencillas gracias a la tecnolog\u00eda. Fundada en 2018, es una de las principales scaleups de la econom\u00eda espa\u00f1ola y se ha expandido a M\u00e9xico, Portugal y Francia.\n\nCompramos viviendas en solo 7 d\u00edas, inyectando liquidez en el mercado, y las devolvemos a \u00e9l mediante venta o alquiler en tiempo r\u00e9cord, renovadas y con su eficiencia energ\u00e9tica mejorada. Nuestras viviendas son lo m\u00e1s cercano a obra nueva sin necesidad de usar nuevo suelo.\n\nEso tiene que ver con nuestra misi\u00f3n, que es modernizar los lugares en los que vivimos y aumentar a oferta de vivienda mediante la renovaci\u00f3n de inmuebles. De hecho, somos el principal regenerador de vivienda de segunda mano de Europa. En ese proceso, acompa\u00f1amos a nuestros clientes a lo largo de su ciclo inmobiliario, facilit\u00e1ndoles todos los servicios que necesitan (arquitectura y dise\u00f1o, b\u00fasqueda de financiaci\u00f3n, servicios legales, etc.).\n\nEn 2021, logramos la mayor ronda de financiaci\u00f3n de una startup en la historia de Espa\u00f1a: m\u00e1s de 460 millones de euros. Entre nuestros inversores est\u00e1n SoftBank, Fifth Wall, Luxor Capital y Mouro Capital, todos con gran conocimiento del sector y foco en la innovaci\u00f3n tecnol\u00f3gica.\n\n\u00bfCu\u00e1les ser\u00e1n algunas de las tareas de la posici\u00f3n?\n\nImplementaci\u00f3n de arquitectura e infraestructura de datos en base a la estrategia de la compa\u00f1\u00eda.\nImplementaci\u00f3n de componentes y aplicaciones, operaci\u00f3n y administraci\u00f3n en base a arquitectura t\u00e9cnica y tech stack.\nImplementaci\u00f3n de componentes para capas de ingesta, almacenamiento y transformaci\u00f3n de datos.\nImplementaci\u00f3n de modelos l\u00f3gicos y f\u00edsicos de datos.\nImplementaci\u00f3n de soluciones para orquestaci\u00f3n de flujos de datos y su gobernanza.\nGesti\u00f3n colaborativa para integraci\u00f3n, automatizaci\u00f3n y operaci\u00f3n de pipelines de datos.\nEvaluaci\u00f3n y desarrollo de pruebas de concepto de tecnolog\u00edas y arquitecturas de datos emergentes.\nDocumentaci\u00f3n de los distintos procesos implicados.\n\n\n\u00bfQu\u00e9 necesitamos de ti?\n\nT\u00edtulo universitario en Ingenier\u00eda Inform\u00e1tica o campo relacionado\nM\u00e1s de 2 a\u00f1os de experiencia en la ejecuci\u00f3n de proyectos de data con participaci\u00f3n en todas sus etapas: ingenier\u00eda y preparaci\u00f3n del dato, almacenamiento y gesti\u00f3n, orquestaci\u00f3n de procesos, desarrollo de modelos y gobernanza de datos.\nConocimientos en tecnolog\u00edas de datos en los principales proveedores de servicios cloud (Google Cloud Platform, Azure)\nConocimientos en tratamiento y gesti\u00f3n de bases de datos SQL/NoSQL (BigQuery, PostgreSQL, MongoDB)\nConocimientos de soluciones y herramientas ETL/ELT (Fivetran, Pipes)\nConocimientos de soluciones de almacenamiento y transformaci\u00f3n de datos (BigQuery, Google Dataform)\nConocimientos de soluciones de gobernanza de datos (Google Dataplex)\nConocimientos de soluciones de orquestaci\u00f3n de datos (Google Dataflow, Apache Airflow, Composer)\n Deseable conocimientos de lenguajes de programaci\u00f3n, como R o Python. \nConocimientos de DataOps como metodolog\u00eda combinada de DevOps y Data Science para el an\u00e1lisis de datos\nTrabajo en equipo y comunicaci\u00f3n eficaz\nHabilidades de resoluci\u00f3n de problemas\n\n\n\u00bfQu\u00e9 ofrecemos?\n\nDesarrollo profesional en una de las proptech de mayor crecimiento en Europa y LATAM.\nFormar parte de un equipo ambicioso orientado a la excelencia en los resultados.\nSalario competitivo y acorde a tus competencias.\nAmbiente meritocr\u00e1tico.\nPosibilidad de cobrar tu salario cuando quieras o marcarte planes de ahorro.\nAlianza con Clicars: podr\u00e1s beneficiarte de descuentos para la compra de coches.\nAdem\u00e1s, por ser empleado de Clikalia tambi\u00e9n contamos con descuentos en la compra de viviendas y en el asesoramiento y gesti\u00f3n hipotecaria.\n\n\n\u00bfTodav\u00eda te lo est\u00e1s pensando?\n\nEn Clikalia queremos integrar el mejor equipo para seguir cambiando, con tecnolog\u00eda y transparencia, la forma en la que se hacen las cosas en torno a algo tan esencial como la vivienda.\n\nEn cumplimiento de la normativa de protecci\u00f3n de datos Clipiso Desarrollo S.L.U. como responsable del tratamiento te informa que los datos ser\u00e1n tratados con la finalidad de gestionar la oferta de empleo dentro del Grupo Clikalia. La base del tratamiento es el establecimiento de la relaci\u00f3n contractual. Los datos no ser\u00e1n conservados una vez finalizada la selecci\u00f3n. En caso de contrataci\u00f3n, los datos formar\u00e1n parte del expediente profesional. Los datos podr\u00e1n ser cedidos a sociedades del Grupo para la gesti\u00f3n. No est\u00e1n previstas transferencias internacionales. Puede ejercitar sus derechos de acceso, rectificaci\u00f3n, supresi\u00f3n, portabilidad y la limitaci\u00f3n u oposici\u00f3n dirigi\u00e9ndose por escrito a dpo@clikalia.com\nClikalia\nFundada en 2018\nM\u00e9xico, Portugal y Francia.\nCompramos viviendas en solo 7 d\u00edas, inyectando liquidez en el mercado\nEn 2021, logramos la mayor ronda de financiaci\u00f3n de una startup en la historia de Espa\u00f1a: m\u00e1s de 460 millones de euros\n.\nSoftBank, Fifth Wall, Luxor Capital y Mouro Capita\n\u00bfCu\u00e1les ser\u00e1n algunas de las tareas de la posici\u00f3n?\nImplementaci\u00f3n de arquitectura e infraestructura de datos en base a la estrategia de la compa\u00f1\u00eda.\nImplementaci\u00f3n de componentes y aplicaciones, operaci\u00f3n y administraci\u00f3n en base a arquitectura t\u00e9cnica y tech stack.\nImplementaci\u00f3n de componentes para capas de ingesta, almacenamiento y transformaci\u00f3n de datos.\nImplementaci\u00f3n de modelos l\u00f3gicos y f\u00edsicos de datos.\nImplementaci\u00f3n de soluciones para orquestaci\u00f3n de flujos de datos y su gobernanza.\nGesti\u00f3n colaborativa para integraci\u00f3n, automatizaci\u00f3n y operaci\u00f3n de pipelines de datos.\nEvaluaci\u00f3n y desarrollo de pruebas de concepto de tecnolog\u00edas y arquitecturas de datos emergentes.\nDocumentaci\u00f3n de los distintos procesos implicados.\nImplementaci\u00f3n de arquitectura e infraestructura de datos en base a la estrategia de la compa\u00f1\u00eda.\nImplementaci\u00f3n de componentes y aplicaciones, operaci\u00f3n y administraci\u00f3n en base a arquitectura t\u00e9cnica y tech stack.\nImplementaci\u00f3n de componentes para capas de ingesta, almacenamiento y transformaci\u00f3n de datos.\nImplementaci\u00f3n de modelos l\u00f3gicos y f\u00edsicos de datos.\nImplementaci\u00f3n de soluciones para orquestaci\u00f3n de flujos de datos y su gobernanza.\nGesti\u00f3n colaborativa para integraci\u00f3n, automatizaci\u00f3n y operaci\u00f3n de pipelines de datos.\nEvaluaci\u00f3n y desarrollo de pruebas de concepto de tecnolog\u00edas y arquitecturas de datos emergentes.\nDocumentaci\u00f3n de los distintos procesos implicados.\n\u00bfQu\u00e9 necesitamos de ti?\nT\u00edtulo universitario en Ingenier\u00eda Inform\u00e1tica o campo relacionado\nM\u00e1s de 2 a\u00f1os de experiencia en la ejecuci\u00f3n de proyectos de data con participaci\u00f3n en todas sus etapas: ingenier\u00eda y preparaci\u00f3n del dato, almacenamiento y gesti\u00f3n, orquestaci\u00f3n de procesos, desarrollo de modelos y gobernanza de datos.\nConocimientos en tecnolog\u00edas de datos en los principales proveedores de servicios cloud (Google Cloud Platform, Azure)\nConocimientos en tratamiento y gesti\u00f3n de bases de datos SQL/NoSQL (BigQuery, PostgreSQL, MongoDB)\nConocimientos de soluciones y herramientas ETL/ELT (Fivetran, Pipes)\nConocimientos de soluciones de almacenamiento y transformaci\u00f3n de datos (BigQuery, Google Dataform)\nConocimientos de soluciones de gobernanza de datos (Google Dataplex)\nConocimientos de soluciones de orquestaci\u00f3n de datos (Google Dataflow, Apache Airflow, Composer)\n Deseable conocimientos de lenguajes de programaci\u00f3n, como R o Python. \nConocimientos de DataOps como metodolog\u00eda combinada de DevOps y Data Science para el an\u00e1lisis de datos\nTrabajo en equipo y comunicaci\u00f3n eficaz\nHabilidades de resoluci\u00f3n de problemas\nT\u00edtulo universitario en Ingenier\u00eda Inform\u00e1tica o campo relacionado\nM\u00e1s de 2 a\u00f1os de experiencia en la ejecuci\u00f3n de proyectos de data con participaci\u00f3n en todas sus etapas: ingenier\u00eda y preparaci\u00f3n del dato, almacenamiento y gesti\u00f3n, orquestaci\u00f3n de procesos, desarrollo de modelos y gobernanza de datos.\nConocimientos en tecnolog\u00edas de datos en los principales proveedores de servicios cloud (Google Cloud Platform, Azure)\nConocimientos en tratamiento y gesti\u00f3n de bases de datos SQL/NoSQL (BigQuery, PostgreSQL, MongoDB)\nConocimientos de soluciones y herramientas ETL/ELT (Fivetran, Pipes)\nConocimientos de soluciones de almacenamiento y transformaci\u00f3n de datos (BigQuery, Google Dataform)\nConocimientos de soluciones de gobernanza de datos (Google Dataplex)\nConocimientos de soluciones de orquestaci\u00f3n de datos (Google Dataflow, Apache Airflow, Composer)\nDeseable conocimientos de lenguajes de programaci\u00f3n, como R o Python.\nConocimientos de DataOps como metodolog\u00eda combinada de DevOps y Data Science para el an\u00e1lisis de datos\nTrabajo en equipo y comunicaci\u00f3n eficaz\nHabilidades de resoluci\u00f3n de problemas\n\u00bfQu\u00e9 ofrecemos?\nDesarrollo profesional en una de las proptech de mayor crecimiento en Europa y LATAM.\nFormar parte de un equipo ambicioso orientado a la excelencia en los resultados.\nSalario competitivo y acorde a tus competencias.\nAmbiente meritocr\u00e1tico.\nPosibilidad de cobrar tu salario cuando quieras o marcarte planes de ahorro.\nAlianza con Clicars: podr\u00e1s beneficiarte de descuentos para la compra de coches.\nAdem\u00e1s, por ser empleado de Clikalia tambi\u00e9n contamos con descuentos en la compra de viviendas y en el asesoramiento y gesti\u00f3n hipotecaria.\nDesarrollo profesional en una de las proptech de mayor crecimiento en Europa y LATAM.\nFormar parte de un equipo ambicioso orientado a la excelencia en los resultados.\nSalario competitivo y acorde a tus competencias.\nAmbiente meritocr\u00e1tico.\nPosibilidad de cobrar tu salario cuando quieras o marcarte planes de ahorro.\nAlianza con Clicars: podr\u00e1s beneficiarte de descuentos para la compra de coches.\nAdem\u00e1s, por ser empleado de Clikalia tambi\u00e9n contamos con descuentos en la compra de viviendas y en el asesoramiento y gesti\u00f3n hipotecaria.\n\u00bfTodav\u00eda te lo est\u00e1s pensando?\nEn cumplimiento de la normativa de protecci\u00f3n de datos Clipiso Desarrollo S.L.U. como responsable del tratamiento te informa que los datos ser\u00e1n tratados con la finalidad de gestionar la oferta de empleo dentro del Grupo Clikalia. La base del tratamiento es el establecimiento de la relaci\u00f3n contractual. Los datos no ser\u00e1n conservados una vez finalizada la selecci\u00f3n. En caso de contrataci\u00f3n, los datos formar\u00e1n parte del expediente profesional. Los datos podr\u00e1n ser cedidos a sociedades del Grupo para la gesti\u00f3n. No est\u00e1n previstas transferencias internacionales. Puede ejercitar sus derechos de acceso, rectificaci\u00f3n, supresi\u00f3n, portabilidad y la limitaci\u00f3n u oposici\u00f3n dirigi\u00e9ndose por escrito a dpo@clikalia.com"
    },
    "4129283971": {
        "title": "Data Engineer Developer",
        "company": "AD4 Oct\u00f3gono",
        "location": "Ja\u00e9n, Andalusia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nEn Ad4 Octogono estamos contratando, buscamos un Data Engineer Developer apasionado y comprometido para unirse a nuestro equipo.\n\nRequisitos:\n\nPyspark: nivel alto\nPython: nivel alto\nSQL: nivel medio\nAWS Services: GLUE, EMR, Lambda, Athena, S3, ControlM: nivel medio/alto (con experiencia pr\u00e1ctica)\nQliksense: nivel medio\n\nHabilidades:\n\nCapacidad para ofrecer soluciones a problemas planteados por el negocio, enfoc\u00e1ndose en la optimizaci\u00f3n y automatizaci\u00f3n de procesos.\nImplementaci\u00f3n de scripts para agilizar c\u00e1lculos.\nFlexibilidad y habilidad para trabajar en equipo.\nAdaptabilidad ante cambios.\nAprendizaje r\u00e1pido y continuo.\n\nExperiencia m\u00ednima: 2 a\u00f1os\n\nNivel de ingl\u00e9s: B2\n\nUbicaci\u00f3n: Ja\u00e9n, C\u00f3rdoba, Granada, Madrid\n\nSi tienes la experiencia y las habilidades necesarias y te entusiasma formar parte de un equipo din\u00e1mico, \u00a1queremos conocerte!\n\nRequisitos:\n\nObligatorios:\n\nPyspark: nivel alto\nPython: nivel alto\nSQL: nivel medio\nAWS Services: GLUE, EMR, Lambda, Athena, S3, ControlM: nivel medio/alto (con experiencia pr\u00e1ctica)\nQliksense: nivel medio\nIngl\u00e9s B2 o superior\nData Engineer Developer\nRequisitos:\nPyspark: nivel alto\nPython: nivel alto\nSQL: nivel medio\nAWS Services: GLUE, EMR, Lambda, Athena, S3, ControlM: nivel medio/alto (con experiencia pr\u00e1ctica)\nQliksense: nivel medio\nPyspark: nivel alto\nPython: nivel alto\nSQL: nivel medio\nAWS Services: GLUE, EMR, Lambda, Athena, S3, ControlM: nivel medio/alto (con experiencia pr\u00e1ctica)\nQliksense: nivel medio\nHabilidades:\nCapacidad para ofrecer soluciones a problemas planteados por el negocio, enfoc\u00e1ndose en la optimizaci\u00f3n y automatizaci\u00f3n de procesos.\nImplementaci\u00f3n de scripts para agilizar c\u00e1lculos.\nFlexibilidad y habilidad para trabajar en equipo.\nAdaptabilidad ante cambios.\nAprendizaje r\u00e1pido y continuo.\nCapacidad para ofrecer soluciones a problemas planteados por el negocio, enfoc\u00e1ndose en la optimizaci\u00f3n y automatizaci\u00f3n de procesos.\nImplementaci\u00f3n de scripts para agilizar c\u00e1lculos.\nFlexibilidad y habilidad para trabajar en equipo.\nAdaptabilidad ante cambios.\nAprendizaje r\u00e1pido y continuo.\nExperiencia m\u00ednima\nNivel de ingl\u00e9s\nUbicaci\u00f3n\nPyspark: nivel alto\nPython: nivel alto\nSQL: nivel medio\nAWS Services: GLUE, EMR, Lambda, Athena, S3, ControlM: nivel medio/alto (con experiencia pr\u00e1ctica)\nQliksense: nivel medio\nIngl\u00e9s B2 o superior\nIngl\u00e9s B2 o superior"
    },
    "4132960302": {
        "title": "Senior Data Platform Engineer",
        "company": "Health Innovation Labs",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nAbout Us\nAt Health Innovation Labs, we are revolutionising healthcare through cutting-edge technology, delivering innovative solutions that transform how care is accessed, delivered, and managed. As a dynamic and fast-paced organisation, we thrive at the intersection of healthcare and technology, empowering providers, patients, and organisations to achieve better outcomes through smarter, more efficient systems.\n\nOur team of forward-thinkers is dedicated to solving complex challenges with creativity and precision. Whether it\u2019s streamlining workflows, enabling real-time data insights, or enhancing patient engagement, Health Innovation Labs is at the forefront of reshaping the future of healthcare.\n\nRooted in innovation and driven by impact, we embrace agility and collaboration as our core strengths. In a rapidly evolving industry, we remain steadfast in our mission: to advance healthcare systems for a healthier, more connected world. Join us as we push boundaries and reimagine what\u2019s possible in healthcare technology.\n\nThe Position\nAs a Senior Data Platform Engineer, you'll lead the design and development of a cutting-edge data platform. Collaborating with cross-functional teams, your work will directly enhance customer experience, making meaningful change for thousands worldwide.\n\nDay to day:\nEnd-to-end development and deployment of a scalable and robust data platform.\nMentor other Data Engineers and Data Platform Engineers, providing technical guidance, and fostering a collaborative and innovative work environment.\nCollaborate with cross-functional teams to understand business requirements, data needs, and use cases, and translate them into effective data platform solutions.\nDeliver the design and architecture of components of the data platform, ensuring high availability, scalability, and security.\nOversee the data acquisition and integration processes, working closely with data providers and external partners to ensure the quality and reliability of incoming data.\nMonitor and optimise the performance of the data platform, proactively identifying and resolving issues to ensure high availability and reliability.\n\nAbout You\nWorking knowledge of the overall SDLC process for data products - including but not limited to building Docker images, provisioning infrastructure, CI/CD and coding best practices.\nStrong technical expertise in data engineering, big data technologies, cloud platforms (such as AWS or GCP), data warehousing, and ELT processes.\nProficiency in programming languages like Typescript and SQL.\nProficiency in infrastructure as code tools such as AWS CDK, CDKTF or Terraform.\nStrong problem-solving skills, with the ability to translate business requirements into technical solutions and make sound decisions in a fast-paced environment.\nSolid understanding of data governance, data privacy, and regulatory compliance requirements.\nExperience working across different time zones\n\nBenefits:\nYou\u2019ll be joining a highly motivated, agile team where your ideas and work will directly influence the direction and progress of an expanding global company in a hyper-growth phase. We pride ourselves on our collaborative and driven culture and offer opportunities for advancement to high achievers.\n\nOther benefits include:\nCompetitive package.\nFull time, remote-first\nAbout Us\nAt Health Innovation Labs, we are revolutionising healthcare through cutting-edge technology, delivering innovative solutions that transform how care is accessed, delivered, and managed. As a dynamic and fast-paced organisation, we thrive at the intersection of healthcare and technology, empowering providers, patients, and organisations to achieve better outcomes through smarter, more efficient systems.\nOur team of forward-thinkers is dedicated to solving complex challenges with creativity and precision. Whether it\u2019s streamlining workflows, enabling real-time data insights, or enhancing patient engagement, Health Innovation Labs is at the forefront of reshaping the future of healthcare.\nRooted in innovation and driven by impact, we embrace agility and collaboration as our core strengths. In a rapidly evolving industry, we remain steadfast in our mission: to advance healthcare systems for a healthier, more connected world. Join us as we push boundaries and reimagine what\u2019s possible in healthcare technology.\nThe Position\nAs a Senior Data Platform Engineer, you'll lead the design and development of a cutting-edge data platform. Collaborating with cross-functional teams, your work will directly enhance customer experience, making meaningful change for thousands worldwide.\nDay to day:\nEnd-to-end development and deployment of a scalable and robust data platform.\nMentor other Data Engineers and Data Platform Engineers, providing technical guidance, and fostering a collaborative and innovative work environment.\nCollaborate with cross-functional teams to understand business requirements, data needs, and use cases, and translate them into effective data platform solutions.\nDeliver the design and architecture of components of the data platform, ensuring high availability, scalability, and security.\nOversee the data acquisition and integration processes, working closely with data providers and external partners to ensure the quality and reliability of incoming data.\nMonitor and optimise the performance of the data platform, proactively identifying and resolving issues to ensure high availability and reliability.\nEnd-to-end development and deployment of a scalable and robust data platform.\nMentor other Data Engineers and Data Platform Engineers, providing technical guidance, and fostering a collaborative and innovative work environment.\nCollaborate with cross-functional teams to understand business requirements, data needs, and use cases, and translate them into effective data platform solutions.\nDeliver the design and architecture of components of the data platform, ensuring high availability, scalability, and security.\nOversee the data acquisition and integration processes, working closely with data providers and external partners to ensure the quality and reliability of incoming data.\nMonitor and optimise the performance of the data platform, proactively identifying and resolving issues to ensure high availability and reliability.\nAbout You\nWorking knowledge of the overall SDLC process for data products - including but not limited to building Docker images, provisioning infrastructure, CI/CD and coding best practices.\nStrong technical expertise in data engineering, big data technologies, cloud platforms (such as AWS or GCP), data warehousing, and ELT processes.\nProficiency in programming languages like Typescript and SQL.\nProficiency in infrastructure as code tools such as AWS CDK, CDKTF or Terraform.\nStrong problem-solving skills, with the ability to translate business requirements into technical solutions and make sound decisions in a fast-paced environment.\nSolid understanding of data governance, data privacy, and regulatory compliance requirements.\nExperience working across different time zones\nWorking knowledge of the overall SDLC process for data products - including but not limited to building Docker images, provisioning infrastructure, CI/CD and coding best practices.\nStrong technical expertise in data engineering, big data technologies, cloud platforms (such as AWS or GCP), data warehousing, and ELT processes.\nProficiency in programming languages like Typescript and SQL.\nProficiency in infrastructure as code tools such as AWS CDK, CDKTF or Terraform.\nStrong problem-solving skills, with the ability to translate business requirements into technical solutions and make sound decisions in a fast-paced environment.\nSolid understanding of data governance, data privacy, and regulatory compliance requirements.\nExperience working across different time zones\nBenefits:\nYou\u2019ll be joining a highly motivated, agile team where your ideas and work will directly influence the direction and progress of an expanding global company in a hyper-growth phase. We pride ourselves on our collaborative and driven culture and offer opportunities for advancement to high achievers.\nOther benefits include:\nCompetitive package.\nFull time, remote-first\nCompetitive package.\nFull time, remote-first"
    },
    "4135654829": {
        "title": "Big Data Engineer",
        "company": "Plain Concepts",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\n\ud83d\ude80 We're Growing Our Data Dream Team!\n\nTitles? Meh, we're not big on them, but let's call this one Data Engineer \ud83d\ude09\n\nAs part of our international Data squad, you'll craft tailor-made solutions that wow our clients. We're hunting for a passionate Data Engineer with a solid technical background. Your mission? Train, deploy, and put groundbreaking developments into action \u2014 and that's just the beginning.\n\nHere's the deal:\n\nYou'll tackle the unique challenges of Data-powered software alongside a team of brilliant minds\nYou'll dive into cutting-edge projects using the latest tech to push boundaries and make an impact\nWhether you prefer working from home or vibing with us at our offices, the choice is yours\nAGILE isn't just a buzzword here \u2014 it's how we roll. Multidisciplinary teams? Check. Full ownership of projects? Double check\n\nReady to take on projects that matter, with a team that's as passionate as you are? Let's make it happen! \ud83d\ude0a\n\nYou will be responsible for:\n\nParticipating in the design and development of Data solutions for challenging projects\nDevelop projects from scratch with minimal supervision and strong team collaboration\nBe a key player in fostering best practices, clean, and reusable code\nDevelop ETLs using Spark (Python/Scala)\nWork on cloud-based projects (Azure/AWS)\nBuild scalable pipelines using a variety of technologies\n\n\nRequirements\n\nAt least 2 years of experience in data engineering\nStrong experience with Python or Scala and Spark, processing large datasets\nSolid experience in Cloud platforms (Azure or AWS)\nHands-on experience building data pipelines (CI/CD)\nExperience with testing (unit, integration, etc.)\nBonus points for experience with Databricks, Snowflake, or Fabric\nBonus points for experience with IaC (Infrastructure as Code)\nKnowledge of SQL and NoSQL databases\n Bonus points if you speak fluent English. \nA strong team player mindset\n\n\nBenefits\n\nSalary determined by the market and your experience \ud83e\udd11\nFlexible schedule 35 Hours / Week \ud83d\ude0e\nFully remote work (optional) \ud83c\udf0d\nFlexible compensation (restaurant, transport, and childcare) \u270c\nFully free health insurance, with a co-payment for dental services \ud83d\ude91\nIndividual budget for training or equipment and free Microsoft certifications \ud83d\udcda\nEnglish lessons \ud83d\uddfd\nBirthday day off \ud83c\udf34\ud83e\udd73\nMonthly bonus for electricity and Internet expenses at home \ud83d\udcbb\nDiscount on gym plan and sports activities \ud83d\udd1d\nPlain Camp (annual team-building event) \ud83c\udfaa\nExtra perks: events attendance and speakers, welcome pack, baby basket, Christmas basket, discount portal for employees \u2795 The pleasure of always working with the latest technological tools!\n\n\nWill you let us know you better?\n\nThe selection process: Simple, just 3 steps.\n\nPhone screen\n2 interviews with the team \ud83e\udd18\n\n\nWhat is Plain Concepts?\n\nPlain Concepts is a global company of over 500 people passionate about technology and innovation. Since our founding, we have grown through technical proficiency and confidence in ideas that others might consider risky, creating custom solutions for our clients. With offices in more than 6 countries, our mission is to continue to drive cuttingedge projects around the world.\n\nWe are highly committed to technical excellence. We are known for developing highly customized projects, offering specialized technical consultancy and training.\n\nThanks to the great work of our technicians, we have been recognized for our ability to lead innovative projects that generate value, from artificial intelligence to blockchain, driving solutions that help companies optimize their performance.\n\nWhat we do at Plain Concepts?\n\nWe pride ourselves on being a 100% technical team, dedicated to crafting custom projects from scratch, offering expert technical consultancy, and providing top-tier training.\n\nOur approach goes beyond traditional outsourcing; we focus on creating value together with our clients\nOur teams are diverse and multidisciplinary, operating in a flat, collaborative structure\nWe live and breathe AGILE principles, ensuring flexibility and efficiency in everything we do\nKnowledge-sharing is at our core: from supporting each other internally to contributing to the broader tech community through conferences, events, and talks\nInnovation drives us \u2014 even the boldest ideas are welcome here\nTransparency underpins all our relationships, fostering trust and long-term partnerships\n\n\nWant to learn more?\n\nCheck out our website! \u27a1 https://www.plainconcepts.com/\n\nAt Plain Concepts, we certainly seek to provide equal opportunities. We want diverse applicants regardless of race, colour, gender, religion, national origin, citizenship, disability, age, sexual orientation, or any other characteristic protected by law.\nWe're Growing Our Data Dream Team!\nData Engineer\nYou'll tackle the unique challenges of Data-powered software alongside a team of brilliant minds\nYou'll dive into cutting-edge projects using the latest tech to push boundaries and make an impact\nWhether you prefer working from home or vibing with us at our offices, the choice is yours\nAGILE isn't just a buzzword here \u2014 it's how we roll. Multidisciplinary teams? Check. Full ownership of projects? Double check\nYou'll tackle the unique challenges of Data-powered software alongside a team of brilliant minds\nYou'll dive into cutting-edge projects using the latest tech to push boundaries and make an impact\nWhether you prefer working from home or vibing with us at our offices, the choice is yours\nAGILE isn't just a buzzword here \u2014 it's how we roll. Multidisciplinary teams? Check. Full ownership of projects? Double check\nYou will be responsible for:\nParticipating in the design and development of Data solutions for challenging projects\nDevelop projects from scratch with minimal supervision and strong team collaboration\nBe a key player in fostering best practices, clean, and reusable code\nDevelop ETLs using Spark (Python/Scala)\nWork on cloud-based projects (Azure/AWS)\nBuild scalable pipelines using a variety of technologies\nParticipating in the design and development of Data solutions for challenging projects\nDevelop projects from scratch with minimal supervision and strong team collaboration\nBe a key player in fostering best practices, clean, and reusable code\nDevelop ETLs using Spark (Python/Scala)\nWork on cloud-based projects (Azure/AWS)\nBuild scalable pipelines using a variety of technologies\nRequirements\nAt least 2 years of experience in data engineering\nStrong experience with Python or Scala and Spark, processing large datasets\nSolid experience in Cloud platforms (Azure or AWS)\nHands-on experience building data pipelines (CI/CD)\nExperience with testing (unit, integration, etc.)\nBonus points for experience with Databricks, Snowflake, or Fabric\nBonus points for experience with IaC (Infrastructure as Code)\nKnowledge of SQL and NoSQL databases\n Bonus points if you speak fluent English. \nA strong team player mindset\nAt least 2 years of experience in data engineering\nStrong experience with Python or Scala and Spark, processing large datasets\nSolid experience in Cloud platforms (Azure or AWS)\nHands-on experience building data pipelines (CI/CD)\nExperience with testing (unit, integration, etc.)\nBonus points for experience with Databricks, Snowflake, or Fabric\nBonus points for experience with IaC (Infrastructure as Code)\nKnowledge of SQL and NoSQL databases\nBonus points if you speak fluent English.\nA strong team player mindset\nBenefits\nSalary determined by the market and your experience \ud83e\udd11\nFlexible schedule 35 Hours / Week \ud83d\ude0e\nFully remote work (optional) \ud83c\udf0d\nFlexible compensation (restaurant, transport, and childcare) \u270c\nFully free health insurance, with a co-payment for dental services \ud83d\ude91\nIndividual budget for training or equipment and free Microsoft certifications \ud83d\udcda\nEnglish lessons \ud83d\uddfd\nBirthday day off \ud83c\udf34\ud83e\udd73\nMonthly bonus for electricity and Internet expenses at home \ud83d\udcbb\nDiscount on gym plan and sports activities \ud83d\udd1d\nPlain Camp (annual team-building event) \ud83c\udfaa\nExtra perks: events attendance and speakers, welcome pack, baby basket, Christmas basket, discount portal for employees \u2795 The pleasure of always working with the latest technological tools!\nSalary determined by the market and your experience \ud83e\udd11\nFlexible schedule 35 Hours / Week \ud83d\ude0e\nFully remote work (optional) \ud83c\udf0d\nFlexible compensation (restaurant, transport, and childcare) \u270c\nFully free health insurance, with a co-payment for dental services \ud83d\ude91\nIndividual budget for training or equipment and free Microsoft certifications \ud83d\udcda\nEnglish lessons \ud83d\uddfd\nBirthday day off \ud83c\udf34\ud83e\udd73\nMonthly bonus for electricity and Internet expenses at home \ud83d\udcbb\nDiscount on gym plan and sports activities \ud83d\udd1d\nPlain Camp (annual team-building event) \ud83c\udfaa\nExtra perks: events attendance and speakers, welcome pack, baby basket, Christmas basket, discount portal for employees \u2795 The pleasure of always working with the latest technological tools!\nWill you let us know you better?\nPhone screen\n2 interviews with the team \ud83e\udd18\nPhone screen\n2 interviews with the team \ud83e\udd18\nWhat is Plain Concepts?\nWhat we do at Plain Concepts?\nOur approach goes beyond traditional outsourcing; we focus on creating value together with our clients\nOur teams are diverse and multidisciplinary, operating in a flat, collaborative structure\nWe live and breathe AGILE principles, ensuring flexibility and efficiency in everything we do\nKnowledge-sharing is at our core: from supporting each other internally to contributing to the broader tech community through conferences, events, and talks\nInnovation drives us \u2014 even the boldest ideas are welcome here\nTransparency underpins all our relationships, fostering trust and long-term partnerships\nOur approach goes beyond traditional outsourcing; we focus on creating value together with our clients\nOur teams are diverse and multidisciplinary, operating in a flat, collaborative structure\nWe live and breathe AGILE principles, ensuring flexibility and efficiency in everything we do\nKnowledge-sharing is at our core: from supporting each other internally to contributing to the broader tech community through conferences, events, and talks\nInnovation drives us \u2014 even the boldest ideas are welcome here\nTransparency underpins all our relationships, fostering trust and long-term partnerships\nWant to learn more?"
    },
    "4145031313": {
        "title": "Data Governance Engineer ",
        "company": "Canonical",
        "location": "EMEA",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nCanonical is a leading provider of open-source software and operating systems for global enterprise and technology markets. Our platform, Ubuntu, is very widely used in breakthrough enterprise initiatives such as public cloud, data science, AI, engineering innovation and IoT. Our customers include the world's leading public cloud and silicon providers, and industry leaders in many sectors. The company is a pioneer of global distributed collaboration, with 1200+ colleagues in more than 80 countries and very few office-based roles. Teams meet two to four times yearly in person, in interesting locations around the world, to align on strategy and execution.\n\nThe company is founder led, profitable and growing.\n\nWe are hiring a Data Governance Engineer with focus on data governance policies, processes, standards, and monitoring in compliance with internal policies and applicable regulatory frameworks, e.g., GDPR, DPA, ISO, etc. A successful candidate will develop and maintain an internal data catalog with automated data labeling and quality metrics, perform access management and ensure data security best practices.\n\nThe Data Governance team in the Commercial Systems unit has a mission to enable a secure and well-governed access to comprehensive data sets originating at many internal and external data sources formed in a data mesh. The team works with well-known open-source data governance tools such as Trino and Ranger, defines and executes data governance processes, and democratizes the data at Canonical.\n\nLocation: This role will be based remotely in the EMEA region.\n\nThe role entails\n\nDefine, monitor, and execute data governance policies\nDesign, implement, and maintain data labeling and quality metrics in the data catalog\nDeploy and operate services developed by the team\nDepending on your seniority, coach, mentor, and offer career development feedback\nDevelop and evangelize great engineering and organizational practices\n\n\nWhat We Are Looking For In You\n\nExceptional academic track record from both high school and university\nUndergraduate degree in a technical subject or a compelling narrative about your alternative chosen path\nTrack record of going above-and-beyond expectations to achieve outstanding results \nExperience with data quality, governance, and security processes and tools\nExperience with software development in Python and SQL\nProfessional written and spoken English with excellent presentation skills\nResult-oriented, with a personal drive to meet commitments \nAbility to travel internationally twice a year, for company events up to two weeks long\n\n\nNice-to-have Skills\n\nPerformance engineering and security experience\nExperience with Airbyte, Ranger, Superset, Temporal, or Trino\n\n\nWhat We Offer Colleagues\n\nWe consider geographical location, experience, and performance in shaping compensation worldwide. We revisit compensation annually (and more often for graduates and associates) to ensure we recognize outstanding performance. In addition to base pay, we offer a performance-driven annual bonus or commission. We provide all team members with additional benefits, which reflect our values and ideals. We balance our programs to meet local needs and ensure fairness globally.\n\nDistributed work environment with twice-yearly team sprints in person\nPersonal learning and development budget of USD 2,000 per year\nAnnual compensation review\nRecognition rewards\nAnnual holiday leave\nMaternity and paternity leave\nEmployee Assistance Program\nOpportunity to travel to new locations to meet colleagues\nPriority Pass, and travel upgrades for long haul company events\n\n\nAbout Canonical\n\nCanonical is a pioneering tech firm at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world of software. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do. Most colleagues at Canonical have worked from home since its inception in 2004. Working here is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game.\n\nCanonical is an equal opportunity employer\n\nWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration.\nData Governance Engineer\nLocation\nThe role entails\nDefine, monitor, and execute data governance policies\nDesign, implement, and maintain data labeling and quality metrics in the data catalog\nDeploy and operate services developed by the team\nDepending on your seniority, coach, mentor, and offer career development feedback\nDevelop and evangelize great engineering and organizational practices\nDefine, monitor, and execute data governance policies\nDesign, implement, and maintain data labeling and quality metrics in the data catalog\nDeploy and operate services developed by the team\nDepending on your seniority, coach, mentor, and offer career development feedback\nDevelop and evangelize great engineering and organizational practices\nWhat We Are Looking For In You\nExceptional academic track record from both high school and university\nUndergraduate degree in a technical subject or a compelling narrative about your alternative chosen path\nTrack record of going above-and-beyond expectations to achieve outstanding results \nExperience with data quality, governance, and security processes and tools\nExperience with software development in Python and SQL\nProfessional written and spoken English with excellent presentation skills\nResult-oriented, with a personal drive to meet commitments \nAbility to travel internationally twice a year, for company events up to two weeks long\nExceptional academic track record from both high school and university\nUndergraduate degree in a technical subject or a compelling narrative about your alternative chosen path\nTrack record of going above-and-beyond expectations to achieve outstanding results\nExperience with data quality, governance, and security processes and tools\nExperience with software development in Python and SQL\nProfessional written and spoken English with excellent presentation skills\nResult-oriented, with a personal drive to meet commitments\nAbility to travel internationally twice a year, for company events up to two weeks long\nNice-to-have Skills\nPerformance engineering and security experience\nExperience with Airbyte, Ranger, Superset, Temporal, or Trino\nPerformance engineering and security experience\nExperience with Airbyte, Ranger, Superset, Temporal, or Trino\nWhat We Offer Colleagues\nDistributed work environment with twice-yearly team sprints in person\nPersonal learning and development budget of USD 2,000 per year\nAnnual compensation review\nRecognition rewards\nAnnual holiday leave\nMaternity and paternity leave\nEmployee Assistance Program\nOpportunity to travel to new locations to meet colleagues\nPriority Pass, and travel upgrades for long haul company events\nDistributed work environment with twice-yearly team sprints in person\nPersonal learning and development budget of USD 2,000 per year\nAnnual compensation review\nRecognition rewards\nAnnual holiday leave\nMaternity and paternity leave\nEmployee Assistance Program\nOpportunity to travel to new locations to meet colleagues\nPriority Pass, and travel upgrades for long haul company events\nAbout Canonical\nCanonical is an equal opportunity employer"
    },
    "4130702716": {
        "title": "Data Engineer Lead",
        "company": "Appodeal, Inc.",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nAppodeal is a dynamic US-based product company with a truly global presence.\n\nWe have offices in Warsaw, Barcelona and Virginia along with remote team members located around the world.\n\nOur company thrives on diversity, collaboration, and innovation, making us a leader in the mobile app monetization space.\n\nWhy Appodeal?\n\nAt Appodeal, we're more than just a company\u2014we're a team united by a common mission: to help every person discover and grow their talents!\n\nWe take pride in our cutting-edge product and our internationally dispersed team of talented professionals.\n\nHere's what we value, and what we hope you do too:\n\nContinuous Learning and Growth: We are passionate about learning, growing personally, and building rewarding careers.\nMaking an Impact: We are committed to building a history-defining company that leaves a lasting impact on the mobile app industry.\nSolving Exciting Challenges: We tackle complex problems every day, supported by a team of world-class professionals and mentors.\nEnjoying the Journey: We believe in having fun while working toward our goals.\n\nWe are seeking an experienced Lead Data Engineer to drive the development and optimization of our data infrastructure within the Vector team. As a key technical leader, you will play a pivotal role in enabling seamless data operations to support machine learning, analytics, and business needs. This is an exciting opportunity to shape the data engineering landscape and collaborate with cross-functional teams to achieve impactful outcomes.\n\nKey Responsibilities\n\nLead the management and evolution of our in-house ML data platform, overseeing tasks such as data collection, storage, transformation, and accessibility to empower machine learning models and analytics.\nArchitect, design, and optimize scalable data workflows, ensuring the seamless orchestration of pipelines.\nEstablish and uphold best practices for data handling, storage, and processing within the ML and analytics ecosystem.\nDrive the development of internal tools to automate and enhance data processes, fostering efficiency and scalability across business workflows.\nCollaborate with product, MLOps, and data science teams to deliver an exceptional data experience for all ML practitioners and data consumers.\nMentor and guide a team of data engineers, fostering professional growth and knowledge-sharing.\n\nRequirements\n\n5+ years of professional experience in data engineering, with at least 2 years in a lead or senior role.\nExpertise in Python 3 and data frameworks (Pandas, PySpark).\nHands-on experience with modern data storage solutions: AWS S3, data lakes, and data warehouses.\nProficiency in data orchestration tools (Dagster, Airflow).\nExperience with streaming technologies (Kafka, Spark).\nSolid understanding of machine learning and data analytics principles.\nBasic knowledge of data visualization and BI tools.\nBased in Barcelona or ready to relocate to Barcelona.\n\nNice to Have\n\nExperience with Databricks.\nFamiliarity with Druid or other OLAP systems.\nBackground in building ML data platforms.\n\nWith an outstanding product and a mission that excites and inspires, Appodeal offers a unique opportunity to make an impact while being part of an amazing team.\n\nJoin us and help shape the future of mobile app success!\nAppodeal\nWarsaw, Barcelona and Virginia\nWhy Appodeal?\nto help every person discover and grow their talents!\nContinuous Learning and Growth: We are passionate about learning, growing personally, and building rewarding careers.\nMaking an Impact: We are committed to building a history-defining company that leaves a lasting impact on the mobile app industry.\nSolving Exciting Challenges: We tackle complex problems every day, supported by a team of world-class professionals and mentors.\nEnjoying the Journey: We believe in having fun while working toward our goals.\nContinuous Learning and Growth: We are passionate about learning, growing personally, and building rewarding careers.\nContinuous Learning and Growth:\nMaking an Impact: We are committed to building a history-defining company that leaves a lasting impact on the mobile app industry.\nMaking an Impact:\nSolving Exciting Challenges: We tackle complex problems every day, supported by a team of world-class professionals and mentors.\nSolving Exciting Challenges:\nEnjoying the Journey: We believe in having fun while working toward our goals.\nEnjoying the Journey:\nLead Data Engineer\nVector team\nKey Responsibilities\nLead the management and evolution of our in-house ML data platform, overseeing tasks such as data collection, storage, transformation, and accessibility to empower machine learning models and analytics.\nArchitect, design, and optimize scalable data workflows, ensuring the seamless orchestration of pipelines.\nEstablish and uphold best practices for data handling, storage, and processing within the ML and analytics ecosystem.\nDrive the development of internal tools to automate and enhance data processes, fostering efficiency and scalability across business workflows.\nCollaborate with product, MLOps, and data science teams to deliver an exceptional data experience for all ML practitioners and data consumers.\nMentor and guide a team of data engineers, fostering professional growth and knowledge-sharing.\nLead the management and evolution of our in-house ML data platform, overseeing tasks such as data collection, storage, transformation, and accessibility to empower machine learning models and analytics.\nArchitect, design, and optimize scalable data workflows, ensuring the seamless orchestration of pipelines.\nEstablish and uphold best practices for data handling, storage, and processing within the ML and analytics ecosystem.\nDrive the development of internal tools to automate and enhance data processes, fostering efficiency and scalability across business workflows.\nCollaborate with product, MLOps, and data science teams to deliver an exceptional data experience for all ML practitioners and data consumers.\nMentor and guide a team of data engineers, fostering professional growth and knowledge-sharing.\nRequirements\n5+ years of professional experience in data engineering, with at least 2 years in a lead or senior role.\nExpertise in Python 3 and data frameworks (Pandas, PySpark).\nHands-on experience with modern data storage solutions: AWS S3, data lakes, and data warehouses.\nProficiency in data orchestration tools (Dagster, Airflow).\nExperience with streaming technologies (Kafka, Spark).\nSolid understanding of machine learning and data analytics principles.\nBasic knowledge of data visualization and BI tools.\nBased in Barcelona or ready to relocate to Barcelona.\n5+ years of professional experience in data engineering, with at least 2 years in a lead or senior role.\n5+ years\nExpertise in Python 3 and data frameworks (Pandas, PySpark).\nHands-on experience with modern data storage solutions: AWS S3, data lakes, and data warehouses.\nProficiency in data orchestration tools (Dagster, Airflow).\nExperience with streaming technologies (Kafka, Spark).\nSolid understanding of machine learning and data analytics principles.\nBasic knowledge of data visualization and BI tools.\nBased in Barcelona or ready to relocate to Barcelona.\nNice to Have\nExperience with Databricks.\nFamiliarity with Druid or other OLAP systems.\nBackground in building ML data platforms.\nExperience with Databricks.\nFamiliarity with Druid or other OLAP systems.\nBackground in building ML data platforms.\nJoin us"
    },
    "4084883864": {
        "title": "Lead Data Software Engineer (Spark/Scala/Databricks) ",
        "company": "EPAM Systems",
        "location": "M\u00e1laga, Andalusia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nAre you an open-minded professional fluent in Scala programming language? If that sounds like you, this could be the perfect opportunity to join EPAM as a Lead Data Engineer. We are looking for a team player with excellent communication and organizational skills, mastery in engineering and a B2+/C1 level of English to communicate fluently with English-speaking stakeholders, share ideas and provide reasoning.\n\nAs part of this project, our Data teams are working on migrating Data Products pipelines from Oracle workloads to Databricks. At EPAM, you will enter a dynamic, agile work environment, serving Fortune 1000 clients while adhering to best engineering practices. If you're a forward-thinking Scala expert with a passion for transforming data solutions, join us in Madrid, M\u00e1laga, or remotely across Spain to be at the forefront of innovation with EPAM!\n\nResponsibilities\n\n\nDevelop, monitor, and operate the most used and most critical curated data pipeline at client\u2019s - Sales Order Data (incl. post-order information, e.g. shipment, return, payment)\nConsulting with analysts, data scientists, and product managers to build and continuously improve \"Single Source of Truth\" KPI for business steering such as the central Profit Contribution measurement (PC II) \nRedevelop old legacy pipelines to new, advanced, and standard versions that are easy to maintain and scalable for future demands \nLeverage and improve a cloud-based tech stack that includes AWS, Databricks, Kubernetes, Spark, Airflow, Python, and Scala \n\n\nRequirements\n\n\nExpertise in Apache Spark along with Spark streaming \nGood hands-on experience with Databricks and delta-lake \nFluency in Scala programming language \nExpertise in SQL \nGood understanding & hands-on experience with CI/CD \nRich working experience with GitHub \nFluency working with AWS landscape \nAbility to build Apache Airflow pipelines \n\n\nNice to have\n\n\nPresto \nSuperset \nStarburst \nOracle & Exasol\n\n\nWe offer\n\n\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\n\n\nEPAM is a leading digital transformation services and product engineering company with over 52,650 EPAMers in more than 55 countries and regions. Since 1993, our multidisciplinary teams have been helping make the future real for our clients and communities around the world. In 2018, we opened an office in Spain that quickly grew to over 1,450 EPAMers distributed between the offices in M\u00e1laga and Madrid as well as remotely across the country. Here you will collaborate with multinational teams, contribute to numerous innovative projects, and have an opportunity to learn and grow continuously.\n\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nLead Data Engineer\nResponsibilities\nDevelop, monitor, and operate the most used and most critical curated data pipeline at client\u2019s - Sales Order Data (incl. post-order information, e.g. shipment, return, payment)\nConsulting with analysts, data scientists, and product managers to build and continuously improve \"Single Source of Truth\" KPI for business steering such as the central Profit Contribution measurement (PC II) \nRedevelop old legacy pipelines to new, advanced, and standard versions that are easy to maintain and scalable for future demands \nLeverage and improve a cloud-based tech stack that includes AWS, Databricks, Kubernetes, Spark, Airflow, Python, and Scala\nDevelop, monitor, and operate the most used and most critical curated data pipeline at client\u2019s - Sales Order Data (incl. post-order information, e.g. shipment, return, payment)\nConsulting with analysts, data scientists, and product managers to build and continuously improve \"Single Source of Truth\" KPI for business steering such as the central Profit Contribution measurement (PC II)\nRedevelop old legacy pipelines to new, advanced, and standard versions that are easy to maintain and scalable for future demands\nLeverage and improve a cloud-based tech stack that includes AWS, Databricks, Kubernetes, Spark, Airflow, Python, and Scala\nRequirements\nExpertise in Apache Spark along with Spark streaming \nGood hands-on experience with Databricks and delta-lake \nFluency in Scala programming language \nExpertise in SQL \nGood understanding & hands-on experience with CI/CD \nRich working experience with GitHub \nFluency working with AWS landscape \nAbility to build Apache Airflow pipelines\nExpertise in Apache Spark along with Spark streaming\nGood hands-on experience with Databricks and delta-lake\nFluency in Scala programming language\nExpertise in SQL\nGood understanding & hands-on experience with CI/CD\nRich working experience with GitHub\nFluency working with AWS landscape\nAbility to build Apache Airflow pipelines\nNice to have\nPresto \nSuperset \nStarburst \nOracle & Exasol\nPresto\nSuperset\nStarburst\nOracle & Exasol\nWe offer\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends."
    },
    "4165299100": {
        "title": "Senior Data Engineer",
        "company": "companies",
        "location": "Valencia, Valencian Community, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nJob DescriptionAre you a data expert with a passion for building scalable data solutions?\n\nAs a Senior Data Engineer at Stefanini, you will design, develop, and optimize modern data architectures, enabling advanced analytics and business intelligence capabilities.\n\nYou'll work with cutting-edge technologies, including Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory, while collaborating with cross-functional teams to deliver high-performance data solutions that drive business success.Key ResponsibilitiesData Architecture & Engineering: Design, build, and maintain robust data pipelines and architectures using Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory.Data Integration & ETL: Develop scalable ETL/ELT processes to extract, transform, and load data from various sources into cloud-based data platforms.Performance Optimization: Optimize data models, queries, and storage strategies for high-performance analytics and reporting.Big Data Processing: Work with large-scale datasets and distributed computing frameworks to support advanced analytics, machine learning, and business intelligence.Cloud Data Solutions: Design and implement data solutions leveraging cloud services, ensuring security, scalability, and cost-efficiency.Collaboration & Stakeholder Engagement: Partner with business analysts, data scientists, and BI teams to understand data needs and deliver efficient, scalable solutions.Data Governance & Security: Implement best practices for data governance, quality, lineage, and compliance with industry standards.Automation & CI/CD: Utilize DevOps principles to automate data workflows, testing, and deployment using CI/CD pipelines.Documentation & Maintenance: Maintain clear documentation of data pipelines, architecture, and processes for transparency and knowledge sharing.\n\nRequired Skills & ExperienceEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, Information Technology, or a related field.Experience: 5+ years of hands-on experience in data engineering, working with large-scale data processing and analytics.Technical Proficiency:Expertise in Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory for data engineering and analytics workloads.Strong skills in SQL for data modeling, querying, and performance tuning.Experience with Python or Scala for data processing and automation.Proficiency in ETL/ELT pipeline development and data integration across cloud platforms.Strong knowledge of data modeling, warehousing, and optimization techniques.Cloud Expertise: Proven experience with Azure cloud services and modern data architectures.Data Governance & Security: Understanding of best practices in data governance, security, compliance, and lineage tracking.\n\nNice to have:Big Data & Distributed Computing: Familiarity with Spark, Delta Lake, and Parquet for large-scale data processing.DevOps & Automation: Experience in CI/CD for data engineering, using tools like Terraform, Git, and Azure DevOps.Additional Tools: Knowledge of SAP BI, SAP Business Warehouse, and integration with cloud-based data platforms is a plus.Machine Learning & AI: Experience supporting machine learning workflows and feature engineering pipelines.Streaming Data & IoT: Familiarity with Kafka, Event Hubs, or real-time data processing is advantageous.API & Integration: Experience with REST APIs, GraphQL, and data API development is a plus.\n\nWhy Join Us?Work with cutting-edge data technologies to shape modern data solutions.Collaborate with a team of data-driven professionals on impactful projects.Gain exposure to AI, machine learning, and advanced analytics use cases.Enjoy a flexible, innovative, and growth-oriented work environment.Competitive salary, benefits, and continuous learning opportunities.\n\nThe preceding job description had been designed to indicate the general nature and level of work performed by employees within this classification.\n\nIt is not designed to contain or be interpreted as a comprehensive inventory of all duties and responsibilities required of employees assigned to this job.What We OfferYou will find here not only a challenging and interesting workplace, but also a rewarding work experience, with competitive compensation and benefit packages:Career development: opportunity to grow within the team;Meal card;Flexible working hours and work from home, aligned with project needs;Friendly team who is eager to meet you.\n\nWhat's NextIt's best to apply today, because job postings can be taken down and we wouldn't want you to miss this opportunity.\n\nIn case you need further information, just send us a message at ****** and we'll be happy to assist!Diversity & inclusionHere at the Stefanini Group, we value plurality and equity, regardless of race, sexual orientation, disability, age, ancestry, religion, gender, and nationality.\n\nWe understand and encourage the importance of being you!About UsWe are a Brazilian company with over 35 years of experience in delivering IT services worldwide, ranging from IT outsourcing to application development or IT staffing.\n\nWe have a direct presence in 41 countries, through our 70 offices located throughout the world.\n\nWe have managed to become the preferred partner of many small-to-midsize local and regional companies as well.\n\nMost of our clients come from industries such as financial services, manufacturing, telecommunications, chemical, services, technology, public sector and utilities.Stefanini has career opportunities locally and around the world for professionals interested in a vibrant, passionate, team-oriented workplace.\n\nIf you are a customer-centric person with a \"get it done\" attitude, come over for coffee and a talk on your future career with us!Learn more about us on www.stefanini.com and join us on LinkedIn , Facebook and Instagram where we regularly post insights from our colleagues."
    },
    "4165298156": {
        "title": "Senior Data Engineer",
        "company": "companies",
        "location": "Granada, Andalusia, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nJob DescriptionAre you a data expert with a passion for building scalable data solutions?\n\nAs a Senior Data Engineer at Stefanini, you will design, develop, and optimize modern data architectures, enabling advanced analytics and business intelligence capabilities.\n\nYou'll work with cutting-edge technologies, including Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory, while collaborating with cross-functional teams to deliver high-performance data solutions that drive business success.Key ResponsibilitiesData Architecture & Engineering: Design, build, and maintain robust data pipelines and architectures using Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory.Data Integration & ETL: Develop scalable ETL/ELT processes to extract, transform, and load data from various sources into cloud-based data platforms.Performance Optimization: Optimize data models, queries, and storage strategies for high-performance analytics and reporting.Big Data Processing: Work with large-scale datasets and distributed computing frameworks to support advanced analytics, machine learning, and business intelligence.Cloud Data Solutions: Design and implement data solutions leveraging cloud services, ensuring security, scalability, and cost-efficiency.Collaboration & Stakeholder Engagement: Partner with business analysts, data scientists, and BI teams to understand data needs and deliver efficient, scalable solutions.Data Governance & Security: Implement best practices for data governance, quality, lineage, and compliance with industry standards.Automation & CI/CD: Utilize DevOps principles to automate data workflows, testing, and deployment using CI/CD pipelines.Documentation & Maintenance: Maintain clear documentation of data pipelines, architecture, and processes for transparency and knowledge sharing.\n\nRequired Skills & ExperienceEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, Information Technology, or a related field.Experience: 5+ years of hands-on experience in data engineering, working with large-scale data processing and analytics.Technical Proficiency:Expertise in Databricks, Microsoft Fabric, Azure Synapse, and Azure Data Factory for data engineering and analytics workloads.Strong skills in SQL for data modeling, querying, and performance tuning.Experience with Python or Scala for data processing and automation.Proficiency in ETL/ELT pipeline development and data integration across cloud platforms.Strong knowledge of data modeling, warehousing, and optimization techniques.Cloud Expertise: Proven experience with Azure cloud services and modern data architectures.Data Governance & Security: Understanding of best practices in data governance, security, compliance, and lineage tracking.\n\nNice to have:Big Data & Distributed Computing: Familiarity with Spark, Delta Lake, and Parquet for large-scale data processing.DevOps & Automation: Experience in CI/CD for data engineering, using tools like Terraform, Git, and Azure DevOps.Additional Tools: Knowledge of SAP BI, SAP Business Warehouse, and integration with cloud-based data platforms is a plus.Machine Learning & AI: Experience supporting machine learning workflows and feature engineering pipelines.Streaming Data & IoT: Familiarity with Kafka, Event Hubs, or real-time data processing is advantageous.API & Integration: Experience with REST APIs, GraphQL, and data API development is a plus.\n\nWhy Join Us?Work with cutting-edge data technologies to shape modern data solutions.Collaborate with a team of data-driven professionals on impactful projects.Gain exposure to AI, machine learning, and advanced analytics use cases.Enjoy a flexible, innovative, and growth-oriented work environment.Competitive salary, benefits, and continuous learning opportunities.\n\nThe preceding job description had been designed to indicate the general nature and level of work performed by employees within this classification.\n\nIt is not designed to contain or be interpreted as a comprehensive inventory of all duties and responsibilities required of employees assigned to this job.What We OfferYou will find here not only a challenging and interesting workplace, but also a rewarding work experience, with competitive compensation and benefit packages:Career development: opportunity to grow within the team;Meal card;Flexible working hours and work from home, aligned with project needs;Friendly team who is eager to meet you.\n\nWhat's NextIt's best to apply today, because job postings can be taken down and we wouldn't want you to miss this opportunity.\n\nIn case you need further information, just send us a message at ****** and we'll be happy to assist!Diversity & inclusionHere at the Stefanini Group, we value plurality and equity, regardless of race, sexual orientation, disability, age, ancestry, religion, gender, and nationality.\n\nWe understand and encourage the importance of being you!About UsWe are a Brazilian company with over 35 years of experience in delivering IT services worldwide, ranging from IT outsourcing to application development or IT staffing.\n\nWe have a direct presence in 41 countries, through our 70 offices located throughout the world.\n\nWe have managed to become the preferred partner of many small-to-midsize local and regional companies as well.\n\nMost of our clients come from industries such as financial services, manufacturing, telecommunications, chemical, services, technology, public sector and utilities.Stefanini has career opportunities locally and around the world for professionals interested in a vibrant, passionate, team-oriented workplace.\n\nIf you are a customer-centric person with a \"get it done\" attitude, come over for coffee and a talk on your future career with us!Learn more about us on www.stefanini.com and join us on LinkedIn , Facebook and Instagram where we regularly post insights from our colleagues."
    },
    "4084883865": {
        "title": "Lead Data Software Engineer (Spark/Scala/Databricks) ",
        "company": "EPAM Systems",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nAre you an open-minded professional fluent in Scala programming language? If that sounds like you, this could be the perfect opportunity to join EPAM as a Lead Data Engineer. We are looking for a team player with excellent communication and organizational skills, mastery in engineering and a B2+/C1 level of English to communicate fluently with English-speaking stakeholders, share ideas and provide reasoning.\n\nAs part of this project, our Data teams are working on migrating Data Products pipelines from Oracle workloads to Databricks. At EPAM, you will enter a dynamic, agile work environment, serving Fortune 1000 clients while adhering to best engineering practices. If you're a forward-thinking Scala expert with a passion for transforming data solutions, join us in Madrid, M\u00e1laga, or remotely across Spain to be at the forefront of innovation with EPAM!\n\nResponsibilities\n\n\nDevelop, monitor, and operate the most used and most critical curated data pipeline at client\u2019s - Sales Order Data (incl. post-order information, e.g. shipment, return, payment)\nConsulting with analysts, data scientists, and product managers to build and continuously improve \"Single Source of Truth\" KPI for business steering such as the central Profit Contribution measurement (PC II) \nRedevelop old legacy pipelines to new, advanced, and standard versions that are easy to maintain and scalable for future demands \nLeverage and improve a cloud-based tech stack that includes AWS, Databricks, Kubernetes, Spark, Airflow, Python, and Scala \n\n\nRequirements\n\n\nExpertise in Apache Spark along with Spark streaming \nGood hands-on experience with Databricks and delta-lake \nFluency in Scala programming language \nExpertise in SQL \nGood understanding & hands-on experience with CI/CD \nRich working experience with GitHub \nFluency working with AWS landscape \nAbility to build Apache Airflow pipelines \n\n\nNice to have\n\n\nPresto \nSuperset \nStarburst \nOracle & Exasol\n\n\nWe offer\n\n\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\n\n\nEPAM is a leading digital transformation services and product engineering company with over 52,650 EPAMers in more than 55 countries and regions. Since 1993, our multidisciplinary teams have been helping make the future real for our clients and communities around the world. In 2018, we opened an office in Spain that quickly grew to over 1,450 EPAMers distributed between the offices in M\u00e1laga and Madrid as well as remotely across the country. Here you will collaborate with multinational teams, contribute to numerous innovative projects, and have an opportunity to learn and grow continuously.\n\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nLead Data Engineer\nResponsibilities\nDevelop, monitor, and operate the most used and most critical curated data pipeline at client\u2019s - Sales Order Data (incl. post-order information, e.g. shipment, return, payment)\nConsulting with analysts, data scientists, and product managers to build and continuously improve \"Single Source of Truth\" KPI for business steering such as the central Profit Contribution measurement (PC II) \nRedevelop old legacy pipelines to new, advanced, and standard versions that are easy to maintain and scalable for future demands \nLeverage and improve a cloud-based tech stack that includes AWS, Databricks, Kubernetes, Spark, Airflow, Python, and Scala\nDevelop, monitor, and operate the most used and most critical curated data pipeline at client\u2019s - Sales Order Data (incl. post-order information, e.g. shipment, return, payment)\nConsulting with analysts, data scientists, and product managers to build and continuously improve \"Single Source of Truth\" KPI for business steering such as the central Profit Contribution measurement (PC II)\nRedevelop old legacy pipelines to new, advanced, and standard versions that are easy to maintain and scalable for future demands\nLeverage and improve a cloud-based tech stack that includes AWS, Databricks, Kubernetes, Spark, Airflow, Python, and Scala\nRequirements\nExpertise in Apache Spark along with Spark streaming \nGood hands-on experience with Databricks and delta-lake \nFluency in Scala programming language \nExpertise in SQL \nGood understanding & hands-on experience with CI/CD \nRich working experience with GitHub \nFluency working with AWS landscape \nAbility to build Apache Airflow pipelines\nExpertise in Apache Spark along with Spark streaming\nGood hands-on experience with Databricks and delta-lake\nFluency in Scala programming language\nExpertise in SQL\nGood understanding & hands-on experience with CI/CD\nRich working experience with GitHub\nFluency working with AWS landscape\nAbility to build Apache Airflow pipelines\nNice to have\nPresto \nSuperset \nStarburst \nOracle & Exasol\nPresto\nSuperset\nStarburst\nOracle & Exasol\nWe offer\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends."
    },
    "4023098692": {
        "title": "Lead Data Engineer",
        "company": "Winning",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nAt Winning Consulting, we are on the lookout for Lead Data Engineer to be part of our dynamic team. This role involves working on an exciting project for one of our key clients in the industry sector, based in our Madrid office. (Hybrid or remote model)\n\nAs an AWS Migration Specialist, your responsibilities will be:\n\nLead cloud migration: Direct the migration of the BI platform and all on-premises processes to AWS, ensuring a smooth and efficient transition\nETL optimization and maintenance: Develop, optimize, and maintain ETL processes to improve data efficiency and availability\nProcess monitoring: Supervise daily processes to ensure their proper functioning, proactively identifying and resolving issues\nData management: Extract, transform, and load data from various sources to generate key insights that drive decision-making\n\nWhat We Are Looking For:\n\nExperience in migrating BI platforms and ETL processes to AWS\nExtensive knowledge and experience with AWS services (AWS S3, AWS Lambda, AWS Glue, AWS RDS, AWS CloudWatch, AWS Transfer)\nStrong background in databases and tools such as SQL Server, SSIS, Snowflake, and scripting experience with Python\nFamiliarity with version control and collaboration tools like Bitbucket, GitHub, Jira, and Confluence\nKnowledge of automation and deployment using Terraform and Jenkins\nExperience with API integration and tools like Mulesoft\n\nWhat We Offer:\n\nOpportunities for growth and professional development in a multinational environment\nWork in a dynamic and collaborative team with access to the latest technologies\nFlexible working arrangements, benefits, and an environment that fosters innovation\n\n\u00bfQui\u00e9nes somos?\n\nWinning Consulting es una empresa de consultor\u00eda que ofrece servicios de\n\nconsultor\u00eda, formaci\u00f3n, reclutamiento e investigaci\u00f3n. Apoyamos a nuestros clientes\n\nen la b\u00fasqueda de soluciones innovadoras y sostenibles, desde la aplicaci\u00f3n del\n\nconocimiento cient\u00edfico a la resoluci\u00f3n de problemas complejos de gesti\u00f3n hasta la\n\ntransformaci\u00f3n digital y tecnol\u00f3gica de las organizaciones.\n\nSi quieres saber m\u00e1s sobre nosotros, visita nuestra web https://www.winning-consulting.com/\n\nTodas las candidaturas se tratan de forma confidencial en virtud del GDPR. Al enviar su candidatura, acepta el tratamiento de su informaci\u00f3n\n\nen el contexto del reclutamiento y su inclusi\u00f3n en nuestra base de datos de candidatos. Si no consiente el tratamiento de estos datos, le\n\nrogamos que no presente su candidatura a este anuncio\n\nPowered by JazzHR\n\n9e6orwuBRb\nLead Data Engineer\nAWS Migration Specialist,\nLead cloud migration: Direct the migration of the BI platform and all on-premises processes to AWS, ensuring a smooth and efficient transition\nETL optimization and maintenance: Develop, optimize, and maintain ETL processes to improve data efficiency and availability\nProcess monitoring: Supervise daily processes to ensure their proper functioning, proactively identifying and resolving issues\nData management: Extract, transform, and load data from various sources to generate key insights that drive decision-making\nLead cloud migration: Direct the migration of the BI platform and all on-premises processes to AWS, ensuring a smooth and efficient transition\nETL optimization and maintenance: Develop, optimize, and maintain ETL processes to improve data efficiency and availability\nProcess monitoring: Supervise daily processes to ensure their proper functioning, proactively identifying and resolving issues\nData management: Extract, transform, and load data from various sources to generate key insights that drive decision-making\nWhat We Are Looking For:\nExperience in migrating BI platforms and ETL processes to AWS\nExtensive knowledge and experience with AWS services (AWS S3, AWS Lambda, AWS Glue, AWS RDS, AWS CloudWatch, AWS Transfer)\nStrong background in databases and tools such as SQL Server, SSIS, Snowflake, and scripting experience with Python\nFamiliarity with version control and collaboration tools like Bitbucket, GitHub, Jira, and Confluence\nKnowledge of automation and deployment using Terraform and Jenkins\nExperience with API integration and tools like Mulesoft\nExperience in migrating BI platforms and ETL processes to AWS\nExtensive knowledge and experience with AWS services (AWS S3, AWS Lambda, AWS Glue, AWS RDS, AWS CloudWatch, AWS Transfer)\nStrong background in databases and tools such as SQL Server, SSIS, Snowflake, and scripting experience with Python\nFamiliarity with version control and collaboration tools like Bitbucket, GitHub, Jira, and Confluence\nKnowledge of automation and deployment using Terraform and Jenkins\nExperience with API integration and tools like Mulesoft\nWhat We Offer:\nOpportunities for growth and professional development in a multinational environment\nWork in a dynamic and collaborative team with access to the latest technologies\nFlexible working arrangements, benefits, and an environment that fosters innovation\nOpportunities for growth and professional development in a multinational environment\nWork in a dynamic and collaborative team with access to the latest technologies\nFlexible working arrangements, benefits, and an environment that fosters innovation\n\u00bfQui\u00e9nes somos?"
    },
    "4121414080": {
        "title": "Senior Data Engineer & BI",
        "company": "Logicalis Spain",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEn Logicalis Spain estamos buscando un perfil experto en iniciativas de Data Engineering / BI sobre entornos Cloud (Azure y AWS; deseable GCP, Snowflake y/o Databricks) dentro de nuestra unidad de negocio de Data & Analytics, ubicada en las oficinas de Madrid o Barcelona.\n\nEl equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\n\nFunciones y Responsabilidades\n\nComprender las necesidades y requisitos de los clientes y elaborar \u00edntegramente propuestas asociadas a proyectos de Data Engineering / BI para cubrir el ciclo de vida de los datos (Ingesta, Almacenamiento, Transformaci\u00f3n, Explotaci\u00f3n) bajo los paradigmas actuales del mercado. Deseable haber trabajado en entornos de datos con metodolog\u00edas de modelado DataVault y arquitecturas DataMesh.\nResponsabilizarse del enfoque de la soluci\u00f3n que mejor puede dar respuesta a las problem\u00e1ticas identificadas y mayores beneficios puede proporcionar, en oportunidades relativas a Integraci\u00f3n de Datos, Modelizaci\u00f3n y Explotaci\u00f3n.\nAcompa\u00f1ar en el dise\u00f1o de propuestas de migraci\u00f3n/modernizaci\u00f3n desde entornos on-premise a entornos Cloud, de la plataforma de datos que pueda tener el cliente.\nDefensa ante los clientes de propuestas y planteamientos.\nPlanteamiento de artefactos y componentes que industrialicen los trabajos de Data Engineering / BI (algoritmos reutilizables, c\u00f3digo en base a metadatos, procesos autom\u00e1ticos de verificaci\u00f3n de calidad, etc).\nAcompa\u00f1ar en la evoluci\u00f3n el Demo Center para contemplar prototipos y demos sobre aplicaciones de datos en cloud.\nAcompa\u00f1ar a los proyectos en las fases iniciales para garantizar y certificar el planteamiento de los proyectos que se realiz\u00f3 en fase de preventa, y ayudar en la resoluci\u00f3n de riesgos o problemas.\n\nBENEFICIOS\n\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n\n> Modalidad de trabajo remoto, con flexibilidad para ir puntualmente a nuestras oficinas/clientes ubicados en Madrid o Barcelona.\n\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n\n> Seguro m\u00e9dico y GYMPASS.\n\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n\n> Portal de descuentos especiales para empleados.\nLogicalis Spain\nexperto en iniciativas de Data Engineering / BI\nData & Analytics de Logicalis\nFunciones y Responsabilidades\nComprender las necesidades y requisitos de los clientes y elaborar \u00edntegramente propuestas asociadas a proyectos de Data Engineering / BI para cubrir el ciclo de vida de los datos (Ingesta, Almacenamiento, Transformaci\u00f3n, Explotaci\u00f3n) bajo los paradigmas actuales del mercado. Deseable haber trabajado en entornos de datos con metodolog\u00edas de modelado DataVault y arquitecturas DataMesh.\nResponsabilizarse del enfoque de la soluci\u00f3n que mejor puede dar respuesta a las problem\u00e1ticas identificadas y mayores beneficios puede proporcionar, en oportunidades relativas a Integraci\u00f3n de Datos, Modelizaci\u00f3n y Explotaci\u00f3n.\nAcompa\u00f1ar en el dise\u00f1o de propuestas de migraci\u00f3n/modernizaci\u00f3n desde entornos on-premise a entornos Cloud, de la plataforma de datos que pueda tener el cliente.\nDefensa ante los clientes de propuestas y planteamientos.\nPlanteamiento de artefactos y componentes que industrialicen los trabajos de Data Engineering / BI (algoritmos reutilizables, c\u00f3digo en base a metadatos, procesos autom\u00e1ticos de verificaci\u00f3n de calidad, etc).\nAcompa\u00f1ar en la evoluci\u00f3n el Demo Center para contemplar prototipos y demos sobre aplicaciones de datos en cloud.\nAcompa\u00f1ar a los proyectos en las fases iniciales para garantizar y certificar el planteamiento de los proyectos que se realiz\u00f3 en fase de preventa, y ayudar en la resoluci\u00f3n de riesgos o problemas.\nComprender las necesidades y requisitos de los clientes y elaborar \u00edntegramente propuestas asociadas a proyectos de Data Engineering / BI para cubrir el ciclo de vida de los datos (Ingesta, Almacenamiento, Transformaci\u00f3n, Explotaci\u00f3n) bajo los paradigmas actuales del mercado. Deseable haber trabajado en entornos de datos con metodolog\u00edas de modelado DataVault y arquitecturas DataMesh.\nResponsabilizarse del enfoque de la soluci\u00f3n que mejor puede dar respuesta a las problem\u00e1ticas identificadas y mayores beneficios puede proporcionar, en oportunidades relativas a Integraci\u00f3n de Datos, Modelizaci\u00f3n y Explotaci\u00f3n.\nAcompa\u00f1ar en el dise\u00f1o de propuestas de migraci\u00f3n/modernizaci\u00f3n desde entornos on-premise a entornos Cloud, de la plataforma de datos que pueda tener el cliente.\nDefensa ante los clientes de propuestas y planteamientos.\nPlanteamiento de artefactos y componentes que industrialicen los trabajos de Data Engineering / BI (algoritmos reutilizables, c\u00f3digo en base a metadatos, procesos autom\u00e1ticos de verificaci\u00f3n de calidad, etc).\nAcompa\u00f1ar en la evoluci\u00f3n el Demo Center para contemplar prototipos y demos sobre aplicaciones de datos en cloud.\nAcompa\u00f1ar a los proyectos en las fases iniciales para garantizar y certificar el planteamiento de los proyectos que se realiz\u00f3 en fase de preventa, y ayudar en la resoluci\u00f3n de riesgos o problemas.\nBENEFICIOS"
    },
    "4142386276": {
        "title": "Ingeniero de Datos de Procesos de negocios Celonis ",
        "company": "Inetum",
        "location": "Talavera de la Reina, Castile-La Mancha, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nMission\n\nEn Inetum buscamos ampliar nuestro equipo de Process Mining. Perfiles con experiencia en ETL y visualizaci\u00f3n de Datos, si adicionalmente aportas experiencia en Celonis, estamos deseando conocerte.\n\nTrabajar\u00e1s en proyectos de miner\u00eda de procesos con Celonis como ingeniero de datos en un proyecto de larga duraci\u00f3n. Se proporcionar\u00e1 formaci\u00f3n en Celonis a los candidatos seleccionados.\n\nModalidad de trabajo h\u00edbrida, presencialidad 2-3 d\u00edas en oficina del cliente en Talavera de la Reina (Toledo).\n\nPerfil\n\nRequisitos Obligatorios\n\nNivel de SQL: Medio-Alto.\nM\u00ednimo 12 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\n\n\nRequisito deseables\n\nGrado o titulaci\u00f3n media en Ingenier\u00eda T\u00e9cnica, Inform\u00e1tica, Telecomunicaciones, Matem\u00e1ticas, F\u00edsica o afines.\nConocimientos de scripting (preferiblemente Python).\nCapacidades anal\u00edticas.\nConocimientos en procesos, miner\u00eda de procesos y mejora de procesos.\nConocimiento avanzado de t\u00e9cnicas de visualizaci\u00f3n y generaci\u00f3n de informes en Power BI.\n60 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\n\n\nBeneficios\n\n\ud83d\ude4b\u200d\u2640\ufe0f\ud83d\udcbb Formar\u00e1s parte de un gran equipo de profesionales con inquietud y motivaci\u00f3n por la tecnolog\u00eda.\n\n\u23f1 Trabajar\u00e1s a jornada completa con un horario flexible y modalidad h\u00edbrida con teletrabajo real.\n\n\ud83d\udc69\ud83c\udffc\u200d\ud83c\udf93 Formaci\u00f3n por parte de la empresa para que puedas seguir desarroll\u00e1ndote y promocionar dentro del plan de carrera que existe para ti.\n\n\ud83c\udfae Podr\u00e1s asistir a eventos y conferencias relevantes del sector.\n\n\u270d\ud83c\udffc Contrato indefinido.\n\n\ud83d\ude04 Estabilidad y buen clima laboral.\n\n\ud83d\udcb0 Salario Competitivo.\n\n\ud83c\udd93 Acceso a ventajas del grupo de empresa\n\n\ud83d\udcb5 Retribuci\u00f3n flexible y m\u00e1s beneficios\n\nOrganizaci\u00f3n\n\nINETUM\n\nSomos un Consultora digital, internacional y \u00e1gil. En la era de la post-transformaci\u00f3n digital, nos esforzamos para que cada uno de nuestros 28.000 profesionales pueda renovarse continuamente.\n\nCada uno de ellos puede dise\u00f1ar su itinerario profesional de acuerdo a sus preferencias, emprender junto a sus clientes para construir en la pr\u00e1ctica un mundo m\u00e1s positivo, innovar en cada uno de los 27 pa\u00edses y conciliar su carrera profesional con su bienestar personal.\n\nNuestros 28.000 atletas digitales est\u00e1n orgullosos de haberse certificado Top Employer Europe 2024.\nMission\nPerfil\nRequisitos Obligatorios\nNivel de SQL: Medio-Alto.\nM\u00ednimo 12 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\nNivel de SQL: Medio-Alto.\nM\u00ednimo 12 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\nRequisito deseables\nGrado o titulaci\u00f3n media en Ingenier\u00eda T\u00e9cnica, Inform\u00e1tica, Telecomunicaciones, Matem\u00e1ticas, F\u00edsica o afines.\nConocimientos de scripting (preferiblemente Python).\nCapacidades anal\u00edticas.\nConocimientos en procesos, miner\u00eda de procesos y mejora de procesos.\nConocimiento avanzado de t\u00e9cnicas de visualizaci\u00f3n y generaci\u00f3n de informes en Power BI.\n60 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\nGrado o titulaci\u00f3n media en Ingenier\u00eda T\u00e9cnica, Inform\u00e1tica, Telecomunicaciones, Matem\u00e1ticas, F\u00edsica o afines.\nConocimientos de scripting (preferiblemente Python).\nCapacidades anal\u00edticas.\nConocimientos en procesos, miner\u00eda de procesos y mejora de procesos.\nConocimiento avanzado de t\u00e9cnicas de visualizaci\u00f3n y generaci\u00f3n de informes en Power BI.\n60 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\nBeneficios\nOrganizaci\u00f3n\nINETUM\nSomos un Consultora digital, internacional y \u00e1gil. En la era de la post-transformaci\u00f3n digital, nos esforzamos para que cada uno de nuestros 28.000 profesionales pueda renovarse continuamente.\nCada uno de ellos puede dise\u00f1ar su itinerario profesional de acuerdo a sus preferencias, emprender junto a sus clientes para construir en la pr\u00e1ctica un mundo m\u00e1s positivo, innovar en cada uno de los 27 pa\u00edses y conciliar su carrera profesional con su bienestar personal.\nNuestros 28.000 atletas digitales est\u00e1n orgullosos de haberse certificado Top Employer Europe 2024."
    },
    "4173759651": {
        "title": "Big Data Engineer with IFRS 15 experience ",
        "company": "W3Global",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nPlease share CVs with the below JD. Immediate joiners preferred.\n\nJob Title: Big Data Development Engineer with IFRS 15 experience\n\nPosition Overview\n\nWe are seeking a skilled Big Data Engineer with specialized knowledge in IFRS 15 to join our team. The successful candidate will be responsible for designing, developing, and implementing systems and processes using Scala, spark, python, SQL, GCP(cloud storage, dataproc, cloud function) that ensure compliance with IFRS 15 revenue recognition standards. This role requires a strong understanding of both data engineering principles and financial reporting standards.\n\nKey Responsibilities\n\n System Development: Design and develop software solutions that facilitate accurate revenue recognition in compliance with IFRS 15.\n Technology : Scala, spark, python, SQL, GCP(cloud storage, dataproc, cloud function), linux. Git and Jenkin as CICD\n Process Implementation: Establish and implement processes to ensure ongoing adherence to IFRS 15 standards across all relevant projects.\n Collaboration: Work closely with cross-functional teams, including finance, accounting, and IT, to integrate IFRS 15 requirements into existing systems and workflows.\n Testing , Documentation\n Continuous Improvement: Stay updated with changes in IFRS 15 standards and proactively adjust systems and processes to maintain compliance.\n\nQualifications\n\n Education: Bachelor's degree in Engineering, Computer Science, Finance, or a related field.\n Experience: Minimum of 3 years of experience in system development with a focus on financial reporting standards, preferably IFRS 15.\n Technical Skills: Proficiency in software development, including experience with relevant programming languages and tools.\n\nPython ,Spark ,Scala\n\nGCP : GCP Cloud Storage , Dataproc,Cloud Function:\n\nWEB: Angular 1.5.8 como front-end\n\nPHP 8.3 \u00b4+ Laravel 9 como back- end\n\nETL (Datahub + Centralizador)\n\nDatabase MySQL 8.0\n\n Analytical Skills: Strong analytical and problem-solving abilities, with attention to detail.\n Communication Skills: Excellent verbal and written communication skills, with the ability to convey complex information clearly.\n Team Collaboration: Proven ability to work effectively in a collaborative team environment.\n Adaptability: Ability to adapt to changing regulations and proactively implement necessary changes.\nImmediate joiners preferred.\nPosition Overview\nKey Responsibilities\nSystem Development: Design and develop software solutions that facilitate accurate revenue recognition in compliance with IFRS 15.\n Technology : Scala, spark, python, SQL, GCP(cloud storage, dataproc, cloud function), linux. Git and Jenkin as CICD\n Process Implementation: Establish and implement processes to ensure ongoing adherence to IFRS 15 standards across all relevant projects.\n Collaboration: Work closely with cross-functional teams, including finance, accounting, and IT, to integrate IFRS 15 requirements into existing systems and workflows.\n Testing , Documentation\n Continuous Improvement: Stay updated with changes in IFRS 15 standards and proactively adjust systems and processes to maintain compliance.\nSystem Development: Design and develop software solutions that facilitate accurate revenue recognition in compliance with IFRS 15.\nTechnology : Scala, spark, python, SQL, GCP(cloud storage, dataproc, cloud function), linux. Git and Jenkin as CICD\nProcess Implementation: Establish and implement processes to ensure ongoing adherence to IFRS 15 standards across all relevant projects.\nCollaboration: Work closely with cross-functional teams, including finance, accounting, and IT, to integrate IFRS 15 requirements into existing systems and workflows.\nTesting , Documentation\nContinuous Improvement: Stay updated with changes in IFRS 15 standards and proactively adjust systems and processes to maintain compliance.\nQualifications\nEducation: Bachelor's degree in Engineering, Computer Science, Finance, or a related field.\n Experience: Minimum of 3 years of experience in system development with a focus on financial reporting standards, preferably IFRS 15.\n Technical Skills: Proficiency in software development, including experience with relevant programming languages and tools.\nEducation: Bachelor's degree in Engineering, Computer Science, Finance, or a related field.\nExperience: Minimum of 3 years of experience in system development with a focus on financial reporting standards, preferably IFRS 15.\nTechnical Skills: Proficiency in software development, including experience with relevant programming languages and tools.\nGCP : GCP Cloud Storage , Dataproc,Cloud Function:\nWEB: Angular 1.5.8 como front-end\nPHP 8.3 \u00b4+ Laravel 9 como back- end\nETL (Datahub + Centralizador)\nDatabase MySQL 8.0\nAnalytical Skills: Strong analytical and problem-solving abilities, with attention to detail.\n Communication Skills: Excellent verbal and written communication skills, with the ability to convey complex information clearly.\n Team Collaboration: Proven ability to work effectively in a collaborative team environment.\n Adaptability: Ability to adapt to changing regulations and proactively implement necessary changes.\nAnalytical Skills: Strong analytical and problem-solving abilities, with attention to detail.\nCommunication Skills: Excellent verbal and written communication skills, with the ability to convey complex information clearly.\nTeam Collaboration: Proven ability to work effectively in a collaborative team environment.\nAdaptability: Ability to adapt to changing regulations and proactively implement necessary changes."
    },
    "4137879692": {
        "title": "Data Engineer Python (Middle) ID27950",
        "company": "AgileEngine",
        "location": "Valencia, Valencian Community, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nAgileEngine is one of the Inc. 5000 fastest-growing companies in the U and a top-3 ranked dev shop according to Clutch. We create award-winning custom software solutions that help companies across 15+ industries change the lives of millions\n\nIf you like a challenging environment where you\u2019re working with the best and are encouraged to learn and experiment every day, there\u2019s no better place - guaranteed! :)\n\nWhat you will do\n\nBuild data pipelines using NiFi (ETL flows) to connect different sources of data together; \nBuild data reports and optimize SQL queries; \nProduce high-quality code; \nIdentify performance bottlenecks; \nCommunicate with Engineering & Product management; \nAlign with existing development teams. \n\nMust haves\n\n3+ years of experience in software development; \nBS in Computer Science or equivalent practical experience; \nStrong analytical SQL knowledge; \nPractical experience building Data Warehouse(s) with Snowflake or other DW-oriented databases (Google BigQuery, Redshift, etc.); \nPractical experience building ETL/ELT data pipelines from scratch or using existing tools like Apache NiFi; \nPractical experience with Python; \nAmazon RDS experience; \nWorking knowledge of Docker; \nUpper-intermediate English level. \n\nThe benefits of joining us\n\nProfessional growth \n\nAccelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps.\n\nCompetitive compensation \n\nWe match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities.\n\nA selection of exciting projects \n\nJoin projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands.\n\nFlextime \n\nTailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office \u2013 whatever makes you the happiest and most productive.\n\nThe next steps of your journey will be shared via email within a few hours. Please check your inbox regularly and watch for updates from our Internal Applicant site, LaunchPod, which will guide you through the process.\nBuild data pipelines using NiFi (ETL flows) to connect different sources of data together; \nBuild data reports and optimize SQL queries; \nProduce high-quality code; \nIdentify performance bottlenecks; \nCommunicate with Engineering & Product management; \nAlign with existing development teams.\nBuild data pipelines using NiFi (ETL flows) to connect different sources of data together;\nBuild data reports and optimize SQL queries;\nProduce high-quality code;\nIdentify performance bottlenecks;\nCommunicate with Engineering & Product management;\nAlign with existing development teams.\n3+ years of experience in software development; \nBS in Computer Science or equivalent practical experience; \nStrong analytical SQL knowledge; \nPractical experience building Data Warehouse(s) with Snowflake or other DW-oriented databases (Google BigQuery, Redshift, etc.); \nPractical experience building ETL/ELT data pipelines from scratch or using existing tools like Apache NiFi; \nPractical experience with Python; \nAmazon RDS experience; \nWorking knowledge of Docker; \nUpper-intermediate English level.\n3+ years of experience in software development;\nBS in Computer Science or equivalent practical experience;\nStrong analytical SQL knowledge;\nPractical experience building Data Warehouse(s) with Snowflake or other DW-oriented databases (Google BigQuery, Redshift, etc.);\nPractical experience building ETL/ELT data pipelines from scratch or using existing tools like Apache NiFi;\nPractical experience with Python;\nAmazon RDS experience;\nWorking knowledge of Docker;\nUpper-intermediate English level.\nProfessional growth\nCompetitive compensation\nA selection of exciting projects\nFlextime"
    },
    "4173921827": {
        "title": "Data Driven Solutions Engineer ",
        "company": "Allianz Partners",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nAllianz Partners is part of Allianz Group and offers B2B2C solutions in the lines of business Travel, Assistance, Mobility & Health. Allianz Partners supports its clients by providing global integrated insurance & service solutions for their customers and employees, based on a deep understanding of the business partners\u2019 value chain. Our ambition is to set the global B2B2C market standard for liaising with global business partners to develop integrated solutions combining insurance, assistance and technology.\n\nA Data Driven Solutions Engineer is responsible for designing, implementing and testing data-driven tools for underwriting and pricing. They design tools in collaboration with respective Pricing Teams, Case Underwriting Departments as well as central functions like the Global Pricing Team and according to AZ Group standards. The process of tool creation includes: defining the way all functionalities work, prototyping the tool, testing the functionalities, assisting users in case of problems, managing the final solution. In their daily work as a Data Driven Solutions Engineer, they use tools for programming, prototyping, process visualization and other. In particular, they assist case underwriters and pricing actuaries in the implementation of tariffs in the tools.\n\nWhat You Do\n\nDevelopment of web tools based on the requirements jointly developed with business experts\nCreation and execution of tests before and after deployment of the solutions into production\nTool documentation and maintenance\nUser onboarding and guidance\nSupporting colleagues and sharing expertise with the underwriting and pricing community in Allianz Partners and Allianz Group\nBuilding and optimizing data and analytics pipelines, with a focus on scalability and efficiency\nWorking closely with data scientists on integrating data analytics and machine learning solutions into the tools\nCommunicating findings and insights to both technical and non-technical stakeholders through clear and concise presentations\n\nWhat You Bring\n\nTech skills like Java, VBA, Python\nImplementation of pragmatic and workable solutions\nExperience in overseeing the directions, development and implementation of software solutions\nApplication of knowledge to specific deliverables (interaction of actuarial decisions with overall business)\nUnderstand underwriting in relevant area of business\nPricing tools (internally developed tools, tools available on the market or from AZ Group)\nWillingness to take over responsibility, develop concepts and drive models into implementation\nExperience in working with agile methodologies in a fast-paced, dynamic, and international environment\nIntercultural sensitivity and international experience\nHighly developed interpersonal skills (i.e., communication, presentation, and negotiation also with non-technical audiences)\n\nWhat We Offer\n\nOur employees play an integral part in our success as a business. We appreciate that each of our employees are unique and have unique needs, ambitions and we enjoy being a part of their journey.\n\nWe are there to empower and encourage you with your personal and professional development ensuring \u202fthat you take control\u202fby offering\u202fa large variety of courses and targeted development programs.\u202fAll that in a global environment where international mobility and career progression are encouraged.\n\nCaring for your health and wellbeing is key priority for us. This is why we build Work Well programs to providing you with peace of mind and give the flexibility in planning and arranging for a better work-life balance.\n\n66691 | Actuarial | Professional | Non-Executive | Allianz Partners | Full-Time | Permanent\n\nAllianz Group is one of the most trusted insurance and asset management companies in the world. Caring for our employees, their ambitions, dreams and challenges, is what makes us a unique employer. Together we can build an environment where everyone feels empowered and has the confidence to explore, to grow and to shape a better future for our customers and the world around us.\n\nWe at Allianz believe in a diverse and inclusive workforce and are proud to be an equal opportunity employer. We encourage you to bring your whole self to work, no matter where you are from, what you look like, who you love or what you believe in.\n\nWe therefore welcome applications regardless of ethnicity or cultural background, age, gender, nationality, religion, disability or sexual orientation.\n\nJoin us. Let's care for tomorrow.\nWhat You Do\nDevelopment of web tools based on the requirements jointly developed with business experts\nCreation and execution of tests before and after deployment of the solutions into production\nTool documentation and maintenance\nUser onboarding and guidance\nSupporting colleagues and sharing expertise with the underwriting and pricing community in Allianz Partners and Allianz Group\nBuilding and optimizing data and analytics pipelines, with a focus on scalability and efficiency\nWorking closely with data scientists on integrating data analytics and machine learning solutions into the tools\nCommunicating findings and insights to both technical and non-technical stakeholders through clear and concise presentations\nDevelopment of web tools based on the requirements jointly developed with business experts\nCreation and execution of tests before and after deployment of the solutions into production\nTool documentation and maintenance\nUser onboarding and guidance\nSupporting colleagues and sharing expertise with the underwriting and pricing community in Allianz Partners and Allianz Group\nBuilding and optimizing data and analytics pipelines, with a focus on scalability and efficiency\nWorking closely with data scientists on integrating data analytics and machine learning solutions into the tools\nCommunicating findings and insights to both technical and non-technical stakeholders through clear and concise presentations\nWhat You Bring\nTech skills like Java, VBA, Python\nImplementation of pragmatic and workable solutions\nExperience in overseeing the directions, development and implementation of software solutions\nApplication of knowledge to specific deliverables (interaction of actuarial decisions with overall business)\nUnderstand underwriting in relevant area of business\nPricing tools (internally developed tools, tools available on the market or from AZ Group)\nWillingness to take over responsibility, develop concepts and drive models into implementation\nExperience in working with agile methodologies in a fast-paced, dynamic, and international environment\nIntercultural sensitivity and international experience\nHighly developed interpersonal skills (i.e., communication, presentation, and negotiation also with non-technical audiences)\nTech skills like Java, VBA, Python\nImplementation of pragmatic and workable solutions\nExperience in overseeing the directions, development and implementation of software solutions\nApplication of knowledge to specific deliverables (interaction of actuarial decisions with overall business)\nUnderstand underwriting in relevant area of business\nPricing tools (internally developed tools, tools available on the market or from AZ Group)\nWillingness to take over responsibility, develop concepts and drive models into implementation\nExperience in working with agile methodologies in a fast-paced, dynamic, and international environment\nIntercultural sensitivity and international experience\nHighly developed interpersonal skills (i.e., communication, presentation, and negotiation also with non-technical audiences)\nWhat We Offer"
    },
    "4139942436": {
        "title": "Data Engineer \u2013 European Institution - MAIA",
        "company": "TheWhiteam",
        "location": "Tres Cantos, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nMinimum Postsecondary Education And Experience In ICT\n\nAt least 3 years of studies attested by a diploma, and a minimum of 5 years postsecondary professional experience\n\nMin. Professional Experience In Profile\n\n2 years\n\nMinimum Certifications\n\nAny data engineer certification, in a domain directly related to the request in question.\nMinimum Postsecondary Education And Experience In ICT\nAt least 3 years of studies attested by a diploma, and a minimum of 5 years postsecondary professional experience\nMin. Professional Experience In Profile\n2 years\nMinimum Certifications\nAny data engineer certification, in a domain directly related to the request in question."
    },
    "4150584797": {
        "title": "Senior Data Engineer ",
        "company": "Philip Morris International",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nMAKE HISTORY WITH US!\n\nAt PMI, we\u2019ve chosen to do something incredible. We\u2019re totally transforming our business and building our future on smoke-free products with the power to deliver a smoke-free future.\n\nWith huge change, comes huge opportunity. So, wherever you join us, you\u2019ll enjoy the freedom to dream up and deliver better, brighter solutions and the space to move your career forward in endlessly different directions.\n\nRoles And Responsibilities\n\n Setting data engineering standards and best practices.\n 3-5 years\u2019 experience working in data warehouse projects\n Overseeing the architecture and development of complex data systems.\n Leading large-scale data integration projects.\n Knowledge of data quality, data lineage, and compliance requirements.\n Effectively communicating technical details to non-technical stakeholders.\n Responsible for designing automated pipelines that handle the ETL of data from various sources into storage systems.\n Manage a DWH (Snowflake), ensuring it's optimized for performance and data integrity.\n Integrating data from diverse sources, such as APIs, DBs, and flat files, is a key responsibility. Ensure smooth integration and consistent data quality.\n\nRequired Skills\n\n ETL Processes: Proficiency in designing and implementing ETL (Extract, Transform, Load) workflows.\n Data Warehousing: Deep understanding of Snowflake\u2019s architecture, including data type, collation, virtual warehouses, micro-partitions, and clustering.\n SQL: Advanced SQL skills for querying, data manipulation, and performance tuning.\n Basic AWS Services: Understanding of EC2, Lambda, and other relevant services for data engineering.\n Terraform: For deploying AWS resources\n AWS EMR, Apache Spark \u2013 big data workloads\n Python: For scripting, automation, and data manipulation.\n Experience in creating pipelines using established CI/CD frameworks in cloud (e.g. AWS specific CICD skills that includes Bitbucket, Jenkins, ...)\n\nDegree Requirements\n\n At least a bachelor\u2019s degree in computer science, information technology, or a related field.\n\nWHAT WE OFFER YOU?\n\nPrivate medical and dental care, life insurance\nLunch Sodexo card to be used in the canteen or outside\nOffice or hybrid working model with flexible working arrangements\nEmployee pension plan\nMultisport program\nCafeteria with various benefits\nFree bike and car parking for all employees.\n\nPlease note that only on-line applications will be taken into consideration.\n\n7603\nRoles And Responsibilities\nSetting data engineering standards and best practices.\n 3-5 years\u2019 experience working in data warehouse projects\n Overseeing the architecture and development of complex data systems.\n Leading large-scale data integration projects.\n Knowledge of data quality, data lineage, and compliance requirements.\n Effectively communicating technical details to non-technical stakeholders.\n Responsible for designing automated pipelines that handle the ETL of data from various sources into storage systems.\n Manage a DWH (Snowflake), ensuring it's optimized for performance and data integrity.\n Integrating data from diverse sources, such as APIs, DBs, and flat files, is a key responsibility. Ensure smooth integration and consistent data quality.\nSetting data engineering standards and best practices.\n3-5 years\u2019 experience working in data warehouse projects\nOverseeing the architecture and development of complex data systems.\nLeading large-scale data integration projects.\nKnowledge of data quality, data lineage, and compliance requirements.\nEffectively communicating technical details to non-technical stakeholders.\nResponsible for designing automated pipelines that handle the ETL of data from various sources into storage systems.\nManage a DWH (Snowflake), ensuring it's optimized for performance and data integrity.\nIntegrating data from diverse sources, such as APIs, DBs, and flat files, is a key responsibility. Ensure smooth integration and consistent data quality.\nRequired Skills\nETL Processes: Proficiency in designing and implementing ETL (Extract, Transform, Load) workflows.\n Data Warehousing: Deep understanding of Snowflake\u2019s architecture, including data type, collation, virtual warehouses, micro-partitions, and clustering.\n SQL: Advanced SQL skills for querying, data manipulation, and performance tuning.\n Basic AWS Services: Understanding of EC2, Lambda, and other relevant services for data engineering.\n Terraform: For deploying AWS resources\n AWS EMR, Apache Spark \u2013 big data workloads\n Python: For scripting, automation, and data manipulation.\n Experience in creating pipelines using established CI/CD frameworks in cloud (e.g. AWS specific CICD skills that includes Bitbucket, Jenkins, ...)\nETL Processes: Proficiency in designing and implementing ETL (Extract, Transform, Load) workflows.\nData Warehousing: Deep understanding of Snowflake\u2019s architecture, including data type, collation, virtual warehouses, micro-partitions, and clustering.\nSQL: Advanced SQL skills for querying, data manipulation, and performance tuning.\nBasic AWS Services: Understanding of EC2, Lambda, and other relevant services for data engineering.\nTerraform: For deploying AWS resources\nAWS EMR, Apache Spark \u2013 big data workloads\nPython: For scripting, automation, and data manipulation.\nExperience in creating pipelines using established CI/CD frameworks in cloud (e.g. AWS specific CICD skills that includes Bitbucket, Jenkins, ...)\nDegree Requirements\nAt least a bachelor\u2019s degree in computer science, information technology, or a related field.\nWHAT WE OFFER YOU?\nPrivate medical and dental care, life insurance\nLunch Sodexo card to be used in the canteen or outside\nOffice or hybrid working model with flexible working arrangements\nEmployee pension plan\nMultisport program\nCafeteria with various benefits\nFree bike and car parking for all employees.\nPrivate medical and dental care, life insurance\nLunch Sodexo card to be used in the canteen or outside\nOffice or hybrid working model with flexible working arrangements\nEmployee pension plan\nMultisport program\nCafeteria with various benefits\nFree bike and car parking for all employees.\n7603"
    },
    "4167174263": {
        "title": "Data Center Junior Operations Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Zaragoza, Aragon, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Junior Operations Engineer to join our Spain-Portugal team, working directly with our Data Center Site Manager and focusing on supporting the site operations team.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.\n\nKey Responsibilities Include but are not limited to:Support Management in Maintenance, Operations, Projects, Security and Cleaning of the assigned facilities.Provide support in the supervision of the execution of services at a technical level.Provide support in personnel management (training, selection, labor relations).Provide support in the supervision of compliance with internal policies and procedures in Risk Prevention, Health and Safety and Quality.Provide support in the supervision of the methodologies implemented.Provide support in the preparation of budgets.Provide support in the generation of reports.Skills amp; RequirementsEducation / ExperienceBachelor#39;s degreeValuable financial knowledgeSpecific Skills / AbilitiesAdvanced Microsoft Office 365 (Power BI, Word, Excel, MS App,\u2026)Knowledge Data Analysis Tools (Phyton, Minitab,\u2026)Fluent in English (B2+ - C1)Position: Junior Operations TeamFull-Time or Part-Time: Full-timeReports to: Data Center Site ManagerLocation: Vitoria / Madrid / BarcelonaContact: ******"
    },
    "4015784910": {
        "title": "Senior Data Engineer",
        "company": "HOLA CONSULTORES SL",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nTus tareas\n\nHola Consultores S.L somos una empresa especializada en la provisi\u00f3n de Servicios TIC e Ingenier\u00eda para la gesti\u00f3n empresarial. Trabajamos dise\u00f1ando soluciones a medida de las necesidades de nuestros clientes.\n\nSomos una empresa 2.0 innovadora donde las personas son muy importantes para nosotros. La \u00e9tica, transparencia, imparcialidad e independencia forman parte de nuestra filosof\u00eda.\n\nTu perfil\n\nPARA BARCELONA, Viladecans.\n\nEstamos seleccionando perfiles DATA ENGINEER, para el sector aeron\u00e1utico ????\n\nResponsabilidades\n\nAdquiera conjuntos de datos que se alineen con las necesidades comerciales\nDesarrollar algoritmos para transformar datos en informaci\u00f3n \u00fatil y procesable.\nCree, pruebe y mantenga arquitecturas de canalizaci\u00f3n de bases de datos\nColaborar con la direcci\u00f3n para comprender los objetivos de la empresa.\nCrear nuevos m\u00e9todos de validaci\u00f3n de datos y herramientas de an\u00e1lisis de datos.\nGarantizar el cumplimiento de las pol\u00edticas de seguridad y gobierno de datos\n\n\u00bfQu\u00e9 necesitamos?\n\nIngl\u00e9s C1\nPython -Pandas , ETL y experiencia con Jupyter.\nEnfoque especial en Calidad de datos.\nLa experiencia en el uso de ReGex es un plus.\nLenguajes SQL como Snowflake y Redshift .\nExperiencia con tecnolog\u00edas de AWS como Redshift , S3, AWS Glue, Lambda y roles y permisos de IAM.\nLa certificaci\u00f3n de AWS como AWS Cloud Practicioner es un plus.\nMetodolog\u00eda \u00e1gil.\nGit.\nConocimientos con IA.\nGitHub/Bitbucket o una herramienta similar durante un a\u00f1o.\nOtros programas como Atlan, DBT...\nLos conocimientos financieros son un plus, algunos proyectos usan ese tipo de informaci\u00f3n.\n\nLa experiencia trabajando en un equipo de datos en una aerol\u00ednea o conocimiento de la industria a\u00e9rea tambi\u00e9n es una ventaja.\n\n\u00bfPor qu\u00e9 nosotros?\n\n\u00bfQu\u00e9 te podemos aportar?\n\nContrato laboral indefinido y estabilidad desde el primer d\u00eda.\n\nTodo nuestro apoyo para tu desarrollo personal/profesional.\n\nJornada laboral h\u00edbrida, 3X2 / 2X3\n\nIncorporaci\u00f3n inmediata.\n\nExcelente clima laboral.\n\nSeguro m\u00e9dico privado con Adeslas (completo + dental).\n\nPago del 100% ex\u00e1menes de certificaci\u00f3n oficial.\n\nPago parcial de cualquier curso de IT/Data.\n\n\u2714Grandes proyectos con tecnolog\u00edas avanzadas.\n\nSi tu perfil se ajusta al puesto y est\u00e1s emocionado/a en formar parte de nuestro equipo,\n\n\u00a1Nos encantar\u00eda saber de ti!\n\n\u00bfQui\u00e9nes somos?\n\nHola, somos una Consultor\u00eda de Barcelona especializada en la selecci\u00f3n, desarrollo y b\u00fasqueda de talento humano.\n\nMisi\u00f3n\n\nEl desarrollo y la excelencia profesional en la gesti\u00f3n de los Recursos Humanos implementando soluciones enfocadas a la consecuci\u00f3n de objetivos concretos.\n\nVisi\u00f3n\n\nAgilizamos los procesos aportando las capacidades tecnol\u00f3gicas adecuadas para la mejora continua de las empresas.\n\nEstrategia\n\nConsultor\u00eda estrat\u00e9gica con un relevante sentido pr\u00e1ctico identificando las necesidades y aplicando soluciones e medida.\nTus tareas\nHola Consultores S.L\nSomos una empresa 2.0 innovadora donde las personas son muy importantes para nosotros. La \u00e9tica, transparencia, imparcialidad e independencia forman parte de nuestra filosof\u00eda.\nTu perfil\nPARA BARCELONA, Viladecans.\nEstamos seleccionando perfiles\nDATA ENGINEER\n,\nResponsabilidades\nAdquiera conjuntos de datos que se alineen con las necesidades comerciales\nDesarrollar algoritmos para transformar datos en informaci\u00f3n \u00fatil y procesable.\nCree, pruebe y mantenga arquitecturas de canalizaci\u00f3n de bases de datos\nColaborar con la direcci\u00f3n para comprender los objetivos de la empresa.\nCrear nuevos m\u00e9todos de validaci\u00f3n de datos y herramientas de an\u00e1lisis de datos.\nGarantizar el cumplimiento de las pol\u00edticas de seguridad y gobierno de datos\nAdquiera conjuntos de datos que se alineen con las necesidades comerciales\nDesarrollar algoritmos para transformar datos en informaci\u00f3n \u00fatil y procesable.\nCree, pruebe y mantenga arquitecturas de canalizaci\u00f3n de bases de datos\nColaborar con la direcci\u00f3n para comprender los objetivos de la empresa.\nCrear nuevos m\u00e9todos de validaci\u00f3n de datos y herramientas de an\u00e1lisis de datos.\nGarantizar el cumplimiento de las pol\u00edticas de seguridad y gobierno de datos\nIngl\u00e9s C1\nPython -Pandas , ETL y experiencia con Jupyter.\nEnfoque especial en Calidad de datos.\nLa experiencia en el uso de ReGex es un plus.\nLenguajes SQL como Snowflake y Redshift .\nExperiencia con tecnolog\u00edas de AWS como Redshift , S3, AWS Glue, Lambda y roles y permisos de IAM.\nLa certificaci\u00f3n de AWS como AWS Cloud Practicioner es un plus.\nMetodolog\u00eda \u00e1gil.\nGit.\nConocimientos con IA.\nGitHub/Bitbucket o una herramienta similar durante un a\u00f1o.\nOtros programas como Atlan, DBT...\nLos conocimientos financieros son un plus, algunos proyectos usan ese tipo de informaci\u00f3n.\nIngl\u00e9s C1\nPython -Pandas , ETL y experiencia con Jupyter.\nEnfoque especial en Calidad de datos.\nLa experiencia en el uso de ReGex es un plus.\nLenguajes SQL como Snowflake y Redshift .\nExperiencia con tecnolog\u00edas de AWS como Redshift , S3, AWS Glue, Lambda y roles y permisos de IAM.\nLa certificaci\u00f3n de AWS como AWS Cloud Practicioner es un plus.\nMetodolog\u00eda \u00e1gil.\nGit.\nConocimientos con IA.\nGitHub/Bitbucket o una herramienta similar durante un a\u00f1o.\nOtros programas como Atlan, DBT...\nLos conocimientos financieros son un plus, algunos proyectos usan ese tipo de informaci\u00f3n.\nLa experiencia trabajando en un equipo de datos en una aerol\u00ednea o conocimiento de la industria a\u00e9rea tambi\u00e9n es una ventaja.\n\u00bfPor qu\u00e9 nosotros?\n\u00bfQu\u00e9 te podemos aportar?\nContrato laboral indefinido y estabilidad desde el primer d\u00eda.\nTodo nuestro apoyo para tu desarrollo personal/profesional.\nJornada laboral h\u00edbrida, 3X2 / 2X3\nIncorporaci\u00f3n inmediata.\nExcelente clima laboral.\nSeguro m\u00e9dico privado con Adeslas (completo + dental).\nPago del 100% ex\u00e1menes de certificaci\u00f3n oficial.\nPago parcial de cualquier curso de IT/Data.\n\u2714Grandes proyectos con tecnolog\u00edas avanzadas.\nSi tu perfil se ajusta al puesto y est\u00e1s emocionado/a en formar parte de nuestro equipo,\n\u00a1Nos encantar\u00eda saber de ti!\n\u00bfQui\u00e9nes somos?\nHola, somos una Consultor\u00eda de Barcelona especializada en la selecci\u00f3n, desarrollo y b\u00fasqueda de talento humano.\nConsultor\u00eda de Barcelona\nMisi\u00f3n\nVisi\u00f3n\nEstrategia"
    },
    "4156641556": {
        "title": "Data Engineer",
        "company": "Interstock Development",
        "location": "\u00c1lava, Principality of Asturias, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nInterstock es una empresa radicada en Donosti dedicada al dise\u00f1o y desarrollo de soluciones de inteligencia artificial para mercados financieros.\n\nSe realizan servicios personalizados aplicando machine learning al desarrollo de estrategias de trading, a la gesti\u00f3n activa de carteras de inversi\u00f3n y a la gesti\u00f3n del riesgo.\n\nBuscamos un Data Engineer altamente cualificado y motivado, que se encargue de asegurar la disponibilidad, rendimiento, actualizaci\u00f3n continua, consistencia e integridad de los datos en toda nuestra organizaci\u00f3n.\n\nEntre Sus Tareas\n\nActualizaci\u00f3n de la infraestructura de datos existente.Integrar APIs para la recogida y procesamiento interno de datos.Gestionar bases de datos y asegurar su disponibilidad y rendimiento.Supervisar la integridad y calidad de los datos.Garantizar la alta disponibilidad y recuperaci\u00f3n de datos cr\u00edticos.Desarrollar interfaces gr\u00e1ficas para el control y la gesti\u00f3n de datos.Realizar migraciones a la nube.Estudios m\u00ednimos Grado en ingenier\u00eda inform\u00e1tica o similaresExperiencia m\u00ednima Al menos 3 a\u00f1osRequisitos m\u00ednimos Experiencia en arquitectura y gesti\u00f3n de infraestructuras de datos.Conocimientos en tecnolog\u00edas de almacenamiento y gesti\u00f3n de bases de datos.Experiencia con APIs, integridad de datos y soluciones en la nube.Lenguajes de programaci\u00f3n: Java, PythonRequisitos deseados Inter\u00e9s en el mundo de los mercados financieros.Conocimientos de arquitecturas basadas en eventos (Kafka,...)Capacidad de autoaprendizaje, iniciativa y proactividad.Se ofrece: Horario flexible y modalidad de trabajo h\u00edbrida (remoto/presencial)Contrato indefinidoOportunidad de crecimiento y desarrollo profesional.Interesados enviar CV a: ******\n\n#J-18808-Ljbffr\nEntre Sus Tareas"
    },
    "4121155716": {
        "title": "Principal Data Software Engineer (Databricks) ",
        "company": "EPAM Systems",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nWe are looking for a Principal Data Software Engineer who possesses expertise in Databricks and has a proactive, open-minded approach. This role offers the opportunity to become a pivotal member of our expert team, making significant contributions in a collaborative environment.\n\nDo you have background and wide experience in Data engineering and strong knowledge in Databricks? Are you an open-minded professional with good English skills? If it sounds like you, this could be the perfect opportunity to join EPAM as a Principal Data Software Engineer.\n\nResponsibilities\n\n\nDesign, develop, and maintain scalable data pipelines and robust data architectures\nOptimize data models and ETL processes using Databricks and complementary technologies\nImplement data quality checks and monitoring systems to maintain high data integrity\nStay current with emerging trends and technologies in data engineering, and advocate for the integration of new tools\nTroubleshoot and resolve data-related issues efficiently\nConduct code reviews to ensure high standards of code quality are maintained\nDrive technology initiatives, including designs, proof of concept, and research and development\nStreamline project processes to enhance data engineering practices\nFoster transparent and effective communication with team members and clients to justify and discuss technical solutions\n\n\nRequirements\n\n\nProven expertise in Spark, using either Scala or PySpark\nStrong background in data architecture, data modeling, and building ETL pipelines using Databricks\nExperience with multiple SDLC phases and technical leadership over complex implementations\nProficiency in cloud-native technologies and software engineering best practices including unit testing and linting\nEngineering background in at least one major cloud platform: AWS, Azure, or GCP\nSkillful in performance optimization for data-intensive applications\nKeen on technological advancements and modernizing legacy systems\nProven ability to present and advocate for technical solutions to stakeholders\nIndependent problem-solving skills and comfort with ambiguity\nHighly proactive with evident client-facing experience\nFluent English communication skills, minimum B2+ level\n\n\nWe offer\n\n\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\n\n\nEPAM is a leading digital transformation services and product engineering company with over 52,650 EPAMers in more than 55 countries and regions. Since 1993, our multidisciplinary teams have been helping make the future real for our clients and communities around the world. In 2018, we opened an office in Spain that quickly grew to over 1,450 EPAMers distributed between the offices in M\u00e1laga and Madrid as well as remotely across the country. Here you will collaborate with multinational teams, contribute to numerous innovative projects, and have an opportunity to learn and grow continuously.\n\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nPrincipal Data Software Engineer\nResponsibilities\nDesign, develop, and maintain scalable data pipelines and robust data architectures\nOptimize data models and ETL processes using Databricks and complementary technologies\nImplement data quality checks and monitoring systems to maintain high data integrity\nStay current with emerging trends and technologies in data engineering, and advocate for the integration of new tools\nTroubleshoot and resolve data-related issues efficiently\nConduct code reviews to ensure high standards of code quality are maintained\nDrive technology initiatives, including designs, proof of concept, and research and development\nStreamline project processes to enhance data engineering practices\nFoster transparent and effective communication with team members and clients to justify and discuss technical solutions\nDesign, develop, and maintain scalable data pipelines and robust data architectures\nOptimize data models and ETL processes using Databricks and complementary technologies\nImplement data quality checks and monitoring systems to maintain high data integrity\nStay current with emerging trends and technologies in data engineering, and advocate for the integration of new tools\nTroubleshoot and resolve data-related issues efficiently\nConduct code reviews to ensure high standards of code quality are maintained\nDrive technology initiatives, including designs, proof of concept, and research and development\nStreamline project processes to enhance data engineering practices\nFoster transparent and effective communication with team members and clients to justify and discuss technical solutions\nRequirements\nProven expertise in Spark, using either Scala or PySpark\nStrong background in data architecture, data modeling, and building ETL pipelines using Databricks\nExperience with multiple SDLC phases and technical leadership over complex implementations\nProficiency in cloud-native technologies and software engineering best practices including unit testing and linting\nEngineering background in at least one major cloud platform: AWS, Azure, or GCP\nSkillful in performance optimization for data-intensive applications\nKeen on technological advancements and modernizing legacy systems\nProven ability to present and advocate for technical solutions to stakeholders\nIndependent problem-solving skills and comfort with ambiguity\nHighly proactive with evident client-facing experience\nFluent English communication skills, minimum B2+ level\nProven expertise in Spark, using either Scala or PySpark\nStrong background in data architecture, data modeling, and building ETL pipelines using Databricks\nExperience with multiple SDLC phases and technical leadership over complex implementations\nProficiency in cloud-native technologies and software engineering best practices including unit testing and linting\nEngineering background in at least one major cloud platform: AWS, Azure, or GCP\nSkillful in performance optimization for data-intensive applications\nKeen on technological advancements and modernizing legacy systems\nProven ability to present and advocate for technical solutions to stakeholders\nIndependent problem-solving skills and comfort with ambiguity\nHighly proactive with evident client-facing experience\nFluent English communication skills, minimum B2+ level\nWe offer\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nPrivate health insurance\nEPAM Employees Stock Purchase Plan\n100% paid sick leave\nReferral Program\nProfessional certification\nLanguage courses\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends.\nWhy Join EPAM\nWORK AND LIFE BALANCE. Enjoy more of your personal time with flexible work options, 24 working days of annual leave and paid time off for numerous public holidays.\nCONTINUOUS LEARNING CULTURE. Craft your personal Career Development Plan to align with your learning objectives. Take advantage of internal training, mentorship, sponsored certifications and LinkedIn courses.\nCLEAR AND DIFFERENT CAREER PATHS. Grow in engineering or managerial direction to become a People Manager, in-depth technical specialist, Solution Architect, or Project/Delivery Manager.\nSTRONG PROFESSIONAL COMMUNITY. Join a global EPAM community of highly skilled experts and connect with them to solve challenges, exchange ideas, share expertise and make friends."
    },
    "4136521101": {
        "title": "Data Engineer Azure",
        "company": "Logicalis Spain",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nEn Logicalis Spain estamos buscando un perfil de Ingeniero de Datos Azure para integrarse en nuestra BU de Data & Analytics. El equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\n\nREQUISITOS T\u00c9CNICOS\nExperiencia demostrable en ingenier\u00eda de datos, con enfoque en Azure y Databricks.\nDominio de Python y PySpark para el desarrollo de pipelines de datos. \nExperiencia en la implementaci\u00f3n y gesti\u00f3n de Delta Live Tables. \nConocimiento en estrategias de Lakehouse Federation y su aplicaci\u00f3n pr\u00e1ctica. \nHabilidad para dise\u00f1ar soluciones de datos escalables y eficientes en entornos de nube.\nExcelentes habilidades de comunicaci\u00f3n y capacidad para trabajar en equipo. \nDeseable: Certificaciones en Azure o Databricks. Conocimiento de otras herramientas y tecnolog\u00edas de big data.\n\nFUNCIONES\nDise\u00f1ar, desarrollar y mantener soluciones de datos escalables y eficientes en Azure y Databricks. \nImplementar y gestionar Delta Live Tables para garantizar la integridad y calidad de los datos.\nDesarrollar y optimizar pipelines de datos utilizando Python y PySpark. \nImplementar estrategias de Lakehouse Federation para integrar y gestionar datos de m\u00faltiples fuentes. \nColaborar con equipos multifuncionales para comprender los requisitos de datos y proporcionar soluciones adecuadas. \nGarantizar la seguridad y el cumplimiento de las pol\u00edticas de datos en todas las soluciones implementadas. \n\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo 100% remoto, desde cualquier punto de Espa\u00f1a.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados.\n> Buen ambiente de trabajo y entorno muy colaborativo.\nEn Logicalis Spain estamos buscando un perfil de Ingeniero de Datos Azure para integrarse en nuestra BU de Data & Analytics. El equipo de Data & Analytics de Logicalis trabaja con clientes nacionales e internacionales, llevando a cabo importantes proyectos de inteligencia artificial y anal\u00edtica avanzada, estrategia del dato, gobierno, integraci\u00f3n y arquitectura, abarcando todo el ciclo de vida del dato.\nLogicalis Spain\nIngeniero de Datos Azure\nData & Analytics\nREQUISITOS T\u00c9CNICOS\nExperiencia demostrable en ingenier\u00eda de datos, con enfoque en Azure y Databricks.\nDominio de Python y PySpark para el desarrollo de pipelines de datos. \nExperiencia en la implementaci\u00f3n y gesti\u00f3n de Delta Live Tables. \nConocimiento en estrategias de Lakehouse Federation y su aplicaci\u00f3n pr\u00e1ctica. \nHabilidad para dise\u00f1ar soluciones de datos escalables y eficientes en entornos de nube.\nExcelentes habilidades de comunicaci\u00f3n y capacidad para trabajar en equipo. \nDeseable: Certificaciones en Azure o Databricks. Conocimiento de otras herramientas y tecnolog\u00edas de big data.\nExperiencia demostrable en ingenier\u00eda de datos, con enfoque en Azure y Databricks.\nDominio de Python y PySpark para el desarrollo de pipelines de datos.\nExperiencia en la implementaci\u00f3n y gesti\u00f3n de Delta Live Tables.\nConocimiento en estrategias de Lakehouse Federation y su aplicaci\u00f3n pr\u00e1ctica.\nHabilidad para dise\u00f1ar soluciones de datos escalables y eficientes en entornos de nube.\nExcelentes habilidades de comunicaci\u00f3n y capacidad para trabajar en equipo.\nDeseable: Certificaciones en Azure o Databricks. Conocimiento de otras herramientas y tecnolog\u00edas de big data.\nFUNCIONES\nDise\u00f1ar, desarrollar y mantener soluciones de datos escalables y eficientes en Azure y Databricks. \nImplementar y gestionar Delta Live Tables para garantizar la integridad y calidad de los datos.\nDesarrollar y optimizar pipelines de datos utilizando Python y PySpark. \nImplementar estrategias de Lakehouse Federation para integrar y gestionar datos de m\u00faltiples fuentes. \nColaborar con equipos multifuncionales para comprender los requisitos de datos y proporcionar soluciones adecuadas. \nGarantizar la seguridad y el cumplimiento de las pol\u00edticas de datos en todas las soluciones implementadas.\nDise\u00f1ar, desarrollar y mantener soluciones de datos escalables y eficientes en Azure y Databricks.\nImplementar y gestionar Delta Live Tables para garantizar la integridad y calidad de los datos.\nDesarrollar y optimizar pipelines de datos utilizando Python y PySpark.\nImplementar estrategias de Lakehouse Federation para integrar y gestionar datos de m\u00faltiples fuentes.\nColaborar con equipos multifuncionales para comprender los requisitos de datos y proporcionar soluciones adecuadas.\nGarantizar la seguridad y el cumplimiento de las pol\u00edticas de datos en todas las soluciones implementadas.\nBENEFICIOS\n> Incorporaci\u00f3n inmediata a compa\u00f1\u00eda l\u00edder del sector IT con un alto grado de expertise en el \u00e1rea de Data & Analytics d\u00f3nde nos encontramos en pleno proceso de expansi\u00f3n.\n> Estabilidad laboral a trav\u00e9s de contrato indefinido con amplias oportunidades de desarrollo profesional y crecimiento en la compa\u00f1\u00eda.\n> Modalidad de trabajo 100% remoto, desde cualquier punto de Espa\u00f1a.\n> Paquete retributivo muy competitivo acorde a la val\u00eda del candidato.\n> Posibilidad de acogerse a planes de retribuci\u00f3n flexible (tarjeta restaurante, tarjeta transporte y tarjeta guarder\u00eda).\n> Seguro m\u00e9dico y GYMPASS.\n> Planes de formaci\u00f3n adaptados a cada perfil (cursos t\u00e9cnicos, certificaciones oficiales, formaci\u00f3n de idiomas...).\n> Portal de descuentos especiales para empleados.\n> Buen ambiente de trabajo y entorno muy colaborativo."
    },
    "4167170755": {
        "title": "Data Center Junior Operations Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Gij\u00f3n, Principality of Asturias, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Junior Operations Engineer to join our Spain-Portugal team, working directly with our Data Center Site Manager and focusing on supporting the site operations team.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.\n\nKey Responsibilities Include but are not limited to:Support Management in Maintenance, Operations, Projects, Security and Cleaning of the assigned facilities.Provide support in the supervision of the execution of services at a technical level.Provide support in personnel management (training, selection, labor relations).Provide support in the supervision of compliance with internal policies and procedures in Risk Prevention, Health and Safety and Quality.Provide support in the supervision of the methodologies implemented.Provide support in the preparation of budgets.Provide support in the generation of reports.Skills & RequirementsEducation / ExperienceBachelor's degreeValuable financial knowledgeSpecific Skills / AbilitiesAdvanced Microsoft Office 365 (Power BI, Word, Excel, MS App,...)Knowledge Data Analysis Tools (Phyton, Minitab,...)Fluent in English (B2+ - C1)Position: Junior Operations TeamFull-Time or Part-Time: Full-timeReports to: Data Center Site ManagerLocation: Vitoria / Madrid / BarcelonaContact: ******"
    },
    "4149724124": {
        "title": "Data Engineer Azure",
        "company": "SEREM Consultor\u00eda Empresarial",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nBuscas nuevas oportunidades? \u00a1Env\u00edanos tu curr\u00edculum y descubre el camino hacia el \u00e9xito! \u00a1Visita nuestro enlace de empleo en https //www.serem.com/language/es/empleo/envianos-tu-curriculum/ y comienza a construir tu futuro con nosotros!\n\nEn SEREM estamos comprometidos con diversos proyectos y queremos contar con los mejores profesionales del sector. Nos encontramos en b\u00fasqueda activa de un/a Data Engineer para trabajar en remoto.\n\nEstamos en b\u00fasqueda de un Data Engineer con experiencia en la conversi\u00f3n tecnol\u00f3gica de SAS a Azure en el contexto de la eliminaci\u00f3n progresiva de SAS GRID. La persona seleccionada ser\u00e1 responsable de evaluar revisar y transformar el c\u00f3digo SAS existente hacia la nueva plataforma en Azure.\n\nEntre las funciones principales se incluyen\n\nEvaluaci\u00f3n y revisi\u00f3n del c\u00f3digo SAS actual.\n\nAn\u00e1lisis de la conversi\u00f3n necesaria hacia la nueva soluci\u00f3n en Azure.\n\nTransformaci\u00f3n del c\u00f3digo SAS utilizando una soluci\u00f3n basada en archivos (Parquet files).\n\nImplementaci\u00f3n de l\u00f3gica de negocio y transformaciones en Azure Synapse (Pyspark).\n\nDocumentaci\u00f3n funcional y t\u00e9cnica de las soluciones migradas.\n\nRequisitos\n\nExperiencia Entre 3 y 6 a\u00f1os en las tecnolog\u00edas requeridas.\n\nTecnolog\u00edas clave Azure Azure Synapse (Pyspark) SAS\n\nNivel de ingl\u00e9s B2 o superior (se valorar\u00e1 capacidad para comunicarse con equipos internacionales).\n\nLo que ofrecemos\n\nProyecto innovador de migraci\u00f3n tecnol\u00f3gica con tecnolog\u00edas punteras.\n\nTrabajo 100 remoto con equipo internacional.\n\nOportunidad de viajar a Holanda 2-3 veces al a\u00f1o para reuniones clave.\n\nDesarrollo profesional en un entorno de transformaci\u00f3n digital.\n\nSi cumples con los requisitos y est\u00e1s listo para asumir este reto \u00a1queremos conocerte!\n\nFomentamos un ambiente de trabajo multicultural e inclusivo no discriminamos por edad g\u00e9nero o creencias as\u00ed como ofrecemos igualdad de oportunidades a todo el personal.\n\nDesarrollamos nuestras actividades bajo los principios del cuidado del medioambiente la sostenibilidad y la responsabilidad social corporativa colaborando en proyectos de reforestaci\u00f3n y sostenibilidad.\n\nApoyamos los 10 principios del Pacto Mundial y los 17 Objetivos de Desarrollo Sostenible en materia de derechos humanos condiciones laborales medio ambiente y anticorrupci\u00f3n.\n\nLos procesos de reclutamiento se desarrollan bajo altos est\u00e1ndares de calidad definiendo la incorporaci\u00f3n en base a la experiencia y habilidades del candidato.\n\nSomos una empresa espa\u00f1ola l\u00edder en servicios tecnol\u00f3gicos y atracci\u00f3n del talento presente en el mercado desde 1995. Contamos con m\u00e1s de 600 empleados en proyectos tanto nacionales como internacionales en sector TI.\n\nAzure, Azure Synapse Pyspark , SAS\nDesired Skills and Experience\nAzure, Azure Synapse Pyspark , SAS"
    },
    "4170840645": {
        "title": "Ingeniero de Datos de Procesos de negocios Celonis ",
        "company": "Inetum",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nDescripci\u00f3n de la empresa\n\nINETUM\n\nSomos un Consultora digital, internacional y \u00e1gil. En la era de la post-transformaci\u00f3n digital, nos esforzamos para que cada uno de nuestros 28.000 profesionales pueda renovarse continuamente.\n\nCada uno de ellos puede dise\u00f1ar su itinerario profesional de acuerdo a sus preferencias, emprender junto a sus clientes para construir en la pr\u00e1ctica un mundo m\u00e1s positivo, innovar en cada uno de los 27 pa\u00edses y conciliar su carrera profesional con su bienestar personal.\n\nNuestros 28.000 atletas digitales est\u00e1n orgullosos de haberse certificado Top Employer Europe 2024.\n\nDescripci\u00f3n del empleo\n\nEn INETUM buscamos ampliar nuestro equipo de PROCESS MINING. Perfiles con experiencia en ETL y visualizaci\u00f3n de Datos, si adicionalmente aportas experiencia en CELONIS, estamos deseando conocerte.\n\nTrabajar\u00e1s en proyectos de miner\u00eda de procesos con Celonis como ingeniero/a de datos en un proyecto de larga duraci\u00f3n. Se proporcionar\u00e1 formaci\u00f3n en Celonis a los candidatos seleccionados.\n\nBuscamos profesionales talentosos, apasionados por el uso de an\u00e1lisis de procesos basados en datos, con el objetivo de ayudar a nuestros clientes a alcanzar sus metas estrat\u00e9gicas.\n\nEl Puesto Que Estamos Ofreciendo No Solo Va a Consistir En Analizar Datos, Sino Que Colaborar\u00e1s De Manera Tangible Al \u00c9xito De Nuestros Clientes Con El Objetivo De Estar\n\nConectando la brecha entre TI y Negocio.\nAnalizando grandes conjuntos de datos para optimizar procesos empresariales mediante la miner\u00eda de procesos.\nCreando valor para nuestros clientes al construir flujos de trabajo automatizados y modelos predictivos.\nElaborando casos de negocio y present\u00e1ndolos a nuestros clientes.\nTrabajando en un entorno din\u00e1mico en constante cambio.\n\nModalidad de trabajo remoto, con disponibilidad de asistencia a presentaciones semanales a cliente.\n\nRequisitos\n\nRequisitos Obligatorios\n\nNivel de SQL: Medio-Alto.\nM\u00ednimo 24 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\n\nRequisito deseables\n\nFormaci\u00f3n universitaria en Inform\u00e1tica, Administraci\u00f3n de Empresas, Econom\u00eda, Matem\u00e1ticas o similar.\nTener entre 1 y 2 a\u00f1os de experiencia en consultor\u00eda de datos y posees fuertes habilidades anal\u00edticas;Conocimientos de scripting (preferiblemente Python).\nSer entusiasta de los datos, el an\u00e1lisis predictivo, la miner\u00eda de procesos y la mejora continua;\nPreferentemente haber trabajado previamente con software de miner\u00eda de procesos como Celonis, Signavio o Minit;\nHaber trabajado y comprender los an\u00e1lisis estad\u00edsticos\nDeseable buen nivel de ingl\u00e9s. B2\nCapacidades anal\u00edticas.\nConocimientos en procesos, miner\u00eda de procesos y mejora de procesos.\nConocimiento avanzado de t\u00e9cnicas de visualizaci\u00f3n y generaci\u00f3n de informes en Power BI.\n60 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\n\nInformaci\u00f3n adicional\n\nBeneficios\n\n\ud83d\ude4b\u200d\u2640\ufe0f\ud83d\udcbb Formar\u00e1s parte de un gran equipo de profesionales con inquietud y motivaci\u00f3n por la tecnolog\u00eda.\n\n\u23f1 Trabajar\u00e1s a jornada completa con un horario flexible y modalidad h\u00edbrida con teletrabajo real.\n\n\ud83d\udc69\ud83c\udffc\u200d\ud83c\udf93 Formaci\u00f3n por parte de la empresa para que puedas seguir desarroll\u00e1ndote y promocionar dentro del plan de carrera que existe para ti.\n\n\ud83c\udfae Podr\u00e1s asistir a eventos y conferencias relevantes del sector.\n\n\u270d\ud83c\udffc Contrato indefinido.\n\n\ud83d\ude04 Estabilidad y buen clima laboral.\n\n\ud83d\udcb0 Salario Competitivo.\n\n\ud83c\udd93 Acceso a ventajas del grupo de empresa\n\n\ud83d\udcb5 Retribuci\u00f3n flexible y m\u00e1s beneficios\nDescripci\u00f3n de la empresa\nINETUM\nSomos un Consultora digital, internacional y \u00e1gil. En la era de la post-transformaci\u00f3n digital, nos esforzamos para que cada uno de nuestros 28.000 profesionales pueda renovarse continuamente.\nCada uno de ellos puede dise\u00f1ar su itinerario profesional de acuerdo a sus preferencias, emprender junto a sus clientes para construir en la pr\u00e1ctica un mundo m\u00e1s positivo, innovar en cada uno de los 27 pa\u00edses y conciliar su carrera profesional con su bienestar personal.\nNuestros 28.000 atletas digitales est\u00e1n orgullosos de haberse certificado Top Employer Europe 2024.\nDescripci\u00f3n del empleo\nPROCESS MINING\nCELONIS\nminer\u00eda de procesos con Celonis\nConectando la brecha entre TI y Negocio.\nAnalizando grandes conjuntos de datos para optimizar procesos empresariales mediante la miner\u00eda de procesos.\nCreando valor para nuestros clientes al construir flujos de trabajo automatizados y modelos predictivos.\nElaborando casos de negocio y present\u00e1ndolos a nuestros clientes.\nTrabajando en un entorno din\u00e1mico en constante cambio.\nConectando la brecha entre TI y Negocio.\nAnalizando grandes conjuntos de datos para optimizar procesos empresariales mediante la miner\u00eda de procesos.\nCreando valor para nuestros clientes al construir flujos de trabajo automatizados y modelos predictivos.\nElaborando casos de negocio y present\u00e1ndolos a nuestros clientes.\nTrabajando en un entorno din\u00e1mico en constante cambio.\nRequisitos\nRequisitos Obligatorios\nNivel de SQL: Medio-Alto.\nM\u00ednimo 24 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\nNivel de SQL: Medio-Alto.\nM\u00ednimo 24 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\nRequisito deseables\nFormaci\u00f3n universitaria en Inform\u00e1tica, Administraci\u00f3n de Empresas, Econom\u00eda, Matem\u00e1ticas o similar.\nTener entre 1 y 2 a\u00f1os de experiencia en consultor\u00eda de datos y posees fuertes habilidades anal\u00edticas;Conocimientos de scripting (preferiblemente Python).\nSer entusiasta de los datos, el an\u00e1lisis predictivo, la miner\u00eda de procesos y la mejora continua;\nPreferentemente haber trabajado previamente con software de miner\u00eda de procesos como Celonis, Signavio o Minit;\nHaber trabajado y comprender los an\u00e1lisis estad\u00edsticos\nDeseable buen nivel de ingl\u00e9s. B2\nCapacidades anal\u00edticas.\nConocimientos en procesos, miner\u00eda de procesos y mejora de procesos.\nConocimiento avanzado de t\u00e9cnicas de visualizaci\u00f3n y generaci\u00f3n de informes en Power BI.\n60 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\nFormaci\u00f3n universitaria en Inform\u00e1tica, Administraci\u00f3n de Empresas, Econom\u00eda, Matem\u00e1ticas o similar.\nTener entre 1 y 2 a\u00f1os de experiencia en consultor\u00eda de datos y posees fuertes habilidades anal\u00edticas;Conocimientos de scripting (preferiblemente Python).\nSer entusiasta de los datos, el an\u00e1lisis predictivo, la miner\u00eda de procesos y la mejora continua;\nPreferentemente haber trabajado previamente con software de miner\u00eda de procesos como Celonis, Signavio o Minit;\nHaber trabajado y comprender los an\u00e1lisis estad\u00edsticos\nDeseable buen nivel de ingl\u00e9s. B2\nCapacidades anal\u00edticas.\nConocimientos en procesos, miner\u00eda de procesos y mejora de procesos.\nConocimiento avanzado de t\u00e9cnicas de visualizaci\u00f3n y generaci\u00f3n de informes en Power BI.\n60 meses de experiencia en proyectos con ETL y carga de datos de gran volumen.\nInformaci\u00f3n adicional\nBeneficios"
    },
    "4177768638": {
        "title": "Senior Data Engineer",
        "company": "Not Found",
        "location": "\u00c1vila, Castilla and Leon, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nSenior Data Engineer \u20ac40,000 - \u20ac50,000 + Bonus Location - Madrid The Ideal profile System Design & Architecture: Define and develop technical specifications, system designs, and scalable architectures for large-scale data infrastructure.Build and optimize ETL/ELT pipelines to process large, diverse datasets with a focus on performance and reliability.Expertise with real-time data streaming platforms like Spark / KafkaIn-depth knowledge of Databricks and cloud-based data architectures for processing and storage at scale.Proven ability to design, build, and optimize ETL/ELT pipelines using tools like Apache Airflow, DBT, and Apache NiFi for both batch and real-time processing. For more info If you feel this is the role for you or you know someone suitable for the role.\n\nEmail me at ******"
    },
    "4177768639": {
        "title": "Senior Data Engineer",
        "company": "Not Found",
        "location": "Greater Madrid Metropolitan Area",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nSenior Data Engineer \u20ac40,000 - \u20ac50,000 + Bonus Location - Madrid The Ideal profile System Design & Architecture: Define and develop technical specifications, system designs, and scalable architectures for large-scale data infrastructure.Build and optimize ETL/ELT pipelines to process large, diverse datasets with a focus on performance and reliability.Expertise with real-time data streaming platforms like Spark / KafkaIn-depth knowledge of Databricks and cloud-based data architectures for processing and storage at scale.Proven ability to design, build, and optimize ETL/ELT pipelines using tools like Apache Airflow, DBT, and Apache NiFi for both batch and real-time processing. For more info If you feel this is the role for you or you know someone suitable for the role.\n\nEmail me at ******"
    },
    "4173371132": {
        "title": "Ingeniero/a control de datos",
        "company": "EOSOL",
        "location": "Murcia, Regi\u00f3n de Murcia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\n\ud83e\uddb8\u200d\u2642\ufe0f Ingeniero/a Control de Datos en Eosol Group \ud83d\ude80\n\n\u00bfEst\u00e1s listo/a para comenzar tu aventura en el apasionante mundo del Data? En Eosol Group buscamos un/a Ingeniero/a de Control de Datos que tenga ganas de aprender y liderar proyectos innovadores. Si eres un amante de los datos y sue\u00f1as con dar forma a los centros de datos del futuro, \u00a1esta es tu oportunidad! \ud83d\udd0d\n\n\u00bfQui\u00e9nes somos?\n\nEosol Group es una empresa l\u00edder en soluciones de ingenier\u00eda. Nuestro equipo es joven, din\u00e1mico y apasionado por la ingenier\u00eda y las Energ\u00edas Renovables. Aqu\u00ed, valoramos la creatividad, la curiosidad y, sobre todo, las ganas de crecer. \ud83c\udf31\n\nLa misi\u00f3n\n\nComo ingeniero/a de control de datos, ser\u00e1s clave en nuestro proceso de an\u00e1lisis y optimizaci\u00f3n de big data. Estar\u00e1s involucrado en proyectos que te permitir\u00e1n aplicar tus conocimientos t\u00e9cnicos de manera pr\u00e1ctica, con la posibilidad de ejecutar ideas innovadoras que mejoren nuestros procesos y resultados.\n\n\u00bfQu\u00e9 har\u00e1s?\n\nLiderar proyectos: Pondr\u00e1s en marcha proyectos de datos que transforman la manera en la que trabajamos y analizamos la informaci\u00f3n.\nColaborar: Trabajar\u00e1s en equipo con diferentes departamentos para garantizar que los centros de datos funcionen de manera eficiente y sin problemas.\nAn\u00e1lisis de datos: Aprender\u00e1s y aplicar\u00e1s t\u00e9cnicas de an\u00e1lisis que te permitir\u00e1n extraer informaci\u00f3n valiosa y hacer recomendaciones basadas en datos.\nMejorar procesos: Te encargar\u00e1s de identificar \u00e1reas de mejora en nuestros sistemas y procesos de gesti\u00f3n de datos.\n\n\u00bfQui\u00e9n eres?\n\nBuscamos a alguien con:\n\nGanas de aprender: No necesitas tener experiencia previa en el sector, pero s\u00ed muchas ganas de sumergirte en el mundo del big data.\nFormaci\u00f3n t\u00e9cnica: Un t\u00edtulo en Ingenier\u00eda, Matem\u00e1ticas, Inform\u00e1tica o \u00e1reas relacionadas que te de una base s\u00f3lida para tu desarrollo.\nPasi\u00f3n por los datos: Debes tener un inter\u00e9s natural por los datos y el an\u00e1lisis. Si te gusta trabajar con n\u00fameros y resolver problemas, \u00a1este es tu lugar!\nHabilidades de comunicaci\u00f3n: Necesitar\u00e1s trabajar en equipo y ser capaz de comunicar tus ideas de manera clara y concisa.\n\n\u00bfQu\u00e9 ofrecemos?\n\nAprendizaje continuo: En Eosol Group valoramos el crecimiento personal y profesional. Te brindamos la oportunidad de aprender y desarrollarte a trav\u00e9s de formaci\u00f3n constante y proyectos desafiantes.\nAmbiente din\u00e1mico: Formar\u00e1s parte de un equipo incre\u00edble donde las ideas fluyen y todos aportan. Aqu\u00ed la creatividad no tiene l\u00edmites. \ud83c\udf1f\nFlexibilidad: Creemos en el equilibrio entre el trabajo y la vida personal. Ofrecemos pol\u00edticas flexibles de trabajo que se adaptan a tus necesidades.\nBeneficios competitivos: Ofrecemos un paquete de beneficios que incluye seguro m\u00e9dico, d\u00edas de vacaciones y opciones de trabajo remoto.\n\n\u00a1\u00danete a nosotros!\n\nSi est\u00e1s listo/a para dar el primer paso en tu carrera como ingeniero/a de control de datos y te entusiasma la idea de envolver y manipular informaci\u00f3n en un ambiente tecnol\u00f3gico innovador, \u00a1queremos conocerte! \u2728 En Eosol Group, tu potencial no tiene l\u00edmites. \u00a1Post\u00falate hoy!\n\nVen y s\u00e9 parte de este emocionante viaje con nosotros. Recuerda, en Eosol Group, \u00a1los datos son solo el comienzo! \ud83c\udf10\ud83d\udcbc\nIngeniero/a de Control de Datos\ndatos\ncentros de datos\nbig data\nLiderar proyectos: Pondr\u00e1s en marcha proyectos de datos que transforman la manera en la que trabajamos y analizamos la informaci\u00f3n.\nColaborar: Trabajar\u00e1s en equipo con diferentes departamentos para garantizar que los centros de datos funcionen de manera eficiente y sin problemas.\nAn\u00e1lisis de datos: Aprender\u00e1s y aplicar\u00e1s t\u00e9cnicas de an\u00e1lisis que te permitir\u00e1n extraer informaci\u00f3n valiosa y hacer recomendaciones basadas en datos.\nMejorar procesos: Te encargar\u00e1s de identificar \u00e1reas de mejora en nuestros sistemas y procesos de gesti\u00f3n de datos.\nLiderar proyectos: Pondr\u00e1s en marcha proyectos de datos que transforman la manera en la que trabajamos y analizamos la informaci\u00f3n.\nColaborar: Trabajar\u00e1s en equipo con diferentes departamentos para garantizar que los centros de datos funcionen de manera eficiente y sin problemas.\nAn\u00e1lisis de datos: Aprender\u00e1s y aplicar\u00e1s t\u00e9cnicas de an\u00e1lisis que te permitir\u00e1n extraer informaci\u00f3n valiosa y hacer recomendaciones basadas en datos.\nMejorar procesos: Te encargar\u00e1s de identificar \u00e1reas de mejora en nuestros sistemas y procesos de gesti\u00f3n de datos.\nGanas de aprender: No necesitas tener experiencia previa en el sector, pero s\u00ed muchas ganas de sumergirte en el mundo del big data.\nFormaci\u00f3n t\u00e9cnica: Un t\u00edtulo en Ingenier\u00eda, Matem\u00e1ticas, Inform\u00e1tica o \u00e1reas relacionadas que te de una base s\u00f3lida para tu desarrollo.\nPasi\u00f3n por los datos: Debes tener un inter\u00e9s natural por los datos y el an\u00e1lisis. Si te gusta trabajar con n\u00fameros y resolver problemas, \u00a1este es tu lugar!\nHabilidades de comunicaci\u00f3n: Necesitar\u00e1s trabajar en equipo y ser capaz de comunicar tus ideas de manera clara y concisa.\nGanas de aprender: No necesitas tener experiencia previa en el sector, pero s\u00ed muchas ganas de sumergirte en el mundo del big data.\nFormaci\u00f3n t\u00e9cnica: Un t\u00edtulo en Ingenier\u00eda, Matem\u00e1ticas, Inform\u00e1tica o \u00e1reas relacionadas que te de una base s\u00f3lida para tu desarrollo.\nPasi\u00f3n por los datos: Debes tener un inter\u00e9s natural por los datos y el an\u00e1lisis. Si te gusta trabajar con n\u00fameros y resolver problemas, \u00a1este es tu lugar!\nHabilidades de comunicaci\u00f3n: Necesitar\u00e1s trabajar en equipo y ser capaz de comunicar tus ideas de manera clara y concisa.\nAprendizaje continuo: En Eosol Group valoramos el crecimiento personal y profesional. Te brindamos la oportunidad de aprender y desarrollarte a trav\u00e9s de formaci\u00f3n constante y proyectos desafiantes.\nAmbiente din\u00e1mico: Formar\u00e1s parte de un equipo incre\u00edble donde las ideas fluyen y todos aportan. Aqu\u00ed la creatividad no tiene l\u00edmites. \ud83c\udf1f\nFlexibilidad: Creemos en el equilibrio entre el trabajo y la vida personal. Ofrecemos pol\u00edticas flexibles de trabajo que se adaptan a tus necesidades.\nBeneficios competitivos: Ofrecemos un paquete de beneficios que incluye seguro m\u00e9dico, d\u00edas de vacaciones y opciones de trabajo remoto.\nAprendizaje continuo: En Eosol Group valoramos el crecimiento personal y profesional. Te brindamos la oportunidad de aprender y desarrollarte a trav\u00e9s de formaci\u00f3n constante y proyectos desafiantes.\nAmbiente din\u00e1mico: Formar\u00e1s parte de un equipo incre\u00edble donde las ideas fluyen y todos aportan. Aqu\u00ed la creatividad no tiene l\u00edmites. \ud83c\udf1f\nFlexibilidad: Creemos en el equilibrio entre el trabajo y la vida personal. Ofrecemos pol\u00edticas flexibles de trabajo que se adaptan a tus necesidades.\nBeneficios competitivos: Ofrecemos un paquete de beneficios que incluye seguro m\u00e9dico, d\u00edas de vacaciones y opciones de trabajo remoto."
    },
    "4167170769": {
        "title": "Data Center Junior Operations Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Ourense, Galicia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Junior Operations Engineer to join our Spain-Portugal team, working directly with our Data Center Site Manager and focusing on supporting the site operations team.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.\n\nKey Responsibilities Include but are not limited to:Support Management in Maintenance, Operations, Projects, Security and Cleaning of the assigned facilities.Provide support in the supervision of the execution of services at a technical level.Provide support in personnel management (training, selection, labor relations).Provide support in the supervision of compliance with internal policies and procedures in Risk Prevention, Health and Safety and Quality.Provide support in the supervision of the methodologies implemented.Provide support in the preparation of budgets.Provide support in the generation of reports.Skills & RequirementsEducation / ExperienceBachelor's degreeValuable financial knowledgeSpecific Skills / AbilitiesAdvanced Microsoft Office 365 (Power BI, Word, Excel, MS App,...)Knowledge Data Analysis Tools (Phyton, Minitab,...)Fluent in English (B2+ - C1)Position: Junior Operations TeamFull-Time or Part-Time: Full-timeReports to: Data Center Site ManagerLocation: Vitoria / Madrid / BarcelonaContact: ******"
    },
    "4124347388": {
        "title": "Data Visualization Engineer ",
        "company": "Inetum",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nMission\n\nAbout The Role\n\nAre you a data enthusiast with a passion for data Visualization? In this key role, you'll be a crucial part of our data squads, responsible for developing and maintaining Power BI and SAP Business Objects reports.\n\nProfile\n\nSkills And Experiences \n\n1 \u2013 3 years of experience (required)\nPrior experience in ITSM or Data Visualization roles i.e. BI developer, BI analyst\nPrior experience working with client\u2019s tech stack\nModern:\nControl-M (preferred)\nSAP Business Objects (required)\nPower BI (preferred)\nSnowflake (required)\nAzure (required)\nEnglish: C1\nOthers: 100% remote work (only from Spain) \n\nWhat We Offer\n\n\ud83d\udcbc Competitive salary based on experience\n\n\ud83d\udcc8 Career development opportunities\n\n\ud83c\udf93 Ongoing training in tech competencies\n\n\ud83d\udd51 Flexible working hours\n\n\ud83d\udccb Flexible benefits (health insurance, meal vouchers, transport cards, etc.)\n\nAre you ready to take your career to the next level? \ud83c\udf1f Join a company that values your talent and growth! \ud83d\udce7 Send us your CV today and let's connect!\n\nOrganization\n\nAbout Inetum\n\nWe are a digital, international, and agile consulting firm. In the era of post-digital transformation, we strive to ensure that each of our 28,000 professionals can continuously renew themselves, positively experiencing their own digital flow.\n\nEach of them can design their career path according to their preferences, partner with clients to practically build a more positive world, innovate in each of the 27 countries, and balance their professional career with their personal well-being.\n\nOur 28,000 digital athletes are proud to have been certified as a Top Employer Europe 2024\nMission\nAbout The Role\nProfile\nSkills And Experiences\n1 \u2013 3 years of experience (required)\nPrior experience in ITSM or Data Visualization roles i.e. BI developer, BI analyst\nPrior experience working with client\u2019s tech stack\nModern:\nControl-M (preferred)\nSAP Business Objects (required)\nPower BI (preferred)\nSnowflake (required)\nAzure (required)\nEnglish: C1\nOthers: 100% remote work (only from Spain)\n1 \u2013 3 years of experience (required)\nPrior experience in ITSM or Data Visualization roles i.e. BI developer, BI analyst\nPrior experience working with client\u2019s tech stack\nModern:\nControl-M (preferred)\nSAP Business Objects (required)\nPower BI (preferred)\nSnowflake (required)\nAzure (required)\nControl-M (preferred)\nSAP Business Objects (required)\nPower BI (preferred)\nSnowflake (required)\nAzure (required)\nEnglish: C1\nOthers: 100% remote work (only from Spain)\nWhat We Offer\nOrganization\nAbout Inetum"
    },
    "4168263568": {
        "title": "Senior Data Engineer ",
        "company": "Tata Consultancy Services",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nAre you a Senior Data Engineer seeking a new interesting challenge? \ud83d\udd0d\nIf your answer is yes, it\u2019s your lucky day so keep reading, it can be just what you're looking for! \ud83d\udc40\n\n\ud83e\uddd0 WHAT ARE WE LOOKING FOR?\nMin. 3 yrs on role Software Engineering + Backend Eng. (one of languages: Python, Java, Scala, Rust - with focus on Python today)\nPractice with high quality codes (unit testing, integration testing, others) + code metrics\nFast discovery what is in new repository code, asking right technical questions about codebase and code quality, CI/CD as a general rule of work (automatic tasks, instead of manual)\nMin. 2 yrs on role near \"data\" subject like Data Engineer or Integrations Engineer\nStrong understanding data pipelines in Apache Spark, Apache Airflow and similar orchestrators\nUnderstanding on mid. level Databricks or other modern platforms BigData solutions inside (i.e. data catalogs, computes engines, SQL engines, observability layers, etc.)\nStrong focus on delivery data pipelines on high quality from data product delivery perspective\nPractical exp. with data streams and Petabytes scale\nMin. 1 yr with exp. near data integrations (data pipelines, connection for diff. types of data sources and sinks)\nBasic AWS knowledge (Lambda, SQS, CloudWatch, Powertools)\nSoft skills for discussion with vendors/partners about possible solutions/limits/properties in technical integrations and different data architectures\nStrong technical arguments in technologies in data integrations and BigData areas (focused on Open Source tools)\n\n\ud83d\udcc5 WHERE AND WHEN?\nWorkplace: Full remote (ALL PROFESSIONALS MUST BE WITHIN THE SPANISH TERRITORY)\nWork Schedule: Business Hours\n\n\ud83e\udd1d WHAT CAN WE OFFER YOU?\nPermanent contract \ud83d\udccb - We offer indefinite contracts from the first day.\nPay and benefits \ud83d\udcb8 - Competitive salary and a flexible compensation plan adapted to your needs (Ticket restaurant plan, Childcare Ticket, Transport Ticket and Health Insurance).\nWork from home \ud83c\udfda\ufe0f - We offer you a financial support every month for your working from home expenses. In addition, in the first month we will give you a bonus to help you to set up your workplace\ud83d\udc68\u200d\ud83d\udcbb\nOpportunity knocks \ud83d\udc4d\ud83c\udffb - Being a part of a growing company, we want to support your path with a career development plan and annual performance-based compensation reviews. \nLearn as you grow \ud83d\udcda - Starting with a fantastic onboarding program, TCS has robust learning platforms that will allow you to learn and grow personal as professionally.\nBring your buddy \ud83d\udc6b - If you have referred a friend for an open position under the BYB Scheme and she/he is hired you\u2019ll receive a very attractive cash award.\nConnect globally \ud83c\udf0f - Work with people from all over the world. You can feel the multicultural workforce.\nBenefit from being a TCSer \ud83c\udf9f\ufe0f \u2013 By being part of the TCS Spain family you can enjoy benefits, offers and corporate discounts on the best brands.\nAnd so on \ud83c\udf89 - Appreciations, incentives, Team Building activities, diversity and inclusion programs, sustainability activities, corporative events... This has only just begun!\n\n\ud83d\udca1WHO ARE WE?\nTata Consultancy Services (TCS) is an Information Technology (IT) company founded in 1968 as part of the Indian Tata Group and is one of the top 3 technology companies globally \ud83d\ude80\n\nWith a presence in 55 countries and more than 600,000 employees, TCS is considered one of the 10 best companies to work for worldwide in 2024 according to the Top Employers Institute \ud83e\udd47\n\nTCS Spain started operations in 2001 and currently has a diverse workforce that collaborates with the main Spanish and multinational companies \ud83c\udf0f\n\nTCS Spain has been certified as a Top Employer 2024 and has also been chosen as one of the 100 Best Companies to Work for in Spain in 2024 according to Forbes\ud83c\udfc6\n\nAmong the portfolio of services, TCS has information technology services, asset-based solutions, global consulting, engineering and industrial services, digital solutions and services, application maintenance and development, quality assurance and testing services, IT infrastructure and BPS \ud83c\udfaf\n\nResponsible for development, TCS Spain is committed to inclusion, diversity and sustainability, and promotes flexibility policies that support wellbeing and work-life balance \ud83d\udcaf\n\nWELCOME, WE ARE WAITING FOR YOU! \ud83d\ude80\nAre you a Senior Data Engineer seeking a new interesting challenge? \ud83d\udd0d\nSenior Data Engineer\nnew interesting challenge\nIf your answer is yes, it\u2019s your lucky day so keep reading, it can be just what you're looking for! \ud83d\udc40\nit can be just what you're looking for\n\ud83e\uddd0 WHAT ARE WE LOOKING FOR?\nWHAT ARE WE LOOKING FOR?\nMin. 3 yrs on role Software Engineering + Backend Eng. (one of languages: Python, Java, Scala, Rust - with focus on Python today)\nPractice with high quality codes (unit testing, integration testing, others) + code metrics\nFast discovery what is in new repository code, asking right technical questions about codebase and code quality, CI/CD as a general rule of work (automatic tasks, instead of manual)\nMin. 2 yrs on role near \"data\" subject like Data Engineer or Integrations Engineer\nStrong understanding data pipelines in Apache Spark, Apache Airflow and similar orchestrators\nUnderstanding on mid. level Databricks or other modern platforms BigData solutions inside (i.e. data catalogs, computes engines, SQL engines, observability layers, etc.)\nStrong focus on delivery data pipelines on high quality from data product delivery perspective\nPractical exp. with data streams and Petabytes scale\nMin. 1 yr with exp. near data integrations (data pipelines, connection for diff. types of data sources and sinks)\nBasic AWS knowledge (Lambda, SQS, CloudWatch, Powertools)\nSoft skills for discussion with vendors/partners about possible solutions/limits/properties in technical integrations and different data architectures\nStrong technical arguments in technologies in data integrations and BigData areas (focused on Open Source tools)\nMin. 3 yrs on role Software Engineering + Backend Eng. (one of languages: Python, Java, Scala, Rust - with focus on Python today)\nMin. 3 yrs on role Software Engineering + Backend Eng.\nPractice with high quality codes (unit testing, integration testing, others) + code metrics\nFast discovery what is in new repository code, asking right technical questions about codebase and code quality, CI/CD as a general rule of work (automatic tasks, instead of manual)\nMin. 2 yrs on role near \"data\" subject like Data Engineer or Integrations Engineer\nStrong understanding data pipelines in Apache Spark, Apache Airflow and similar orchestrators\nApache Spark, Apache Airflow\nUnderstanding on mid. level Databricks or other modern platforms BigData solutions inside (i.e. data catalogs, computes engines, SQL engines, observability layers, etc.)\nDatabricks\nStrong focus on delivery data pipelines on high quality from data product delivery perspective\nPractical exp. with data streams and Petabytes scale\nMin. 1 yr with exp. near data integrations (data pipelines, connection for diff. types of data sources and sinks)\nMin. 1 yr with exp. near data integrations\nBasic AWS knowledge (Lambda, SQS, CloudWatch, Powertools)\nSoft skills for discussion with vendors/partners about possible solutions/limits/properties in technical integrations and different data architectures\nStrong technical arguments in technologies in data integrations and BigData areas (focused on Open Source tools)\n\ud83d\udcc5 WHERE AND WHEN?\nWHERE AND WHEN?\nWorkplace: Full remote (ALL PROFESSIONALS MUST BE WITHIN THE SPANISH TERRITORY)\nWork Schedule: Business Hours\nWorkplace: Full remote (ALL PROFESSIONALS MUST BE WITHIN THE SPANISH TERRITORY)\nWork Schedule: Business Hours\n\ud83e\udd1d WHAT CAN WE OFFER YOU?\nWHAT CAN WE OFFER YOU?\nPermanent contract \ud83d\udccb - We offer indefinite contracts from the first day.\nPay and benefits \ud83d\udcb8 - Competitive salary and a flexible compensation plan adapted to your needs (Ticket restaurant plan, Childcare Ticket, Transport Ticket and Health Insurance).\nWork from home \ud83c\udfda\ufe0f - We offer you a financial support every month for your working from home expenses. In addition, in the first month we will give you a bonus to help you to set up your workplace\ud83d\udc68\u200d\ud83d\udcbb\nOpportunity knocks \ud83d\udc4d\ud83c\udffb - Being a part of a growing company, we want to support your path with a career development plan and annual performance-based compensation reviews. \nLearn as you grow \ud83d\udcda - Starting with a fantastic onboarding program, TCS has robust learning platforms that will allow you to learn and grow personal as professionally.\nBring your buddy \ud83d\udc6b - If you have referred a friend for an open position under the BYB Scheme and she/he is hired you\u2019ll receive a very attractive cash award.\nConnect globally \ud83c\udf0f - Work with people from all over the world. You can feel the multicultural workforce.\nBenefit from being a TCSer \ud83c\udf9f\ufe0f \u2013 By being part of the TCS Spain family you can enjoy benefits, offers and corporate discounts on the best brands.\nAnd so on \ud83c\udf89 - Appreciations, incentives, Team Building activities, diversity and inclusion programs, sustainability activities, corporative events... This has only just begun!\nPermanent contract \ud83d\udccb - We offer indefinite contracts from the first day.\nPermanent contract\nindefinite contracts\nPay and benefits \ud83d\udcb8 - Competitive salary and a flexible compensation plan adapted to your needs (Ticket restaurant plan, Childcare Ticket, Transport Ticket and Health Insurance).\nPay and benefits\nCompetitive salary\nflexible compensation plan\nWork from home \ud83c\udfda\ufe0f - We offer you a financial support every month for your working from home expenses. In addition, in the first month we will give you a bonus to help you to set up your workplace\ud83d\udc68\u200d\ud83d\udcbb\nWork from home\nfinancial support\nworking from home expenses\nbonus\nto set up your workplace\nOpportunity knocks \ud83d\udc4d\ud83c\udffb - Being a part of a growing company, we want to support your path with a career development plan and annual performance-based compensation reviews.\nOpportunity knocks\ncareer development plan\nannual performance-based compensation reviews\nLearn as you grow \ud83d\udcda - Starting with a fantastic onboarding program, TCS has robust learning platforms that will allow you to learn and grow personal as professionally.\nLearn as you grow\nonboarding program\nrobust learning platforms\nBring your buddy \ud83d\udc6b - If you have referred a friend for an open position under the BYB Scheme and she/he is hired you\u2019ll receive a very attractive cash award.\nBring your buddy\nyou have referred a friend for an open position\nshe/he\nis hired\nvery attractive\ncash award.\nConnect globally \ud83c\udf0f - Work with people from all over the world. You can feel the multicultural workforce.\nConnect globally\npeople from all over the world\nmulticultural workforce\nBenefit from being a TCSer \ud83c\udf9f\ufe0f \u2013 By being part of the TCS Spain family you can enjoy benefits, offers and corporate discounts on the best brands.\nBenefit from being a TCSer\nbenefits, offers and corporate discounts on the best brands\nAnd so on \ud83c\udf89 - Appreciations, incentives, Team Building activities, diversity and inclusion programs, sustainability activities, corporative events... This has only just begun!\nAnd so on\nAppreciations, incentives, Team Building activities, diversity and inclusion programs, sustainability activities, corporative events...\n\ud83d\udca1WHO ARE WE?\nWHO ARE WE?\nTata Consultancy Services (TCS) is an Information Technology (IT) company founded in 1968 as part of the Indian Tata Group and is one of the top 3 technology companies globally \ud83d\ude80\nTata Consultancy Services (TCS)\nInformation Technology (IT) company\nIndian Tata Group\none of the top 3 technology companies globally\nWith a presence in 55 countries and more than 600,000 employees, TCS is considered one of the 10 best companies to work for worldwide in 2024 according to the Top Employers Institute \ud83e\udd47\n55 countries\nemployees\none of the 10 best companies to work for worldwide in 2024\nTop Employers Institute\nTCS Spain started operations in 2001 and currently has a diverse workforce that collaborates with the main Spanish and multinational companies \ud83c\udf0f\nTCS Spain\ndiverse workforce\nmain Spanish and multinational companies\n\ud83c\udf0f\nTCS Spain has been certified as a Top Employer 2024 and has also been chosen as one of the 100 Best Companies to Work for in Spain in 2024 according to Forbes\ud83c\udfc6\nTop Employer 2024\none of the 100 Best Companies to Work for in Spain in 2024\nForbes\nAmong the portfolio of services, TCS has information technology services, asset-based solutions, global consulting, engineering and industrial services, digital solutions and services, application maintenance and development, quality assurance and testing services, IT infrastructure and BPS \ud83c\udfaf\ninformation technology services\nasset-based solutions\nglobal consulting\nengineering and industrial services\ndigital solutions and services\napplication maintenance and development\nquality assurance and testing services\nIT infrastructure\nBPS\nResponsible for development, TCS Spain is committed to inclusion, diversity and sustainability, and promotes flexibility policies that support wellbeing and work-life balance \ud83d\udcaf\ndevelopment\ninclusion, diversity\nsustainability\nflexibility policies\nwellbeing and work-life balance\nWELCOME, WE ARE WAITING FOR YOU! \ud83d\ude80"
    },
    "4162260442": {
        "title": "Senior Data Engineer",
        "company": "Straumann Group",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nThe Straumann Group is a globally leading provider in the field of aesthetic dentistry. We combine experience, scientific evidence, and innovation with passion and are committed to uncompromising quality to offer dental professionals and patients worldwide the best possible solutions. As a pioneer and market leader in dental implantology, we have consistently expanded our product portfolio in recent years. Today, we are active in all areas of aesthetic dentistry, ranging from tooth preservation, regeneration, restoration, orthodontics with transparent aligner trays to tooth loss.\n\nAbout The Role\n\nWe are looking for a Senior Data Engineer to join the Customer Insights Team, reporting to the Head of Customer Insights. You will contribute the purpose of the team, which is:\n\n\u201cTo provide data insights that fuel smarter decisions and drive business value.\"\n\nMain Tasks And Responsibilities\n\n Lead the data engineering and architecture strategy for the team and be the subject matter expert in this area.\n Oversee and improve Power BI data models including data modelling and data ingestion with Power Query Editor.\n Monitor and maintain data quality and integrity in our pipelines and models.\n Be the data owner for IDT data sources.\n Work with members of the data insights team to identify new sources of data and ingest them into the data lake and use them in Power BI data models.\n Collaborate with the data and technology teams to work with the data lake platform.\n Understand the IDT/Straumann business, data and processes to model data structures, relationships and measures.\n Prioritize data engineering work, based on business value and enabling smarter decisions.\n\nRequirements\n\n At least 5-10 commercial experience in data/analytics, in a data engineer or data analyst role.\n Comfortable leading projects and building relationships across the organization, with a desire to progress into a leadership role.\n Able to understand business data, reporting and analytics requirements and build data structures, models and measures to support them.\n Understanding data modeling and ETL concepts, technology and pipeline development. Ideally with leading platforms such as Azure and AWS.\n Experience of building and maintaining Power BI or similar data models.\n Knowledge of the latest technology developments and experience of using them to improve the data infrastructure.\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or disability.\n\nEmployment Type: Full Time\n\nAlternative Locations: Spain : Madrid\n\nTravel Percentage: 0 - 10%\n\nRequisition ID: 17042\n\n17042\nAbout The Role\nMain Tasks And Responsibilities\nLead the data engineering and architecture strategy for the team and be the subject matter expert in this area.\n Oversee and improve Power BI data models including data modelling and data ingestion with Power Query Editor.\n Monitor and maintain data quality and integrity in our pipelines and models.\n Be the data owner for IDT data sources.\n Work with members of the data insights team to identify new sources of data and ingest them into the data lake and use them in Power BI data models.\n Collaborate with the data and technology teams to work with the data lake platform.\n Understand the IDT/Straumann business, data and processes to model data structures, relationships and measures.\n Prioritize data engineering work, based on business value and enabling smarter decisions.\nLead the data engineering and architecture strategy for the team and be the subject matter expert in this area.\nOversee and improve Power BI data models including data modelling and data ingestion with Power Query Editor.\nMonitor and maintain data quality and integrity in our pipelines and models.\nBe the data owner for IDT data sources.\nWork with members of the data insights team to identify new sources of data and ingest them into the data lake and use them in Power BI data models.\nCollaborate with the data and technology teams to work with the data lake platform.\nUnderstand the IDT/Straumann business, data and processes to model data structures, relationships and measures.\nPrioritize data engineering work, based on business value and enabling smarter decisions.\nRequirements\nAt least 5-10 commercial experience in data/analytics, in a data engineer or data analyst role.\n Comfortable leading projects and building relationships across the organization, with a desire to progress into a leadership role.\n Able to understand business data, reporting and analytics requirements and build data structures, models and measures to support them.\n Understanding data modeling and ETL concepts, technology and pipeline development. Ideally with leading platforms such as Azure and AWS.\n Experience of building and maintaining Power BI or similar data models.\n Knowledge of the latest technology developments and experience of using them to improve the data infrastructure.\nAt least 5-10 commercial experience in data/analytics, in a data engineer or data analyst role.\nComfortable leading projects and building relationships across the organization, with a desire to progress into a leadership role.\nAble to understand business data, reporting and analytics requirements and build data structures, models and measures to support them.\nUnderstanding data modeling and ETL concepts, technology and pipeline development. Ideally with leading platforms such as Azure and AWS.\nExperience of building and maintaining Power BI or similar data models.\nKnowledge of the latest technology developments and experience of using them to improve the data infrastructure.\nEmployment Type:\nAlternative Locations:\nTravel Percentage:\nRequisition ID:\n17042"
    },
    "4172652686": {
        "title": "Analytics Data Engineer (m/f/d) ",
        "company": "BSH Electrodom\u00e9sticos Espa\u00f1a, S.A.",
        "location": "Valencia, Valencian Community, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nTomorrow is our home.\n\nSee how easy it is to explore new ideas at BSH Home Appliances Group: As a leading manufacturer of home appliances, we improve people\u2019s quality of life by thinking in solutions. With our global brands Bosch, Siemens, Gaggenau and Neff as well as our local brands, we explore innovative technologies from voice recognition to artificial intelligence. As we utilize these to create new user experiences, we think, prototype, build \u2013 and keep on learning. Join us now and give your career a home.\n\nBSH Electrodom\u00e9sticos Espa\u00f1a, S.A. | Full time |\n\nValencia\n\nYour Responsibilities\n\nYou think it is time for something different? Then you at the right place! You will be working in an exciting international environment and contributing personally to the company\u2019s success. Expect exciting projects and versatile tasks, such as:\n\nDesign, build, and maintain scalable data pipelines to process large datasets efficiently.\nImplement and manage data warehouse and virtualization solutions using AWS Redshift, Denodo, and SAP Datasphere.\nSupport the development and deployment of machine learning models, leveraging tools such as AWS EMR, AWS Airflow, and Alteryx.\nPartner with analytics teams and data scientists to understand data needs, ensure data accuracy, and provide reliable data access solutions.\nRegularly assess system performance, troubleshoot issues, and implement enhancements for better efficiency.\n\nYour profile\n\nBachelors Degree in Engineering or related field\nMinimum 3 years of experience in data engineering or related fields.\nProven expertise in AWS services such as Redshift, Data Lake, EMR, and Airflow.\nKnowledge in SAP HANA, SAP Datasphere, and familiarity in machine learning is a strong advantage.\nExcellent written and verbal communication skills in English.\n\nYour Benefits\n\nFlexible working time schemes and possibility of home office (50%).\nDiscounts in BSH homeappliances and other benefits.\nFree access to our online learning platform, including languages.\nSubsidised food.\n\nFurther information\n\nPlease visit bsh-group.com/career. We would love to welcome you in our team!\n\nFind a New Home For Your Professional Ambitions\n\nBSH Electrodom\u00e9sticos Espa\u00f1a, S.A is the controller of the processing and processes your personal data with the only purpose of managing your application. If you want to exercise your data protection rights, please send an email to Data-Protection-ES@bshg.com.\n\nYour application will also be forwarded to individual employees of the following company for the purpose of filling the position: BSH Ev Aletleri Sanayi ve Ticaret A.\u015e. (Turkey).\nYour Responsibilities\nDesign, build, and maintain scalable data pipelines to process large datasets efficiently.\nImplement and manage data warehouse and virtualization solutions using AWS Redshift, Denodo, and SAP Datasphere.\nSupport the development and deployment of machine learning models, leveraging tools such as AWS EMR, AWS Airflow, and Alteryx.\nPartner with analytics teams and data scientists to understand data needs, ensure data accuracy, and provide reliable data access solutions.\nRegularly assess system performance, troubleshoot issues, and implement enhancements for better efficiency.\nDesign, build, and maintain scalable data pipelines to process large datasets efficiently.\nImplement and manage data warehouse and virtualization solutions using AWS Redshift, Denodo, and SAP Datasphere.\nSupport the development and deployment of machine learning models, leveraging tools such as AWS EMR, AWS Airflow, and Alteryx.\nPartner with analytics teams and data scientists to understand data needs, ensure data accuracy, and provide reliable data access solutions.\nRegularly assess system performance, troubleshoot issues, and implement enhancements for better efficiency.\nYour profile\nBachelors Degree in Engineering or related field\nMinimum 3 years of experience in data engineering or related fields.\nProven expertise in AWS services such as Redshift, Data Lake, EMR, and Airflow.\nKnowledge in SAP HANA, SAP Datasphere, and familiarity in machine learning is a strong advantage.\nExcellent written and verbal communication skills in English.\nBachelors Degree in Engineering or related field\nMinimum 3 years of experience in data engineering or related fields.\nProven expertise in AWS services such as Redshift, Data Lake, EMR, and Airflow.\nKnowledge in SAP HANA, SAP Datasphere, and familiarity in machine learning is a strong advantage.\nExcellent written and verbal communication skills in English.\nYour Benefits\nFlexible working time schemes and possibility of home office (50%).\nDiscounts in BSH homeappliances and other benefits.\nFree access to our online learning platform, including languages.\nSubsidised food.\nFlexible working time schemes and possibility of home office (50%).\nDiscounts in BSH homeappliances and other benefits.\nFree access to our online learning platform, including languages.\nSubsidised food.\nFurther information\nFind a New Home For Your Professional Ambitions"
    },
    "4163520468": {
        "title": "Data Engineer",
        "company": "Remobi",
        "location": "European Union",
        "work_mode": "Remote",
        "job_type": "Contract",
        "description": "About the job\nWe are building the world's greatest community of remote technologists!\n\nToday, organizations that understand the value of remote working will reap the rewards. Because it doesn\u2019t just provide team members with a healthier work-life balance, it gives you the opportunity to access the brightest minds in the world.\n\nOur clients access our community to build or extend their existing teams. All are made up of remote, distributed software engineering experts\u2014best-in-class professionals. Rapidly deployed without compromising on quality.\n\nJoin our Remobi community to have access to meaningful, innovative freelance projects and play a key role in shaping how companies operate.\n\nDuration: Until end of September 2025 (possible extension)\nLocation: Remote\nContract Type: Freelance / B2B / Contract (Candidates must be based in EU)\n\nSummary:\n\nWe are seeking a highly skilled Data Engineer to join our dynamic team and support the implementation of Data Quality rules related to Salesforce objects in a custom-built Data Quality Solution using AWS and Power BI. You will play a key role in improving Power BI functionality, developing scalable data pipelines, and ensuring data quality and integrity. If you thrive in a fast-paced, cloud-driven environment, this role is for you!\n\nHow You\u2019ll Make an Impact\n\nYou will be responsible for:\n\nDeveloping and maintaining data visualization solutions in Power BI for data products.\nDesigning, building, and optimizing data pipelines and ETLs to extract, transform, and load data efficiently.\nImplementing data quality checks using Soda SQL and ensuring data integrity in Snowflake.\nManaging and integrating APIs to enhance data access and usability.\nSupporting cloud infrastructure development and deploying solutions using AWS and CI/CD pipelines in GitLab.\nBuilding reusable components for multiple data solutions.\nCollaborating with business stakeholders to translate business requirements into technical solutions.\nImplementing data engineering best practices for scalability and performance.\nDocumenting all developed components to ensure clear knowledge transfer.\nAssisting in solution architecture design and implementation.\nWriting complex queries in SQL and Snowflake to solve analytical problems.\nCreating, maintaining, and scaling data models for efficient reporting and analysis.\n\nWhat You Bring (Skills, Capabilities)\n\n5+ years of experience in Data Engineering, with a focus on cloud technologies.\nHands-on experience with AWS, Snowflake, and Power BI.\nProficiency in Soda SQL for data quality monitoring.\nStrong expertise in SQL and relational databases, with experience writing and optimizing complex queries.\nExperience with ETL processes, data modeling, and data warehousing concepts.\nFamiliarity with AWS services (S3, Lambda, Glue, Redshift) and CI/CD workflows in GitLab.\nStrong analytical and problem-solving skills with the ability to troubleshoot data pipeline issues.\nExcellent communication and collaboration skills to work effectively in a distributed team.\nWe are building the world's greatest community of remote technologists!\nToday, organizations that understand the value of remote working will reap the rewards. Because it doesn\u2019t just provide team members with a healthier work-life balance, it gives you the opportunity to access the brightest minds in the world.\nOur clients access our community to build or extend their existing teams. All are made up of remote, distributed software engineering experts\u2014best-in-class professionals. Rapidly deployed without compromising on quality.\nJoin our Remobi community to have access to meaningful, innovative freelance projects and play a key role in shaping how companies operate.\nDuration: Until end of September 2025 (possible extension)\nDuration:\nLocation: Remote\nLocation:\nContract Type: Freelance / B2B / Contract (Candidates must be based in EU)\nContract Type:\nSummary:\nWe are seeking a highly skilled Data Engineer to join our dynamic team and support the implementation of Data Quality rules related to Salesforce objects in a custom-built Data Quality Solution using AWS and Power BI. You will play a key role in improving Power BI functionality, developing scalable data pipelines, and ensuring data quality and integrity. If you thrive in a fast-paced, cloud-driven environment, this role is for you!\nData Engineer\nData Quality rules related to Salesforce objects\nData Quality Solution\nAWS and Power BI\nHow You\u2019ll Make an Impact\nYou will be responsible for:\nDeveloping and maintaining data visualization solutions in Power BI for data products.\nDesigning, building, and optimizing data pipelines and ETLs to extract, transform, and load data efficiently.\nImplementing data quality checks using Soda SQL and ensuring data integrity in Snowflake.\nManaging and integrating APIs to enhance data access and usability.\nSupporting cloud infrastructure development and deploying solutions using AWS and CI/CD pipelines in GitLab.\nBuilding reusable components for multiple data solutions.\nCollaborating with business stakeholders to translate business requirements into technical solutions.\nImplementing data engineering best practices for scalability and performance.\nDocumenting all developed components to ensure clear knowledge transfer.\nAssisting in solution architecture design and implementation.\nWriting complex queries in SQL and Snowflake to solve analytical problems.\nCreating, maintaining, and scaling data models for efficient reporting and analysis.\nDeveloping and maintaining data visualization solutions in Power BI for data products.\ndata visualization solutions\nPower BI\nDesigning, building, and optimizing data pipelines and ETLs to extract, transform, and load data efficiently.\ndata pipelines and ETLs\nImplementing data quality checks using Soda SQL and ensuring data integrity in Snowflake.\ndata quality checks\nSoda SQL\ndata integrity\nSnowflake\nManaging and integrating APIs to enhance data access and usability.\nAPIs\nSupporting cloud infrastructure development and deploying solutions using AWS and CI/CD pipelines in GitLab.\ncloud infrastructure development\nAWS\nCI/CD pipelines in GitLab\nBuilding reusable components for multiple data solutions.\nreusable components\nCollaborating with business stakeholders to translate business requirements into technical solutions.\ntranslate business requirements into technical solutions\nImplementing data engineering best practices for scalability and performance.\ndata engineering best practices\nDocumenting all developed components to ensure clear knowledge transfer.\nAssisting in solution architecture design and implementation.\nsolution architecture design\nWriting complex queries in SQL and Snowflake to solve analytical problems.\ncomplex queries\nCreating, maintaining, and scaling data models for efficient reporting and analysis.\ndata models\nWhat You Bring (Skills, Capabilities)\n5+ years of experience in Data Engineering, with a focus on cloud technologies.\nHands-on experience with AWS, Snowflake, and Power BI.\nProficiency in Soda SQL for data quality monitoring.\nStrong expertise in SQL and relational databases, with experience writing and optimizing complex queries.\nExperience with ETL processes, data modeling, and data warehousing concepts.\nFamiliarity with AWS services (S3, Lambda, Glue, Redshift) and CI/CD workflows in GitLab.\nStrong analytical and problem-solving skills with the ability to troubleshoot data pipeline issues.\nExcellent communication and collaboration skills to work effectively in a distributed team.\n5+ years of experience in Data Engineering, with a focus on cloud technologies.\n5+ years of experience\nHands-on experience with AWS, Snowflake, and Power BI.\nAWS, Snowflake, and Power BI\nProficiency in Soda SQL for data quality monitoring.\nStrong expertise in SQL and relational databases, with experience writing and optimizing complex queries.\nSQL and relational databases\nExperience with ETL processes, data modeling, and data warehousing concepts.\nETL processes, data modeling, and data warehousing\nFamiliarity with AWS services (S3, Lambda, Glue, Redshift) and CI/CD workflows in GitLab.\nAWS services\nGitLab\nStrong analytical and problem-solving skills with the ability to troubleshoot data pipeline issues.\ntroubleshoot data pipeline issues\nExcellent communication and collaboration skills to work effectively in a distributed team."
    },
    "4142675985": {
        "title": "Data Engineer",
        "company": "Allianz Technology",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "NULL",
        "job_type": "NULL",
        "description": "About the job\nAre you passionate about data and its transformative potential for businesses? Do you thrive in a dynamic environment where technical expertise and collaboration are key? We\u2019re looking for a Data Engineer to play a pivotal role in developing and operationalizing an analytics solution for our customer. This is your chance to contribute to a cutting-edge project that integrates and automates data for business use, helping the organization harness the full power of analytics.\n\nAs a key member of a cross-functional team, you will collaborate with business SMEs and technical experts to design scalable data pipelines, optimize cloud-based data warehouses, and ensure that data is ready for impactful business insights. You will be at the forefront of data security, quality, and integration, actively championing the value of analytics across the organization.\n\nIf you're a skilled data professional with experience in cloud technologies, data manipulation, and process automation, and you're eager to make a meaningful impact through your technical expertise, we want to hear from you!\n\nWhat you can find at Allianz Technology:\n\nWork Life - Balance: We offer flexible working hours so you can start your working day according to your needs. In addition, during the summer and every Friday of the year, you have the option of intensive working hours, which will allow you to enjoy more free time for yourself.\nInternational and Multicultural Environment: We value the diversity and richness that comes from working in an international and multicultural environment. With more than 52 different nationalities represented in our company, we offer our employees the opportunity to collaborate with people from all over the world, enriching the work environment and providing the opportunity to learn and grow in a truly global setting. This diversity allows us to have a broader perspective, foster creativity and innovation, and promote an inclusive work environment where every voice is heard and valued.\nProfessional Growth Opportunities: We are committed to the professional development of our employees and provide numerous opportunities for growth within the company. We value internal talent and promote internal advancement, offering training programs, mentoring, and skills development that allow our employees to advance in their careers. \nCompensation and Benefits Package: It includes a company bonus plan, pension, employee stock program, and multiple employee discounts. \n\nWhat will make you succeed in this position?\n\n Data manipulation and extraction tools. \n Knowledge and understanding of the tools and processes to ensure data security, quality and accuracy. \n Knowledge and capability of Microsoft Azure toolset. \n Significant Knowledge and capability of market leading BI and analytics tools / processes outputs etc. \n Proven experience of detailed data analysis and manipulation. \n Knowledge in PySpark \n Proven experience and knowledge of and can demonstrate use of best practice data management and in particular data administration. \n Experience of working in a busy environment whilst maintaining a high degree of accuracy at all times and meeting tight deadlines. \n Experience gained to proactively build and maintain relationships both externally and with customers throughout the company. \n Proven experience of leading data centric or technical projects. \n Proven experience in mentoring or managing technical teams in the use of technical software, design, modelling and information management. \n\nYour mission in the role will be:\n\n To liaise with business SMEs to achieve a common understanding of the business challenge, their needs, data requirements and expected output. \n To be responsible for the development and maintenance of automated, repeatable processes for integrating data and providing it to business analytics users in an appropriate format. \n Working with architects and developers, provide technical input into matching the best technology to the use case and develop an ongoing process. \n To work closely within a cross functional team to scope, develop, articulate progress and results including business case to progress further. (End to end Technical to business use). \n Work with SMEs to analyze the data. \n Work with architect to develop process for production use cases. \n Actively championing analytics across all areas and levels of the business. \n Working on all aspects of the project as needed by the overall team. \n To be responsible for maintaining standards around the use and access to data, ensuring adherence to internal and external regulatory factors. \n\nAbout Allianz Technology\n\nAllianz Technology is the global IT service provider for Allianz and delivers IT solutions that drive the digitalization of the Group. With more than 12,000 employees located in 51 countries around the globe, Allianz Technology works together with other Allianz entities in pioneering the digitalization of the financial services industry.\n\nWe oversee the full digitalization spectrum \u2013 from one of the industry\u2019s largest IT infrastructure projects that includes data centers, networking and security, to application platforms that span from workplace services to digital interaction. In short, we deliver full-scale, end-to-end IT solutions for Allianz in the digital age.\n\nD&I statement\n\nAllianz Technology is proud to be an equal opportunity employer encouraging diversity in the working environment. We are interested in your strengths and experience. We welcome all applications from all people regardless of gender identity and/or expression, sexual orientation, ethnicity and cultural background, age, nationality, religion, disability, or philosophy of life.\n\nJoin us. Let\u00b4s care for tomorrow.\n\nYou. IT\n\n58278 | Ingenier\u00eda inform\u00e1tica y tecnol\u00f3gica | Profesional / Senior | Non-Executive | Allianz Technology | Jornada completa | Indefinido\nData Engineer\nWhat you can find at Allianz Technology:\nWork Life - Balance: We offer flexible working hours so you can start your working day according to your needs. In addition, during the summer and every Friday of the year, you have the option of intensive working hours, which will allow you to enjoy more free time for yourself.\nInternational and Multicultural Environment: We value the diversity and richness that comes from working in an international and multicultural environment. With more than 52 different nationalities represented in our company, we offer our employees the opportunity to collaborate with people from all over the world, enriching the work environment and providing the opportunity to learn and grow in a truly global setting. This diversity allows us to have a broader perspective, foster creativity and innovation, and promote an inclusive work environment where every voice is heard and valued.\nProfessional Growth Opportunities: We are committed to the professional development of our employees and provide numerous opportunities for growth within the company. We value internal talent and promote internal advancement, offering training programs, mentoring, and skills development that allow our employees to advance in their careers. \nCompensation and Benefits Package: It includes a company bonus plan, pension, employee stock program, and multiple employee discounts.\nWork Life - Balance: We offer flexible working hours so you can start your working day according to your needs. In addition, during the summer and every Friday of the year, you have the option of intensive working hours, which will allow you to enjoy more free time for yourself.\nInternational and Multicultural Environment: We value the diversity and richness that comes from working in an international and multicultural environment. With more than 52 different nationalities represented in our company, we offer our employees the opportunity to collaborate with people from all over the world, enriching the work environment and providing the opportunity to learn and grow in a truly global setting. This diversity allows us to have a broader perspective, foster creativity and innovation, and promote an inclusive work environment where every voice is heard and valued.\nProfessional Growth Opportunities: We are committed to the professional development of our employees and provide numerous opportunities for growth within the company. We value internal talent and promote internal advancement, offering training programs, mentoring, and skills development that allow our employees to advance in their careers.\nCompensation and Benefits Package: It includes a company bonus plan, pension, employee stock program, and multiple employee discounts.\nWhat will make you succeed in this position?\nData manipulation and extraction tools. \n Knowledge and understanding of the tools and processes to ensure data security, quality and accuracy. \n Knowledge and capability of Microsoft Azure toolset. \n Significant Knowledge and capability of market leading BI and analytics tools / processes outputs etc. \n Proven experience of detailed data analysis and manipulation. \n Knowledge in PySpark \n Proven experience and knowledge of and can demonstrate use of best practice data management and in particular data administration. \n Experience of working in a busy environment whilst maintaining a high degree of accuracy at all times and meeting tight deadlines. \n Experience gained to proactively build and maintain relationships both externally and with customers throughout the company. \n Proven experience of leading data centric or technical projects. \n Proven experience in mentoring or managing technical teams in the use of technical software, design, modelling and information management.\nData manipulation and extraction tools.\nKnowledge and understanding of the tools and processes to ensure data security, quality and accuracy.\nKnowledge and capability of Microsoft Azure toolset.\nSignificant Knowledge and capability of market leading BI and analytics tools / processes outputs etc.\nProven experience of detailed data analysis and manipulation.\nKnowledge in PySpark\nProven experience and knowledge of and can demonstrate use of best practice data management and in particular data administration.\nExperience of working in a busy environment whilst maintaining a high degree of accuracy at all times and meeting tight deadlines.\nExperience gained to proactively build and maintain relationships both externally and with customers throughout the company.\nProven experience of leading data centric or technical projects.\nProven experience in mentoring or managing technical teams in the use of technical software, design, modelling and information management.\nYour mission in the role will be:\nTo liaise with business SMEs to achieve a common understanding of the business challenge, their needs, data requirements and expected output. \n To be responsible for the development and maintenance of automated, repeatable processes for integrating data and providing it to business analytics users in an appropriate format. \n Working with architects and developers, provide technical input into matching the best technology to the use case and develop an ongoing process. \n To work closely within a cross functional team to scope, develop, articulate progress and results including business case to progress further. (End to end Technical to business use). \n Work with SMEs to analyze the data. \n Work with architect to develop process for production use cases. \n Actively championing analytics across all areas and levels of the business. \n Working on all aspects of the project as needed by the overall team. \n To be responsible for maintaining standards around the use and access to data, ensuring adherence to internal and external regulatory factors.\nTo liaise with business SMEs to achieve a common understanding of the business challenge, their needs, data requirements and expected output.\nTo be responsible for the development and maintenance of automated, repeatable processes for integrating data and providing it to business analytics users in an appropriate format.\nWorking with architects and developers, provide technical input into matching the best technology to the use case and develop an ongoing process.\nTo work closely within a cross functional team to scope, develop, articulate progress and results including business case to progress further. (End to end Technical to business use).\nWork with SMEs to analyze the data.\nWork with architect to develop process for production use cases.\nActively championing analytics across all areas and levels of the business.\nWorking on all aspects of the project as needed by the overall team.\nTo be responsible for maintaining standards around the use and access to data, ensuring adherence to internal and external regulatory factors.\nAbout Allianz Technology\nD&I statement"
    },
    "4173569027": {
        "title": "Data Governance & Quality Engineer ",
        "company": "Inetum",
        "location": "Greater Madrid Metropolitan Area",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nInetum is an agile IT services company that provides digital services and solutions, and a global group that helps companies and institutions to get the most out of digital flow. In a context of perpetual movement, where needs and usages are constantly being reinvented, the Inetum group is committed to all these players to innovate, continue to adapt, and stay ahead. With its multi-expert profile, Inetum offers its clients a unique combination of proximity, a breakdown by sector, and state of the art solutions.\n\n\ud83d\udce2 We are looking for a Data Governance & Quality Engineer! \ud83d\udce2\nWe are seeking a specialist in data governance and quality management platforms, with expertise in Informatica, Collibra, or Palantir. You will be responsible for the deployment, development, and optimization of these platforms within the PF Information System, ensuring compatibility with existing infrastructure.\n\n\ud83d\udccd Hybrid work model in the client's offices in Madrid city.\n\n\n\ud83d\udd39 Key Responsibilities:\n\u2705 Implementation and maintenance of Informatica, Collibra, or Palantir\n\ud83d\uddc4\ufe0f Data migration and governance strategy development\n\ud83d\udee0\ufe0f Supporting IT OPS teams in pre-production and production environments\n\ud83d\udcd1 Technical documentation and infrastructure setup\n\ud83d\udd0d Continuous support, troubleshooting, and performance optimization\n\n\n\ud83d\udd39 Required Skills:\n\ud83d\udca1 Expertise in Informatica, Collibra, or Palantir\n\ud83d\udda5\ufe0f DevOps & SQL queries\n\ud83d\udc27 Knowledge of Linux & Oracle (preferred)\n\u2601\ufe0f Cloud infrastructure experience (a plus)\n\ud83d\udde3\ufe0f English level B2 or higher (French is a plus)\n\n\nIf you're passionate about data governance, platform deployment, and infrastructure optimization, let\u2019s connect! \ud83e\udd1d\nInetum is an agile IT services company that provides digital services and solutions, and a global group that helps companies and institutions to get the most out of digital flow. In a context of perpetual movement, where needs and usages are constantly being reinvented, the Inetum group is committed to all these players to innovate, continue to adapt, and stay ahead. With its multi-expert profile, Inetum offers its clients a unique combination of proximity, a breakdown by sector, and state of the art solutions.\nInetum\nis an agile IT services company that provides digital services and solutions, and a global group that helps companies and institutions to get the most out of digital flow. In a context of perpetual movement, where needs and usages are constantly being reinvented, the Inetum group is committed to all these players to innovate, continue to adapt, and stay ahead. With its multi-expert profile, Inetum offers its clients a unique combination of proximity, a breakdown by sector, and state of the art solutions.\n\ud83d\udce2 We are looking for a Data Governance & Quality Engineer! \ud83d\udce2\nWe are looking for a Data Governance & Quality Engineer!\nWe are seeking a specialist in data governance and quality management platforms, with expertise in Informatica, Collibra, or Palantir. You will be responsible for the deployment, development, and optimization of these platforms within the PF Information System, ensuring compatibility with existing infrastructure.\ndata governance and quality management platforms\nInformatica, Collibra, or Palantir\ndeployment, development, and optimization\n\ud83d\udccd Hybrid work model in the client's offices in Madrid city.\nHybrid work model in the client's offices in Madrid city.\n\ud83d\udd39 Key Responsibilities:\nKey Responsibilities:\n\u2705 Implementation and maintenance of Informatica, Collibra, or Palantir\n\ud83d\uddc4\ufe0f Data migration and governance strategy development\n\ud83d\udee0\ufe0f Supporting IT OPS teams in pre-production and production environments\n\ud83d\udcd1 Technical documentation and infrastructure setup\n\ud83d\udd0d Continuous support, troubleshooting, and performance optimization\n\ud83d\udd39 Required Skills:\nRequired Skills:\n\ud83d\udca1 Expertise in Informatica, Collibra, or Palantir\nExpertise in Informatica, Collibra, or Palantir\n\ud83d\udda5\ufe0f DevOps & SQL queries\n\ud83d\udc27 Knowledge of Linux & Oracle (preferred)\n\u2601\ufe0f Cloud infrastructure experience (a plus)\n\ud83d\udde3\ufe0f English level B2 or higher (French is a plus)\nIf you're passionate about data governance, platform deployment, and infrastructure optimization, let\u2019s connect! \ud83e\udd1d\ndata governance, platform deployment, and infrastructure optimization"
    },
    "4161087739": {
        "title": "Busqueda Urgente Data Engineer",
        "company": "Interstock Development",
        "location": "\u00c1lava, Principality of Asturias, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nInterstock es una empresa radicada en Donosti dedicada al dise\u00f1o y desarrollo de soluciones de inteligencia artificial para mercados financieros.\n\nSe realizan servicios personalizados aplicando machine learning al desarrollo de estrategias de trading, a la gesti\u00f3n activa de carteras de inversi\u00f3n y a la gesti\u00f3n del riesgo.Buscamos un Data Engineer altamente cualificado y motivado, que se encargue de asegurar la disponibilidad, rendimiento, actualizaci\u00f3n continua, consistencia e integridad de los datos en toda nuestra organizaci\u00f3n.\n\nEntre sus tareas:Actualizaci\u00f3n de la infraestructura de datos existente.Integrar APIs para la recogida y procesamiento interno de datos.Gestionar bases de datos y asegurar su disponibilidad y rendimiento.Supervisar la integridad y calidad de los datos.Garantizar la alta disponibilidad y recuperaci\u00f3n de datos cr\u00edticos.Desarrollar interfaces gr\u00e1ficas para el control y la gesti\u00f3n de datos.Realizar migraciones a la nube.Estudios m\u00ednimos- Grado en ingenier\u00eda inform\u00e1tica o similaresExperiencia m\u00ednima- Al menos 3 a\u00f1osRequisitos m\u00ednimosExperiencia en arquitectura y gesti\u00f3n de infraestructuras de datos.Conocimientos en tecnolog\u00edas de almacenamiento y gesti\u00f3n de bases de datos.Experiencia con APIs, integridad de datos y soluciones en la nube.Lenguajes de programaci\u00f3n: Java, PythonRequisitos deseadosInter\u00e9s en el mundo de los mercados financieros.Conocimientos de arquitecturas basadas en eventos (Kafka,...)Capacidad de autoaprendizaje, iniciativa y proactividad.Se ofrece:Horario flexible y modalidad de trabajo h\u00edbrida (remoto/presencial)Contrato indefinidoOportunidad de crecimiento y desarrollo profesional.Interesados enviar CV a: ******"
    },
    "4167173503": {
        "title": "Data Center Junior Operations Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Salamanca, Castilla and Leon, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Junior Operations Engineer to join our Spain-Portugal team, working directly with our Data Center Site Manager and focusing on supporting the site operations team.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.\n\nKey Responsibilities Include but are not limited to:Support Management in Maintenance, Operations, Projects, Security and Cleaning of the assigned facilities.Provide support in the supervision of the execution of services at a technical level.Provide support in personnel management (training, selection, labor relations).Provide support in the supervision of compliance with internal policies and procedures in Risk Prevention, Health and Safety and Quality.Provide support in the supervision of the methodologies implemented.Provide support in the preparation of budgets.Provide support in the generation of reports.Skills & RequirementsEducation / ExperienceBachelor's degreeValuable financial knowledgeSpecific Skills / AbilitiesAdvanced Microsoft Office 365 (Power BI, Word, Excel, MS App,...)Knowledge Data Analysis Tools (Phyton, Minitab,...)Fluent in English (B2+ - C1)Position: Junior Operations TeamFull-Time or Part-Time: Full-timeReports to: Data Center Site ManagerLocation: Vitoria / Madrid / BarcelonaContact: ******"
    },
    "4167170955": {
        "title": "Data Center Junior Operations Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Torrej\u00f3n de Ardoz, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Junior Operations Engineer to join our Spain-Portugal team, working directly with our Data Center Site Manager and focusing on supporting the site operations team.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.\n\nKey Responsibilities Include but are not limited to:Support Management in Maintenance, Operations, Projects, Security and Cleaning of the assigned facilities.Provide support in the supervision of the execution of services at a technical level.Provide support in personnel management (training, selection, labor relations).Provide support in the supervision of compliance with internal policies and procedures in Risk Prevention, Health and Safety and Quality.Provide support in the supervision of the methodologies implemented.Provide support in the preparation of budgets.Provide support in the generation of reports.Skills & RequirementsEducation / ExperienceBachelor's degreeValuable financial knowledgeSpecific Skills / AbilitiesAdvanced Microsoft Office 365 (Power BI, Word, Excel, MS App,...)Knowledge Data Analysis Tools (Phyton, Minitab,...)Fluent in English (B2+ - C1)Position: Junior Operations TeamFull-Time or Part-Time: Full-timeReports to: Data Center Site ManagerLocation: Vitoria / Madrid / BarcelonaContact: ******"
    },
    "4167175058": {
        "title": "Data Center Junior Operations Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Legan\u00e9s, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Junior Operations Engineer to join our Spain-Portugal team, working directly with our Data Center Site Manager and focusing on supporting the site operations team.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.\n\nKey Responsibilities Include but are not limited to:Support Management in Maintenance, Operations, Projects, Security and Cleaning of the assigned facilities.Provide support in the supervision of the execution of services at a technical level.Provide support in personnel management (training, selection, labor relations).Provide support in the supervision of compliance with internal policies and procedures in Risk Prevention, Health and Safety and Quality.Provide support in the supervision of the methodologies implemented.Provide support in the preparation of budgets.Provide support in the generation of reports.Skills & RequirementsEducation / ExperienceBachelor's degreeValuable financial knowledgeSpecific Skills / AbilitiesAdvanced Microsoft Office 365 (Power BI, Word, Excel, MS App,...)Knowledge Data Analysis Tools (Phyton, Minitab,...)Fluent in English (B2+ - C1)Position: Junior Operations TeamFull-Time or Part-Time: Full-timeReports to: Data Center Site ManagerLocation: Vitoria / Madrid / BarcelonaContact: ******"
    },
    "4178438469": {
        "title": "Data Engineer Scala-Spark (Market Risk) ",
        "company": "Common Management Solutions",
        "location": "Spain",
        "work_mode": "Remote",
        "job_type": "Full-time",
        "description": "About the job\nData Engineer Scala Spark - Finance\n\nSobre el Rol\nDesde Common MS, estamos buscando un Ingeniero de Datos para apoyar en las tareas del Datahub de Markets a trav\u00e9s de BigData (Scala - Spark) en Market Risk\n\n\nRequisitos\nSolida experiencia trabajando con SQL\nExperiencia en dise\u00f1o de modelado de datos\nDominio en la realizaci\u00f3n de pruebas t\u00e9cnicas/funcionales exhaustivas y validaci\u00f3n de datos\nExperiencia en proyectos tecnol\u00f3gicos en el sector banca\nIngl\u00e9s medio/alto\n\nRequisitos Deseados\nConocimiento t\u00e9cnico de Scala, Spark & Hadoop\nConocimiento de productos financieros o m\u00e9tricas de riesgos de mercado\n\nLo que Ofrecemos\nLa oportunidad de construir una plataforma de datos innovadora con alta visibilidad dentro del banco.\nAmbiente colaborativo y \u00e1gil en un banco de inversi\u00f3n global.\nSalario competitivo y oportunidades de desarrollo profesional.\nModalidad remota (Espa\u00f1a)\n\n\u00a1\u00danete a nosotros y forma parte del futuro de la toma de decisiones basada en datos en la banca de inversi\u00f3n!\nData Engineer Scala Spark - Finance\nSobre el Rol\nDesde Common MS, estamos buscando un Ingeniero de Datos para apoyar en las tareas del Datahub de Markets a trav\u00e9s de BigData (Scala - Spark) en Market Risk\nRequisitos\nSolida experiencia trabajando con SQL\nExperiencia en dise\u00f1o de modelado de datos\nDominio en la realizaci\u00f3n de pruebas t\u00e9cnicas/funcionales exhaustivas y validaci\u00f3n de datos\nExperiencia en proyectos tecnol\u00f3gicos en el sector banca\nIngl\u00e9s medio/alto\nSolida experiencia trabajando con SQL\nExperiencia en dise\u00f1o de modelado de datos\nDominio en la realizaci\u00f3n de pruebas t\u00e9cnicas/funcionales exhaustivas y validaci\u00f3n de datos\nExperiencia en proyectos tecnol\u00f3gicos en el sector banca\nIngl\u00e9s medio/alto\nRequisitos Deseados\nConocimiento t\u00e9cnico de Scala, Spark & Hadoop\nConocimiento de productos financieros o m\u00e9tricas de riesgos de mercado\nConocimiento t\u00e9cnico de Scala, Spark & Hadoop\nConocimiento de productos financieros o m\u00e9tricas de riesgos de mercado\nLo que Ofrecemos\nLa oportunidad de construir una plataforma de datos innovadora con alta visibilidad dentro del banco.\nAmbiente colaborativo y \u00e1gil en un banco de inversi\u00f3n global.\nSalario competitivo y oportunidades de desarrollo profesional.\nModalidad remota (Espa\u00f1a)\nLa oportunidad de construir una plataforma de datos innovadora con alta visibilidad dentro del banco.\nconstruir una plataforma de datos innovadora\nAmbiente colaborativo y \u00e1gil en un banco de inversi\u00f3n global.\nAmbiente colaborativo y \u00e1gil\nSalario competitivo y oportunidades de desarrollo profesional.\noportunidades de desarrollo profesional\nModalidad remota (Espa\u00f1a)\n\u00a1\u00danete a nosotros y forma parte del futuro de la toma de decisiones basada en datos en la banca de inversi\u00f3n!\ntoma de decisiones basada en datos en la banca de inversi\u00f3n"
    },
    "4153824303": {
        "title": "Data Engineer Lead ",
        "company": "Hays",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "Hybrid",
        "job_type": "Full-time",
        "description": "About the job\nTu nueva empresa:\nConsultora internacional, l\u00edder global en el sector de Data&Analytics\n\nTu nuevo puesto:\nDise\u00f1o y construcci\u00f3n de infraestructuras de datos\nGesti\u00f3n del flujo de datos\nOptimizaci\u00f3n de bases de datos\nLimpieza y preparaci\u00f3n de datos\nDise\u00f1ar e Implementar pipelines de datos \nGestionar clientes, detectar sus necesidades y desarrollar oportunidades\nGesti\u00f3n de un equipo \n\n\nRequisitos:\n  Estudios en ingenier\u00eda, matem\u00e1ticas, ciencia de datos o similares\nM\u00e1s de 3 a\u00f1os de experiencia en el campo de los datos \nConocimientos avanzados de SQL\nConocimiento en Pyspark + Databricks y/o Snowflake \nIngl\u00e9s avanzado es un PLUS\nCapacidad de gestionar equipos peque\u00f1os es un PLUS\n\nBeneficios:\nEmpresa internacional, joven y din\u00e1mica\nModalidad de trabajo hibrida \nHorario flexible\nViernes horario intensivo\nVerano horario intensivo (meses de julio y agosto)\nUbicaci\u00f3n: Barcelona \nPlan de carrera\nSalario variable (hasta 45.000 euros b/a)\nTu nueva empresa:\nConsultora internacional, l\u00edder global en el sector de Data&Analytics\nTu nuevo puesto:\nDise\u00f1o y construcci\u00f3n de infraestructuras de datos\nGesti\u00f3n del flujo de datos\nOptimizaci\u00f3n de bases de datos\nLimpieza y preparaci\u00f3n de datos\nDise\u00f1ar e Implementar pipelines de datos \nGestionar clientes, detectar sus necesidades y desarrollar oportunidades\nGesti\u00f3n de un equipo\nDise\u00f1o y construcci\u00f3n de infraestructuras de datos\nGesti\u00f3n del flujo de datos\nOptimizaci\u00f3n de bases de datos\nLimpieza y preparaci\u00f3n de datos\nDise\u00f1ar e Implementar pipelines de datos\nGestionar clientes, detectar sus necesidades y desarrollar oportunidades\nGesti\u00f3n de un equipo\nRequisitos:\nEstudios en ingenier\u00eda, matem\u00e1ticas, ciencia de datos o similares\nM\u00e1s de 3 a\u00f1os de experiencia en el campo de los datos \nConocimientos avanzados de SQL\nConocimiento en Pyspark + Databricks y/o Snowflake \nIngl\u00e9s avanzado es un PLUS\nCapacidad de gestionar equipos peque\u00f1os es un PLUS\nEstudios en ingenier\u00eda, matem\u00e1ticas, ciencia de datos o similares\nM\u00e1s de 3 a\u00f1os de experiencia en el campo de los datos\nConocimientos avanzados de SQL\nConocimiento en Pyspark + Databricks y/o Snowflake\nIngl\u00e9s avanzado es un PLUS\nCapacidad de gestionar equipos peque\u00f1os es un PLUS\nBeneficios:\nEmpresa internacional, joven y din\u00e1mica\nModalidad de trabajo hibrida \nHorario flexible\nViernes horario intensivo\nVerano horario intensivo (meses de julio y agosto)\nUbicaci\u00f3n: Barcelona \nPlan de carrera\nSalario variable (hasta 45.000 euros b/a)\nEmpresa internacional, joven y din\u00e1mica\nModalidad de trabajo hibrida\nHorario flexible\nViernes horario intensivo\nVerano horario intensivo (meses de julio y agosto)\nUbicaci\u00f3n: Barcelona\nPlan de carrera\nSalario variable (hasta 45.000 euros b/a)"
    },
    "4163471105": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Nava, Principality of Asturias, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163466990": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Jerez de la Frontera, Andalusia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network amp; OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network amp; connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report amp; management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global amp; local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills amp; RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies amp; methodologiesAbility to improve performance management amp; critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.Position: Data Center Network amp; OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4167173579": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Las Palmas de Gran Canaria, Canary Islands, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163472027": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Barcelona, Catalonia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163472021": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Seville, Andalusia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163471104": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Santander, Cantabria, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4167172353": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "\u00c1lava, Principality of Asturias, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies methodologiesAbility to improve performance management critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4158746777": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Madrid, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities\n\nInclude But Are Not Limited To\n\nEnsure implementation plans, test plans, and project timelines for network projects are completed to our standards.\n\nLead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.\n\nManage the interconnection between data centers: report & management capacity of physical traces between data centers.\n\nOversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.\n\nLong haul evaluation and schedule development with cost certainty for last mile\n\nCoordination and Level 3 of attention in case of incident.\n\nResponsible for establishing and maintaining the network in the Data Center Facilities,\n\nWork with multiple internal teams and provide IT Network Engineering support for their various projects.\n\nWork with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.\n\nManagement NOC and SOC providers.\n\nStrategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.\n\nCarry out supervisory responsibilities in accordance with the organization's policies and procedures.\n\nCoordinate support of change management resources in partnership with electrical and mechanical operations teams.\n\nImplementing global & local standardization.\n\nProvide technical guidance and expertise to the team on project related topics.\n\nBuilds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & Requirements\n\nEducation / Experience\n\n3+ years of experience in Data Center or similar roles.\n\nProven track record of success in delivering complex projects.\n\nPrior ownership of the operation of a mission-critical team and/or product.\n\nAbility to consistently deliver results despite competing priorities and distractions.\n\nExperience working with and deploying telecom and network providers, specifically in new projects\n\nProject Management Experience.\n\nCisco, meraki, aruba certifications is a plus\n\nCapacity Planning And Budgetary Experience.Specific Skills / Abilities\n\nTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environment\n\nStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actions\n\nAbility to interpret industry data center standards, policies & methodologies\n\nAbility to improve performance management & critical awareness methods and implementation standards thinking out the box\n\nStrong interpersonal, presentation and communication skills\n\nStrong organizational skills and detail oriented\n\nAbility to manage budgets and analyzing expenses\n\nExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)\n\nFluent in EnglishGreat Place to Work\n\nEnjoy\u2026\n\nA fast-paced, entrepreneurial culture focused on innovation.\n\nA flexible, autonomous work environment.\n\nA culture of respect, learning, and excellence.\n\nExperienced, highly talented experts as team peers.\n\nGrowth and travel opportunities.\n\nA team of change-makers having a significant impact on people and the planet.About Edged Spain-Portugal\n\nEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.\n\nPosition:Data Center Network & OT Engineer\n\nFull-Time or Part-Time:Full-Time\n\nReports to:Regional Network Manager\n\nLocation:Vitoria or Madrid or Barcelona (On-Site)Contact: ******\nInclude But Are Not Limited To\nCapacity Planning And Budgetary Experience.Specific Skills / Abilities"
    },
    "4163467816": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Gij\u00f3n, Principality of Asturias, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4167170656": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Chartered Community of Navarre, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies methodologiesAbility to improve performance management critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163472028": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Balearic Islands, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4167175069": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Morc\u00edn, Principality of Asturias, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies methodologiesAbility to improve performance management critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163471130": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Valderredible, Cantabria, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163472048": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Ribamont\u00e1n al Mar, Cantabria, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163470385": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "C\u00e1ceres, Extremadura, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4167167965": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Le\u00f3n, Castilla and Leon, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies methodologiesAbility to improve performance management critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4167181201": {
        "title": "Data Center Network & Ot Engineer | [N113]",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Alcobendas, Community of Madrid, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nOpportunityEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network connectivity.\n\nEdged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.We seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mile coordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities.Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projects.Project Management Experience.Cisco, meraki, aruba certifications is a plus.Capacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environment.Strong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actions.Ability to interpret industry data center standards, policies methodologies.Ability to improve performance management critical awareness methods and implementation standards thinking out the box.Strong interpersonal, presentation and communication skills.Strong organizational skills and detail oriented.Ability to manage budgets and analyzing expenses.Experience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM).Fluent in English.Great Place to Work Enjoy\u2026A fast-paced, entrepreneurial culture focused on innovation.\n\nA flexible, autonomous work environment.\n\nA culture of respect, learning, and excellence.\n\nExperienced, highly talented experts as team peers.\n\nGrowth and travel opportunities.\n\nA team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIts energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.Position:Data Center Network OT EngineerFull-Time or Part-Time:Full-TimeReports to:Regional Network ManagerLocation:Vitoria or Madrid or Barcelona (On-Site)Contact:******\n\n#J-18808-Ljbffr"
    },
    "4165293942": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Greater Madrid Metropolitan Area",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.\n\nEdged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.\n\nKey Responsibilities Include but are not limited to: Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues. Skills & Requirements Education / Experience 3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience. Specific Skills / Abilities Team, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in English Great Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.\n\nA flexible, autonomous work environment.\n\nA culture of respect, learning, and excellence.\n\nExperienced, highly talented experts as team peers.\n\nGrowth and travel opportunities.\n\nA team of change-makers having a significant impact on people and the planet.\n\nAbout Edged Spain-Portugal Edged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.\n\nPosition: Data Center Network & OT Engineer\n\nFull-Time or Part-Time: Full-Time\n\nReports to: Regional Network Manager\n\nLocation: Vitoria or Madrid or Barcelona (On-Site)\n\nContact: ******"
    },
    "4163465986": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Villafranca, Castilla and Leon, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163466987": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Cartagena, Regi\u00f3n de Murcia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4167172438": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Lugo, Galicia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies methodologiesAbility to improve performance management critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163470349": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Valladolid, Castilla and Leon, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163474010": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "O\u00f1a, Castilla and Leon, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies methodologiesAbility to improve performance management critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4167170681": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Murcia, Regi\u00f3n de Murcia, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies methodologiesAbility to improve performance management critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4167170688": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Badajoz, Extremadura, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies methodologiesAbility to improve performance management critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    },
    "4163468793": {
        "title": "Data Center Network & Ot Engineer",
        "company": "Endeavour. Inspired Infrastructure.",
        "location": "Toledo, Castile-La Mancha, Spain",
        "work_mode": "On-site",
        "job_type": "Full-time",
        "description": "About the job\nThis job is sourced from a job board. Learn More\nEdged Spain-Portugal has an exciting opportunity for someone passionate about sustainability and eager to be part of an innovative company that's on a journey to transform the world's data center infrastructure.\n\nWe are seeking a Data Center Network & OT Engineer to join our Spain-Portugal team, working directly with our Regional Network Manager and Data Center Site Managers and focusing on Data Center Network & connectivity.Edged is highly selective about the people we bring on board because our mission depends on it.\n\nProgress happens quickly and we must be able to trust one another to be transparent, communicative, strategic, reliable, and driven.\n\nWe seek ultra-creatives and superstar performers with a sense of humility and a hunger to make a positive impact in the world.\n\nWe offer flexibility and endless growth opportunities to those who can harness their skills and talents and identify how and where to use them to add value.Key Responsibilities Include but are not limited to:Ensure implementation plans, test plans, and project timelines for network projects are completed to our standards.Lead and supervise a team or supplier which is installing, maintaining, and decommissioning network and server hardware.Manage the interconnection between data centers: report & management capacity of physical traces between data centers.Oversee the safety, security, availability, quality, and performance of the team, while driving a positive customer experience.Long haul evaluation and schedule development with cost certainty for last mileCoordination and Level 3 of attention in case of incident.Responsible for establishing and maintaining the network in the Data Center Facilities,Work with multiple internal teams and provide IT Network Engineering support for their various projects.Work with business and internal partners to provide connectivity to the Edged Spain environment using secure site-to-site VPN tunnels.Management NOC and SOC providers.Strategic planning and forecasting; manage team and individual performance; delivering performance reviews, and address staffing needs.Carry out supervisory responsibilities in accordance with the organization's policies and procedures.Coordinate support of change management resources in partnership with electrical and mechanical operations teams.Implementing global & local standardization.Provide technical guidance and expertise to the team on project related topics.Builds and develops relevant business relationships, critically thinks about departmental problems, and takes action on resolving these issues.Skills & RequirementsEducation / Experience3+ years of experience in Data Center or similar roles.Proven track record of success in delivering complex projects.Prior ownership of the operation of a mission-critical team and/or product.Ability to consistently deliver results despite competing priorities and distractions.Experience working with and deploying telecom and network providers, specifically in new projectsProject Management Experience.Cisco, meraki, aruba certifications is a plusCapacity planning and budgetary experience.Specific Skills / AbilitiesTeam, organizational and planning skills, as well as stringent time management skills to balance workloads in a complex and dynamic environmentStrong quantitative and qualitative reasoning skills, with demonstrated ability to determine event root causes, performance shortfalls and required corrective actionsAbility to interpret industry data center standards, policies & methodologiesAbility to improve performance management & critical awareness methods and implementation standards thinking out the boxStrong interpersonal, presentation and communication skillsStrong organizational skills and detail orientedAbility to manage budgets and analyzing expensesExperience with Computerized Maintenance Management Systems (CMMS), Data Center Infrastructure Management (DCIM)Fluent in EnglishGreat Place to Work Enjoy\u2026 A fast-paced, entrepreneurial culture focused on innovation.A flexible, autonomous work environment.A culture of respect, learning, and excellence.Experienced, highly talented experts as team peers.Growth and travel opportunities.A team of change-makers having a significant impact on people and the planet.About Edged Spain-PortugalEdged is launching a global network of sustainable datacenters and energy infrastructure.\n\nIt's energy solution combines rapid deployment with sustainable operation.\n\nModular components, including zero-water cooling and advanced electrical systems help to lower costs, minimize the environment footprint, and enable a new generation of on-demand infrastructure as a service.?Position: Data Center Network & OT EngineerFull-Time or Part-Time: Full-TimeReports to: Regional Network ManagerLocation: Vitoria or Madrid or Barcelona (On-Site) Contact: ******"
    }
}